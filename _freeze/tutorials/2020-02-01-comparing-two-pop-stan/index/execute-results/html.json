{
  "hash": "80fa87f02ec378adde6865e3b55162d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Comparing Two Populations (Stan)\"\ndescription: \"\"\nauthor:\n  - name: Andrea Gabrio\n    url: https://angabrio.github.io/agabriosite2/\n    orcid: 0000-0002-7650-4534\n    email: a.gabrio@maastrichtuniversity.nl\n    corresponding: true    \n    affiliation: Maastricht University\n    affiliation-url: https://www.maastrichtuniversity.nl/research/methodology-and-statistics\ndate: 2020-02-01\ncategories: [Quarto, R, Academia, Software, Statistics] # self-defined categories\n#image: featured.jpg\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nabstract: > \n  [This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models ...]{style=\"font-size: 85%\"}\nkeywords:\n  - Software\n  - Statistics\n  - Stan\n#license: \"GPL-2\"\n#copyright: \n#  holder: CRAN\n#  year: 2023\n#citation: \n#  title: missingHE\n#  author: Andrea Gabrio\n#  note: R package version 4.4.2\n#  url: https://cran.r-project.org/web/packages/missingHE\n#funding: \"The author received no specific funding for this work.\"\nbibliography: citations_stan2.bib\n#nocite: |\n#  @gabrio2017handling\n---\n\n\nThis tutorial will focus on the use of Bayesian estimation to explore differences between two populations. `BUGS` (Bayesian inference Using *Gibbs Sampling*) is an algorithm and supporting language (resembling `R`) dedicated to performing the Gibbs sampling implementation of *Markov Chain Monte Carlo* (MCMC) method. Dialects of the `BUGS` language are implemented within three main projects:\n\n1. **OpenBUGS** - written in component pascal.\n \n2. **JAGS** - (Just Another Gibbs Sampler) - written in `C++`. \n\n3. **Stan** - a dedicated Bayesian modelling framework written in `C++` and implementing *Hamiltonian* MCMC samplers.\n\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within `R` itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\n* *R2OpenBUGS* - interfaces with `OpenBUGS`\n\n* *R2jags* - interfaces with `JAGS`\n\n* *rstan* - interfaces with `Stan`\n\nThe `BUGS/JAGS/Stan` languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.\n\n* Within the model:\n\n  1. The likelihood function relating the response to the predictors.\n\n  2. The definition of the priors.\n  \n* Chain properties:\n\n  1. The number of chains.\n  \n  2. The length of chains (number of iterations).\n  \n  3. The burn-in length (number of initial iterations to ignore).\n  \n  4. The thinning rate (number of iterations to count on before storing a sample).\n  \n* The initial estimates to start an MCMC chain. If there are multiple chains, these starting values can differ between chains.\n\n* The list of model parameters and derivatives to monitor (and return the posterior distributions of)\n\nThis tutorial will demonstrate how to fit models in `Stan` (@gelman2015stan) using the package `rstan` (@rstanpackage) as interface, which also requires to load some other packages.\n\n# Data generation\n\nWe will start by generating a random data set. Note, I am creating two versions of the predictor variable (a numeric version and a factorial version).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nnA <- 60  #sample size from Population A\nnB <- 40  #sample size from Population B\nmuA <- 105  #population mean of Population A\nmuB <- 77.5  #population mean of Population B\nsigma <- 3  #standard deviation of both populations (equally varied)\nyA <- rnorm(nA, muA, sigma)  #Population A sample\nyB <- rnorm(nB, muB, sigma)  #Population B sample\ny <- c(yA, yB)\nx <- factor(rep(c(\"A\", \"B\"), c(nA, nB)))  #categorical listing of the populations\nxn <- as.numeric(x)  #numerical version of the population category for means parameterization. # Should not start at 0.\ndata <- data.frame(y, x, xn)  # dataset\n```\n:::\n\n\nLet inspect the first few rows of the dataset using the command `head`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA          y x xn\nNA 1 103.3186 A  1\nNA 2 104.3095 A  1\nNA 3 109.6761 A  1\nNA 4 105.2115 A  1\nNA 5 105.3879 A  1\nNA 6 110.1452 A  1\n```\n\n\n:::\n:::\n\n\nWe can also perform some exploratory data analysis - in this case, a boxplot of the response for each level of the predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(y ~ x, data)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# The One Sample t-test\n\nA *t-test* is essentially just a simple regression model in which the categorical predictor is represented by a binary variable in which one level is coded as $0$ and the other $1$. For the model itself, the observed response $y_i$ are assumed to be drawn from a normal distribution with a given mean $\\mu$ and standard deviation $\\sigma$. The expected values are themselves determined by the linear predictor $\\mu_i=\\beta_0+\\beta_1x_i$, where $\\beta_0$ represents the mean of the first treatment group and $\\beta_1$ represents the difference between the mean of the first group and the mean of the second group (the effect of interest).\n\nMCMC sampling requires priors on all parameters. We will employ weakly informative priors. Specifying \"uninformative\" priors is always a bit of a balancing act. If the priors are too vague (wide) the MCMC sampler can wander off into nonscence areas of likelihood rather than concentrate around areas of highest likelihood (desired when wanting the outcomes to be largely driven by the data). On the other hand, if the priors are too strong, they may have an influence on the parameters. In such a simple model, this balance is very forgiving - it is for more complex models that prior choice becomes more important. For this simple model, we will go with zero-centered Gaussian (normal) priors with relatively large standard deviations ($1000$) for both the intercept and the treatment effect and a wide half-cauchy (scale=$25$) for the standard deviation (@gelman2006prior).\n\n$$\ny_i \\sim \\text{Normal}(\\mu_i, \\sigma),  \n$$\n\nwhere $\\mu_i=\\beta_0+\\beta_1x_i$. \n\nPriors are defined as:\n\n$$\n\\beta_j \\sim \\text{Normal}(0,1000),  \\;\\;\\; \\text{and} \\;\\;\\; \\sigma \\sim \\text{Cauchy}(0,25),\n$$\n\nfor $j=0,1$.\n\n## Fitting the model in Stan\n\nBroadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (*Means parameterisation*) or we can estimate the mean of one group and the difference between this group and the other group(s) (*Effects parameterisation*). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).\n\n1. **Effects parameterisation**\n\n$$\ny_i = \\beta_0 + \\beta_{j}x_i + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n$$\n\nEach $y_i$ is modelled by an intercept $\\beta_0$ (mean of group A) plus a difference parameter $\\beta_j$ (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from ($x_i$), plus a residual drawn from a normal distribution with mean $0$ and standard deviation $\\sigma$. Actually, there are as many $\\beta_j$ parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of $y$ are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: $\\beta_0$, $\\beta_1$ and $\\sigma$.\n\n2. **Means parameterisation**\n\n$$\ny_i = \\beta_{j} + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n$$\n\nEach $y_i$ is modelled as the mean $\\beta_j$ of each group ($j=1,2$) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of $\\sigma$. Actually, $\\boldsymbol \\beta$ is a set of $j$ coefficients corresponding to the $j$ dummy coded factor levels. Expected values of $y$ are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: $\\beta_1$, $\\beta_2$ and $\\sigma$.\n\nWhilst the `Stan` language broadly resembles `BUGS/JAGS`, there are numerous important differences. Some of these differences are to support translation to `c++` for compilation (such as declaring variables). Others reflect leveraging of vectorization to speed up run time. Here are some important notes about `Stan`:\n\n* All variables must be declared\n\n* Variables declared in the parameters block will be collected\n\n* Anything in the transformed block will be collected as samples. Also, checks will be made every loop\n\nNow I will demonstrate fitting the models with `Stan`. Note, I am using the `refresh=0` option so as to suppress the larger regular output in the interest of keeping output to what is necessary for this tutorial. When running outside of a tutorial context, the regular verbose output is useful as it provides a way to gauge progress.\n\n**Effects Parameterisation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanString = \" \ndata {\n int n;\n vector [n] y;\n vector [n] x;\n }\n parameters {\n real <lower=0, upper=100> sigma;\n real beta0;\n real beta;\n }\n transformed parameters {\n }\n model {\n vector [n] mu;\n \n //Priors\n beta0 ~ normal(0,1000);\n beta ~ normal(0,1000);\n sigma ~ cauchy(0,25);\n \n mu = beta0 + beta*x;\n //Likelihood\n y ~ normal(mu, sigma);\n }\n generated quantities {\n vector [2] Group_means;\n real CohensD;\n //Other Derived parameters \n //# Group means (note, beta is a vector)\n Group_means[1] = beta0;\n Group_means[2] = beta0+beta;\n \n CohensD = beta /sigma;  \n }\n \n \"\n## write the model to a text file\nwriteLines(stanString, con = \"ttestModel.stan\")\n```\n:::\n\n\n**Means Parameterisation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanString.means = \"  \n data {\n int n;\n int nX;\n vector [n] y;\n matrix [n,nX] x;\n }\n parameters {\n real <lower=0, upper=100> sigma;\n vector [nX] beta;\n }\n transformed parameters {\n }\n model {\n vector [n] mu;\n \n //Priors\n beta ~ normal(0,1000);\n sigma ~ cauchy(0,25);\n \n mu = x*beta;\n //Likelihood\n y ~ normal(mu, sigma);\n }\n generated quantities {\n vector [2] Group_means;\n real CohensD;\n \n //Other Derived parameters \n Group_means[1] = beta[1];\n Group_means[2] = beta[1]+beta[2];\n \n CohensD = beta[2] /sigma;  \n }\n \n \"\n## write the model to a text file\nwriteLines(stanString.means, con = \"ttestModelMeans.stan\")\n```\n:::\n\n\nArrange the data as a list (as required by `Stan`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.list <- with(data, list(y = y, x = (xn - 1), n = nrow(data)))\nX <- model.matrix(~x, data)\ndata.list.means = with(data, list(y = y, x = X, n = nrow(data), nX = ncol(X)))\n```\n:::\n\n\nDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninits <- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,\n    data$x, mean))), sigma = sd(data$y/2))\ninits.means <- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))\n```\n:::\n\n\nDefine the nodes (parameters and derivatives) to monitor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- c(\"beta0\", \"beta\", \"sigma\", \"Group_means\", \"CohensD\")\nparams.means <- c(\"beta\", \"sigma\", \"Group_means\",\"CohensD\")\n```\n:::\n\n\nDefine the chain parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nburnInSteps = 500  # the number of initial samples to discard\nnChains = 2  # the number of independed sampling chains to perform \nthinSteps = 1  # the thinning rate\nnIter = 2000\n```\n:::\n\n\nStart the `Stan` model (check the model, load data into the model, specify the number of chains and compile the model). Load the `rstan` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\n```\n:::\n\n\nWhen using the `stan` function (`rtsan` package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\n\n**Effects Parameterisation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.stan = stan(file = \"ttestModel.stan\", \n  data = data.list, \n  pars = params,\n  iter = nIter,\n  warmup = burnInSteps, \n  chains = nChains, \n  thin = thinSteps, \n  init = \"random\", #or inits=list(inits,inits)\n  refresh = 0)\n\n#print results\nprint(data.stan)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                   mean se_mean   sd    2.5%     25%     50%     75%   97.5%\nNA beta0           105.20    0.01 0.35  104.51  104.97  105.20  105.43  105.89\nNA beta            -27.31    0.01 0.55  -28.37  -27.67  -27.32  -26.95  -26.22\nNA sigma             2.79    0.00 0.20    2.43    2.64    2.78    2.91    3.20\nNA Group_means[1]  105.20    0.01 0.35  104.51  104.97  105.20  105.43  105.89\nNA Group_means[2]   77.89    0.01 0.44   77.03   77.60   77.89   78.18   78.75\nNA CohensD          -9.86    0.02 0.73  -11.31  -10.35   -9.83   -9.35   -8.45\nNA lp__           -150.69    0.04 1.23 -153.81 -151.23 -150.35 -149.81 -149.31\nNA                n_eff Rhat\nNA beta0           1726    1\nNA beta            1543    1\nNA sigma           1752    1\nNA Group_means[1]  1726    1\nNA Group_means[2]  3066    1\nNA CohensD         1795    1\nNA lp__            1188    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:16:01 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n**Means Parameterisation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.stan.means = stan(file = \"ttestModelMeans.stan\", \n  data = data.list.means, \n  pars = params.means,\n  iter = nIter,\n  warmup = burnInSteps, \n  chains = nChains, \n  thin = thinSteps, \n  init = \"random\", #or inits=list(inits.means,inits.means)\n  refresh = 0)\n\n#print results\nprint(data.stan.means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                   mean se_mean   sd    2.5%     25%     50%     75%   97.5%\nNA beta[1]         105.21    0.01 0.35  104.51  104.97  105.21  105.44  105.91\nNA beta[2]         -27.33    0.01 0.56  -28.43  -27.69  -27.33  -26.96  -26.22\nNA sigma             2.78    0.00 0.20    2.41    2.64    2.77    2.91    3.20\nNA Group_means[1]  105.21    0.01 0.35  104.51  104.97  105.21  105.44  105.91\nNA Group_means[2]   77.88    0.01 0.43   77.05   77.58   77.88   78.18   78.75\nNA CohensD          -9.88    0.02 0.73  -11.32  -10.39   -9.86   -9.37   -8.51\nNA lp__           -150.71    0.03 1.20 -153.75 -151.26 -150.40 -149.83 -149.35\nNA                n_eff Rhat\nNA beta[1]         1986    1\nNA beta[2]         2007    1\nNA sigma           2144    1\nNA Group_means[1]  1986    1\nNA Group_means[2]  3201    1\nNA CohensD         2202    1\nNA lp__            1429    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:16:32 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n**Notes**\n\n* If `inits=\"random\"` the `stan` function will randomly generate initial values between $-2$ and $2$ on the *unconstrained support*. The optional additional parameter `init_r` can be set to some value other than $2$ to change the range of the randomly generated inits. Other available options include: set  `inits=\"0\"` to initialize all parameters to zero on the unconstrained support; set inital values by providing a list equal in length to the number of chains; set initial values by providing a function that returns a list for specifying the initial values of parameters for a chain.\n\n* In addition to the mean and quantiles of each of the sample nodes, the `stan` function will calculate.\n\n  1. The *effective sample size* for each sample - if `n.eff` for a node is substantially less than the number of iterations, then it suggests poor mixing.\n  \n  2. The *Potential scale reduction factor* or `Rhat` values for each sample - these are a convergence diagnostic (values of $1$ indicate full convergence, values greater than $1.01$ are indicative of non-convergence.\n\nThe total number samples collected is $3000$. That is, there are $3000$ samples collected from the multidimensional posterior distribution and thus, $3000$ samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed. \n\n# MCMC diagnostics\n\nAgain, prior to examining the summaries, we should have explored the convergence diagnostics. There are numerous ways of working with `Stan` model fits (for exploring diagnostics and summarisation).\n\n1. extract the mcmc samples and convert them into a mcmc.list to leverage the various `mcmcplots` routines\n\n2. use the numerous routines that come with the `rstan` package\n\n3. use the routines that come with the `bayesplot` package\n\nWe will explore all of these.\n\n* **mcmcplots**\n\nFirst, we need to convert the `rtsan` object into an `mcmc.list` object to apply the functions in the `mcmcplots` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mcmcplots)\ns = as.array(data.stan.means)\nmcmc <- do.call(mcmc.list, plyr:::alply(s[, , -(length(s[1, 1, ]))], 2, as.mcmc))\n```\n:::\n\n\nNext we look at density and trace plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndenplot(mcmc, parms = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntraplot(mcmc, parms = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\nThese plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. \n\n* **rstan**\n\nMCMC diagnostic measures that can be directly applied to `rstan` objects via the `rstan` package include: traceplots, autocorrelation, effective sample size and Rhat diagnostics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#traceplots\nstan_trace(data.stan.means, pars = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#autocorrelation\nstan_ac(data.stan.means, pars = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#rhat\nstan_rhat(data.stan.means, pars = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-3.png){width=672}\n:::\n\n```{.r .cell-code}\n#ess\nstan_ess(data.stan.means, pars = c(\"Group_means\", \"CohensD\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-4.png){width=672}\n:::\n:::\n\n\nNote:\n\n  * Rhat values are a measure of sampling efficiency/effectiveness. Ideally, all values should be less than $1.05$. If there are values of 1.05 or greater it suggests that the sampler was not very efficient or effective. Not only does this mean that the sampler was potentiall slower than it could have been, more importantly, it could indicate that the sampler spent time sampling in a region of the likelihood that is less informative. Such a situation can arise from either a misspecified model or overly vague priors that permit sampling in otherwise nonscence parameter space.\n\n  * ESS indicates the number samples (or proportion of samples that the sampling algorithm) deamed effective. The sampler rejects samples on the basis of certain criterion and when it does so, the previous sample value is used. Hence while the MCMC sampling chain may contain $1000$ samples, if there are only $10$ effective samples ($1$%), the estimated properties are not likely to be reliable.\n\n* **bayesplot**\n\nAnother alternative is to use the package `bayesplot`, which provides a range of standardised diagnostic measures for assessing MCMC convergence and issues, which can be directly applied to the `rstan` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\n\n#density and trace plots\nmcmc_combo(as.array(data.stan.means), regex_pars = \"Group_means|CohensD\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n# Model validation\n\nResiduals are not computed directly within `rstan`. However, we can calculate them manually form the posteriors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nmcmc = as.matrix(data.stan.means)[, c(\"beta[1]\", \"beta[2]\")]\n# generate a model matrix\nnewdata = data.frame(x = data$x)\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\ncoefs = apply(mcmc, 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data$y - fit\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThere is no evidence that the mcmc chain did not converge on a stable posterior distribution. We are now in a position to examine the summaries of the parameters.\n\n# Parameter estimates\n\nA quick look at posterior summaries can be obtained through the command `summary` which can be directly applied to our `rstan` object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data.stan.means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA $summary\nNA                       mean     se_mean        sd        2.5%         25%\nNA beta[1]         105.211757 0.007942873 0.3539769  104.505843  104.971055\nNA beta[2]         -27.327628 0.012473019 0.5587401  -28.427901  -27.692813\nNA sigma             2.779126 0.004297321 0.1990000    2.412497    2.636107\nNA Group_means[1]  105.211757 0.007942873 0.3539769  104.505843  104.971055\nNA Group_means[2]   77.884129 0.007683369 0.4346719   77.046396   77.583389\nNA CohensD          -9.883437 0.015622139 0.7331354  -11.321870  -10.394390\nNA lp__           -150.705879 0.031681466 1.1974657 -153.748876 -151.261985\nNA                        50%         75%       97.5%    n_eff      Rhat\nNA beta[1]         105.214946  105.436841  105.907771 1986.071 1.0004906\nNA beta[2]         -27.330669  -26.961516  -26.218176 2006.672 1.0004700\nNA sigma             2.772915    2.910585    3.196914 2144.424 1.0005267\nNA Group_means[1]  105.214946  105.436841  105.907771 1986.071 1.0004906\nNA Group_means[2]   77.882431   78.183886   78.745200 3200.514 0.9996750\nNA CohensD          -9.860548   -9.367286   -8.506401 2202.355 1.0007762\nNA lp__           -150.404119 -149.833903 -149.348309 1428.616 0.9995937\nNA \nNA $c_summary\nNA , , chains = chain:1\nNA \nNA                 stats\nNA parameter               mean        sd        2.5%         25%         50%\nNA   beta[1]         105.216258 0.3578476  104.503903  104.968222  105.217655\nNA   beta[2]         -27.336027 0.5706157  -28.447036  -27.711968  -27.356123\nNA   sigma             2.776761 0.1979930    2.421528    2.629522    2.776330\nNA   Group_means[1]  105.216258 0.3578476  104.503903  104.968222  105.217655\nNA   Group_means[2]   77.880231 0.4303583   77.044172   77.584218   77.881041\nNA   CohensD          -9.894731 0.7359107  -11.337046  -10.395515   -9.860308\nNA   lp__           -150.705058 1.2004089 -153.917640 -151.256032 -150.408954\nNA                 stats\nNA parameter                75%       97.5%\nNA   beta[1]         105.444986  105.920025\nNA   beta[2]         -26.955343  -26.209090\nNA   sigma             2.908990    3.184759\nNA   Group_means[1]  105.444986  105.920025\nNA   Group_means[2]   78.173031   78.705582\nNA   CohensD          -9.379888   -8.517788\nNA   lp__           -149.832114 -149.354270\nNA \nNA , , chains = chain:2\nNA \nNA                 stats\nNA parameter               mean        sd        2.5%         25%         50%\nNA   beta[1]         105.207257 0.3501249  104.506351  104.972152  105.210589\nNA   beta[2]         -27.319230 0.5466678  -28.379874  -27.668085  -27.317132\nNA   sigma             2.781492 0.2000399    2.409108    2.642792    2.769325\nNA   Group_means[1]  105.207257 0.3501249  104.506351  104.972152  105.210589\nNA   Group_means[2]   77.888027 0.4390520   77.062758   77.577941   77.885929\nNA   CohensD          -9.872143 0.7304204  -11.308201  -10.383455   -9.862055\nNA   lp__           -150.706700 1.1949150 -153.696913 -151.262612 -150.392368\nNA                 stats\nNA parameter                75%       97.5%\nNA   beta[1]         105.424876  105.889371\nNA   beta[2]         -26.970533  -26.243809\nNA   sigma             2.912221    3.201516\nNA   Group_means[1]  105.424876  105.889371\nNA   Group_means[2]   78.193687   78.776276\nNA   CohensD          -9.359816   -8.505181\nNA   lp__           -149.835948 -149.336959\n```\n\n\n:::\n:::\n\n\nThe Group A is typically $27.3$ units greater than Group B. The $95$% confidence interval for the difference between Group A and B does not overlap with $0$ implying a significant difference between the two groups.\n\n# Graphical summaries\n\nA nice graphic is often a great accompaniment to a statistical analysis. Although there are no fixed assumptions associated with graphing (in contrast to statistical analyses), we often want the graphical summaries to reflect the associated statistical analyses. After all, the sample is just one perspective on the population(s). What we are more interested in is being able to estimate and depict likely population parameters/trends. Thus, whilst we could easily provide a plot displaying the raw data along with simple measures of location and spread, arguably, we should use estimates that reflect the fitted model. In this case, it would be appropriate to plot the credibility interval associated with each group. We do this by loading functions in the package `broom` and `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(dplyr)\nmcmc = as.matrix(data.stan.means)\n## Calculate the fitted values\nnewdata = data.frame(x = levels(data$x))\nXmat = model.matrix(~x, newdata)\ncoefs = mcmc[, c(\"beta[1]\", \"beta[2]\")]\nfit = coefs %*% t(Xmat)\nnewdata = newdata %>% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\nnewdata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA   x term  estimate std.error  conf.low conf.high\nNA 1 A    1 105.21176 0.3539769 104.49829 105.89151\nNA 2 B    2  77.88413 0.4346719  77.03334  78.71763\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\n    ymax = conf.high)) + scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") +\n    theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data, aes(y = y,\n    x = x), color = \"gray\") + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n    scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since $\\text{resid}=\\text{obs}−\\text{fitted}$ and the fitted values depend only on the single predictor we are interested in.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate partial residuals fitted values\nfdata = rdata = data\nfMat = rMat = model.matrix(~x, fdata)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data$y - apply(coefs, 2, median) %*% t(rMat))\nrdata = rdata %>% mutate(partial.resid = resid + fit)\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\n    color = \"gray\") + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n    scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n# Effect sizes\n\nWe can compute summaries for our effect size of interest (e.g. Cohen's or the percentage ES) by post-processing our posterior distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc = as.matrix(data.stan.means)\n## Cohen's D\ncohenD = mcmc[, \"beta[2]\"]/mcmc[, \"sigma\"]\ntidyMCMC(as.mcmc(cohenD), conf.int = TRUE, conf.method = \"HPDinterval\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA # A tibble: 1 × 5\nNA   term  estimate std.error conf.low conf.high\nNA   <chr>    <dbl>     <dbl>    <dbl>     <dbl>\nNA 1 var1     -9.88     0.733    -11.3     -8.49\n```\n\n\n:::\n\n```{.r .cell-code}\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"beta[2]\"]/mcmc[, \"beta[1]\"]\n\n# Probability that the effect is greater than 10% (a decline of >10%)\nsum(-1 * ES > 10)/length(ES)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [1] 1\n```\n\n\n:::\n:::\n\n\n# Probability statements\n\nAny sort of probability statements of interest about our effect size can be computed in a relatively easy way by playing around with the posteriors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc = as.matrix(data.stan.means)\n\n# Percentage change (relative to Group A)\nES = 100 * mcmc[, \"beta[2]\"]/mcmc[, \"beta[1]\"]\nhist(ES)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Probability that the effect is greater than 10% (a decline of >10%)\nsum(-1 * ES > 10)/length(ES)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Probability that the effect is greater than 25% (a decline of >25%)\nsum(-1 * ES > 25)/length(ES)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [1] 0.978\n```\n\n\n:::\n:::\n\n\n# Finite population standard deviations\n\nEstimates for the variability associated with between and within group differences can also be easily obtained. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nNA # A tibble: 2 × 5\nNA   term     estimate std.error conf.low conf.high\nNA   <chr>       <dbl>     <dbl>    <dbl>     <dbl>\nNA 1 sd.x        19.3     0.395     18.5      20.1 \nNA 2 sd.resid     2.75    0.0196     2.74      2.79\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA # A tibble: 2 × 5\nNA   term     estimate std.error conf.low conf.high\nNA   <chr>       <dbl>     <dbl>    <dbl>     <dbl>\nNA 1 sd.x         87.5     0.240     87.1      87.8\nNA 2 sd.resid     12.5     0.240     12.2      12.9\n```\n\n\n:::\n:::\n\n\n# Unequally varied populations\n\nWe can also generate data assuming two populations with different variances, e.g. between male and female subgroups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn1 <- 60  #sample size from population 1\nn2 <- 40  #sample size from population 2\nmu1 <- 105  #population mean of population 1\nmu2 <- 77.5  #population mean of population 2\nsigma1 <- 3  #standard deviation of population 1\nsigma2 <- 2  #standard deviation of population 2\nn <- n1 + n2  #total sample size\ny1 <- rnorm(n1, mu1, sigma1)  #population 1 sample\ny2 <- rnorm(n2, mu2, sigma2)  #population 2 sample\ny <- c(y1, y2)\nx <- factor(rep(c(\"A\", \"B\"), c(n1, n2)))  #categorical listing of the populations\nxn <- rep(c(0, 1), c(n1, n2))  #numerical version of the population category\ndata2 <- data.frame(y, x, xn)  # dataset\nhead(data2)  #print out the first six rows of the data set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA          y x xn\nNA 1 103.3186 A  0\nNA 2 104.3095 A  0\nNA 3 109.6761 A  0\nNA 4 105.2115 A  0\nNA 5 105.3879 A  0\nNA 6 110.1452 A  0\n```\n\n\n:::\n:::\n\n\nStart by defining the model\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon, \n$$\n\nwhere $\\epsilon_1 \\sim \\text{Normal}(0,\\sigma_1)$ for $x_1=0$ (females), and $\\epsilon_2 \\sim \\text{Normal}(0,\\sigma_2)$ for $x_2=1$ (males). In `Stan` code, the model becomes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanStringv3 = \" \n data {\n int n;\n vector [n] y;\n vector [n] x;\n int<lower=1,upper=2> xn[n];\n }\n parameters {\n vector <lower=0, upper=100>[2] sigma;\n real beta0;\n real beta;\n }\n transformed parameters {\n }\n model {\n vector [n] mu;\n //Priors\n beta0 ~ normal(0,1000);\n beta ~ normal(0,1000);\n sigma ~ cauchy(0,25);\n\n mu = beta0 + beta*x;\n //Likelihood\n for (i in 1:n) y[i] ~ normal(mu[i], sigma[xn[i]]);\n }\n generated quantities {\n vector [2] Group_means;\n real CohensD;\n real CLES;\n\n Group_means[1] = beta0;\n Group_means[2] = beta0+beta;\n CohensD = beta /(sum(sigma)/2);\n CLES = normal_cdf(beta /sum(sigma),0,1);  \n }\n \n \"\n\n## write the model to a text file \nwriteLines(stanStringv3,con=\"ttestModelv3.stan\")\n```\n:::\n\n\nWe specify priors directly on $\\sigma_1$ and $\\sigma_2$ using Cauchy distributions with a scale of $25$. Next, arrange the data as a list (as required by `Stan`) and define the MCMC parameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata2.list <- with(data, list(y = y, x = (xn - 1), xn = xn, n = nrow(data)))\nparamsv3 <- c(\"beta0\",\"beta\",\"sigma\",\"Group_means\",\"CohensD\", \"CLES\")\nburnInSteps = 500\nnChains = 2\nthinSteps = 1\nnIter = 2000\n```\n:::\n\n\nFinally, fit the model in `Stan` and print the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.stanv3 = stan(file = \"ttestModelv3.stan\", \n  data = data2.list, \n  pars = paramsv3,\n  iter = nIter,\n  warmup = burnInSteps, \n  chains = nChains, \n  thin = thinSteps, \n  init = \"random\", #or inits=list(inits,inits)\n  refresh = 0)\n\n#print results\nprint(data.stanv3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                   mean se_mean   sd    2.5%     25%     50%     75%   97.5%\nNA beta0           105.20    0.01 0.36  104.49  104.96  105.20  105.44  105.92\nNA beta            -27.32    0.01 0.60  -28.52  -27.71  -27.32  -26.93  -26.14\nNA sigma[1]          2.79    0.01 0.26    2.35    2.61    2.77    2.95    3.34\nNA sigma[2]          2.87    0.01 0.33    2.31    2.63    2.84    3.07    3.59\nNA Group_means[1]  105.20    0.01 0.36  104.49  104.96  105.20  105.44  105.92\nNA Group_means[2]   77.88    0.01 0.47   76.97   77.56   77.88   78.19   78.78\nNA CohensD          -9.71    0.01 0.74  -11.14  -10.22   -9.69   -9.21   -8.27\nNA CLES              0.00    0.00 0.00    0.00    0.00    0.00    0.00    0.00\nNA lp__           -150.25    0.04 1.43 -154.08 -150.92 -149.94 -149.21 -148.49\nNA                n_eff Rhat\nNA beta0           2243    1\nNA beta            2272    1\nNA sigma[1]        2549    1\nNA sigma[2]        2504    1\nNA Group_means[1]  2243    1\nNA Group_means[2]  3134    1\nNA CohensD         2727    1\nNA CLES            2487    1\nNA lp__            1151    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:17:07 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n# References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}