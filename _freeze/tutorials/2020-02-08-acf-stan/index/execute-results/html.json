{
  "hash": "7b68e0a2d556f486c177b7d724a7f283",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Temporal Autocorrelation (Stan)\"\ndescription: \"\"\nauthor:\n  - name: Andrea Gabrio\n    url: https://angabrio.github.io/agabriosite2/\n    orcid: 0000-0002-7650-4534\n    email: a.gabrio@maastrichtuniversity.nl\n    corresponding: true    \n    affiliation: Maastricht University\n    affiliation-url: https://www.maastrichtuniversity.nl/research/methodology-and-statistics\ndate: 2020-02-08\ncategories: [Quarto, R, Academia, Software, Statistics] # self-defined categories\n#image: featured.jpg\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nabstract: > \n  [This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models ...]{style=\"font-size: 85%\"}\nkeywords:\n  - Software\n  - Statistics\n  - Stan\n#license: \"GPL-2\"\n#copyright: \n#  holder: CRAN\n#  year: 2023\n#citation: \n#  title: missingHE\n#  author: Andrea Gabrio\n#  note: R package version 4.4.2\n#  url: https://cran.r-project.org/web/packages/missingHE\n#funding: \"The author received no specific funding for this work.\"\nbibliography: citations_stan9.bib\n#nocite: |\n#  @gabrio2017handling\n---\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. `BUGS` (Bayesian inference Using *Gibbs Sampling*) is an algorithm and supporting language (resembling `R`) dedicated to performing the Gibbs sampling implementation of *Markov Chain Monte Carlo* (MCMC) method. Dialects of the `BUGS` language are implemented within three main projects:\n\n1. **OpenBUGS** - written in component pascal.\n \n2. **JAGS** - (Just Another Gibbs Sampler) - written in `C++`. \n\n3. **Stan** - a dedicated Bayesian modelling framework written in `C++` and implementing *Hamiltonian* MCMC samplers.\n\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of `R`, and thus, they are best accessed from within `R` itself. As such there are multiple packages devoted to interfacing with the various software implementations:\n\n* *R2OpenBUGS* - interfaces with `OpenBUGS`\n\n* *R2jags* - interfaces with `JAGS`\n\n* *rstan* - interfaces with `Stan`\n\nThis tutorial will demonstrate how to fit models in `Stan` (@gelman2015stan) using the package `rstan` (@rstanpackage) as interface, which also requires to load some other packages.\n\n# Overview\n\n## Introduction\n\nUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let's explore a simple linear regression model to see how each of the assumptions relate to the model.\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\n$$\n\nThe above simple statistical model models the **linear relationship** of $y_i$ against $x_i$. The residuals ($\\epsilon$) are assumed to be **normally distributed** with a mean of zero and a constant (yet unknown) variance ($\\sigma$, **homogeneity of variance**). The residuals (and thus observations) are also assumed to all be **independent**.\n\nHomogeneity of variance and independence are encapsulated within the single symbol for variance ($\\sigma^2$). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a $N\\times N$ covariance matrix are the same, $\\sigma^2$) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don't otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\n\nIn order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not \"reset\" itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:\n\n$$\ny_i \\sim Dist(\\mu_i),\n$$\n\nwhere $\\mu_i=\\boldsymbol X \\boldsymbol \\beta + \\boldsymbol Z \\boldsymbol \\gamma$, with $\\boldsymbol X$ and $\\boldsymbol \\beta$ representing the *fixed data structure* and *fixed effects*, respectively, while with $\\boldsymbol Z$ and $\\boldsymbol \\gamma$ represent the *varying data structure* and *varying effects*, respectively. In simple regression, there are no \"varying\" effects, and thus:\n\n$$\n\\boldsymbol \\gamma \\sim MVN(\\boldsymbol 0, \\boldsymbol \\Sigma),\n$$\n\nwhere $\\boldsymbol \\Sigma$ is a variance-covariance matrix of the form\n\n$$\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho^{\\phi_{1,2}} & \\ldots & \\rho^{\\phi_{1,n}} \\\\\n   \\rho^{\\phi_{2,1}} & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\phi_{n,1}} & \\ldots & \\ldots & 1\n   \\end{bmatrix}.\n$$\n\nNotice that this introduces a very large number of additional parameters that require estimating: $\\sigma^2$ (error variance), $\\rho$ (base autocorrelation) and each of the individual covariances ($\\rho^{\\phi_{n,n}}$). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.\n\n* When we assume that all residuals are independent (regular regression), i.e. $\\rho=0$, $\\boldsymbol \\Sigma$ is essentially equal to $\\sigma^2 \\boldsymbol I$ and we simply use:\n\n$$\n\\boldsymbol \\gamma \\sim N( 0,\\sigma^2).\n$$\n\n* We could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a *first order autoregressive* (AR1) structure in which exponent on the correlation declines linearly according to the time lag ($\\mid t - s\\mid$).\n\n$$\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho & \\ldots & \\rho^{\\mid t-s \\mid} \\\\\n   \\rho & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\mid t-s \\mid } & \\ldots & \\ldots & 1\n   \\end{bmatrix}. \n$$\n\nNote, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag $2$ pairs will have the same correlation and so on.\n\n# First order autocorrelation\n\nConsider an example, in which the number of individuals at time $2$ will be partly dependent on the number of individuals present at time $1$. Clearly then, the observations (and thus residuals) are not fully independent - there is an auto-regressive correlation dependency structure. We could accommodate this lack of independence by fitting a model that incorporates a AR1 variance-covariance structure. Alternatively, we fit the following model:\n\n$$\ny_{it} \\sim Dist(\\mu_{it}),\n$$\n\nwhere \n\n$$\n\\mu_{it}=\\boldsymbol X \\boldsymbol \\beta + \\rho \\epsilon_{i,t-1} + \\gamma_{it},\n$$\n\nand where $\\gamma \\sim N(0, \\sigma^2)$. In this version of the model, we are stating that the expected value of an observation is equal to the regular linear predictor plus the autocorrelation parameter ($\\rho$) multipled by the residual associated with the previous observation plus the regular independently distributed noise ($\\sigma^2$). Such a model is substantially faster to fit, although along with stationarity assumes in estimating the autocorrelation parameter, only the smallest lags are used. To see this in action, we will first generate some temporally auto-correlated data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(126)\nn = 50\na <- 20  #intercept\nb <- 0.2  #slope\nx <- round(runif(n, 1, n), 1)  #values of the year covariate\nyear <- 1:n\nsigma <- 20\nrho <- 0.8\n\nlibrary(nlme)\n## define a constructor for a first-order\n## correlation structure\nar1 <- corAR1(form = ~year, value = rho)\n## initialize this constructor against our data\nAR1 <- Initialize(ar1, data = data.frame(year))\n## generate a correlation matrix\nV <- corMatrix(AR1)\n## Cholesky factorization of V\nCv <- chol(V)\n## simulate AR1 errors\ne <- t(Cv) %*% rnorm(n, 0, sigma)  # cov(e) = V * sig^2\n## generate response\ny <- a + b * x + e\ndata.temporalCor = data.frame(y = y, x = x, year = year)\nwrite.table(data.temporalCor, file = \"data.temporalCor.csv\",\n    sep = \",\", quote = F, row.names = FALSE)\n\npairs(data.temporalCor)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWe will now proceed to analyse these data via both of the above techniques for `JAGS`:\n\n* incorporating AR1 residual autocorrelation structure\n\n* incorporating lagged residuals into the model\n\n# Incorporating lagged residuals\n\n## Model fitting\n\nWe proceed to code the model into `JAGS` (remember that in this software normal distribution are parameterised in terms of precisions $\\tau$ rather than variances, where $\\tau=\\frac{1}{\\sigma^2}$). Define the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanString = \"\n  data {\n  int<lower=1> n;\n  vector [n] y;\n  int<lower=1> nX;\n  matrix[n,nX] X;\n  }\n  transformed data {\n  }\n  parameters {\n  vector[nX] beta;\n  real<lower=0> sigma;\n  real<lower=-1,upper=1> phi;\n  }\n  transformed parameters {\n  vector[n] mu;\n  vector[n] epsilon;\n  mu = X*beta;\n  epsilon[1] = y[1] - mu[1];\n  for (i in 2:n) {\n  epsilon[i] = (y[i] - mu[i]);\n  mu[i] = mu[i] + phi*epsilon[i-1];\n  }\n  }\n  model {\n  phi ~ uniform(-1,1);\n  beta ~ normal(0,100);\n  sigma ~ cauchy(0,5);\n  y ~ normal(mu, sigma);\n  }\n  generated quantities {\n  }\n  \n  \"\n\n## write the model to a text file\nwriteLines(stanString, con = \"tempModel.stan\")\n```\n:::\n\n\nArrange the data as a list (as required by `Stan`). As input, `Stan` will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list <- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n```\n:::\n\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [1] 1500\n```\n\n\n:::\n:::\n\n\nNow compile and run the Stan code via the `rstan` interface.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\n```\n:::\n\n\nDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate ($0.8$ or $80$% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth ($10$). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum ($10$ by default).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.temporalCor.rstan <- stan(data = data.temporalCor.list, file = \"tempModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.4e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.038 seconds (Warm-up)\nNA Chain 1:                0.038 seconds (Sampling)\nNA Chain 1:                0.076 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 7e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nNA Chain 2:                0.032 seconds (Sampling)\nNA Chain 2:                0.068 seconds (Total)\nNA Chain 2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.temporalCor.rstan, par = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA          mean se_mean    sd 2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 30.71    0.36 11.95 8.68 22.30 30.40 38.27 55.41  1082    1\nNA beta[2]  0.22    0.00  0.10 0.02  0.16  0.22  0.30  0.43  1407    1\nNA sigma   12.11    0.03  1.26 9.93 11.25 12.01 12.86 14.88  1359    1\nNA phi      0.91    0.00  0.05 0.79  0.88  0.92  0.95  0.99  1117    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:39:47 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n## MCMC diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mcmcplots)\nmcmc = As.mcmc.list(data.temporalCor.rstan)\ndenplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntraplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Raftery diagnostic\nraftery.diag(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA             beta[1]      beta[2]         sigma          phi         lp__\nNA Lag 0   1.000000000  1.000000000  1.0000000000  1.000000000  1.000000000\nNA Lag 1   0.244876071  0.131202988  0.1861700732  0.155207971  0.469635120\nNA Lag 5   0.030363677  0.006018787 -0.0230078904  0.037502444  0.016158672\nNA Lag 10 -0.003669667 -0.014831709 -0.0006623927 -0.009372815  0.022517151\nNA Lag 50  0.014609742  0.005511481 -0.0400835388  0.018467269 -0.003215701\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_ac(data.temporalCor.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nstan_rhat(data.temporalCor.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nstan_ess(data.temporalCor.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\nAll diagnostics seem fine.\n\n## Model validation\n\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by `sigma`.\n\n$$\nRes_i = Y_i - \\mu_i\n$$\n\n$$\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n$$\n\n$$\nRes_i = \\frac{Res_i}{\\sigma} \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc = as.matrix(data.temporalCor.rstan)\nwch = grep(\"beta\", colnames(mcmc))\n# generate a model matrix\nnewdata = data.frame(x = data.temporalCor$x)\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nlibrary(ggplot2)\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(acf(resid, lag = 40))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-4.png){width=672}\n:::\n\n```{.r .cell-code}\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\n    fit[i, ], mcmc[i, \"sigma\"]))\nggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\n    fill = \"Model\"), alpha = 0.5) + geom_density(data = data.temporalCor,\n    aes(x = y, fill = \"Obs\"), alpha = 0.5) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-5.png){width=672}\n:::\n:::\n\n\nNo obvious autocorrelation or other issues with residuals remaining.\n\n## Parameter estimates\n\nExplore parameter estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(broom.mixed)\ntidyMCMC(data.temporalCor.rstan, par = c(\"beta\", \"phi\", \"sigma\"),\n    conf.int = TRUE, conf.method = \"HPDinterval\", rhat = TRUE,\n    ess = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA # A tibble: 4 × 7\nNA   term    estimate std.error   conf.low conf.high  rhat   ess\nNA   <chr>      <dbl>     <dbl>      <dbl>     <dbl> <dbl> <int>\nNA 1 beta[1]   30.7     11.9     7.96         54.4    1.00  1082\nNA 2 beta[2]    0.224    0.104  -0.0000273     0.414  1.00  1407\nNA 3 phi        0.910    0.0541  0.810         0.998  1.00  1117\nNA 4 sigma     12.1      1.26    9.88         14.7    1.00  1359\n```\n\n\n:::\n:::\n\n\n# Incorporating AR1 residual autocorrelation structure\n\n## Model fitting\n\nWe proceed to code the model into `JAGS` (remember that in this software normal distribution are parameterised in terms of precisions $\\tau$ rather than variances, where $\\tau=\\frac{1}{\\sigma^2}$). Define the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstanString = \"\nfunctions { \n\tmatrix cov_matrix_ar1(real ar, real sigma, int nrows) { \n  matrix[nrows, nrows] mat; \n\tvector[nrows - 1] gamma; \n\tmat = diag_matrix(rep_vector(1, nrows)); \n\t\tfor (i in 2:nrows) { \n\t\t\tgamma[i - 1] = pow(ar, i - 1); \n\t\t\t\tfor (j in 1:(i - 1)) { \n\t\t\t\t\t\tmat[i, j] = gamma[i - j]; \n\t\t\t\t\t\tmat[j, i] = gamma[i - j]; \n\t\t\t\t\t} \n\t\t\t\t} \n\t\t\t\treturn sigma^2 / (1 - ar^2) * mat; \n\t\t}\n} \n\t  \n\t    data { \n\t\t\t int<lower=1> n;  // total number of observations \n\t\t\t\t vector[n] y;  // response variable\n\t\t\t\t int<lower=1> nX;\n\t\t\t\t\t matrix[n,nX] X;\n\t\t   } \n\t\t\t transformed data {\n\t\t\t\tvector[n] se2 = rep_vector(0, n); \n\t\t\t } \n\t\t\t parameters { \n\t\t\t\tvector[nX] beta;\n\t\t\t\t\treal<lower=0> sigma;  // residual SD \n\t\t\t\t\treal <lower=-1,upper=1> phi;  // autoregressive effects \n\t\t\t\t} \n\t\t\t\ttransformed parameters { \n\t\t\t\t} \n\t\t\t\tmodel {\n\t\t\t\t\tmatrix[n, n] res_cov_matrix;\n\t\t\t\t\tmatrix[n, n] Sigma; \n\t\t\t\t\tvector[n] mu = X*beta;\n\t\t\t\t\tres_cov_matrix = cov_matrix_ar1(phi, sigma, n);\n\t\t\t\t\tSigma = res_cov_matrix + diag_matrix(se2);\n\t\t\t\t\tSigma = cholesky_decompose(Sigma); \n\n\t\t\t\t\t// priors including all constants\n\t\t\t\t\tbeta ~ student_t(3,30,30);\n\t\t\t\t\tsigma ~ cauchy(0,5);\n\t\t\t\t\ty ~ multi_normal_cholesky(mu,Sigma);\n\t\t\t\t} \n\t\t\t\tgenerated quantities { \n\t\t\t\t}\n  \n  \"\n\n## write the model to a text file\nwriteLines(stanString, con = \"tempModel2.stan\")\n```\n:::\n\n\nArrange the data as a list (as required by `Stan`). As input, `Stan` will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list <- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n```\n:::\n\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [1] 1500\n```\n\n\n:::\n:::\n\n\nNow compile and run the Stan code via the `rstan` interface. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.temporalCor2.rstan <- stan(data = data.temporalCor.list, file = \"tempModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.00024 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.4 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.097 seconds (Warm-up)\nNA Chain 1:                0.665 seconds (Sampling)\nNA Chain 1:                1.762 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.000209 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 1.176 seconds (Warm-up)\nNA Chain 2:                0.788 seconds (Sampling)\nNA Chain 2:                1.964 seconds (Total)\nNA Chain 2:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(data.temporalCor2.rstan, par = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA          mean se_mean    sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 21.61    0.46 16.08 -8.13 12.64 20.76 29.68 55.24  1230 1.00\nNA beta[2]  0.22    0.00  0.10  0.01  0.15  0.22  0.29  0.42  1392 1.00\nNA sigma   12.02    0.03  1.31  9.85 11.06 11.92 12.82 14.89  1462 1.00\nNA phi      0.89    0.00  0.05  0.78  0.86  0.90  0.93  0.98  1131 1.01\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:40:28 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n## MCMC diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc = As.mcmc.list(data.temporalCor2.rstan)\ndenplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntraplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Raftery diagnostic\nraftery.diag(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA            beta[1]      beta[2]        sigma          phi         lp__\nNA Lag 0   1.00000000  1.000000000  1.000000000  1.000000000  1.000000000\nNA Lag 1   0.21452456  0.112473437  0.181672833  0.124546428  0.527409040\nNA Lag 5  -0.04090736  0.011068413 -0.001147177 -0.026057078  0.008368655\nNA Lag 10 -0.02972948 -0.061470014 -0.027729823 -0.008138883  0.027081616\nNA Lag 50 -0.01132752 -0.009636609  0.057698527  0.024742960 -0.018426417\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_ac(data.temporalCor2.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nstan_rhat(data.temporalCor2.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n\n```{.r .cell-code}\nstan_ess(data.temporalCor2.rstan)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n:::\n\n\nAll diagnostics seem fine.\n\n## Model validation\n\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by `sigma`.\n\n$$\nRes_i = Y_i - \\mu_i\n$$\n\n$$\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n$$\n\n$$\nRes_i = \\frac{Res_i}{\\sigma} \n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc = as.matrix(data.temporalCor2.rstan)\nwch = grep(\"beta\", colnames(mcmc))\n# generate a model matrix\nnewdata = data.frame(x = data.temporalCor$x)\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(acf(resid, lag = 40))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-4.png){width=672}\n:::\n\n```{.r .cell-code}\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\n    fit[i, ], mcmc[i, \"sigma\"]))\nggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\n    fill = \"Model\"), alpha = 0.5) + geom_density(data = data.temporalCor,\n    aes(x = y, fill = \"Obs\"), alpha = 0.5) + theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-5.png){width=672}\n:::\n:::\n\n\nNo obvious autocorrelation or other issues with residuals remaining.\n\n## Parameter estimates\n\nExplore parameter estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyMCMC(data.temporalCor2.rstan, par = c(\"beta\", \"phi\", \"sigma\"),\n    conf.int = TRUE, conf.method = \"HPDinterval\", rhat = TRUE,\n    ess = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNA # A tibble: 4 × 7\nNA   term    estimate std.error conf.low conf.high  rhat   ess\nNA   <chr>      <dbl>     <dbl>    <dbl>     <dbl> <dbl> <int>\nNA 1 beta[1]   21.6     16.1    -9.11       52.5   1.00   1230\nNA 2 beta[2]    0.220    0.104   0.00806     0.410 0.999  1392\nNA 3 phi        0.894    0.0541  0.795       0.990 1.01   1131\nNA 4 sigma     12.0      1.31    9.63       14.5   1.00   1462\n```\n\n\n:::\n:::\n\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}