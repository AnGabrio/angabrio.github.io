{
  "hash": "c2abafcbab661b68c72c7d65762be99c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian statistics in health economic evaluations - part 2\"\ndescription: \"\"\nauthor:\n  - name: Andrea Gabrio\n    url: https://angabrio.github.io/agabriosite2/\n    orcid: 0000-0002-7650-4534\n    affiliation: Maastricht University\n    affiliation-url: https://www.maastrichtuniversity.nl/research/methodology-and-statistics\ndate: 2023-10-05\ncategories: [Quarto, R, Academia, health economics] # self-defined categories\n#citation: \n#  url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ \nimage: featured.png\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n![](featured.png){fig-align=\"center\"}\n\n\nHello dear audience and welcome back to my regular post updates. This time I would like to continue discussing the application of Bayesian methods in HTA that I picked-up in my previous post and show some customisation options of the modelling strategy that may come in handy when dealing with such analyses. More specifically, a notorious problem affecting trial-based economic evaluations is the presence of skewed data which, when coupled with small sample sizes, may question the validity of mean estimate based on normal distribution assumptions. In the previous post I provided a simple modelling framework based on normal distributions for both CEA aggregated outcomes, namely effectiveness (usually in terms of QALYs) and total costs computed over the study period. Today, I would like to show how, within a Bayesian setting, the use of alternative parametric distributions is relatively simple and allows to tackle issues such as skewness while simultaneously dealing with uncertainty quantification for each unknown quantity in the model. This is extremely helpful in that there is no need to reply on re-sampling methods (e.g. bootstrapping) to generate uncertainty around the desired estimates and to combine such methods with alternative modelling approaches, e.g. need to specify how bootstrapping should be done when using non-Normal distributions, multilevel models, or even missing data.\n\nLet's take the same example I simulated in my previous post and re-perform the exact same analysis based on Normal distributions and then try to compare its results to an analysis based on alternative, more flexible, parametric distributions that can better capture the features of the data (e.g. skewness). We start by simulating some non-Normal bivariate cost and QALY data from an hypothetical study for a total of $300$ patients assigned to two competing intervention groups ($t=0,1$). When generating the data, we can try to mimic the typical skewness features of the outcome data by using alternative distributions such as Gamma for costs and Beta for QALYs. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(768)\nn <- 300\nid <- seq(1:n)\ntrt <- c(rep(0, n/2),rep(1, n/2))\nmean_e1 <- c(0.5)\nmean_e2 <- c(0.7)\nsigma_e <- 0.15\ntau1_e <- ((mean_e1*(1-mean_e1))/(sigma_e^2)-1)\ntau2_e <- ((mean_e2*(1-mean_e2))/(sigma_e^2)-1)\nalpha1_beta <- tau1_e*mean_e1\nbeta1_beta <- tau1_e*(1-mean_e1)\nalpha2_beta <- tau2_e*mean_e2\nbeta2_beta <- tau2_e*(1-mean_e2)\ne1 <- rbeta(n/2, alpha1_beta, beta1_beta)\ne2 <- rbeta(n/2, alpha2_beta, beta2_beta)\n\nmean_c1 <- 500\nmean_c2 <- 1000\nsigma_c <- 300\ntau1_c <- mean_c1/(sigma_c^2)\ntau2_c <- mean_c2/(sigma_c^2)\nln.mean_c1 <- log(500) + 5*(e1-mean(e1)) \nc1 <- rgamma(n/2, (exp(ln.mean_c1)/sigma_c)^2, exp(ln.mean_c1)/(sigma_c^2))\nln.mean_c2 <- log(1000) + 5*(e2-mean(e2)) + rgamma(n/2,0,tau2_c)\nc2 <- rgamma(n/2, (exp(ln.mean_c2)/sigma_c)^2, exp(ln.mean_c2)/(sigma_c^2))\n\nQALYs <- c(e1,e2)\nCosts <- c(c1,c2)\n\ndata_sim_ec <- data.frame(id, trt, QALYs, Costs)\ndata_sim_ec <- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\n```\n:::\n\n\nWe can now inspect the empirical distributions of the two outcomes by treatment group to have an idea of the level of the associated skewness.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf <- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) <- c(\"old\",\"new\")\n\ndata_sim_ec$trtf <- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) <- c(\"old\",\"new\")\nQALY_hist <- ggplot(data_sim_ec, aes(x=QALYs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist <- ggplot(data_sim_ec, aes(x=Costs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n## Step 1: fit a standard normal model\n\nAs in the previous post, we rely on factoring the joint distribution $p(e,c\\mid  \\boldsymbol \\theta)$ into the product of a marginal distribution $p(e\\mid \\boldsymbol \\theta_e)$ of the effects and a conditional distribution $p(c \\mid e, \\boldsymbol \\theta_c)$ of the cost given the effects, each indexed by corresponding set of parameters. We can rely on the freely-available Bayesian software named [**JAGS**](https://mcmc-jags.sourceforge.io/) to fit the model. We write the model code into a txt file and save it into our current wd to be called from R. Then, we load the package **R2jags** which allows to call the software from R through the function **jags** and after providing the data and some technical parameters needed to run the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#save data input\nn <- dim(data_sim_ec)[1]\nQALYs <- data_sim_ec$QALYs\nCosts <- data_sim_ec$Costs\ntrt <- data_sim_ec$trt\n\n#load package and provide algorithm parameters\nlibrary(R2jags)\nset.seed(2345) #set seed for reproducibility\ndatalist<-list(\"n\",\"QALYs\",\"Costs\",\"trt\") #pass data into a list\n#set up initial values for algorithm\ninits1 <- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 1)\ninits2 <- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 2)\n#set parameter easimates to save\nparams<-c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\",\"gamma2\",\"s_c\",\"s_e\",\"nu_c\",\"nu_e\")\nfilein<-\"model_bn.txt\" #name of model file\nn.iter<-20000 #n of iterations\n\n#fit model\njmodel_bn<-jags(data=datalist,inits=list(inits1,inits2),\n                parameters.to.save=params,model.file=filein,\n                n.chains=2,n.iter=n.iter,n.thin=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 7\n   Total graph size: 1522\n\nInitializing model\n```\n\n\n:::\n:::\n\n\nWe then proceed to extract the key quantities of interest of this analysis by post-processing the model estimates via simulation methods, with the aim to obtain the mean effects and costs outcome posterior distributions $(\\mu_e,\\mu_c)$ in each treatment group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#obtain estimates of means by arm\n\n#extract estimates for each mean parameter by trt group\nnu_e0 <- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==0]\nnu_e1 <- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==1]\nnu_c0 <- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==0]\nnu_c1 <- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==1]\n#extract estimates for std\ns_e <- jmodel_bn$BUGSoutput$sims.list$s_e\ns_c <- jmodel_bn$BUGSoutput$sims.list$s_c\n\n#create empty vectors to contain results for means by trt group\nmu_e0 <- mu_c0 <- c()\nmu_e1 <- mu_c1 <- c()\n\n#set number of replications\nL <- 5000\n\nset.seed(2345) #set seed for reproducibility\n#generate replications and take mean at each iteration of the posterior\nfor(i in 1:n.iter){\n mu_e0[i] <- mean(rnorm(L,nu_e0[i,],s_e[i])) \n mu_e1[i] <- mean(rnorm(L,nu_e1[i,],s_e[i]))\n mu_c0[i] <- mean(rnorm(L,nu_c0[i,],s_c[i])) \n mu_c1[i] <- mean(rnorm(L,nu_c1[i,],s_c[i])) \n}\n\n#calculate mean differences\nDelta_e <- mu_e1 - mu_e0\nDelta_c <- mu_c1 - mu_c0\n```\n:::\n\n\n## Step 2: fit a Beta-Gamma model\n\nAs a comparative approach, we now try to directly address the skewness in the observed data by using a Beta distribution for modelling the QALYs and a Gamma distribution for the Total costs. The main idea is that, by using distributions that can naturally allow for non-symmetric data, estimates will likely be more robust and uncertainty concerning such estimates will be more properly quantified. We begin by writing the model file and save it as a txt file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_bg <- \"\nmodel {\n\n#model specification\nfor(i in 1:n){\n      QALYs[i]~dbeta(shape1[i],shape2[i])\n      shape1[i]<-nu_e[i]*(nu_e[i]*(1-nu_e[i])/pow(s_e,2) - 1)\n      shape2[i]<-(1-nu_e[i])*(nu_e[i]*(1-nu_e[i])/pow(s_e,2) - 1)\n      logit(nu_e[i])<-beta0 + beta1*trt[i]\n\n      Costs[i]~dgamma(shape[i],rate[i])\n      shape[i]<-pow(nu_c[i],2)/pow(s_c,2)\n      rate[i]<-nu_c[i]/pow(s_c,2)\n      log(nu_c[i])<-gamma0 + gamma1*trt[i] + gamma2*QALYs[i]\n}\n\n#prior specification\ns_c ~ dunif(0,1000)\ns_e_limit<- sqrt(0.5885263*(1-0.5885263))\ns_e ~ dunif(0,s_e_limit)\nbeta0 ~ dnorm(0,0.001)\nbeta1 ~ dnorm(0,0.001)\ngamma0 ~ dnorm(0,0.001)\ngamma1 ~ dnorm(0,0.001)\ngamma2 ~ dnorm(0,0.001)\n\n}\n\"\nwriteLines(model_bg, con = \"model_bg.txt\")\n```\n:::\n\n\nWe then proceed to pass the data and algorithm parameters to fit the model in JAGS. It is important to stress that, since Gamma and Beta are defined within bounds of $(0,+\\infty)$ and $(0,1)$, then we need to \"modify\" the data to ensure that such extremes are not present (in case they occur). This is often done by adding/subtracting a small constant to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#save data input\nn <- dim(data_sim_ec)[1]\nQALYs <- data_sim_ec$QALYs - 0.001\nCosts <- data_sim_ec$Costs + 0.001\ntrt <- data_sim_ec$trt\n\n#load package and provide algorithm parameters\nlibrary(R2jags)\nset.seed(2345) #set seed for reproducibility\ndatalist<-list(\"n\",\"QALYs\",\"Costs\",\"trt\") #pass data into a list\n#set up initial values for algorithm\ninits1 <- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 1)\ninits2 <- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 2)\n#set parameter easimates to save\nparams<-c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\",\"gamma2\",\"s_c\",\"s_e\",\"nu_c\",\"nu_e\",\n          \"shape\",\"rate\",\"shape1\",\"shape2\")\nfilein<-\"model_bg.txt\" #name of model file\nn.iter<-20000 #n of iterations\n\n#fit model\njmodel_bg<-jags(data=datalist,inits=list(inits1,inits2),\n                parameters.to.save=params,model.file=filein,\n                n.chains=2,n.iter=n.iter,n.thin=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 7\n   Total graph size: 2739\n\nInitializing model\n```\n\n\n:::\n:::\n\n\nWe then proceed to extract the key quantities of interest of this analysis by post-processing the model estimates via simulation methods, with the aim to obtain the mean effects and costs outcome posterior distributions $(\\mu_e,\\mu_c)$ in each treatment group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#obtain estimates of means by arm\n\n#extract estimates for each mean parameter by trt group\nshape1_e0 <- jmodel_bg$BUGSoutput$sims.list$shape1[,trt==0]\nshape1_e1 <- jmodel_bg$BUGSoutput$sims.list$shape1[,trt==1]\nshape2_e0 <- jmodel_bg$BUGSoutput$sims.list$shape2[,trt==0]\nshape2_e1 <- jmodel_bg$BUGSoutput$sims.list$shape2[,trt==1]\n\nshape_c0 <- jmodel_bg$BUGSoutput$sims.list$shape[,trt==0]\nshape_c1 <- jmodel_bg$BUGSoutput$sims.list$shape[,trt==1]\nrate_c0 <- jmodel_bg$BUGSoutput$sims.list$rate[,trt==0]\nrate_c1 <- jmodel_bg$BUGSoutput$sims.list$rate[,trt==1]\nscale_c0 <- 1/rate_c0\nscale_c1 <- 1/rate_c1\n\n\n#create empty vectors to contain results for means by trt group\nmu_e0_bg <- mu_c0_bg <- c()\nmu_e1_bg <- mu_c1_bg <- c()\n\n#set number of replications\nL <- 5000\n\nset.seed(2345) #set seed for reproducibility\n#generate replications and take mean at each iteration of the posterior\nfor(i in 1:n.iter){\n mu_e0_bg[i] <- mean(rbeta(L,shape1 =  shape1_e0[i,], shape2 =  shape2_e0[i])) \n mu_e1_bg[i] <- mean(rbeta(L,shape1 =  shape1_e1[i,], shape2 =  shape2_e1[i]))\n mu_c0_bg[i] <- mean(rgamma(L,shape =  shape_c0[i,],scale =  scale_c0[i])) \n mu_c1_bg[i] <- mean(rgamma(L,shape =  shape_c1[i,],scale =  scale_c1[i])) \n}\n\n#calculate mean differences\nDelta_e_bg <- mu_e1_bg - mu_e0_bg\nDelta_c_bg <- mu_c1_bg - mu_c0_bg\n```\n:::\n\n\nAt this point it would be wise to check the results of both models and compare them in terms of some measures of relative or absolute performance, such as information criteria or posterior predictive checks. In general, the idea is that IC should reveal a better performance of the distributions that better fit the observed data, while PPCs can be used to detect possible problems in the model replications that may fail to capture some aspects of the observed data. These can be used to decide which model should be trusted more in terms of relative fit to the data. For the sake of simplicity, here I omit these comparisons as it is something I would like to dedicate a more substantial discussion.\n\nFinally, we can then produce all standard CEA output, e.g. CEAC or CE Plane, by post-processing these posterior distributions and compare the cost-effectiveness results of the two models fitted. If you want to skip the fun, we can take advantage of the `R` package **BCEA** which is dedicated to post-processing the results from a Bayesian CEA model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load package and provide means e and c by group as input \nlibrary(BCEA)\nmu_e_bn <- cbind(mu_e0,mu_e1)\nmu_c_bn <- cbind(mu_c0,mu_c1)\nmu_e_bg <- cbind(mu_e0_bg,mu_e1_bg)\nmu_c_bg <- cbind(mu_c0_bg,mu_c1_bg)\n\n#produce CEA output\ncea_res_bn <- bcea(eff = mu_e_bn, cost = mu_c_bn, ref = 2)\ncea_res_bg <- bcea(eff = mu_e_bg, cost = mu_c_bg, ref = 2)\n\n#CE Plane (set wtp value)\ncep_bn <- ceplane.plot(cea_res_bn, graph = \"ggplot2\", wtp = 10000)\ncep_bg <- ceplane.plot(cea_res_bg, graph = \"ggplot2\", wtp = 10000)\n\n#CEAC \nceac_bn <- ceac.plot(cea_res_bn, graph = \"ggplot2\")\nceac_bg <- ceac.plot(cea_res_bg, graph = \"ggplot2\")\n\ngridExtra::grid.arrange(cep_bn, cep_bg, ceac_bn, ceac_bg, nrow = 2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=768}\n:::\n\n```{.r .cell-code}\n#other output\nsummary(cea_res_bn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCost-effectiveness analysis summary \n\nReference intervention:  intervention 2\nComparator intervention: intervention 1\n\nOptimal decision: choose intervention 1 for k < 4000 and intervention 2 for k >= 4000\n\n\nAnalysis for willingness to pay parameter k = 25000\n\n               Expected net benefit\nintervention 1                11911\nintervention 2                15635\n\n                                    EIB CEAC   ICER\nintervention 2 vs intervention 1 3724.1    1 3959.2\n\nOptimal intervention (max expected net benefit) for k = 25000: intervention 2\n                \nEVPI -1.5852e-13\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(cea_res_bg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCost-effectiveness analysis summary \n\nReference intervention:  intervention 2\nComparator intervention: intervention 1\n\nintervention 2 dominates for all k in [0 - 50000] \n\n\nAnalysis for willingness to pay parameter k = 25000\n\n               Expected net benefit\nintervention 1               8507.6\nintervention 2              14523.2\n\n                                    EIB CEAC    ICER\nintervention 2 vs intervention 1 6015.6    1 -7817.7\n\nOptimal intervention (max expected net benefit) for k = 25000: intervention 2\n                \nEVPI -6.3236e-13\n```\n\n\n:::\n:::\n\n\nIn the end results may be quite different between the models used and it is crucial that model selection is done based on appropriate criteria in order to decide which results to trust more according to how well the model fit the data and potential issues in MCMC convergence (i.e. need to check algorithm diagnostics to make sure posterior estimates are reliable). I really think that, especially in a HTA context, Bayesian methods are naturally suited for the job since they allow to flexibly define your model to tackle different aspects of the data while also addressing uncertainty in a structural and consistent way, being it uncertainty about model estimates, missing values, predictions, etc .... Although the same can also be achieved within a frequentist framework, this comes at the cost of an increasingly complex implementation effort (e.g. need to combine bootstrapping and non-normal distributions) which, instead, in a Bayesian setting can be achieved in a relatively easy way. As the models we need to fit become increasingly complex, it makes sense to me that more flexible approaches that can be tailored to deal with different situations and peculiarities should become more and useful to analysts to accomplish their tasks.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}