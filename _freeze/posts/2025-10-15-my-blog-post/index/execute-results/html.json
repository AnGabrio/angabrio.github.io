{
  "hash": "f604a866bb2f8756502f43569c76e02d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Conducting trial-based cost-effectiveness analyses:\"\nsubtitle: \"A tutorial\"\ndescription: \"\"\nauthor:\n  - name: Andrea Gabrio\n    url: https://angabrio.github.io/agabriosite2/\n    orcid: 0000-0002-7650-4534\n    affiliation: Maastricht University\n    affiliation-url: https://www.maastrichtuniversity.nl/research/methodology-and-statistics\ndate: 2025-10-15\ncategories: [Quarto, R, Academia, HTA, Cost-Effectiveness, Tutorial] # self-defined categories\n#citation: \n#  url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ \nimage: featured.png\nbibliography: references.bib\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\nHello folks, I hope you are well and welcome back to my blog. Today, after taking a break from HTA in the past posts, I would like to resume one of my favourite topics: statistical methods for cost-effectiveness analyses. One of the reasons that pushed me to cover once more this topic is the fact that I will shortly give a presentation about this within my university with the objective to highlight the current state of play in the field. I would like then to take this opportunity (otherwise it could take a long time for me to organise my code again) to provide here a short summary of the different elements I will touch in my talk. Perhaps I will need to split the topics in two posts as the amount of things that will be discussed is quite a lot, but I am not sure yet about this. Let's see how it goes.\n\nFor now, I just want to introduce the topic and put out the disclaimer that, given that I will mostly focus on typical methods used in trial-based analyses, I will strictly focus on frequentist methods and their implementation using standard `R` functions and packages. Thus, no Bayesian methods will be discussed/coded today, unfortunately! But no worries, there will be plenty of time to discuss Bayesian methods for HTA in the future. Actually, if interested, a new book for which I also collaborated has been recently published [@baio2025r] which I highly recommend if you want to have a more in-depth look at how to code advanced methods for HTA in `R`, including Bayesian! The chapter I coordinated and written is the one on **missing data** (I believe chapter 3?), naturally. I think you will find very helpful if you are too scared about statistics and programming (I promise that things are not that difficult and everything is well described and illustrated).\n\nMoving on, let's get into the content of today's post! The focus will be around the typical complexities that affect trial-based CE data and what has been done and is currently recommended in the literature to deal with them when conducting the analyses. I will also partly follow the structure and order of topics discussed in a relatively recent review [@el2022scoping], which I strongly recommend to look at for references. Given the limited available, I will focus on those methods and issues that are directly mentioned in the recent national guidelines from the Dutch HTA agency *ZorgInstituut Nederland*[@nederland2024guideline] about the statistical analysis of (empirical) trial-based health economic evaluations in the Netherlands. In particular, I will focus on the following aspects:\n\n  - Imbalances in mean baseline effects/costs between arms, if not accounted for, may distort CE results[@manca2005estimating;@van2009deal]\n  \n  - Ignoring the correlation between effects \\& costs is inappropriate and will lead to a loss in efficiency for the estimates [@willan2004regression]\n  \n  - Skewness} in effects/costs in small samples undermines the validity of asymptotic tests and may lead to incorrect results [@barber2000analysis]\n  \n  - Clustering of data linked to treatment (eg cluster RCTs) invalidates the assumptions of standard methods (eg OLS), underestimates variance and possibly bias results [@gomes2012developing]\n  \n  - Missing effects/costs during follow-up are common and may introduce bias. Methods that appropriately account for missing data uncertainty and assess the sensitivity of results are needed [@gabrio2017handling;@leurent2018missing]\n\n\n# Regression Adjustment for Baseline Imbalances {#sec-baseadj}\n\nWe start by considering the problem of the possible occurrence of some imbalances between treatment arms in some baseline variables. In the context of randomised trial-based CUAs, a typical example of these variables are baseline utilities [@manca2005estimating] or costs [@van2009deal]. Despite randomisation, some baseline imbalances are likely to occur, especially in those baseline variables that are likely strongly associated with the main outcomes of interest for the analysis, i.e. QALYs and Total Costs. This is a particular issue for baseline utilities, which are directly used in the calculation of QALYs through the *Area Under the Curve* (AUC) method [@drummond2015methods]. If these imbalances are not adjusted for, estimates from the main analysis are likely to be affected (eg mean QALY difference or ICER estimates), especially in the event of substantial differences, and possibly lead to misleading CE conclusions.   \n\n## Data generation\n\nThe following (folded) code is simply used to generate some artificial CEA data for exemplary purposes to demonstrate how regression adjustment may be conducted in `R`. If not of interest, you may skip the folded code and jump to the actual implementation code in the next section.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#generate multivariate trial CUA data (utilities)\nlibrary(mvtnorm) #load library to generate multivariate normal data\nn0 <- n1 <- 2000 #sample size for control (0) and new trt (1)\ntime <- 3 #n of time points\n\n#mean utilities at each time in control\nmu0 <- c(0.63,0.63,0.67) \n#covariance matrix for utilities in control\nSigma0 <- diag(3)  \ndiag(Sigma0) <- c(0.1^2,0.1^2,0.1^2)\nrho0 <- 0.85 #correlation across time in control\nSigma0[1,2] <- Sigma0[2,1] <- rho0*0.1*0.1\nSigma0[3,1] <- Sigma0[1,3] <- rho0*0.1*0.1\nSigma0[3,2] <- Sigma0[2,3] <- rho0*0.1*0.1\n#mean utilities at each time in new trt\nmu1 <- c(0.55,0.63,0.67) \nSigma1 <- diag(3) \ndiag(Sigma1) <- c(0.1^2,0.1^2,0.1^2) \nrho1 <- 0.85 #correlation across time in new trt\n#covariance matrix for utilities in new trt\nSigma1[1,2] <- Sigma1[2,1] <- rho1*0.1*0.1\nSigma1[3,1] <- Sigma1[1,3] <- rho1*0.1*0.1\nSigma1[3,2] <- Sigma1[2,3] <- rho1*0.1*0.1\n\n#set rng seed for reproducibility\nset.seed(2345)\n#simulate data by arm\nu0 <- rmvnorm(n0, mean = mu0, sigma = Sigma0)\nu1 <- rmvnorm(n1, mean = mu1, sigma = Sigma1)\n#compute QALYs via AUC by arm\nQALY0 <- ((u0[,1]+u0[,2])/2)*(6/12) + ((u0[,2]+u0[,3])/2)*(6/12)\nQALY1 <- ((u1[,1]+u1[,2])/2)*(6/12) + ((u1[,2]+u1[,3])/2)*(6/12)\n\n#rename variables and create dataset\nu_base <- c(u0[,1],u1[,1])\nu_6m <- c(u0[,2],u1[,2])\nu_12m <- c(u0[,3],u1[,3])\nQALY <- c(QALY0, QALY1)\ntrt <- c(rep(\"old\",n0),rep(\"new\",n1))\ndataset <- data.frame(u_base,u_6m,u_12m,QALY,trt)\n#make trt a factor variable (old,new)\ndataset$trt <- factor(trt, levels = c(\"old\", \"new\"))\n#randomly shuffle rows of the dataset\ndataset <- dataset[sample(1:nrow(dataset)), ]\n```\n:::\n\n\nWe can inspect the first few rows of the generated data stored in the `R` object `dataset`, for example by typing the following command\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(dataset, n=8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        u_base      u_6m     u_12m      QALY trt\n1851 0.6649710 0.6695179 0.6385095 0.6606291 old\n3885 0.5024305 0.6252957 0.6733950 0.6066042 new\n2100 0.5250719 0.6123696 0.6940127 0.6109560 new\n1430 0.6435694 0.6345586 0.6374612 0.6375370 old\n2705 0.4929146 0.5621082 0.5388496 0.5389951 new\n2774 0.6824814 0.7805227 0.8651590 0.7771715 new\n537  0.5938100 0.5543479 0.6344838 0.5842474 old\n3667 0.4878960 0.5223296 0.6684593 0.5502536 new\n```\n\n\n:::\n:::\n\n\nwhich shows for a few rows (individuals) their related values for the following hypothetical variables: baseline utility, 6 and 12 months follow-up utility, QALY and treatment allocation. \n\n## Method application\n\nRegression adjustment is a technique which allows to obtained estimates of parameters of interest, i.e. mean QALY differential $\\Delta_e=\\text{E}[\\text{QALY}\\mid \\text{New}]-\\text{E}[\\text{QALY}\\mid \\text{old}]$, while also controlling for possible imbalances in some baseline variables. Although alternative adjustment methods exist, regression-based adjustment is by far the most popular and recommended in the literature since it can be easily implemented and can provide valid adjusted estimates with respect to multiple baseline variables [@manca2005estimating].    \n\nUsually, estimates for $\\Delta_e$ are retrieved after fitting a standard *Ordinary Least Square* (OLS) linear regression to QALYs using the treatment arm indicator as the key independent variable into the model. \n\n$$\n\\text{QALY}_i = \\beta_0+\\beta_1\\times \\text{arm}_i + \\varepsilon_i\n$$ {#eq-1}\n\nwhere $\\text{QALY}_i$ and $\\text{arm}_i$ denote the outcome and treatment indicator value for individual $i=1,\\ldots,N$ in the trial, while $\\varepsilon_i$ denote the individual-level error term assumed to follow a Normal distribution with mean $0$ and variance $\\sigma^2$.\n\nEstimates of the regression coefficients $\\hat{\\beta}=(\\hat{\\beta}_0,\\hat{\\beta}_1)$ in @eq-1 can then be used to derive the **unadjusted** mean QALYs in each treatment group \n\n$$\n\\begin{aligned}\n\\text{E}[\\text{QALY}\\mid \\text{arm}=\\text{old}] &= \\hat{\\beta}_0\\\\\n\\text{E}[\\text{QALY}\\mid \\text{arm}=\\text{new}] &= \\hat{\\beta}_0+\\hat{\\beta}_1,\\\\\n\\end{aligned}\n$$\nwith $\\hat{\\beta}_1$ representing the mean difference between the reference group (eg New) with respect to the comparator (eg Old).\n\nAdjustment for some baseline variable, eg baseline utilities $u_{i0}$, can be easily achieved by including the corresponding baseline variable into the regression as an additional independent variable. Thus, the model regression becomes\n\n$$\n\\text{QALY}_i = \\beta_0+\\beta_1\\times \\text{arm}_i + \\beta_2\\times \\text{u}_{i0}+ \\varepsilon_i\n$$ {#eq-2}\n\nwhere $\\hat{\\beta}_2$ represents the coefficient associated with $u_{i0}$ (ie how much QALY changes for a unit change in baseline utility). By simply including $u_{i0}$ into the model, **adjusted** estimates for $\\hat{\\beta}_1$ from @eq-2 can be obtained in a similar way to the unadjusted model. The only difference is in case we need to estimate mean QALYs in each treatment group:\n\n$$\n\\begin{aligned}\n\\text{E}[\\text{QALY}\\mid \\text{arm}=\\text{old}] &= \\hat{\\beta}_0+\\hat{\\beta}_2\\times \\bar{\\text{u}}_{0}\\\\\n\\text{E}[\\text{QALY}\\mid \\text{arm}=\\text{new}] &= \\hat{\\beta}_0+\\hat{\\beta}_1+\\hat{\\beta}_2\\times \\bar{\\text{u}}_{0},\\\\\n\\end{aligned}\n$$ {#eq-3}\n\n\nwhich can be obtained after setting the value of $\\text{u}_{i0}$ to its sample mean across treatment groups (denoted with $\\bar{\\text{u}}_{0}$).  \n\nNow, let's all do this in `R`. First, let's fit the adjusted model and summarise the output\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#fit OLS regression adjusting for baseline utility\nlm_adj <- lm(QALY ~ trt + u_base, data = dataset)\n#summarise output\nsummary(lm_adj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = QALY ~ trt + u_base, data = dataset)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.133578 -0.023659 -0.000639  0.023359  0.116394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.084463   0.003556   23.75   <2e-16 ***\ntrtnew      0.051304   0.001178   43.56   <2e-16 ***\nu_base      0.880692   0.005528  159.31   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03454 on 3997 degrees of freedom\nMultiple R-squared:  0.8653,\tAdjusted R-squared:  0.8652 \nF-statistic: 1.284e+04 on 2 and 3997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nWe can see in the coefficients \"Estimate\" column the values for: $\\hat{\\beta}_0=0.084$, $\\hat{\\beta}_1=0.051$ and $\\hat{\\beta}_2=0.881$. In addition, we can also find the value for the \"Residual standard error\" $\\sigma=0.035$. \n\nIn the following (folded) code part I will show how to manually compute the estimates in @eq-3 based on the regression output.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#extract regression coefficients\nbetas <- coef(lm_adj)\n#extract error standard deviation\nsigma <- summary(lm_adj)$sigma\n#extract all independent variables\nXs <- model.matrix(lm_adj)\n#compute matrix multiplication \nXtX.inv <- solve(t(Xs) %*% Xs) \n#select profile to be estimated in terms of: intercept, trt, u_base\nprof_old <- c(1,0,mean(dataset$u_base)) #1=intercept,0=trt,u_base=mean(u)\nprof_new <- c(1,1,mean(dataset$u_base)) #1=intercept,1=trt,u_base=mean(u)\n#compute linear combination of regression parameters\nmean_old <- betas[\"(Intercept)\"] + betas[\"u_base\"]*mean(dataset$u_base)\nmean_new <- betas[\"(Intercept)\"] + betas[\"trtnew\"] + betas[\"u_base\"]*mean(dataset$u_base)\n#compute associated standard errors\nstd.err_old <- sigma * sqrt(t(prof_old) %*% XtX.inv %*% prof_old)\nstd.err_new <- sigma * sqrt(t(prof_new) %*% XtX.inv %*% prof_new)\n#compute associated confidence intervals \nalpha<-0.05 #95% CI\nn <- dim(dataset)[1] #total sample size\ndf <- n-c(length(betas)-1) #degrees of freedom\nt.critic <- qt(1-alpha/2,df=df) #critical value for t distribution and 95% CI\nmean_old_lower.CI <- mean_old - t.critic*std.err_old\nmean_old_upper.CI <- mean_old + t.critic*std.err_old\nmean_new_lower.CI <- mean_new - t.critic*std.err_new\nmean_new_upper.CI <- mean_new + t.critic*std.err_new\n#combine all results\nmean_old_summary <- c(mean_old,std.err_old,mean_old_lower.CI,mean_old_upper.CI)\nmean_new_summary <- c(mean_new,std.err_new,mean_new_lower.CI,mean_new_upper.CI)\n#attach names to each value\nnames(mean_old_summary) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\nnames(mean_new_summary) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\n```\n:::\n\n\nWe can also rely on the pre-built function `emmean` from the `R` package `emmeans` to compute the mean QALYs in each treatment group in a more automatic way. For example, we can type the following \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(emmeans) #load library to obtain marginal means\nlm_adj_em <- emmeans(lm_adj, ~ trt) #compute mean outcome by level of trt\nlm_adj_em\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n trt emmean       SE   df lower.CL upper.CL\n old 0.6023 0.000803 3997   0.6007   0.6039\n new 0.6536 0.000803 3997   0.6520   0.6552\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nto directly obtain estimates of the mean QALYs, their standard errors and $95\\%$ confidence intervals by treatment group (hopefully they are the same as those computed manually before!). In addition, it is also possible to use the function `contrast` to derive estimates of any linear combination of the quantities above. For example, we may obtain estimates for the mean QALY difference (New - Old) by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#take difference as - Old + New = New - Old\nQALY_new_vs_old <- list(\"New vs Old\" = c(-1, 1))\n#compute linear combination\nlm_adj_em_delta_e <- contrast(lm_adj_em, QALY_new_vs_old) \n#obtain results in terms of confidence intervals\nconfint(lm_adj_em_delta_e, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast   estimate      SE   df lower.CL upper.CL\n New vs Old   0.0513 0.00118 3997    0.049   0.0536\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nAlthough I have not shown here an application of regression adjustment to control for baseline costs when deriving Total Costs mean and mean differential estimates, these can be easily obtained in a similar way to what shown for QALYs and baseline utilities. In the following sections I will show how these computations can be embedded within a bootstrapping procedure for both outcome variables and how to obtain bootstrapped estimates for the quantities of interest for the CEA. \n\n## Summary\n\nIn general, it is good to keep in mind the following features when implementing regression-based adjustment:\n\n- **Advantages**: \n\n  + Easy to implement\n  + Allow to control for multiple variables\n  + Assess impact of specific variables on marginal CE outcomes (eg via\ninteraction terms)\n\n- **Drawbacks**:\n\n  + Assume variables’ distribution is the same across arms\n  + Adjusting for many variables may result in “overfitting\"\n\nThere are also alternatives to regression-based adjustment, although these have been less used in the CEA literature and have their own advantages and drawbacks. Examples include: *Propensity score adjustment* [@indurkhya2006using], *Propensity Score Matching* \\& *Genetic Matching* [@sekhon2012matching]\n\n# Correlation between CE outcomes {#sec-corr}\n\nA typical feature of trial-based CE data is the presence of some form of correlation between the effect and cost variables. This may happen because: new treatments come from intensive research and are *positively* associated with higher unit costs, or new treatments *negatively* affect care pathway costs (eg fewer hospitalisations, side effects, etc.). When this association, either positive or negative, is substantial, simply running separate OLS models for each outcome (as in @sec-baseadj) will provide *inefficient* estimates in the sense that the level of uncertainty around the parameter estimates will be **overestimated**, thus resulting in higher standard errors and wider confidence intervals. \n\nTo address this problem, **joint modelling** of both outcome variables is typically recommended to properly characterise the level of uncertainty around parameter estimates and CE results [@o2001framework]. Indeed, by simultaneously modelling both outcomes within a *multivariate* analysis it is possible to borrow information across variables to estimate variance components and standard errors more efficiently with respect to separate univariate analyses. A general issue, however, is that multivariate modelling, while also allowing for regression adjustment for each outcome separately (ie using different baseline variables per outcome), is not straightforward to implement using standard software packages, especially wihtin a frequentist statistical framework.  \n\nTo overcome this practical issue, correlation between outcomes can also be taken into account using alternative approaches, which have become increasingly popular in trial-based CEA. These include: **Seemingly Unrelated Regression** (SUR) equations framework [@willan2004regression], (non-parametric) **bootstrapping** procedure [@nixon2010non], or even a combination of these two methods.\n\n## Data generation\n\nAs usual, let's start by generating some artificial data that will be used to show how to implement the methods in `R`. The code used to generate these data is provided in the following (folded) code part and, if not of interest, you may skip it and jump to the actual implementation code in the next section.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#remove scientific notation to be displayed by R\noptions(scipen=999) \n#generate data\nlibrary(mvtnorm) #load package to generate multivariate normal data\nn0 <- n1 <- 150 #arm-specific sample sizes (0=old,1=new)\nmu0 <- c(0.58,3) #mean QALY and TC in old arm\n#2x2 Covariance matrix for QALY and TC in old arm\nSigma0 <- diag(2) \ndiag(Sigma0) <- c(0.1^2,1^2) #set variances for each outcome in old arm\nrho0 <- 0.75 #set correlation between outcomes in old arm\nSigma0[1,2] <- Sigma0[2,1] <- rho0*0.1*1 #set covariances in old arm\n#do the same for new arm\nmu1 <- c(0.65,3)\nSigma1 <- diag(2) \ndiag(Sigma1) <- c(0.1^2,1^2) \nrho1 <- 0.75\nSigma1[1,2] <- Sigma1[2,1] <- rho1*0.1*1\n#set rng for reproducibility\nset.seed(2345)\n#generate QALY and TC data by arm\nec0 <- rmvnorm(n0, mean = mu0, sigma = Sigma0)\nec1 <- rmvnorm(n1, mean = mu1, sigma = Sigma1)\n#extract each outcome variable by arm\nQALY0 <- ec0[,1]\nQALY1 <- ec1[,1]\nTC0 <- ec0[,2]*100 #rescale costs\nTC1 <- ec1[,2]*100 #rescale costs\n#generate baseline values for each outcome and arm\nset.seed(2345)\nu0 <- 0.1 + 0.001*QALY0 + rnorm(n0,0,0.25)\nu1 <- 0.1 + 0.001*QALY1 + rnorm(n1,0,0.25)\nc0 <- 100 + 0.001*TC0 + rnorm(n0,0,85)\nc1 <- 100 + 0.001*TC1 + rnorm(n1,0,85)\n#combine variables into a dataframe\nQALY <- c(QALY0, QALY1)\nTC <- c(TC0, TC1)\nu <- c(u0,u1)\nc <- c(c0,c1)\ntrt <- c(rep(\"old\",n0),rep(\"new\",n1))\ndataset <- data.frame(QALY,TC,u,c,trt)\ndataset$trt <- factor(trt, levels = c(\"old\", \"new\"))\n#randomly shuffle rows of the dataset\ndataset <- dataset[sample(1:nrow(dataset)), ]\n```\n:::\n\n\nAgain, let's inspect the first few rows of the newly-generated data by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(dataset, n=8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         QALY       TC           u         c trt\n16  0.5702695 324.0702 -0.13975216  23.94076 old\n77  0.6240807 412.6261  0.57226207  51.83082 old\n83  0.5269099 281.0698  0.07965101  90.18762 old\n112 0.4696755 186.8044  0.22252636 135.76419 old\n66  0.5655207 313.6860 -0.14142596  24.48376 old\n80  0.5576176 366.6674 -0.22197673 -37.31306 old\n107 0.7560502 291.5428 -0.15671117 156.94553 old\n84  0.5313837 297.7465  0.45782877 120.80797 old\n```\n\n\n:::\n:::\n\n\nWe can also check the level of *Pearson*'s correlation between QALY and TC variables by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncor(dataset$QALY,dataset$TC, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6244816\n```\n\n\n:::\n:::\n\n\n## Method application\n\n**Seemingly Unrelated Regression** consists in an approximation to joint of different outcome variables while also allowing separate regression model specifications for each outcome variable. The modelling approach is similar to that of OLS (see @eq-1), where each outcome is regressed on given sets of independent variables. For example, considering the case of regression adjustment for baseline utilities ($u_{i0}$) and costs ($c_{i0}$), the SUR model would look like:\n\n$$\n\\begin{aligned}\n\\text{QALY}_i &= \\beta_0 + \\beta_1\\times \\text{arm}_i + \\beta_2\\times u_{i0} + \\varepsilon_{ie} \\\\\n\\text{TC}_i &= \\alpha_0 + \\alpha_1\\times \\text{arm}_i + \\alpha_2\\times c_{i0} + \\varepsilon_{ic} \\\\\n\\end{aligned}\n$$ {#eq-4}\n\nThe main difference compared to running separate OLS models is that correlation between the error terms of the two equations is taken into account by assuming they follow a multivariate normal distribution:\n\n$$\n\\begin{aligned}\n\\begin{pmatrix}\n\\varepsilon_{ie}\\\\\n\\varepsilon_{ic}\\\\\n\\end{pmatrix} &\\sim  \\text{Normal}\n\\begin{bmatrix}\n\\mu=\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\Sigma =\\begin{pmatrix}\n\\sigma^2_e & \\rho\\sigma_e\\sigma_c\\\\\n\\rho\\sigma_c\\sigma_e & \\sigma^2_c\n\\end{pmatrix}\n\\end{bmatrix},\n\\end{aligned}\n$$\n\nwith mean vector $\\mu$ and covariance matrix $\\Sigma$, where: $\\sigma^2_e$ and $\\sigma^2_c$ denote the variances of the QALY and TC variables, while $\\rho$ represents the correlation parameter capturing the association between the outcomes.\n\nIn `R`, the following code may be used to fit SUR equations as shown in @eq-4 to the generated data and summarise the output:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(systemfit) #load package to fit SUR\n#fit SUR to QALY and TC separate regressions\nsur_ec <- systemfit(list(QALYreg = QALY~trt + u, TCreg = TC~trt + c), \n                    method=\"SUR\", data=dataset)\n#summarise output\nsummary(sur_ec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nsystemfit results \nmethod: SUR \n\n         N  DF     SSR detRCov   OLS-R2 McElroy-R2\nsystem 600 594 2266697  36.535 0.000907   0.135509\n\n          N  DF           SSR         MSE      RMSE       R2    Adj R2\nQALYreg 300 297       2.61745    0.008813  0.093877 0.144406  0.138645\nTCreg   300 297 2266694.46182 7631.967885 87.361135 0.000907 -0.005821\n\nThe covariance matrix of the residuals used for estimation\n          QALYreg     TCreg\nQALYreg 0.0088027    5.5284\nTCreg   5.5283958 7631.5290\n\nThe covariance matrix of the residuals\n           QALYreg      TCreg\nQALYreg 0.00881297    5.54305\nTCreg   5.54304723 7631.96788\n\nThe correlations of the residuals\n         QALYreg    TCreg\nQALYreg 1.000000 0.675879\nTCreg   0.675879 1.000000\n\n\nSUR estimates for 'QALYreg' (equation 1)\nModel Formula: QALY ~ trt + u\n\n               Estimate  Std. Error  t value               Pr(>|t|)    \n(Intercept)  0.57629408  0.00781616 73.73115 < 0.000000000000000222 ***\ntrtnew       0.07641998  0.01083463  7.05331      0.000000000012319 ***\nu           -0.02836204  0.01656608 -1.71205                0.08793 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.093877 on 297 degrees of freedom\nNumber of observations: 300 Degrees of Freedom: 297 \nSSR: 2.617453 MSE: 0.008813 Root MSE: 0.093877 \nMultiple R-Squared: 0.144406 Adjusted R-Squared: 0.138645 \n\n\nSUR estimates for 'TCreg' (equation 2)\nModel Formula: TC ~ trt + c\n\n               Estimate  Std. Error  t value             Pr(>|t|)    \n(Intercept) 294.9174847   8.7883442 33.55780 < 0.0000000000000002 ***\ntrtnew        1.8624126  10.1335467  0.18379              0.85431    \nc             0.0242418   0.0462881  0.52371              0.60087    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 87.361135 on 297 degrees of freedom\nNumber of observations: 300 Degrees of Freedom: 297 \nSSR: 2266694.461815 MSE: 7631.967885 Root MSE: 87.361135 \nMultiple R-Squared: 0.000907 Adjusted R-Squared: -0.005821 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nWe can see in for the first equation (`QALYreg`) the coefficients \"Estimate\" column shows the values for: $\\hat{\\beta}_0=0.576$, $\\hat{\\beta}_1=0.076$ and $\\hat{\\beta}_2=-0.028$. In addition, we can also find the value for the \"Residual standard error\" $\\sigma=0.094$. Similarly for the second equation (`TCreg`) we have: $\\hat{\\alpha}_0=294.917$, $\\hat{\\alpha}_1=1.862$, $\\hat{\\alpha}_2=0.024$ and $\\sigma_c=87.361$. Finally, we also have estimates for the \"covariance matrix of the residuals\" and the \"correlations of the residuals\" (with an estimated correlation of $0.676$) which we would not be able to get from fitting separate OLS models.\n\nIn the following (folded) code part I will show how to manually compute the estimates for mean QALY and TC variables by treatment group from the models' regression coefficients as specified in @eq-4.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#extract regression coefficients for QALY model\nbetas <- coef(sur_ec$eq[[1]])\n#extract error standard deviation for QALY model\nsigma <- summary(sur_ec$eq[[1]])$sigma\n#extract all independent variables for QALY model\nXs_e <- model.matrix(sur_ec$eq[[1]])\n#compute matrix multiplication \nXtX.inv_e <- solve(t(Xs_e) %*% Xs_e) \n#select profile to be estimated in terms of: intercept, trt, u\nprof_old_e <- c(1,0,mean(dataset$u)) #1=intercept,0=trt,u=mean(u)\nprof_new_e <- c(1,1,mean(dataset$u)) #1=intercept,1=trt,u=mean(u)\n#compute linear combination of regression parameters\nmean_old_e <- betas[\"(Intercept)\"] + betas[\"u\"]*mean(dataset$u)\nmean_new_e <- betas[\"(Intercept)\"] + betas[\"trtnew\"] + betas[\"u\"]*mean(dataset$u)\n#compute associated standard errors\nstd.err_old_e <- sigma_e * sqrt(t(prof_old_e) %*% XtX.inv_e %*% prof_old_e)\nstd.err_new_e <- sigma_e * sqrt(t(prof_new_e) %*% XtX.inv_e %*% prof_new_e)\n#compute associated confidence intervals \nalpha<-0.05 #95% CI\nn_e <- dim(dataset)[1] #total sample size\ndf_e <- n_e-c(length(betas)-1) #degrees of freedom\nt.critic_e <- qt(1-alpha/2,df=df_e) #critical value for t distribution and 95% CI\nmean_old_lower.CI_e <- mean_old_e - t.critic_e*std.err_old_e\nmean_old_upper.CI_e <- mean_old_e + t.critic_e*std.err_old_e\nmean_new_lower.CI_e <- mean_new_e - t.critic_e*std.err_new_e\nmean_new_upper.CI_e <- mean_new_e + t.critic_e*std.err_new_e\n#combine all results\nmean_old_summary_e <- c(mean_old_e,std.err_old_e,mean_old_lower.CI_e,mean_old_upper.CI_e)\nmean_new_summary_e <- c(mean_new_e,std.err_new_e,mean_new_lower.CI_e,mean_new_upper.CI_e)\n#attach names to each value\nnames(mean_old_summary_e) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\nnames(mean_new_summary_e) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\n\n#do the same for TC model\nalphas <- coef(sur_ec$eq[[2]])\nsigma <- summary(sur_ec$eq[[2]])$sigma\nXs_c <- model.matrix(sur_ec$eq[[2]])\nXtX.inv_c <- solve(t(Xs_c) %*% Xs_c) \nprof_old_c <- c(1,0,mean(dataset$c)) #1=intercept,0=trt,c=mean(c)\nprof_new_c <- c(1,1,mean(dataset$c)) #1=intercept,1=trt,c=mean(c)\nmean_old_c <- alphas[\"(Intercept)\"] + alphas[\"c\"]*mean(dataset$c)\nmean_new_c <- alphas[\"(Intercept)\"] + alphas[\"trtnew\"] + alphas[\"c\"]*mean(dataset$c)\nstd.err_old_c <- sigma_c * sqrt(t(prof_old_c) %*% XtX.inv_c %*% prof_old_c)\nstd.err_new_c <- sigma_c * sqrt(t(prof_new_c) %*% XtX.inv_c %*% prof_new_c)\nalpha<-0.05 #95% CI\nn_c <- dim(dataset)[1] #total sample size\ndf_c <- n_c-c(length(alphas)-1) #degrees of freedom\nt.critic_c <- qt(1-alpha/2,df=df_c) #critical value for t distribution and 95% CI\nmean_old_lower.CI_c <- mean_old_c - t.critic_c*std.err_old_c\nmean_old_upper.CI_c <- mean_old_c + t.critic_c*std.err_old_c\nmean_new_lower.CI_c <- mean_new_c - t.critic_c*std.err_new_c\nmean_new_upper.CI_c <- mean_new_c + t.critic_c*std.err_new_c\nmean_old_summary_c <- c(mean_old_c,std.err_old_c,mean_old_lower.CI_c,mean_old_upper.CI_c)\nmean_new_summary_c <- c(mean_new_c,std.err_new_c,mean_new_lower.CI_c,mean_new_upper.CI_c)\nnames(mean_old_summary_c) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\nnames(mean_new_summary_c) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\n```\n:::\n\n\nUnfortunately, the convenient functions to automatically derive marginal mean estimates from regression models in the `R` package `emmeans` are not compatible with `R` objects generated via functions from the `systemfit` package. This means that estimates for the marginal means of each outcome need to be derived manually as shown in the (folded) code part above. As a quick example, I will show now how estimates and related measures of uncertainty may be derived for the mean QALY/TC difference represented by the parameters $\\beta_1$ and $\\alpha_1$ in @eq-4.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#extract coefficient for QALY difference from model\nbeta1 <- coef(sur_ec$eq[[1]])[\"trtnew\"]\n#extract standard error for QALY difference from model\nSE_beta1 <- summary(sur_ec$eq[[1]])$coefficients[\"trtnew\",\"Std. Error\"]\n#extract CI bounds for for QALY difference from model\nCI_lower_beta1 <- confint(sur_ec$eq[[1]], level = 0.95)[\"trtnew\",1]\nCI_upper_beta1 <- confint(sur_ec$eq[[1]], level = 0.95)[\"trtnew\",2]\n#combine all estimates in a single object\ndelta_e_sur <- c(beta1,SE_beta1,CI_lower_beta1,CI_upper_beta1)\n#rename elements of object\nnames(delta_e_sur) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\n\n#do the same for TC difference \nalpha1 <- coef(sur_ec$eq[[2]])[\"trtnew\"]\nSE_alpha1 <- summary(sur_ec$eq[[2]])$coefficients[\"trtnew\",\"Std. Error\"]\nCI_lower_alpha1 <- confint(sur_ec$eq[[2]], level = 0.95)[\"trtnew\",1]\nCI_upper_alpha1 <- confint(sur_ec$eq[[2]], level = 0.95)[\"trtnew\",2]\ndelta_c_sur <- c(alpha1,SE_alpha1,CI_lower_alpha1,CI_upper_alpha1)\nnames(delta_c_sur) <- c(\"Estimate\",\"SE\",\"CI(low)\",\"CI(high)\")\n\n#print results\ndelta_e_sur\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimate         SE    CI(low)   CI(high) \n0.07641998 0.01083463 0.05509761 0.09774235 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\ndelta_c_sur\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimate         SE    CI(low)   CI(high) \n  1.862413  10.133547 -18.080240  21.805066 \n```\n\n\n:::\n:::\n\n\nAnother approach which has been suggested in the literature for \"indirectly\" taking into account the correlation between outcomes is the so-called (non-parametric) **paired bootstrapping** [@nixon2010non]. The underlying procedure is as follows:\n\n  1. Generate a \"bootstrap\" sample by *sampling with replacement* QALY and TC individual values together from the original dataset\n  2. Fit the desired model to the newly obtained bootstrap sample to derive bootstrap estimates for the quantities of interest (eg mean outcome differences), for example using OLS or SUR models (ie $\\hat{\\beta}^b_1$ and $\\hat{\\alpha}^b_1$)\n  3. Repeat step 1-2 for a large number of bootstrap iterations $B$ (eg $5000$) and store the results to generate a set of $B$ bootstrap estimates for $b=1,\\ldots,B$\n  4. Use the stored sets of bootstrap estimates (ie $\\hat{\\beta}^b_1$ and $\\hat{\\alpha}^b_1$) to *empirically* approximate the sampling distribution of the parameters of interest.\n  5. Use this distribution of estimates to quantify the level of uncertainty around the quantities of interest, eg in terms of confidence intervals.\n  \nSince at each bootstrap iteration, in step 1, the outcome values are sampled \"together\" for the same individual, then correlation between the variables is preserved in the newly drawn bootstrapped samples. The following (folded) code part shows how to construct an `R` function which allows to implement a non-parametric bootstrap procedure for QALY and TC variables and derive bootstrapped estimates for marginal and incremental mean estimates by fitting a OLS or SUR model to each bootstrapped sample.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(data.table) #package to handle datasets more efficiently\nlibrary(bootstrap) #package to use bootstrap procedure \nlibrary(rlang)\nboot_ec <- function(data, B, QALYreg, TCreg, method = \"OLS\",\n                    profile_QALY=\"default\", profile_TC=\"default\", trt_pos = 2){\n  #the following lines are needed to make sure proper inputs are given\n  if(!is.data.frame(data)){stop(\"data needs to be a data frame object\")}\n  if(!is.numeric(B)){stop(\"please provide number of bootstrap iterations\")}\n  if(B<=0 | !B%%1==0){stop(\"please provide number of bootstrap iterations\")}\n  if(!is_formula(QALYreg)){stop(\"please provide formula for QALY model\")}\n  if(!is_formula(TCreg)){stop(\"please provide formula for TC model\")}\n  if(!method %in% c(\"OLS\",\"SUR\")){stop(\"please provide valid method name\")}\n  if(!is.numeric(trt_pos) | length(trt_pos)!=1 | trt_pos<=0){stop(\"please provide valid trt indicator position in regressions\")}\n  n <- dim(data)[1] #original sample size\n  #n covariates \n  nX_e <- dim(model.matrix(QALYreg, data))[2]\n  nX_c <- dim(model.matrix(TCreg, data))[2]\n  #extract name of trt indicator and outcomes from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  if(trt_name_e != trt_name_c){stop(\"please provide same trt variable name and position in QALY and TC formuale\")}\n  QALY_name <- all.vars(QALYreg)[1]\n  TC_name <- all.vars(TCreg)[1]\n  #check if trt indicator is factor and store its levels\n  if(is.factor(data[,trt_name_e])){\n    trt_fact <- TRUE\n    trt_lev <- levels(data[,trt_name_e])} else {\n    trt_fact <- FALSE\n    trt_lev <- unique(data[,trt_name_e])}\n  if(length(trt_lev)!=2){stop(\"The function only allows comparison between two trt groups\")}  \n  #check that correct profile provided or set default\n  if(profile_QALY != \"default\"){\n    if(!is.vector(profile_QALY) | length(profile_QALY)!=nX_e){stop(\"provide valid profile for QALYreg\")}}\n  if(profile_TC != \"default\"){\n    if(!is.vector(profile_TC) | length(profile_TC)!=nX_c){stop(\"provide valid profile for TCreg\")}}\n  #prepare empty objects to contain bootstrapped estimates\n  data_ec_b_list <- list()\n  coeff_e <- c()\n  coeff_c <- c()\n  em_e_ctr <- em_e_int <- c()\n  em_c_ctr <- em_c_int <- c()\n  dataset.dt <- data.table(data) #convert data into data.table object\n  for(i in 1:B){\n    #sample with replacement\n    data_ec_b_list[[i]] <- dataset.dt[sample(.N, n, replace = T)]\n    #fit model\n    model_ec <- systemfit(list(QALYreg = QALYreg, TCreg = TCreg), \n                          method=method, data=data_ec_b_list[[i]])\n    #extract covariate values\n    X_e <- model.matrix(model_ec$eq[[1]])\n    X_c <- model.matrix(model_ec$eq[[2]])\n    #define QALYreg profile\n    if(profile_QALY == \"default\"){\n     profile_b_QALY <- apply(X_e, 2, mean, na.rm=T)\n    } else {profile_b_QALY <- profile_QALY}\n    profile_b_QALY_ctr <- profile_b_QALY_int <- profile_b_QALY\n    profile_b_QALY_ctr[trt_pos] <- 0 #set profile for comparator\n    profile_b_QALY_int[trt_pos] <- 1 #set profile for reference\n    #define TCreg profile\n    if(profile_TC == \"default\"){\n     profile_b_TC <- apply(X_c, 2, mean, na.rm=T)\n    } else {profile_b_TC <- profile_TC}\n    profile_b_TC_ctr <- profile_b_TC_int <- profile_b_TC\n    profile_b_TC_ctr[trt_pos] <- 0 #set profile for comparator\n    profile_b_TC_int[trt_pos] <- 1 #set profile for reference\n    #extract coefficient estimates from each model\n    coeff_e[i] <- summary(model_ec$eq[[1]])$coefficients[trt_pos,\"Estimate\"]\n    coeff_c[i] <- summary(model_ec$eq[[2]])$coefficients[trt_pos,\"Estimate\"]\n    #compute linear combination of parameters\n    em_e_ctr[i] <- t(profile_b_QALY_ctr) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"] \n    em_e_int[i] <- t(profile_b_QALY_int) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"] \n    em_c_ctr[i] <- t(profile_b_TC_ctr) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"] \n    em_c_int[i] <- t(profile_b_TC_int) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"] \n  }\n  #create list objects to store all results \n  res_e_b_list <-list(\"Delta_e\"=coeff_e,\"mu_e_ctr\"=em_e_ctr,\"mu_e_int\"=em_e_int)\n  res_c_b_list <-list(\"Delta_c\"=coeff_c,\"mu_c_ctr\"=em_c_ctr,\"mu_c_int\"=em_c_int)\n  input_list <- list(\"data\"=data, \"method\"=method, \"trt_pos\"=trt_pos, \"QALYreg\"=QALYreg,\n                     \"TCreg\"=TCreg,\"profile_QALY_ctr\"=profile_b_QALY_ctr,\n                     \"profile_QALY_int\"=profile_b_QALY_int,\"profile_TC_ctr\"=profile_b_TC_ctr,\n                     \"profile_TC_int\"=profile_b_TC_int)\n  #compute overall list and return it as output from the function\n  res_ec_b_list <- list(\"QALY_boot\"=res_e_b_list,\"TC_boot\"=res_c_b_list,\"inputs\"=input_list)\n  class(res_ec_b_list) <- \"bootCE\"\n  return(res_ec_b_list)\n}\n```\n:::\n\n  \nWe can now apply the newly created bootstrap function called `boot_res` to our generated dataset to obtain $B=200$ bootstrapped estimates for the parameters of interest, which are here stored in a list object called `boot_res`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#set rng for reproducibility\nset.seed(2345)\n#apply function to dataset\nboot_res <- boot_ec(data = dataset, QALYreg = QALY ~ trt + u,\n                    TCreg = TC ~ trt + c, method = \"OLS\", B=200)\n```\n:::\n\n\nWe can access the bootstrapped estimates for a given quantity, eg mean QALY and TC difference between treatment groups (New vs Old), by typing `boot_res$QALY_boot$Delta_e` and `boot_res$QALY_boot$Delta_c`, respectively. Once extracted, we can then inspect the distribution of $\\Delta_e$, for example, with an histogram.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=480}\n:::\n:::\n\n\nIn a similar way, the function also produces bootstrapped estimates for the mean QALY values in the comparator (Old) and reference (New) arm by typing `boot_res$QALY_boot$mu_e_ctr` and `boot_res$QALY_boot$mu_e_int`, which were derived at each bootstrap iteration using the same approach as in @sec-baseadj. Similar results with respect to the TC parameters can be obtained using the corresponding list names `Delta_c`,`mu_c_ctr` and `mu_c_int`.\n\n**Important Disclaimer**. The above bootstrap function is quite flexible in that it allows to derive bootstrapped estimates for different CE quantities. However, it should **NOT** be implemented blindly. For example, when computing these quantities, it assumes that the treatment indicator is always placed as the first independent variable in both QALY and TC regression model formulae. If this is not true, then the estimates produced will be **incorrect** since they will correspond to the coefficients of some other independent variable! The position of the treatment indicator may be changed through a specific (optional) argument called `trt_pos`, by default set to $2$ (since position $1$ corresponds to the intercept in both models). In addition, mean outcome values for each treatment group are obtained as linear combination of the QALY/TC regression parameters in combination with specific covariate profiles (ie one for the comparator and one for the reference intervention). These profiles are computed under the assumption that, except the treatment indicator, all other independent variables should be set at their mean value. This is generally ok when interest is in the average estimates but makes little sense when interest is instead in retrieving estimates for specific covariate profiles. A typical example of this would be when a categorical or factor covariate (eg gender) is included in the models and that estimates for mean QALY/TC values should be produced for a given value for that variable (eg only for males). In this latter case, estimates produced by the default settings of the function will be **incorrect** since they are evaluated at the mean values for all covariates (except the treatment indicator). The function is provided with (optional) arguments called `profile_QALY` and `profile_TC` which allow the user to specify a custom covariate profile for each regression but, unless the user is familiar with how to correctly specify these profiles, care should be used in the interpretation of the generated estimates. **All this to say, be careful about blindly using non-standard functions as, if you do not know how they work or how to interpret the output they produce, you may run the risk of generating completely nonsensical results!**.\n\nThe entire bootstrap distribution of mean QALY/TC difference and/or those for the mean outcomes in each treatment group obtained through the function `boot_ec` may be used to generate standard CEA graphs such as the CE plane [@black1990plane] and CE acceptability curve [@fenwick2001representing]. However, for the purpose of summarising uncertainty about the derived quantities, standard statistical measures such as confidence intervals should also be produced. Alternative ways to compute CIs based on bootstrapped estimates for a given quantity have been developed and compared, including in the context of CEA [@briggs1997pulling;@briggs1999constructing]. Although initial methods focussed on the computation of CIs for Incremental *Cost-Effectiveness Ratio* (ICER), its ratio nature makes it quite difficult to provide sensible interpretations to confidence bounds in many scenarios [@glick2014economic]. As a result, interest has been shifted towards the quantification of uncertainty via CIs for alternative quantities, such as the *Net Monetary Benefit* (NMB) which is a linear function of the mean differences in QALYs and TCs [@stinnett1998net]. \n\nThe following (folded) code part shows how to obtain confidence intervals for any of the quantity derived from the `boot_ec` function using two popular approaches in the CEA literature: the *empirical percentile* (perc) method and the *bias-corrected and accelerated* (BCa) method. Although other approaches exist to derive confidence intervals for bootstrapped estimates, and a clear \"best\" approach in all possible scenarios has not been identified, these two approaches are by far those most commonly implemented in trial-based CEAs. In general, the perc approach is very easy to implement but has well known limitations, especially in small samples, in that it produces intervals that tend to be too narrow. The BCa approach, conversely, is generally regarded as a more robust version for computing intervals as it computes the CI bounds while introducing a bias and skewness correction term to improve the coverage performance in many scenarios [@briggs1997pulling]. I have coded two functions, one called `jk_ec`, which is used to obtain jackknife estimates based on the original sample (needed to compute the BCa intervals), and another called `boot_ci`, which is the main function used to get the intervals. At the moment, both need to be loaded into the `R` workspace in order to compute the intervals and they also require as input the results generated by the previous function `boot_ec`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#jackknife sampling function (used to compute BCa interval inside main boot_ci function)\njk_ec <- function(data,QALYreg,TCreg,trt_pos,profile_QALY_ctr,profile_QALY_int,\n                   profile_TC_ctr,profile_TC_int,or_method){\n  n <- dim(data)[1]\n  #prepare objects to store results\n  jk_delta_c_i <- jk_delta_e_i <- c()\n  jk_mu0_c_i <- jk_mu0_e_i <- c()\n  jk_mu1_c_i <- jk_mu1_e_i <- c()\n  for(i in 1:n){\n    #apply jackknife re-sampling\n    data_i <- data[-i,]\n    #obtain estimates of interest\n    model_ec_i <- systemfit(list(QALYreg = QALYreg, TCreg = TCreg), \n                          method=or_method, data=data_i)\n    jk_delta_e_i[i] <- summary(model_ec_i$eq[[1]])$coefficients[trt_pos,\"Estimate\"]\n    jk_delta_c_i[i] <- summary(model_ec_i$eq[[2]])$coefficients[trt_pos,\"Estimate\"]\n    jk_mu0_e_i[i] <- as.numeric(t(profile_QALY_ctr) %*% summary(model_ec_i$eq[[1]])$coefficients[,\"Estimate\"])\n    jk_mu1_e_i[i] <- as.numeric(t(profile_QALY_int) %*% summary(model_ec_i$eq[[1]])$coefficients[,\"Estimate\"])\n    jk_mu0_c_i[i] <- as.numeric(t(profile_TC_ctr) %*% summary(model_ec_i$eq[[2]])$coefficients[,\"Estimate\"])\n    jk_mu1_c_i[i] <- as.numeric(t(profile_TC_int) %*% summary(model_ec_i$eq[[2]])$coefficients[,\"Estimate\"])\n  }\n  jk_est_i <- cbind.data.frame(jk_delta_e_i,jk_delta_c_i,jk_mu0_e_i,jk_mu1_e_i,jk_mu0_c_i,jk_mu1_c_i)\n  names(jk_est_i) <- c(\"jk_Delta_e\",\"jk_Delta_c\",\"jk_mu_e_ctr\",\"jk_mu_e_int\",\"jk_mu_c_ctr\",\"jk_mu_c_int\")\n  return(jk_est_i)\n}\n\nboot_ci <- function(x, method = \"perc\", confidence = 0.95){\n  #the following lines are needed to make sure proper inputs are given\n  if(!inherits(x, c(\"bootCE\",\"tsbootCE\"))) {stop(\"Only objects of class 'bootCE' or can 'tsbootCE' be used\")}\n  if(!method %in% c(\"perc\",\"BCa\")){stop(\"please provide valid method name\")}\n  if(!is.numeric(confidence)){stop(\"please provide valid confidence level\")}\n  if(confidence<=0 | confidence>=1){stop(\"please provide valid confidence level\")}\n  #extract information from inputs\n  B <- length(x$QALY_boot$Delta_e)\n  mu_e_ctr <- x$QALY_boot$mu_e_ctr\n  mu_e_int <- x$QALY_boot$mu_e_int\n  Delta_e <- x$QALY_boot$Delta_e\n  mu_c_ctr <- x$TC_boot$mu_c_ctr\n  mu_c_int <- x$TC_boot$mu_c_int\n  Delta_c <- x$TC_boot$Delta_c\n  alpha <- 1 - confidence\n  #compute CI bounds according to method chosen\n  if(method == \"perc\"){\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_e <- quantile(Delta_e, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_c <- quantile(Delta_c, probs = c(alpha/2,(1-alpha/2)))\n  }\n  if(method == \"BCa\"){\n    or_method <- x$inputs$method\n    data <- x$inputs$data\n    trt_pos <- x$inputs$trt_pos\n    QALYreg <- x$inputs$QALYreg\n    TCreg <- x$inputs$TCreg\n    profile_QALY_ctr <- x$inputs$profile_QALY_ctr\n    profile_QALY_int <- x$inputs$profile_QALY_int\n    profile_TC_ctr <- x$inputs$profile_TC_ctr\n    profile_TC_int <- x$inputs$profile_TC_int\n    #obtain avg BCa estimates based on original sample\n    model_ec <- systemfit(list(QALYreg = QALYreg, TCreg = TCreg), \n                          method=or_method, data=data)\n    avg_BCa_Delta_e <- summary(model_ec$eq[[1]])$coefficients[trt_pos,\"Estimate\"]\n    avg_BCa_Delta_c <- summary(model_ec$eq[[2]])$coefficients[trt_pos,\"Estimate\"]\n    avg_BCa_mu_e_ctr <- as.numeric(t(profile_QALY_ctr) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"])\n    avg_BCa_mu_e_int <- as.numeric(t(profile_QALY_int) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"])\n    avg_BCa_mu_c_ctr <- as.numeric(t(profile_TC_ctr) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"])\n    avg_BCa_mu_c_int <- as.numeric(t(profile_TC_int) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"])\n    #compute proportion of samples below avg estimates\n    plower_Delta_e <- length(Delta_e[Delta_e<avg_BCa_Delta_e])/B\n    plower_Delta_c <- length(Delta_c[Delta_c<avg_BCa_Delta_c])/B\n    plower_mu_e_ctr <- length(mu_e_ctr[mu_e_ctr<avg_BCa_mu_e_ctr])/B\n    plower_mu_e_int <- length(mu_e_int[mu_e_int<avg_BCa_mu_e_int])/B\n    plower_mu_c_ctr <- length(mu_c_ctr[mu_c_ctr<avg_BCa_mu_c_ctr])/B\n    plower_mu_c_int <- length(mu_c_int[mu_c_int<avg_BCa_mu_c_int])/B\n    #compute bias-correction term\n    z0_Delta_e <- qnorm(plower_Delta_e, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_Delta_c <- qnorm(plower_Delta_c, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_ctr <- qnorm(plower_mu_e_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_int <- qnorm(plower_mu_e_int, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_ctr <- qnorm(plower_mu_c_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_int <- qnorm(plower_mu_c_int, mean = 0, sd = 1, lower.tail = TRUE)\n    #apply jackknife sampling functions to get jeckknife estimates\n    jk_res <- jk_ec(data = data, QALYreg=QALYreg,TCreg=TCreg,trt_pos=trt_pos,\n                        profile_QALY_ctr=profile_QALY_ctr,profile_QALY_int=profile_QALY_int,\n                        profile_TC_ctr=profile_TC_ctr,profile_TC_int=profile_TC_int,or_method=or_method)\n    #compute avg of jk estimates\n    jk_res_avg <- apply(jk_res, 2, mean, na.rm=T) \n    #compute skewness correction term\n    a_Delta_e <- sum((jk_res_avg[\"jk_Delta_e\"] - jk_res[,\"jk_Delta_e\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_e\"] - jk_res[,\"jk_Delta_e\"])^2))^(3/2))\n    a_Delta_c <- sum((jk_res_avg[\"jk_Delta_c\"] - jk_res[,\"jk_Delta_c\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_c\"] - jk_res[,\"jk_Delta_c\"])^2))^(3/2))\n    a_mu_e_ctr <- sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res[,\"jk_mu_e_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res[,\"jk_mu_e_ctr\"])^2))^(3/2))\n    a_mu_e_int <- sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res[,\"jk_mu_e_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res[,\"jk_mu_e_int\"])^2))^(3/2))\n    a_mu_c_ctr <- sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res[,\"jk_mu_c_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res[,\"jk_mu_c_ctr\"])^2))^(3/2))\n    a_mu_c_int <- sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res[,\"jk_mu_c_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res[,\"jk_mu_c_int\"])^2))^(3/2))    \n    #compute adjusted probs for getting desired confidence level\n    z_alpha1 <- qnorm(alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    z_alpha2 <- qnorm(1-alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    ci_l_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha1)/(1-a_Delta_e*(z0_Delta_e+z_alpha1))))\n    ci_u_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha2)/(1-a_Delta_e*(z0_Delta_e+z_alpha2))))\n    ci_l_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha1)/(1-a_Delta_c*(z0_Delta_c+z_alpha1))))\n    ci_u_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha2)/(1-a_Delta_c*(z0_Delta_c+z_alpha2))))\n    ci_l_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha1)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha1))))\n    ci_u_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha2)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha2))))\n    ci_l_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha1)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha1))))\n    ci_u_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha2)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha2))))\n    ci_l_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha1)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha1))))\n    ci_u_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha2)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha2))))\n    ci_l_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha1)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha1))))\n    ci_u_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha2)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha2))))\n    #obtain quantiles on original scale\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(ci_l_mu_e_ctr,ci_u_mu_e_ctr))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(ci_l_mu_e_int,ci_u_mu_e_int))\n    ci_Delta_e <- quantile(Delta_e, probs = c(ci_l_Delta_e,ci_u_Delta_e))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(ci_l_mu_c_ctr,ci_u_mu_c_ctr))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(ci_l_mu_c_int,ci_u_mu_c_int))\n    ci_Delta_c <- quantile(Delta_c, probs = c(ci_l_Delta_c,ci_u_Delta_c))\n  }\n  #organise and return results\n  res_list <- list(\"Delta_e\"=ci_Delta_e,\"Delta_c\"=ci_Delta_c,\"mu_e_ctr\"=ci_mu_e_ctr,\"mu_e_int\"=ci_mu_e_int,\"mu_c_ctr\"=ci_mu_c_ctr,\"mu_c_int\"=ci_mu_c_int)\n  return(res_list)\n}\n```\n:::\n\n\nAs an example, we can now apply the newly created bootstrap function called `boot_ci` to our bootstrap results stored in the object `boot_res` to compute bootstrapped confidence intervals for all stored quantities using either the percentile or BCa approach (selected through the argument `method`). \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#apply function to bootstrap results\nboot_ci_bca <- boot_ci(x = boot_res, method = \"BCa\", confidence = 0.95)\nboot_ci_bca\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Delta_e\n 2.655003%  97.64746% \n0.05583106 0.09830066 \n\n$Delta_c\n1.886311% 96.72076% \n-15.25787  21.70418 \n\n$mu_e_ctr\n 3.48715% 98.23841% \n0.5612709 0.5915057 \n\n$mu_e_int\n2.478735% 97.47866% \n0.6361601 0.6636482 \n\n$mu_c_ctr\n2.907112% 97.85471% \n 283.1276  309.8906 \n\n$mu_c_int\n3.023147% 97.94824% \n 285.7260  313.8358 \n```\n\n\n:::\n:::\n\n\nThe above code computes and displays $95\\%$ BCa intervals for each store quantity from the previous bootstrap function, namely the mean QALY and TC difference ($\\Delta_e, \\Delta_c$), and the mean QALY/TC in each treatment group ($\\mu_e,\\mu_c$), here referred to as \"control\" (ctr) and \"intervetion\" (int).\n\n## Summary\n\nWhile **SUR** has been widely applied in trial-based CEA to account for the correlation between outcomes, mainly for its ease of implementation, its standalone implementation still has similar limitations to those of standard OLS approaches, eg reliance on Normality assumptions (see @sec-baseadj). For this reason it has been advocated the use of non-parametric **bootstrapping** as an alternative approach, which have been shown to produce adequate inferences while also retaining the correlation structure in the data through paired resampling methods. In addition, it is also possible to combine SUR and bootstrapping together within a single procedure, as also the code developed in this section showed.\n\nHowever, due to its simulation-based nature, the performance of non-parametric bootstrapping in CEA for handling correlation **has not been fully assessed** and some areas of uncertainty remains. These include:\n\n- Performance of bootstrapping with respect to **proper joint models**, which can formally take into account the correlation among multivariate outcomes. While joint models typically perform better for handling normally distributed outcomes, their relative performance with respect to non-parametric bootstrapping for non-normally distributed outcomes in small samples is still unclear.\n\n- Alternative approaches for confidence intervals computation. Although, in the use of **BCa** methods is recommended, the relative performance of alternative approaches in general scenarios has not been fully assessed. In some case, eg small samples and high degree of skewness, some approaches to CI computation can lead to quite different results and are potentially biased/inefficient [@briggs1997pulling;@briggs1999constructing]. \n\nDespite the performance of bootstrapping not being fully assessed, its use in CEA is needed in that CE results need to be expressed in the form of \"distributions\" for the parameters of interest, eg mean QALY/TC incrementals and, within a frequentist framework, resampling methods allow to obtain empirical estimates of these distributions which can then be used to generate standard CEA output (eg CE plane). We note that, when a Bayesian framework is used instead, implementation of non-normal joint models becomes considerably easier and have been shown to perform quite well in the context of CEA [@nixon2005methods;@lambert2008estimating;@conigliani2009bayesian;@gabrio2019full;@achana2021multivariate].\n\n# Skewness in CE outcomes {#sec-skew}\n\nA general concern in trial-based CEA is the presence of high degrees of skewness in the outcome variables, especially costs. This is due to the specific nature of outcome data:\n\n  - Costs are bounded below at $0$ and in many cases are characterised by empirical distributions that are highly **skewed to the right** (positively skewed), with many individuals associated with no or very low costs and a few individuals associated with very high costs.\n  \n  - QoL utilities (used to construct QALYs) are bounded above at $1$ and, especially in the context of non-life threatening interventions, are characterised by empirical distributions that are **skewed to the left** (negatively skewed), with many individuals associated with a perfect or high utility score (health state) and a few individuals associated with lower scores.\n\nThese features of CE data pose a threat to the validity of results derived from methods that rely on *normality assumptions* such as OLS and SUR models, or even parametric (eg Central Limit Theorem) and non-parametric (non-parametric tests or bootstrapping) methods that rely on *asymptotic results*, especially in the presence of high degree of skeweness and small samples [@thompson2000should;@o2003assessing;@nixon2010non]. The main problem is related to the definition of \"too high\" degree of skewness or \"too small\" sample size, on which a general consensus is lacking since performance of the methods can be highly affected by the specific combination of these two elements. In addition, options such as **data transformation** (eg take log) are not desirable since CE inference should be made on the arithmetic mean of the outcome variables on their original scale, which are usually not easily accessible when applying some transformation to the data [@barber2000analysis].\n\nTo deal with skewness in small samples in CEA, the recommended approach in the literature consists in some form of **Generalised Linear Model** (GLM) regression framework [@thompson2005sensitive;@barber2004multiple], which allow to choose alternative parametric distributional assumptions (eg Gamma) about the individual residuals to formally account for high levels of skewness. Although *(non-parametric) bootstrapping* is often advocated as a possible approach to handle skewness, its performance has been shown to be not optimal when facing high degrees of skewness in small samples [@nixon2010non]. However, it is important to remember that, within a frequentist framework for CEA, use of bootstrapping is often required in order to generate approximate empirical distributions of the quantities of interest and use them to obtain standard CE output.\n\n\n## Data generation\n\nIn this section I will once more generate some artificial data that will be used to show how the desired methods can be implemented in `R`. The code used to generate these data is provided in the following (folded) code part and, if not of interest, you may skip it and jump to the actual implementation code in the next section.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#set parameter values for generating data\nn_sam <- 20 #sample size\nmu_u <- c(0.86,0.9)\nsigma_u <- c(0.2,0.2)\ntau_u <- (mu_u*(1-mu_u)/sigma_u^2-1)\nshape1_u <- mu_u*tau_u\nshape2_u <- (1-mu_u)*tau_u\nmu_c <- c(100,125) \nsigma_c <- c(17,17)\nshape_c <- mu_c^2/sigma_c^2\nscale_c <- sigma_c^2/mu_c\n#generaten QALY and TC data assuming Beta and Gamma distributions by arm\nset.seed(2345)\nsam0_u <- rbeta(n_sam/2,shape1 = shape1_u[1], shape2 = shape2_u[1])\nsam1_u <- rbeta(n_sam/2,shape1 = shape1_u[2], shape2 = shape2_u[2])\nsam0_c <- rgamma(n_sam/2, shape = shape_c[1], scale = scale_c[1])\nsam1_c <- rgamma(n_sam/2, shape = shape_c[2], scale = scale_c[2])\n#combine arm data and generate baseline u and c values\nsam_u <- c(sam0_u,sam1_u)\nbase_u <- 0.75 + 0.5*sam_u + rnorm(n_sam, 0, 0.05) \nsam_c <- c(sam0_c,sam1_c)\nbase_c <- 50 + 0.01*sam_c + rnorm(n_sam, 0, 0.5) \n#create trt indicator\ntrt <- c(rep(\"old\",n_sam/2),rep(\"new\",n_sam/2))\n#combine variables into a dataframe\ndataset_sam_skew.df <- data.frame(sam_u,sam_c,base_u,base_c,trt)\nnames(dataset_sam_skew.df) <- c(\"QALY\",\"TC\",\"u\",\"c\",\"trt\")\n#randomly shuffle rows of the dataset\ndataset_sam_skew.df <- dataset_sam_skew.df[sample(1:nrow(dataset_sam_skew.df)), ]\n#define trt indicator as factor\ndataset_sam_skew.df$trt <- factor(dataset_sam_skew.df$trt, levels = c(\"old\",\"new\"))\n```\n:::\n\n\nWe can inspect the first few rows of the newly-generated data by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(dataset_sam_skew.df, n=8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        QALY        TC        u        c trt\n16 0.9966611 118.19067 1.315029 51.47245 new\n14 0.9970156 107.64264 1.253033 50.77574 new\n12 0.9457835 132.33424 1.196667 51.32995 new\n10 0.9496251  87.06468 1.294377 51.07543 old\n3  0.9652897 102.36302 1.273476 51.03935 old\n13 0.8985183 149.15977 1.259549 52.20984 new\n4  0.6181245  86.65457 1.023924 50.95642 old\n1  0.9931256 120.79056 1.229032 50.91415 old\n```\n\n\n:::\n:::\n\n\nAs an example, we can visually check the level of *skewness* for QALYs using boxplots, by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nboxplot(dataset_sam_skew.df$QALY, ylab=\"\",xlab=\"QALY\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Method application\n\n**Generalised Linear Models** (GLM) share a similar regression framework to those of OLS and SUR (@sec-baseadj and @sec-corr), where a given dependent variable, eg QALY or TC, is assumed to be a function of some independent variables through some regression coefficients $\\beta$ and some random error term $\\varepsilon_i$. A (simplified) representation of the GLM regression equations for the two CE outcomes may look something like:\n\n$$\n\\begin{aligned}\n\\text{E}[\\text{QALY}_i] &= g_e(\\beta_0 + \\beta_1\\times \\text{arm}_i + \\beta_2\\times u_{i0})^{-1} \\\\\n\\text{E}[\\text{TC}_i] &= g_c(\\alpha_0 + \\alpha_1\\times \\text{arm}_i + \\alpha_2\\times c_{i0})^{-1} \\\\\n\\end{aligned}\n$$ {#eq-5}\n\nwhere $\\text{E}[\\text{QALY}_i]$ and $\\text{E}[\\text{TC}_i]$ denote the expected value of the two variables, expressed as a function of some regression coefficients and independent variables. Compared to standard regression, there are two main differences:\n\n  - The expected value is not a direct linear function of the parameters and variables, which are included linearly in the equation after some function $g(\\cdot)$ is applied. This is typical of GLM in that in many cases the range of the outcomes is bounded (above or below some values) and therefore a so-called **link function** $g(\\cdot)$ is used to allow a linear regression specification of the model on an unrestricted scale, which would otherwise be difficult to achieve. Estimates are then back-transformed on the original scale by applying the inverse function $g(\\cdot)^{-1}$. Examples of typical link functions include: *log, logit, square root, and identity* (ie no scaling). Their choice is mainly drive by the specific type of outcome variables and distributional assumptions made about the error term.\n  \n  - The individual error term $\\varepsilon_i$ is not assumed to follow a normal distribution, but **alternative parametric distributional forms** are used, such as *Beta, Gamma, Bernoulli, or Negative Binomial*. The specific distributional choice is mainly driven again by the type of outcome variable which needs to be modelled.\n\nIn `R`, the following code may be used to fit GLMs as shown in @eq-5 assuming Beta-distributions for the QALY and Gamma distributions for the TC data, assuming a logit link function for the former and an identity link function for the latter. The code also shows how to summarise the generated output.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(mfx) #load package to fit beta regression\n#fit Beta regression to QALY assuming logit link\nglm_e <- betareg(QALY ~ trt + u, data = dataset_sam_skew.df,  link = \"logit\")\n#fit Gamma regression to TC assuming identity link\nglm_c <- glm(TC ~ trt + c, data = dataset_sam_skew.df, family = Gamma(link = \"identity\"))\n#summarise output\nsummary(glm_e)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nbetareg(formula = QALY ~ trt + u, data = dataset_sam_skew.df, link = \"logit\")\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.0387 -0.4169  0.0253  0.5370  1.8036 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value            Pr(>|z|)    \n(Intercept) -12.0426     1.6704  -7.209 0.00000000000056158 ***\ntrtnew        0.4970     0.4229   1.175                0.24    \nu            12.2652     1.5441   7.943 0.00000000000000197 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(>|z|)   \n(phi)   26.244      9.543    2.75  0.00596 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 54.13 on 4 Df\nPseudo R-squared: 0.4825\nNumber of iterations: 37 (BFGS) + 5 (Fisher scoring) \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nsummary(glm_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = TC ~ trt + c, family = Gamma(link = \"identity\"), \n    data = dataset_sam_skew.df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -463.632    284.001  -1.633  0.12096   \ntrtnew        25.654      6.729   3.812  0.00139 **\nc             11.047      5.586   1.978  0.06442 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.01431784)\n\n    Null deviance: 0.66693  on 19  degrees of freedom\nResidual deviance: 0.24186  on 17  degrees of freedom\nAIC: 165.32\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nNote that direct interpretation of the above model coefficients can be difficult due to the use of different link functions and non-normal distributional assumptions. However, we can use the convenient `emmeans` function from the package `emmean` to compute point and uncertainty estimates for mean QALY and TC variables on the original scale by treatment group (and the difference between groups using the `contrast` function) from the model output. The code part below shows how to do this.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#compute mean QALY and TC by arm on original scale\nglm_e.em <- emmeans(glm_e, ~ trt)\nglm_c.em <- emmeans(glm_c, ~ trt)\n\n#print results for QALYs (example)\nglm_e.em\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n trt emmean     SE  df asymp.LCL asymp.UCL\n old  0.948 0.0144 Inf     0.920     0.976\n new  0.968 0.0108 Inf     0.947     0.989\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n#specify and compute mean differences between groups (New - Old)\ndiff_NewvsOld <- list(\"New vs Old\" = c(-1, 1))\nglm_em_delta_e <- confint(contrast(glm_e.em, diff_NewvsOld))\nglm_em_delta_c <- confint(contrast(glm_c.em, diff_NewvsOld))\n\n#print results for QALYs difference (example)\nglm_em_delta_e\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast   estimate     SE  df asymp.LCL asymp.UCL\n New vs Old   0.0196 0.0168 Inf   -0.0133    0.0525\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nThe following (folded) code part shows how to construct an `R` function which allows to implement a non-parametric bootstrap procedure for QALY and TC variables and derive bootstrapped estimates for marginal and incremental mean estimates by fitting a GLM to the outcomes for each bootstrapped sample.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#note that compared to the function developed before for OLS/SUR models, this one\n#does not require to provide profiles for the computation of mean estimates but instead relies on the emmeans function to obtain these estimates without manual computation. Because of this the function is less flexible and can only evaluate mean estimates assuming a single profile for both QALY and TC models (the one used by emmeans to compute these quantities). However, the function needs the user to specify different distributions and link functions for the QALY and TC models. \nlibrary(data.table)\nlibrary(rlang)\nlibrary(mfx) \nlibrary(MASS)\nlibrary(bootstrap)\nlibrary(emmeans)\nboot_ec_glm <- function(data, B, QALYreg, TCreg, QALY_dist, TC_dist, \n                        QALY_link, TC_link, trt_pos = 2){\n  #the following lines are needed to make sure proper inputs are given\n  if(!is.data.frame(data)){stop(\"data needs to be a data frame object\")}\n  if(!is.numeric(B)){stop(\"please provide number of bootstrap iterations\")}\n  if(B<=0 | !B%%1==0){stop(\"please provide number of bootstrap iterations\")}\n  if(!is_formula(QALYreg)){stop(\"please provide formula for QALY model\")}\n  if(!is_formula(TCreg)){stop(\"please provide formula for TC model\")}\n  if(!QALY_dist %in% c(\"Beta\",\"Binomial\",\"NegBinomial\",\"Gamma\",\"InvGaussian\",\"Poisson\",\"Gaussian\")){stop(\"please provide valid distribution name\")}\n  if(!TC_dist %in% c(\"Beta\",\"Binomial\",\"NegBinomial\",\"Gamma\",\"InvGaussian\",\"Poisson\",\"Gaussian\")){stop(\"please provide valid distribution name\")}\n  if(!QALY_link %in% c(\"logit\",\"probit\",\"cauchit\", \"cloglog\", \"identity\", \"log\", \"sqrt\", \"1/mu^2\", \"inverse\")){stop(\"please provide valid link function name\")}\n  if(!TC_link %in% c(\"logit\",\"probit\",\"cauchit\", \"cloglog\", \"identity\", \"log\", \"sqrt\", \"1/mu^2\", \"inverse\")){stop(\"please provide valid link function name\")}\n  if(!is.numeric(trt_pos) | length(trt_pos)!=1 | trt_pos<=0){stop(\"please provide valid trt indicator position in regressions\")}\n  n <- dim(data)[1] #original sample size\n  #n covariates \n  nX_e <- dim(model.matrix(QALYreg, data))[2]\n  nX_c <- dim(model.matrix(TCreg, data))[2]\n  #extract name of trt indicator and outcomes from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  if(trt_name_e != trt_name_c){stop(\"please provide same trt variable name and position in QALY and TC formuale\")}\n  QALY_name <- all.vars(QALYreg)[1]\n  TC_name <- all.vars(TCreg)[1]\n  #check if trt indicator is factor and store its levels\n  if(is.factor(data[,trt_name_e])){\n    trt_fact <- TRUE\n    trt_lev <- levels(data[,trt_name_e])} else {\n    trt_fact <- FALSE\n    trt_lev <- unique(data[,trt_name_e])}\n  if(length(trt_lev)!=2){stop(\"The function only allows comparison between two trt groups\")}  \n  #prepare empty objects to contain bootstrapped estimates\n  data_ec_b_list <- list()\n  coeff_e <- c()\n  coeff_c <- c()\n  em_e_ctr <- em_e_int <- c()\n  em_c_ctr <- em_c_int <- c()\n  dataset.dt <- data.table(data) #convert data into data.table object\n  for(i in 1:B){\n    #sample with replacement\n    data_ec_b_list[[i]] <- dataset.dt[sample(.N, n, replace = T)]\n    #select and fit GLM based on distribution and link function (QALY)\n    if(QALY_dist==\"Beta\"){\n      glm_e <- betareg(QALYreg, data = data_ec_b_list[[i]], link = QALY_link)}\n    if(QALY_dist==\"NegBinomial\"){\n      glm_e <- glm.nb(QALYreg, data = data_ec_b_list[[i]], link = QALY_link)}\n    if(QALY_dist==\"Binomial\"){\n      glm_e <- glm(QALYreg, data = data_ec_b_list[[i]], family = binomial(link = QALY_link))}\n    if(QALY_dist==\"Gamma\"){\n      glm_e <- glm(QALYreg, data = data_ec_b_list[[i]], family = Gamma(link = QALY_link))}\n    if(QALY_dist==\"InvGaussian\"){\n      glm_e <- glm(QALYreg, data = data_ec_b_list[[i]], family = inverse.gaussian(link = QALY_link))}\n    if(QALY_dist==\"Poisson\"){\n      glm_e <- glm(QALYreg, data = data_ec_b_list[[i]], family = poisson(link = QALY_link))} \n    if(QALY_dist==\"Gaussian\"){\n      glm_e <- glm(QALYreg, data = data_ec_b_list[[i]], family = gaussian(link = QALY_link))}\n    #select and fit GLM based on distribution and link function (TC)\n    if(TC_dist==\"Beta\"){\n      glm_c <- betareg(TCreg, data = data_ec_b_list[[i]], link = TC_link)}\n    if(TC_dist==\"NegBinomial\"){\n      glm_c <- glm.nb(TCreg, data = data_ec_b_list[[i]], link = TC_link)}\n    if(TC_dist==\"Binomial\"){\n      glm_c <- glm(TCreg, data = data_ec_b_list[[i]], family = binomial(link = TC_link))}\n    if(TC_dist==\"Gamma\"){\n      glm_c <- glm(TCreg, data = data_ec_b_list[[i]], family = Gamma(link = TC_link))}\n    if(TC_dist==\"InvGaussian\"){\n      glm_c <- glm(TCreg, data = data_ec_b_list[[i]], family = inverse.gaussian(link = TC_link))}\n    if(TC_dist==\"Poisson\"){\n      glm_c <- glm(TCreg, data = data_ec_b_list[[i]], family = poisson(link = TC_link))} \n    if(TC_dist==\"Gaussian\"){\n      glm_c <- glm(TCreg, data = data_ec_b_list[[i]], family = gaussian(link = TC_link))}\n    #use emmeans function to get mean outcomes for each arm\n    glm_e.em <- emmeans(glm_e, trt_name_e, type = \"response\", data = data_ec_b_list[[i]])\n    glm_c.em <- emmeans(glm_c, trt_name_c, type = \"response\", data = data_ec_b_list[[i]])\n    em_e_ctr[i] <- summary(glm_e.em)[1,2]\n    em_e_int[i] <- summary(glm_e.em)[2,2]\n    em_c_ctr[i] <- summary(glm_c.em)[1,2]\n    em_c_int[i] <- summary(glm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    coeff_e[i] <- em_e_int[i] - em_e_ctr[i]\n    coeff_c[i] <- em_c_int[i] - em_c_ctr[i]\n  }\n  #create list objects to store all results \n  res_e_b_list <-list(\"Delta_e\"=coeff_e,\"mu_e_ctr\"=em_e_ctr,\"mu_e_int\"=em_e_int)\n  res_c_b_list <-list(\"Delta_c\"=coeff_c,\"mu_c_ctr\"=em_c_ctr,\"mu_c_int\"=em_c_int)\n  input_list <- list(\"data\"=data, \"trt_pos\"=trt_pos, \"QALYreg\"=QALYreg,\n                     \"TCreg\"=TCreg,\"QALY_link\"=QALY_link,\"QALY_dist\"=QALY_dist,\n                     \"TC_dist\"=TC_dist,\"TC_link\"=TC_link)\n  #compute overall list and return it as output from the function\n  res_ec_b_list <- list(\"QALY_boot\"=res_e_b_list,\"TC_boot\"=res_c_b_list,\"inputs\"=input_list)\n  class(res_ec_b_list) <- \"bootCE_glm\"\n  return(res_ec_b_list)\n}\n```\n:::\n\n  \nWe can now apply the newly created bootstrap function called `boot_ec_glm` to our generated dataset to obtain $B=200$ bootstrapped estimates for the parameters of interest, which are here stored in a list object called `boot_res_glm`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#set rng for reproducibility\nset.seed(2345)\n#apply function to dataset\nboot_res_glm <- boot_ec_glm(data = dataset, QALYreg = QALY ~ trt + u,\n                    TCreg = TC ~ trt + c, QALY_dist = \"Beta\", TC_dist = \"Gamma\",\n                    QALY_link = \"logit\", TC_link = \"inverse\", B=200)\n```\n:::\n\n\nNote that, depending on the selected distribution and link function, GLMs may encounter problems in estimation and sometimes may not be even fit to some specific data. For example, if QALYs contain some negative values (which is theoretically possible), then Beta distributions will never be able to fit since these are defined on the interval $(0,1)$. These problems are also more likely to occur when implementing GLMs within a bootstrap procedure, since the resampling done at each iteration may lead to some data structures that do not fit well the assumed distributions.\n\nSimilarly to what done in @sec-corr, the following (folded) code part shows how to modify the function for generating bootstrap confidence intervals to allow the use of GLM results (produced through the function `boot_ec_glm`) instead of those from OLS and SUR models.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#jackknife sampling function (used to compute BCa interval inside main boot_ci function)\njk_ec_glm <- function(data,QALYreg,TCreg,trt_pos,QALY_dist,TC_dist,\n                   QALY_link,TC_link){\n  n <- dim(data)[1]\n  #extract name of trt indicator from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  #prepare objects to store results\n  jk_delta_c_i <- jk_delta_e_i <- c()\n  jk_mu0_c_i <- jk_mu0_e_i <- c()\n  jk_mu1_c_i <- jk_mu1_e_i <- c()\n  for(i in 1:n){\n    #apply jackknife re-sampling\n    data_i <- data[-i,]\n    #obtain estimates of interest\n    if(QALY_dist==\"Beta\"){\n      glm_e <- betareg(QALYreg, data = data_i, link = QALY_link)}\n    if(QALY_dist==\"NegBinomial\"){\n      glm_e <- glm.nb(QALYreg, data = data_i, link = QALY_link)}\n    if(QALY_dist==\"Binomial\"){\n      glm_e <- glm(QALYreg, data = data_i, family = binomial(link = QALY_link))}\n    if(QALY_dist==\"Gamma\"){\n      glm_e <- glm(QALYreg, data = data_i, family = Gamma(link = QALY_link))}\n    if(QALY_dist==\"InvGaussian\"){\n      glm_e <- glm(QALYreg, data = data_i, family = inverse.gaussian(link = QALY_link))}\n    if(QALY_dist==\"Poisson\"){\n      glm_e <- glm(QALYreg, data = data_i, family = poisson(link = QALY_link))} \n    if(QALY_dist==\"Gaussian\"){\n      glm_e <- glm(QALYreg, data = data_i, family = gaussian(link = QALY_link))}\n    #select and fit GLM based on distribution and link function (TC)\n    if(TC_dist==\"Beta\"){\n      glm_c <- betareg(TCreg, data = data_i, link = TC_link)}\n    if(TC_dist==\"NegBinomial\"){\n      glm_c <- glm.nb(TCreg, data = data_i, link = TC_link)}\n    if(TC_dist==\"Binomial\"){\n      glm_c <- glm(TCreg, data = data_i, family = binomial(link = TC_link))}\n    if(TC_dist==\"Gamma\"){\n      glm_c <- glm(TCreg, data = data_i, family = Gamma(link = TC_link))}\n    if(TC_dist==\"InvGaussian\"){\n      glm_c <- glm(TCreg, data = data_i, family = inverse.gaussian(link = TC_link))}\n    if(TC_dist==\"Poisson\"){\n      glm_c <- glm(TCreg, data = data_i, family = poisson(link = TC_link))} \n    if(TC_dist==\"Gaussian\"){\n      glm_c <- glm(TCreg, data = data_i, family = gaussian(link = TC_link))}    \n    #use emmeans function to get mean outcomes for each arm\n    glm_e.em <- emmeans(glm_e, trt_name_e, type = \"response\", data = data_i)\n    glm_c.em <- emmeans(glm_c, trt_name_c, type = \"response\", data = data_i)\n    jk_mu0_e_i[i] <- summary(glm_e.em)[1,2]\n    jk_mu1_e_i[i] <- summary(glm_e.em)[2,2]\n    jk_mu0_c_i[i] <- summary(glm_c.em)[1,2]\n    jk_mu1_c_i[i] <- summary(glm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    jk_delta_e_i[i] <- jk_mu1_e_i[i] - jk_mu0_e_i[i]\n    jk_delta_c_i[i] <- jk_mu1_c_i[i] - jk_mu0_c_i[i]   \n  }\n  jk_est_i <- cbind.data.frame(jk_delta_e_i,jk_delta_c_i,jk_mu0_e_i,jk_mu1_e_i,jk_mu0_c_i,jk_mu1_c_i)\n  names(jk_est_i) <- c(\"jk_Delta_e\",\"jk_Delta_c\",\"jk_mu_e_ctr\",\"jk_mu_e_int\",\"jk_mu_c_ctr\",\"jk_mu_c_int\")\n  return(jk_est_i)\n}\n\nboot_ci_glm <- function(x, method = \"perc\", confidence = 0.95){\n  #the following lines are needed to make sure proper inputs are given\n  if(!inherits(x, c(\"bootCE_glm\",\"tsbootCE_glm\"))) {stop(\"Only objects of class 'bootCE_glm' or 'tsbootCE_glm' can be used\")}\n  if(!method %in% c(\"perc\",\"BCa\")){stop(\"please provide valid method name\")}\n  if(!is.numeric(confidence)){stop(\"please provide valid confidence level\")}\n  if(confidence<=0 | confidence>=1){stop(\"please provide valid confidence level\")}\n  #extract information from inputs\n  B <- length(x$QALY_boot$Delta_e)\n  mu_e_ctr <- x$QALY_boot$mu_e_ctr\n  mu_e_int <- x$QALY_boot$mu_e_int\n  Delta_e <- x$QALY_boot$Delta_e\n  mu_c_ctr <- x$TC_boot$mu_c_ctr\n  mu_c_int <- x$TC_boot$mu_c_int\n  Delta_c <- x$TC_boot$Delta_c\n  alpha <- 1 - confidence\n  #compute CI bounds according to method chosen\n  if(method == \"perc\"){\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_e <- quantile(Delta_e, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_c <- quantile(Delta_c, probs = c(alpha/2,(1-alpha/2)))\n  }\n  if(method == \"BCa\"){\n    QALY_dist <- x$inputs$QALY_dist\n    TC_dist <- x$inputs$TC_dist\n    QALY_link <- x$inputs$QALY_link\n    TC_link <- x$inputs$TC_link\n    data <- x$inputs$data\n    trt_pos <- x$inputs$trt_pos\n    QALYreg <- x$inputs$QALYreg\n    TCreg <- x$inputs$TCreg\n    #obtain avg BCa estimates based on original sample\n    if(QALY_dist==\"Beta\"){\n      glm_e <- betareg(QALYreg, data = data, link = QALY_link)}\n    if(QALY_dist==\"NegBinomial\"){\n      glm_e <- glm.nb(QALYreg, data = data, link = QALY_link)}\n    if(QALY_dist==\"Binomial\"){\n      glm_e <- glm(QALYreg, data = data, family = binomial(link = QALY_link))}\n    if(QALY_dist==\"Gamma\"){\n      glm_e <- glm(QALYreg, data = data, family = Gamma(link = QALY_link))}\n    if(QALY_dist==\"InvGaussian\"){\n      glm_e <- glm(QALYreg, data = data, family = inverse.gaussian(link = QALY_link))}\n    if(QALY_dist==\"Poisson\"){\n      glm_e <- glm(QALYreg, data = data, family = poisson(link = QALY_link))} \n    if(QALY_dist==\"Gaussian\"){\n      glm_e <- glm(QALYreg, data = data, family = gaussian(link = QALY_link))}\n    #select and fit GLM based on distribution and link function (TC)\n    if(TC_dist==\"Beta\"){\n      glm_c <- betareg(TCreg, data = data, link = TC_link)}\n    if(TC_dist==\"NegBinomial\"){\n      glm_c <- glm.nb(TCreg, data = data, link = TC_link)}\n    if(TC_dist==\"Binomial\"){\n      glm_c <- glm(TCreg, data = data, family = binomial(link = TC_link))}\n    if(TC_dist==\"Gamma\"){\n      glm_c <- glm(TCreg, data = data, family = Gamma(link = TC_link))}\n    if(TC_dist==\"InvGaussian\"){\n      glm_c <- glm(TCreg, data = data, family = inverse.gaussian(link = TC_link))}\n    if(TC_dist==\"Poisson\"){\n      glm_c <- glm(TCreg, data = data, family = poisson(link = TC_link))} \n    if(TC_dist==\"Gaussian\"){\n      glm_c <- glm(TCreg, data = data, family = gaussian(link = TC_link))}    \n    #extract name of trt indicator from provided formula\n    trt_name_e <- all.vars(QALYreg)[trt_pos]\n    trt_name_c <- all.vars(TCreg)[trt_pos]\n    #use emmeans function to get mean outcomes for each arm\n    glm_e.em <- emmeans(glm_e, trt_name_e, type = \"response\", data = data)\n    glm_c.em <- emmeans(glm_c, trt_name_c, type = \"response\", data = data)\n    avg_BCa_mu_e_ctr <- summary(glm_e.em)[1,2]\n    avg_BCa_mu_e_int <- summary(glm_e.em)[2,2]\n    avg_BCa_mu_c_ctr <- summary(glm_c.em)[1,2]\n    avg_BCa_mu_c_int <- summary(glm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    avg_BCa_Delta_e <- avg_BCa_mu_e_int - avg_BCa_mu_e_ctr\n    avg_BCa_Delta_c <- avg_BCa_mu_c_int - avg_BCa_mu_c_ctr    \n    #compute proportion of samples below avg estimates\n    plower_Delta_e <- length(Delta_e[Delta_e<avg_BCa_Delta_e])/B\n    plower_Delta_c <- length(Delta_c[Delta_c<avg_BCa_Delta_c])/B\n    plower_mu_e_ctr <- length(mu_e_ctr[mu_e_ctr<avg_BCa_mu_e_ctr])/B\n    plower_mu_e_int <- length(mu_e_int[mu_e_int<avg_BCa_mu_e_int])/B\n    plower_mu_c_ctr <- length(mu_c_ctr[mu_c_ctr<avg_BCa_mu_c_ctr])/B\n    plower_mu_c_int <- length(mu_c_int[mu_c_int<avg_BCa_mu_c_int])/B\n    #compute bias-correction term\n    z0_Delta_e <- qnorm(plower_Delta_e, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_Delta_c <- qnorm(plower_Delta_c, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_ctr <- qnorm(plower_mu_e_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_int <- qnorm(plower_mu_e_int, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_ctr <- qnorm(plower_mu_c_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_int <- qnorm(plower_mu_c_int, mean = 0, sd = 1, lower.tail = TRUE)\n    #apply jackknife sampling functions to get jeckknife estimates\n    jk_res_glm <- jk_ec_glm(data = data, QALYreg=QALYreg,TCreg=TCreg,trt_pos=trt_pos,\n                        QALY_dist=QALY_dist,TC_dist=TC_dist,\n                        QALY_link=QALY_link,TC_link=TC_link)\n    #compute avg of jk estimates\n    jk_res_avg <- apply(jk_res_glm, 2, mean, na.rm = T) \n    #compute skewness correction term\n    a_Delta_e <- sum((jk_res_avg[\"jk_Delta_e\"] - jk_res_glm[,\"jk_Delta_e\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_e\"] - jk_res_glm[,\"jk_Delta_e\"])^2))^(3/2))\n    a_Delta_c <- sum((jk_res_avg[\"jk_Delta_c\"] - jk_res_glm[,\"jk_Delta_c\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_c\"] - jk_res_glm[,\"jk_Delta_c\"])^2))^(3/2))\n    a_mu_e_ctr <- sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res_glm[,\"jk_mu_e_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res_glm[,\"jk_mu_e_ctr\"])^2))^(3/2))\n    a_mu_e_int <- sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res_glm[,\"jk_mu_e_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res_glm[,\"jk_mu_e_int\"])^2))^(3/2))\n    a_mu_c_ctr <- sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res_glm[,\"jk_mu_c_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res_glm[,\"jk_mu_c_ctr\"])^2))^(3/2))\n    a_mu_c_int <- sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res_glm[,\"jk_mu_c_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res_glm[,\"jk_mu_c_int\"])^2))^(3/2))    \n    #compute adjusted probs for getting desired confidence level\n    z_alpha1 <- qnorm(alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    z_alpha2 <- qnorm(1-alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    ci_l_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha1)/(1-a_Delta_e*(z0_Delta_e+z_alpha1))))\n    ci_u_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha2)/(1-a_Delta_e*(z0_Delta_e+z_alpha2))))\n    ci_l_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha1)/(1-a_Delta_c*(z0_Delta_c+z_alpha1))))\n    ci_u_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha2)/(1-a_Delta_c*(z0_Delta_c+z_alpha2))))\n    ci_l_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha1)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha1))))\n    ci_u_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha2)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha2))))\n    ci_l_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha1)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha1))))\n    ci_u_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha2)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha2))))\n    ci_l_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha1)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha1))))\n    ci_u_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha2)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha2))))\n    ci_l_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha1)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha1))))\n    ci_u_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha2)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha2))))\n    #obtain quantiles on original scale\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(ci_l_mu_e_ctr,ci_u_mu_e_ctr))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(ci_l_mu_e_int,ci_u_mu_e_int))\n    ci_Delta_e <- quantile(Delta_e, probs = c(ci_l_Delta_e,ci_u_Delta_e))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(ci_l_mu_c_ctr,ci_u_mu_c_ctr))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(ci_l_mu_c_int,ci_u_mu_c_int))\n    ci_Delta_c <- quantile(Delta_c, probs = c(ci_l_Delta_c,ci_u_Delta_c))\n  }\n  #organise and return results\n  res_list <- list(\"Delta_e\"=ci_Delta_e,\"Delta_c\"=ci_Delta_c,\"mu_e_ctr\"=ci_mu_e_ctr,\"mu_e_int\"=ci_mu_e_int,\"mu_c_ctr\"=ci_mu_c_ctr,\"mu_c_int\"=ci_mu_c_int)\n  return(res_list)\n}\n\n#apply function to bootstrap results generated using the function boot_ec_glm to compute bootstrapped confidence intervals for all stored quantities using either the percentile or BCa approach (selected through the argument `method`).\nboot_ci_glm_bca <- boot_ci_glm(x = boot_res_glm, method = \"BCa\", confidence = 0.95)\n```\n:::\n\n\n## Summary\n\nGLMs have been recommended as a valid approach to handle skewed data in CEA, although they are currently characterised by some drawbacks:\n\n  - Performance of the model may change substantially according to the specific parametric distributional assumptions made, with different distributions possibly leading to quite different results [@thompson2005sensitive]. In theory, choice of an adequate distributional assumption allows to obtain more reliable estimates for the quantities of interest compared to methods that ignore skewness but assessment of a \"good enough\" fit of the selected distribution to the available data is difficult to establish.\n  \n  - Choice of the link function may also have considerable impact on the fit of the model to the data- In general, link functions specified on the original scale are preferred to ease interpretation (eg identity link) but these often lead to instability of the estimates due to the non-symmetric nature of CE data, especially when multiple covariates are included in the models.\n  \n  - Within a frequentist framework, implementation of **joint** GLMs is often difficult and impractical. This forces analysts to rely on fitting separate models to QALY and TC variables, therefore ignoring their possible correlation. This can be indirectly taken into account by embedding the models within a paired bootstrapping procedure, although its performance with respect to joint modelling has not been fully assessed.\n\nMore recently, methodological developments have been made in the context of Bayesian analysis of CE data, which allows a more flexible modelling framework compared to the standard frequentist approach. This lead to the development of so-called **hurdle** or two-part regression models in combination with non-normal distributional assumptions for CE outcomes to handle extremely skewed data even in small samples [@baio2014bayesian;@gabrio2019full]. However, implementation of these methods within a frequentist framework can be very challenging, especially when these need to be combined with bootstrapping to generate empricial distributions for the estimates of interest required to perform the CE assessment.\n\n# Clustering of data {#sec-cluster}\n\nSome trials, for ethical or practical reasons, may randomise clusters rather than individual patients to each treatment arm. This is typically the case for *cluster* RCT, where the unit of randomisation becomes the treatment arm itself, ie individuals in a given cluster all receive the same treatment. When present, this design feature of the trial poses a threat to the validity of results derived from methods that ignore the clustering structure in the data, such as OLS, SUR or GLM approaches. \n\nIt has been shown that methods that ignore clustering (ie assume independence across all observations) will **underestimate** statistical uncertainty around parameter estimates, and may even result in biases and misleading conclusions [@gomes2012developing]. The problem is further exacerbated in the context of CEA where clustering needs to be addressed jointly with the other *typical complexities* affecting the data, such as the need to deal with baseline imbalances, correlation between the outcomes, and the level of skewness in both CE outcomes [@gomes2012methods]. \n\nDifferent approaches have been proposed in the literature to deal with clustering in CEA data and, among those assessed and compared, two main approaches have been suggested: **Multilevel Models** (MLM) and (non-parametric) **Two-Stage Bootstrapping** (TSB). The two approaches tackle the clustering problem from different perspectives but have been shown to perform good in general situations, although MLMs seem to have a generally better performance across a range of data structure scenarios [@gomes2012developing]. \n\n## Data generation\n\nLet's start by generating some artificial data that will be used to show how the desired methods can be implemented in `R`. The code used to generate these data is provided in the following (folded) code part and, if not of interest, you may skip it and jump to the actual implementation code in the next section.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#set parameter values for generating data\nn <- 100 #sample size by cluster\nJ <- 26 #n of clusters\nbeta0_c <- 100\nbeta1_c <- 20\ntau_c <- 10*2\ntrt_j <- c(rep(0,J/2),rep(1,J/2))\n#simulate cluster-level means\nset.seed(2345)\nphi_cj <- rnorm(J,beta0_c+beta1_c*trt_j, tau_c) #cost cluster means\nc_ij <- matrix(NA, nrow = n, ncol = J)\nsigma_c <- 15*2\n#simulate individual-level values\nfor(j in 1:J){\n  c_ij[,j] <- rnorm(n,phi_cj[j],sigma_c) #cost data\n}\nbeta0_e <- 0.5\nbeta1_e <- 0.1\ntau_e <- 0.1*2\ngamma <- tau_e/tau_c  #parameter capturing correlation between cluster QALY and cost means\n#simulate cluster-level means\nphi_ej <- rnorm(J,beta0_e+beta1_e+gamma*(phi_cj - (beta0_c+beta1_c*trt_j)), tau_e) #QALY cluster means\ne_ij <- matrix(NA, nrow = n, ncol = J)\nsigma_e <- 0.15*2\nrho_ec <- 0.5\ntheta <- rho_ec*(sigma_e/sigma_c) #parameter capturing correlation between QALY and cost\n#simulate individual-level values\nfor(j in 1:J){\n  e_ij[,j] <- rnorm(n,phi_ej[j]+theta*(c_ij[,j]-phi_cj[j]),sigma_e)+rnorm(n,0,0.15) #QALY data\n}\n#compute ICC by outcome (cluster variance/total variance)\nicc_c <- tau_c^2/(tau_c^2+sigma_c^2)\nicc_e <- tau_e^2/(tau_e^2+sigma_e^2)\n#generate dataset\ncluster <- rep(1:J, each=n)\nTC <- c(c_ij)\nQALY <- c(e_ij)\ntrt <- ifelse(cluster<=13,\"old\",\"new\")\nid <- rep(1:n*J) #individual id number\ndata.clus.df <- data.frame(id, QALY,TC,trt,cluster)\ndata.clus.df$trt <- factor(data.clus.df$trt, levels = c(\"old\",\"new\"))\n#randomly shuffle rows of the dataset\ndata.clus.df <- data.clus.df[sample(1:nrow(data.clus.df)), ]\n```\n:::\n\n\nWe can inspect the first few rows of the newly-generated data by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(data.clus.df, n=8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id       QALY       TC trt cluster\n2228  728 1.23285753 108.9805 new      23\n1351 1326 0.90037472 171.1320 new      14\n374  1924 0.47126061 154.6768 old       4\n1382 2132 0.67579513 135.2535 new      14\n247  1222 0.10535644 109.3631 old       3\n2324  624 0.04669856 104.4512 new      24\n2128  728 1.22815612 104.2594 new      22\n1656 1456 1.02208538 191.0768 new      17\n```\n\n\n:::\n:::\n\n\nand, as an example, we look at summary statistics for TC between two different clusters by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nsummary(data.clus.df$TC[data.clus.df$cluster==1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.10   54.12   74.48   76.33   95.71  139.58 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nsummary(data.clus.df$TC[data.clus.df$cluster==15])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  63.24  105.11  128.03  125.58  143.45  190.73 \n```\n\n\n:::\n:::\n\n\nWe can see that summary statistics can be quite different between clusters, therefore suggesting the violation of the independence assumption typical of standard statistical methods, such as OLS. We can also compute the *Intraclass Correlation Coefficients* (ICC) for the QALY (0.308) and TC (0.308) variables, which give an indication of the proportion of total variability in each outcome that is due to variability between clusters.   \n\n## Method application\n\nThe general idea underlying MLM is to extend the standard OLS/SUR regression framework by adding some cluster-specific **random terms** to capture differences between cluster outcome means from the overall means computed across clusters. The regression model for the two CE outcomes can be expressed as:\n\n$$\n\\begin{aligned}\n\\text{QALY}_{ij} &= \\beta_0 + \\beta_1\\times \\text{arm}_j + u_{je} + \\varepsilon_{ije} \\\\\n\\text{TC}_{ij} &= \\alpha_0 + \\alpha_1\\times \\text{arm}_j + u_{jc} + \\varepsilon_{ijc} \\\\\n\\end{aligned}\n$$ {#eq-6}\n\nwhere $u_{je}$ and $u_{jc}$ denote the random terms that are specific to cluster $j$. Similarly to what discussed for SUR models (@eq-4), these error terms can be assumed to follow a joint normal distribution    \n\n$$\n\\begin{aligned}\n\\begin{pmatrix}\nu_{je}\\\\\nu_{jc}\\\\\n\\end{pmatrix} &\\sim  \\text{Normal}\n\\begin{bmatrix}\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau^2_e & \\psi\\tau_e\\tau_c\\\\\n\\psi\\tau_c\\tau_e & \\tau^2_c\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nwhere the parameter $\\psi$ allows to link the two random terms to capture possible correlation between cluster QALY and TC means, although it is often assumed $0$ to ease implementation of the models.\n\nIn `R`, the following code may be used to fit separate MLM equations as shown in @eq-6 to the generated data and summarise the output:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(lme4) #load package to fit MLM\nlibrary(nlme) #load package to fit MLM\n#fit MLM to QALY regression\nmlm_e <- lme(QALY ~ trt, random = ~1|cluster, data = data.clus.df)\nsummary(mlm_e) #summarise output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed-effects model fit by REML\n  Data: data.clus.df \n       AIC      BIC    logLik\n  2244.443 2267.893 -1118.222\n\nRandom effects:\n Formula: ~1 | cluster\n        (Intercept)  Residual\nStdDev:   0.2747145 0.3642086\n\nFixed effects:  QALY ~ trt \n                Value  Std.Error   DF  t-value p-value\n(Intercept) 0.5955004 0.07685878 2574 7.747981  0.0000\ntrtnew      0.0256487 0.10869473   24 0.235970  0.8155\n Correlation: \n       (Intr)\ntrtnew -0.707\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.45114121 -0.70888295 -0.01460118  0.66186518  4.39502802 \n\nNumber of Observations: 2600\nNumber of Groups: 26 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n#fit MLM to QALY regression\nmlm_c <- lme(TC ~ trt, random = ~1|cluster, data = data.clus.df)\nsummary(mlm_c) #summarise output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed-effects model fit by REML\n  Data: data.clus.df \n       AIC      BIC    logLik\n  25064.68 25088.13 -12528.34\n\nRandom effects:\n Formula: ~1 | cluster\n        (Intercept) Residual\nStdDev:    19.13406 29.46526\n\nFixed effects:  TC ~ trt \n               Value Std.Error   DF   t-value p-value\n(Intercept) 99.40969  5.369388 2574 18.514155  0.0000\ntrtnew      20.25840  7.593462   24  2.667874  0.0135\n Correlation: \n       (Intr)\ntrtnew -0.707\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.77623879 -0.68339399 -0.01845129  0.67128677  3.20459691 \n\nNumber of Observations: 2600\nNumber of Groups: 26 \n```\n\n\n:::\n:::\n\n\nWe can use the function `emmean` from the `R` package `emmeans` to easily compute the mean QALYs in each treatment group. For example, we can type the following \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(emmeans) #load library to obtain marginal means\nmlm1em_e <- emmeans(mlm_e, ~ trt) #compute mean outcome by level of trt\nmlm1em_e\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n trt emmean     SE df lower.CL upper.CL\n old  0.596 0.0769 25    0.437    0.754\n new  0.621 0.0769 24    0.463    0.780\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nto directly obtain estimates of the mean QALYs, their standard errors and $95\\%$ confidence intervals by treatment group. We can then use the function `contrast` to derive estimates of any linear combination of the quantities above. For example, we may obtain estimates for the mean QALY difference (New - Old) by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#take difference as - Old + New = New - Old\nQALY_new_vs_old <- list(\"New vs Old\" = c(-1, 1))\n#compute linear combination\nmlm1em_delta_e <- contrast(mlm1em_e, QALY_new_vs_old)\n#obtain results in terms of confidence intervals\nconfint(mlm1em_delta_e, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast   estimate    SE df lower.CL upper.CL\n New vs Old   0.0256 0.109 24   -0.199     0.25\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nAlthough I have not shown here an application of MLM for TC estimates, these can be easily obtained in a similar way to what shown for QALYs. \n\nIn the (folded) code part below, I additionally provide `R` functions to implement MLMs within a boostrap procedure to generate bootstrapped estimates of the parameters of interest and their associated confidence intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(data.table) #package to handle datasets more efficiently\nlibrary(bootstrap) #package to use bootstrap procedure \nlibrary(rlang)\nlibrary(lme4)\nlibrary(nlme)\nlibrary(emmeans)\nboot_ec_mlm <- function(data, B, QALYreg, TCreg, QALYrandom, TCrandom, trt_pos = 2){\n  #the following lines are needed to make sure proper inputs are given\n  if(!is.data.frame(data)){stop(\"data needs to be a data frame object\")}\n  if(!is.numeric(B)){stop(\"please provide number of bootstrap iterations\")}\n  if(B<=0 | !B%%1==0){stop(\"please provide number of bootstrap iterations\")}\n  if(!is_formula(QALYreg)){stop(\"please provide formula for QALY model\")}\n  if(!is_formula(TCreg)){stop(\"please provide formula for TC model\")}\n  if(!is_formula(QALYrandom)){stop(\"please provide formula for QALY random effects\")}\n  if(!is_formula(TCrandom)){stop(\"please provide formula for TC random effects\")}\n  if(!is.numeric(trt_pos) | length(trt_pos)!=1 | trt_pos<=0){stop(\"please provide valid trt indicator position in regressions\")}\n  n <- dim(data)[1] #original sample size\n  #n covariates \n  nX_e <- dim(model.matrix(QALYreg, data))[2]\n  nX_c <- dim(model.matrix(TCreg, data))[2]\n  #extract name of trt indicator and outcomes from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  if(trt_name_e != trt_name_c){stop(\"please provide same trt variable name and position in QALY and TC formuale\")}\n  QALY_name <- all.vars(QALYreg)[1]\n  TC_name <- all.vars(TCreg)[1]\n  #check if trt indicator is factor and store its levels\n  if(is.factor(data[,trt_name_e])){\n    trt_fact <- TRUE\n    trt_lev <- levels(data[,trt_name_e])} else {\n    trt_fact <- FALSE\n    trt_lev <- unique(data[,trt_name_e])}\n  if(length(trt_lev)!=2){stop(\"The function only allows comparison between two trt groups\")}  \n  #prepare empty objects to contain bootstrapped estimates\n  data_ec_b_list <- list()\n  coeff_e <- c()\n  coeff_c <- c()\n  em_e_ctr <- em_e_int <- c()\n  em_c_ctr <- em_c_int <- c()\n  dataset.dt <- data.table(data) #convert data into data.table object\n  for(i in 1:B){\n    #sample with replacement\n    data_ec_b_list[[i]] <- dataset.dt[sample(.N, n, replace = T)]\n    #fit models for QALY and TC\n    mlm_e <- lme(QALYreg, random = QALYrandom, data = data_ec_b_list[[i]])\n    mlm_c <- lme(TCreg, random = TCrandom, data = data_ec_b_list[[i]])\n    #use emmeans function to get mean outcomes for each arm\n    mlm_e.em <- emmeans(mlm_e, trt_name_e, type = \"response\", data = data_ec_b_list[[i]])\n    mlm_c.em <- emmeans(mlm_c, trt_name_c, type = \"response\", data = data_ec_b_list[[i]])\n    em_e_ctr[i] <- summary(mlm_e.em)[1,2]\n    em_e_int[i] <- summary(mlm_e.em)[2,2]\n    em_c_ctr[i] <- summary(mlm_c.em)[1,2]\n    em_c_int[i] <- summary(mlm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    coeff_e[i] <- em_e_int[i] - em_e_ctr[i]\n    coeff_c[i] <- em_c_int[i] - em_c_ctr[i]\n  }\n  #create list objects to store all results \n  res_e_b_list <-list(\"Delta_e\"=coeff_e,\"mu_e_ctr\"=em_e_ctr,\"mu_e_int\"=em_e_int)\n  res_c_b_list <-list(\"Delta_c\"=coeff_c,\"mu_c_ctr\"=em_c_ctr,\"mu_c_int\"=em_c_int)\n  input_list <- list(\"data\"=data, \"trt_pos\"=trt_pos, \"QALYreg\"=QALYreg,\n                     \"TCreg\"=TCreg,\"QALYrandom\"=QALYrandom,\"TCrandom\"=TCrandom)\n  #compute overall list and return it as output from the function\n  res_ec_b_list <- list(\"QALY_boot\"=res_e_b_list,\"TC_boot\"=res_c_b_list,\"inputs\"=input_list)\n  class(res_ec_b_list) <- \"bootCE_mlm\"\n  return(res_ec_b_list)\n}\n\n#apply function to obtain bootstrap estimates from MLM\n#set rng for reproducibility\nset.seed(2345)\n#the function fit separate MLM to QALY and TC data using normal random effects term specified for the variables indicated in the arguments QALYrandom and TCrandom, which must be specified as formulae following the notation indicated for random effects terms in the function lme from the package nlme\nboot_res_mlm <- boot_ec_mlm(data = data.clus.df, QALYreg = QALY ~ trt, \n                            QALYrandom = ~1|cluster, TCreg = TC ~ trt, \n                            TCrandom = ~1|cluster, B=200)\n\n\n#jackknife sampling function (used to compute BCa interval inside main boot_ci function)\njk_ec_mlm <- function(data,QALYreg,TCreg,trt_pos,QALYrandom,TCrandom){\n  n <- dim(data)[1]\n  #extract name of trt indicator from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  #prepare objects to store results\n  jk_delta_c_i <- jk_delta_e_i <- c()\n  jk_mu0_c_i <- jk_mu0_e_i <- c()\n  jk_mu1_c_i <- jk_mu1_e_i <- c()\n  for(i in 1:n){\n    #apply jackknife re-sampling\n    data_i <- data[-i,]\n    #fit models\n    mlm_e <- lme(QALYreg, random = QALYrandom, data = data_i)\n    mlm_c <- lme(TCreg, random = TCrandom, data = data_i)\n    #use emmeans function to get mean outcomes for each arm\n    mlm_e.em <- emmeans(mlm_e, trt_name_e, type = \"response\", data = data_i)\n    mlm_c.em <- emmeans(mlm_c, trt_name_c, type = \"response\", data = data_i)\n    jk_mu0_e_i[i] <- summary(mlm_e.em)[1,2]\n    jk_mu1_e_i[i] <- summary(mlm_e.em)[2,2]\n    jk_mu0_c_i[i] <- summary(mlm_c.em)[1,2]\n    jk_mu1_c_i[i] <- summary(mlm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    jk_delta_e_i[i] <- jk_mu1_e_i[i] - jk_mu0_e_i[i]\n    jk_delta_c_i[i] <- jk_mu1_c_i[i] - jk_mu0_c_i[i]    \n  }\n  jk_est_i <- cbind.data.frame(jk_delta_e_i,jk_delta_c_i,jk_mu0_e_i,jk_mu1_e_i,jk_mu0_c_i,jk_mu1_c_i)\n  names(jk_est_i) <- c(\"jk_Delta_e\",\"jk_Delta_c\",\"jk_mu_e_ctr\",\"jk_mu_e_int\",\"jk_mu_c_ctr\",\"jk_mu_c_int\")\n  return(jk_est_i)\n}\n\nboot_ci_mlm <- function(x, method = \"perc\", confidence = 0.95){\n  #the following lines are needed to make sure proper inputs are given\n  if(!inherits(x, c(\"bootCE_mlm\"))) {stop(\"Only objects of class 'bootCE_mlm' can be used\")}\n  if(!method %in% c(\"perc\",\"BCa\")){stop(\"please provide valid method name\")}\n  if(!is.numeric(confidence)){stop(\"please provide valid confidence level\")}\n  if(confidence<=0 | confidence>=1){stop(\"please provide valid confidence level\")}\n  #extract information from inputs\n  B <- length(x$QALY_boot$Delta_e)\n  mu_e_ctr <- x$QALY_boot$mu_e_ctr\n  mu_e_int <- x$QALY_boot$mu_e_int\n  Delta_e <- x$QALY_boot$Delta_e\n  mu_c_ctr <- x$TC_boot$mu_c_ctr\n  mu_c_int <- x$TC_boot$mu_c_int\n  Delta_c <- x$TC_boot$Delta_c\n  alpha <- 1 - confidence\n  #compute CI bounds according to method chosen\n  if(method == \"perc\"){\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_e <- quantile(Delta_e, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(alpha/2,(1-alpha/2)))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(alpha/2,(1-alpha/2)))\n    ci_Delta_c <- quantile(Delta_c, probs = c(alpha/2,(1-alpha/2)))\n  }\n  if(method == \"BCa\"){\n    data <- x$inputs$data\n    trt_pos <- x$inputs$trt_pos\n    QALYreg <- x$inputs$QALYreg\n    TCreg <- x$inputs$TCreg\n    QALYrandom <- x$inputs$QALYrandom\n    TCrandom <- x$inputs$TCrandom\n    #obtain avg BCa estimates based on original sample\n    mlm_e <- lme(QALYreg, random = QALYrandom, data = data)\n    mlm_c <- lme(TCreg, random = TCrandom, data = data)\n    #extract name of trt indicator from provided formula\n    trt_name_e <- all.vars(QALYreg)[trt_pos]\n    trt_name_c <- all.vars(TCreg)[trt_pos]\n    #use emmeans function to get mean outcomes for each arm\n    mlm_e.em <- emmeans(mlm_e, trt_name_e, type = \"response\", data = data)\n    mlm_c.em <- emmeans(mlm_c, trt_name_c, type = \"response\", data = data)\n    avg_BCa_mu_e_ctr <- summary(mlm_e.em)[1,2]\n    avg_BCa_mu_e_int <- summary(mlm_e.em)[2,2]\n    avg_BCa_mu_c_ctr <- summary(mlm_c.em)[1,2]\n    avg_BCa_mu_c_int <- summary(mlm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    avg_BCa_Delta_e <- avg_BCa_mu_e_int - avg_BCa_mu_e_ctr\n    avg_BCa_Delta_c <- avg_BCa_mu_c_int - avg_BCa_mu_c_ctr     \n    #compute proportion of samples below avg estimates\n    plower_Delta_e <- length(Delta_e[Delta_e<avg_BCa_Delta_e])/B\n    plower_Delta_c <- length(Delta_c[Delta_c<avg_BCa_Delta_c])/B\n    plower_mu_e_ctr <- length(mu_e_ctr[mu_e_ctr<avg_BCa_mu_e_ctr])/B\n    plower_mu_e_int <- length(mu_e_int[mu_e_int<avg_BCa_mu_e_int])/B\n    plower_mu_c_ctr <- length(mu_c_ctr[mu_c_ctr<avg_BCa_mu_c_ctr])/B\n    plower_mu_c_int <- length(mu_c_int[mu_c_int<avg_BCa_mu_c_int])/B\n    #compute bias-correction term\n    z0_Delta_e <- qnorm(plower_Delta_e, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_Delta_c <- qnorm(plower_Delta_c, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_ctr <- qnorm(plower_mu_e_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_e_int <- qnorm(plower_mu_e_int, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_ctr <- qnorm(plower_mu_c_ctr, mean = 0, sd = 1, lower.tail = TRUE)\n    z0_mu_c_int <- qnorm(plower_mu_c_int, mean = 0, sd = 1, lower.tail = TRUE)\n    #apply jackknife sampling functions to get jeckknife estimates\n    jk_res_mlm <- jk_ec_mlm(data = data, QALYreg=QALYreg,TCreg=TCreg,trt_pos=trt_pos,\n                        QALYrandom=QALYrandom,TCrandom=TCrandom)\n    #compute avg of jk estimates\n    jk_res_avg <- apply(jk_res_mlm, 2, mean, na.rm=T) \n    #compute skewness correction term\n    a_Delta_e <- sum((jk_res_avg[\"jk_Delta_e\"] - jk_res_mlm[,\"jk_Delta_e\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_e\"] - jk_res_mlm[,\"jk_Delta_e\"])^2))^(3/2))\n    a_Delta_c <- sum((jk_res_avg[\"jk_Delta_c\"] - jk_res_mlm[,\"jk_Delta_c\"])^3) / (6*(sum((jk_res_avg[\"jk_Delta_c\"] - jk_res_mlm[,\"jk_Delta_c\"])^2))^(3/2))\n    a_mu_e_ctr <- sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res_mlm[,\"jk_mu_e_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_ctr\"] - jk_res_mlm[,\"jk_mu_e_ctr\"])^2))^(3/2))\n    a_mu_e_int <- sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res_mlm[,\"jk_mu_e_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_e_int\"] - jk_res_mlm[,\"jk_mu_e_int\"])^2))^(3/2))\n    a_mu_c_ctr <- sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res_mlm[,\"jk_mu_c_ctr\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_ctr\"] - jk_res_mlm[,\"jk_mu_c_ctr\"])^2))^(3/2))\n    a_mu_c_int <- sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res_mlm[,\"jk_mu_c_int\"])^3) / (6*(sum((jk_res_avg[\"jk_mu_c_int\"] - jk_res_mlm[,\"jk_mu_c_int\"])^2))^(3/2))    \n    #compute adjusted probs for getting desired confidence level\n    z_alpha1 <- qnorm(alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    z_alpha2 <- qnorm(1-alpha/2, mean = 0, sd = 1, lower.tail = TRUE)\n    ci_l_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha1)/(1-a_Delta_e*(z0_Delta_e+z_alpha1))))\n    ci_u_Delta_e <- pnorm(z0_Delta_e + ((z0_Delta_e+z_alpha2)/(1-a_Delta_e*(z0_Delta_e+z_alpha2))))\n    ci_l_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha1)/(1-a_Delta_c*(z0_Delta_c+z_alpha1))))\n    ci_u_Delta_c <- pnorm(z0_Delta_c + ((z0_Delta_c+z_alpha2)/(1-a_Delta_c*(z0_Delta_c+z_alpha2))))\n    ci_l_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha1)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha1))))\n    ci_u_mu_e_ctr <- pnorm(z0_mu_e_ctr + ((z0_mu_e_ctr+z_alpha2)/(1-a_mu_e_ctr*(z0_mu_e_ctr+z_alpha2))))\n    ci_l_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha1)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha1))))\n    ci_u_mu_e_int <- pnorm(z0_mu_e_int + ((z0_mu_e_int+z_alpha2)/(1-a_mu_e_int*(z0_mu_e_int+z_alpha2))))\n    ci_l_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha1)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha1))))\n    ci_u_mu_c_ctr <- pnorm(z0_mu_c_ctr + ((z0_mu_c_ctr+z_alpha2)/(1-a_mu_c_ctr*(z0_mu_c_ctr+z_alpha2))))\n    ci_l_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha1)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha1))))\n    ci_u_mu_c_int <- pnorm(z0_mu_c_int + ((z0_mu_c_int+z_alpha2)/(1-a_mu_c_int*(z0_mu_c_int+z_alpha2))))\n    #obtain quantiles on original scale\n    ci_mu_e_ctr <- quantile(mu_e_ctr, probs = c(ci_l_mu_e_ctr,ci_u_mu_e_ctr))\n    ci_mu_e_int <- quantile(mu_e_int, probs = c(ci_l_mu_e_int,ci_u_mu_e_int))\n    ci_Delta_e <- quantile(Delta_e, probs = c(ci_l_Delta_e,ci_u_Delta_e))\n    ci_mu_c_ctr <- quantile(mu_c_ctr, probs = c(ci_l_mu_c_ctr,ci_u_mu_c_ctr))\n    ci_mu_c_int <- quantile(mu_c_int, probs = c(ci_l_mu_c_int,ci_u_mu_c_int))\n    ci_Delta_c <- quantile(Delta_c, probs = c(ci_l_Delta_c,ci_u_Delta_c))\n  }\n  #organise and return results\n  res_list <- list(\"Delta_e\"=ci_Delta_e,\"Delta_c\"=ci_Delta_c,\"mu_e_ctr\"=ci_mu_e_ctr,\"mu_e_int\"=ci_mu_e_int,\"mu_c_ctr\"=ci_mu_c_ctr,\"mu_c_int\"=ci_mu_c_int)\n  return(res_list)\n}\n\n#apply function to MLM results\nboot_ci_mlm_bca <- boot_ci_mlm(x = boot_res_mlm, method = \"BCa\", confidence = 0.95)\n```\n:::\n\n\n\nAn alternative approach to handle clustering in CEA is to combine standard statistical methods that ignore clustering (eg OLS, SUR or GLM) with a modified version of non-parametric bootstrapping, which takes the name of **Two-Stage Bootstrapping** (TSB). The key change consists in the way the sampling with replacement is performed from the original sample so to take into account the two-level data structure represented by clusters and individuals within clusters at each bootstrap iteration [@gomes2012developing]. In short, the TSB procedure can be summarised in the following steps:\n\n  1. Generate a \"bootstrap\" sample by *sampling with replacement* clusters from the original dataset and then sample QALY and TC individual values together within each resampled cluster\n  2. Fit the desired model to the newly obtained bootstrap sample to derive bootstrap estimates for the quantities of interest (eg mean outcome differences), for example using OLS or SUR models (ie $\\hat{\\beta}^b_1$ and $\\hat{\\alpha}^b_1$)\n  3. Repeat step 1-2 for a large number of bootstrap iterations $B$ (eg $5000$) and store the results to generate a set of $B$ bootstrap estimates for $b=1,\\ldots,B$\n  4. Use the stored sets of bootstrap estimates (ie $\\hat{\\beta}^b_1$ and $\\hat{\\alpha}^b_1$) to *empirically* approximate the sampling distribution of the parameters of interest.\n  5. Use this distribution of estimates to quantify the level of uncertainty around the quantities of interest, eg in terms of confidence intervals.\n\nA limitation of standard TSB is that, due to the two-stage sampling procedure, it is very likely that the level of uncertainty, and so the variance of the estimated quantities, is **overestimated**. Thus, some form of *shrinkage correction* should be implemented within the TSB procedure to correct for variance overestimation [@gomes2012developing]. When using this correction, the TSB procedure is modified by shrinking cluster means and estimating individual residuals from the cluster means before any sampling is performed. Next, resampling is first applied to the shrunk cluster means, and then to the individual residuals. Finally, the resampled cluster means and individual residuals are combined to obtain a complete bootstrap sample, for which steps 2-5 are then followed.\n\nThe following (folded) code part shows how to construct an `R` function which allows to implement a non-parametric TSB procedure with shrinkage correction for QALY and TC variables and derive bootstrapped estimates for marginal and incremental mean estimates by fitting a OLS or SUR model to each bootstrapped sample.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(data.table) #package to handle datasets more efficiently\nlibrary(bootstrap) #package to use bootstrap procedure \nlibrary(rlang)\ntsboot_ec <- function(data, B, QALYreg, TCreg, method = \"OLS\", cluster, unbalclus=\"donner\",\n                    profile_QALY=\"default\", profile_TC=\"default\", trt_pos = 2){\n  #the following lines are needed to make sure proper inputs are given\n  if(!is.data.frame(data)){stop(\"data needs to be a data frame object\")}\n  if(!is.numeric(B)){stop(\"please provide number of bootstrap iterations\")}\n  if(B<=0 | !B%%1==0){stop(\"please provide number of bootstrap iterations\")}\n  if(!is_formula(QALYreg)){stop(\"please provide formula for QALY model\")}\n  if(!is_formula(TCreg)){stop(\"please provide formula for TC model\")}\n  if(!method %in% c(\"OLS\",\"SUR\")){stop(\"please provide valid method name\")}\n  if(!is.numeric(trt_pos) | length(trt_pos)!=1 | trt_pos<=0){stop(\"please provide valid trt indicator position in regressions\")}\n  if(!is.character(cluster)){stop(\"please provide valid cluster variable name\")}\n  if(!cluster %in% names(data)){stop(\"please provide valid cluster variable name\")}\n  if(!unbalclus %in% c(\"donner\",\"median\",\"mean\")){stop(\"please provide valid method to compute avg cluster size when standardising\")}\n  #convert cluster as factor and then numeric \n  data[,cluster] <- as.numeric(as.factor(data[,cluster]))\n  #check that cluster variable is integer \n  if(!all(data[,cluster] - floor(data[,cluster]) == 0)){stop(\"cluster values should be integers\")}\n  n_size <- dim(data)[1] #original sample size\n  #n covariates \n  nX_e <- dim(model.matrix(QALYreg, data))[2]\n  nX_c <- dim(model.matrix(TCreg, data))[2]\n  #check that correct profile provided or set default\n  if(profile_QALY != \"default\"){\n    if(!is.vector(profile_QALY) | length(profile_QALY)!=nX_e){stop(\"provide valid profile for QALYreg\")}}\n  if(profile_TC != \"default\"){\n    if(!is.vector(profile_TC) | length(profile_TC)!=nX_c){stop(\"provide valid profile for TCreg\")}}\n  #extract name of trt indicator and outcomes from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  if(trt_name_e != trt_name_c){stop(\"please provide same trt variable name and position in QALY and TC formuale\")}\n  QALY_name <- all.vars(QALYreg)[1]\n  TC_name <- all.vars(TCreg)[1]\n  #check if trt indicator is factor and store its levels\n  if(is.factor(data[,trt_name_e])){\n    trt_fact <- TRUE\n    trt_lev <- levels(data[,trt_name_e])} else {\n    trt_fact <- FALSE\n    trt_lev <- unique(data[,trt_name_e])}\n  if(length(trt_lev)!=2){stop(\"The function only allows comparison between two trt groups\")}\n  #prepare empty objects to contain bootstrapped estimates\n  coeff_e <- c()\n  coeff_c <- c()\n  em_e_ctr <- em_e_int <- c()\n  em_c_ctr <- em_c_int <- c()\nfor(i in 1:B){\n    count <- 0 #set count for while loop across strata\n    n.strata <- length(unique(data[,trt_name_e]))\n    shrunk.data <- c()\n  while (count<n.strata){\n    count <- count+1\n    data1 <- data.frame(data[data[,trt_name_e]==unique(data[,trt_name_e])[count],])\n    clus.size <- table(data1[,cluster])\n    cost.x <- tapply(data1[,TC_name],data1[,cluster],mean) # calc cluster means\n    qaly.x <- tapply(data1[,QALY_name],data1[,cluster],mean) # calc cluster means\n    # STANDARDIZE Z: calc b for standardiwing z\n    a <- length(unique(data1[,cluster]))\n    if (var(clus.size)==0){\n     b <- unique(clus.size)\n   } else {\n    if (unbalclus==\"donner\"){\n     ifelse(warning,print(\"'average' clus size = Donner\"),NA)\n     n <- sum(clus.size)\n     b <- (n-(sum(clus.size^2)/n))/(a-1)\n    } else if (unbalclus==\"median\"){\n     ifelse(warning,print(\"'average' clus size = median\"),NA)\n     b <- median(clus.size)\n    } else if (unbalclus==\"mean\"){\n     ifelse(warning,print(\"'average' clus size = mean\"),NA)\n     b <- mean(clus.size)\n    } else {}\n   } # End of 'else'\n    # standardise z using cluster means (dfm = deviation from cluster mean)\n    cost.dfm <- data1[,TC_name]-rep(cost.x,times=clus.size)\n    qaly.dfm <- data1[,QALY_name]-rep(qaly.x,times=clus.size)\n    cost.z <- (cost.dfm)/sqrt(1-1/b)\n    qaly.z <- (qaly.dfm)/sqrt(1-1/b)\n    # SHRINKAGE: calc c for shrinking x\n    cost.ssw <- sum(cost.dfm^2)\n    qaly.ssw <- sum(qaly.dfm^2)\n    cost.ssb <- sum((cost.x-mean(cost.x))^2)\n    qaly.ssb <- sum((qaly.x-mean(qaly.x))^2)\n    cost.rhs <- a/(a-1) - cost.ssw/(b*(b-1)*cost.ssb)\n    qaly.rhs <- a/(a-1) - qaly.ssw/(b*(b-1)*qaly.ssb)\n    ifelse(cost.rhs<0, cost.c<-1, cost.c<-1-sqrt(cost.rhs))\n    ifelse(qaly.rhs<0, qaly.c<-1, qaly.c<-1-sqrt(qaly.rhs))\n    ## re-calc x\n    cost.x <- cost.c*mean(data1[,TC_name]) + (1-cost.c)*cost.x\n    qaly.x <- qaly.c*mean(data1[,QALY_name]) + (1-qaly.c)*qaly.x\n    # TWO-STAGE SAMPLING & RE-CONSTRUCT OBS WITH SHRUNKEN MEANS AND STANDARDISED RESIDUALS\n    # gen random clus (order) id with replacement\n    sampled.x.cid <- sample(1:length(unique(data1[,cluster])),replace=T)\n    sampled.z.iid <- sample(1:length(cost.z),sum(clus.size[sampled.x.cid]),replace=T) # chosen ind ids     for varying stratum sizes\n    sampled.cost <- rep(cost.x[sampled.x.cid],times=clus.size[sampled.x.cid])+cost.z[sampled.z.iid]\n    sampled.qaly <- rep(qaly.x[sampled.x.cid],times=clus.size[sampled.x.cid])+qaly.z[sampled.z.iid]\n    # bind data from multiple strata together\n    shrunk.data <- as.data.frame(rbind(shrunk.data,cbind(sampled.cost,sampled.qaly,\n    rep(unique(data1[,cluster])[sampled.x.cid],times=clus.size[sampled.x.cid]),\n    rep(unique(data[,trt_name_e])[count],times=sum(clus.size[sampled.x.cid])))))\n  } # end of while\n  #rename variables\n  colnames(shrunk.data) <- c(TC_name,QALY_name,cluster,trt_name_e)\n  #copy trt levels if factor\n  if(is_true(trt_fact)){ \n    shrunk.data[,trt_name_e] <- factor(shrunk.data[,trt_name_e], levels=sort(unique(shrunk.data[,trt_name_e])), labels = trt_lev)}\n    #create a dt object\n    dataset_tsb.dt <- data.table(shrunk.data)\n    #fit model\n    model_ec <- systemfit(list(QALYreg = QALYreg, TCreg = TCreg), \n                          method=method, data=dataset_tsb.dt)\n    #extract covariate values\n    X_e <- model.matrix(model_ec$eq[[1]])\n    X_c <- model.matrix(model_ec$eq[[2]])\n    #define QALYreg profile\n    if(profile_QALY == \"default\"){\n     profile_b_QALY <- apply(X_e, 2, mean, na.rm=T)\n    } else {profile_b_QALY <- profile_QALY}\n    profile_b_QALY_ctr <- profile_b_QALY_int <- profile_b_QALY\n    profile_b_QALY_ctr[trt_pos] <- 0 #set profile for comparator\n    profile_b_QALY_int[trt_pos] <- 1 #set profile for reference\n    #define TCreg profile\n    if(profile_TC == \"default\"){\n     profile_b_TC <- apply(X_c, 2, mean, na.rm=T)\n    } else {profile_b_TC <- profile_TC}\n    profile_b_TC_ctr <- profile_b_TC_int <- profile_b_TC\n    profile_b_TC_ctr[trt_pos] <- 0 #set profile for comparator\n    profile_b_TC_int[trt_pos] <- 1 #set profile for reference\n    #extract coefficient estimates from each model\n    coeff_e[i] <- summary(model_ec$eq[[1]])$coefficients[trt_pos,\"Estimate\"]\n    coeff_c[i] <- summary(model_ec$eq[[2]])$coefficients[trt_pos,\"Estimate\"]\n    #compute linear combination of parameters\n    em_e_ctr[i] <- t(profile_b_QALY_ctr) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"] \n    em_e_int[i] <- t(profile_b_QALY_int) %*% summary(model_ec$eq[[1]])$coefficients[,\"Estimate\"] \n    em_c_ctr[i] <- t(profile_b_TC_ctr) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"] \n    em_c_int[i] <- t(profile_b_TC_int) %*% summary(model_ec$eq[[2]])$coefficients[,\"Estimate\"] \n  }\n  #create list objects to store all results \n  res_e_tsb_list <-list(\"Delta_e\"=coeff_e,\"mu_e_ctr\"=em_e_ctr,\"mu_e_int\"=em_e_int)\n  res_c_tsb_list <-list(\"Delta_c\"=coeff_c,\"mu_c_ctr\"=em_c_ctr,\"mu_c_int\"=em_c_int)\n  input_list <- list(\"data\"=data, \"method\"=method, \"trt_pos\"=trt_pos, \"QALYreg\"=QALYreg,\n                     \"TCreg\"=TCreg,\"profile_QALY_ctr\"=profile_b_QALY_ctr,\n                     \"profile_QALY_int\"=profile_b_QALY_int,\"profile_TC_ctr\"=profile_b_TC_ctr,\n                     \"profile_TC_int\"=profile_b_TC_int, \"cluster\"=cluster, \"unbalclus\"=unbalclus)\n  #compute overall list and return it as output from the function\n  res_ec_tsb_list <- list(\"QALY_boot\"=res_e_tsb_list,\"TC_boot\"=res_c_tsb_list,\"inputs\"=input_list)\n  class(res_ec_tsb_list) <- \"tsbootCE\"\n  return(res_ec_tsb_list)\n}\n```\n:::\n\n  \nWe can now apply the newly created TSB function called `tsboot_res` to our generated dataset to obtain $B=200$ bootstrapped estimates for the parameters of interest, which are here stored in a list object called `tsboot_res`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#set rng for reproducibility\nset.seed(2345)\n#apply function to dataset\ntsboot_res <- tsboot_ec(data = data.clus.df, QALYreg = QALY ~ trt, cluster = \"cluster\", TCreg = TC ~ trt, method = \"OLS\", B=200)\n```\n:::\n\n\nNext, we can use the functions `jk_ec` and `boot_ci` shown in @sec-corr to compute bootstrapped confidence intervals. As an example, we can now apply the newly created bootstrap function called `boot_ci` to our bootstrap results stored in the object `tsboot_res` to compute bootstrapped confidence intervals for all stored quantities using either the BCa approach (selected through the argument `method`). \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#apply function to TSB results\ntsboot_ci_bca <- boot_ci(x = tsboot_res, method = \"BCa\", confidence = 0.95)\ntsboot_ci_bca\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Delta_e\n0.5842263%  91.91954% \n-0.1961036  0.1850060 \n\n$Delta_c\n0.8417625%  93.69878% \n  2.949009  32.641264 \n\n$mu_e_ctr\n5.173335% 98.90882% \n0.4810873 0.7642355 \n\n$mu_e_int\n1.631128% 96.27396% \n0.4918447 0.7677400 \n\n$mu_c_ctr\n8.490612% 99.46002% \n 92.51988 112.04817 \n\n$mu_c_int\n1.857534% 96.67969% \n 109.7356  130.1179 \n```\n\n\n:::\n:::\n\n\nFor completeness, in the following (folded) code part, I also provide the same TSB functions but tailored to the specification of independent GLMs rather than OLS/SUR models only possible with `tsboot_res`. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#note that compared to the function developed before for OLS/SUR models, this one\n#does not require to provide profiles for the computation of mean estimates but instead relies on the emmeans function to obtain these estimates without manual computation. Because of this the function is less flexible and can only evaluate mean estimates assuming a single profile for both QALY and TC models (the one used by emmeans to compute these quantities). However, the function needs the user to specify different distributions and link functions for the QALY and TC models. \nlibrary(data.table)\nlibrary(rlang)\nlibrary(mfx) \nlibrary(MASS)\nlibrary(bootstrap)\nlibrary(emmeans)\ntsboot_ec_glm <- function(data, B, QALYreg, TCreg, QALY_dist, TC_dist, \n                        QALY_link, TC_link, cluster, unbalclus=\"donner\", trt_pos = 2){\n  #the following lines are needed to make sure proper inputs are given\n  if(!is.data.frame(data)){stop(\"data needs to be a data frame object\")}\n  if(!is.numeric(B)){stop(\"please provide number of bootstrap iterations\")}\n  if(B<=0 | !B%%1==0){stop(\"please provide number of bootstrap iterations\")}\n  if(!is_formula(QALYreg)){stop(\"please provide formula for QALY model\")}\n  if(!is_formula(TCreg)){stop(\"please provide formula for TC model\")}\n  if(!QALY_dist %in% c(\"Beta\",\"Binomial\",\"NegBinomial\",\"Gamma\",\"InvGaussian\",\"Poisson\",\"Gaussian\")){stop(\"please provide valid distribution name\")}\n  if(!TC_dist %in% c(\"Beta\",\"Binomial\",\"NegBinomial\",\"Gamma\",\"InvGaussian\",\"Poisson\",\"Gaussian\")){stop(\"please provide valid distribution name\")}\n  if(!QALY_link %in% c(\"logit\",\"probit\",\"cauchit\", \"cloglog\", \"identity\", \"log\", \"sqrt\", \"1/mu^2\", \"inverse\")){stop(\"please provide valid link function name\")}\n  if(!TC_link %in% c(\"logit\",\"probit\",\"cauchit\", \"cloglog\", \"identity\", \"log\", \"sqrt\", \"1/mu^2\", \"inverse\")){stop(\"please provide valid link function name\")}\n  if(!is.numeric(trt_pos) | length(trt_pos)!=1 | trt_pos<=0){stop(\"please provide valid trt indicator position in regressions\")}\n  if(!is.character(cluster)){stop(\"please provide valid cluster variable name\")}\n  if(!cluster %in% names(data)){stop(\"please provide valid cluster variable name\")}\n  if(!unbalclus %in% c(\"donner\",\"median\",\"mean\")){stop(\"please provide valid method to compute avg cluster size when standardising\")}\n  #convert cluster as factor and then numeric \n  data[,cluster] <- as.numeric(as.factor(data[,cluster]))\n  #check that cluster variable is integer \n  if(!all(data[,cluster] - floor(data[,cluster]) == 0)){stop(\"cluster values should be integers\")}\n  n_size <- dim(data)[1] #original sample size\n  #n covariates \n  nX_e <- dim(model.matrix(QALYreg, data))[2]\n  nX_c <- dim(model.matrix(TCreg, data))[2]\n  #extract name of trt indicator and outcomes from provided formula\n  trt_name_e <- all.vars(QALYreg)[trt_pos]\n  trt_name_c <- all.vars(TCreg)[trt_pos]\n  if(trt_name_e != trt_name_c){stop(\"please provide same trt variable name and position in QALY and TC formuale\")}\n  QALY_name <- all.vars(QALYreg)[1]\n  TC_name <- all.vars(TCreg)[1]\n  #check if trt indicator is factor and store its levels\n  if(is.factor(data[,trt_name_e])){\n    trt_fact <- TRUE\n    trt_lev <- levels(data[,trt_name_e])} else {\n    trt_fact <- FALSE\n    trt_lev <- unique(data[,trt_name_e])}\n  if(length(trt_lev)!=2){stop(\"The function only allows comparison between two trt groups\")}\n  #prepare empty objects to contain bootstrapped estimates\n  coeff_e <- c()\n  coeff_c <- c()\n  em_e_ctr <- em_e_int <- c()\n  em_c_ctr <- em_c_int <- c()\nfor(i in 1:B){\n    count <- 0 #set count for while loop across strata\n    n.strata <- length(unique(data[,trt_name_e]))\n    shrunk.data <- c()\n  while (count<n.strata){\n    count <- count+1\n    data1 <- data.frame(data[data[,trt_name_e]==unique(data[,trt_name_e])[count],])\n    clus.size <- table(data1[,cluster])\n    cost.x <- tapply(data1[,TC_name],data1[,cluster],mean) # calc cluster means\n    qaly.x <- tapply(data1[,QALY_name],data1[,cluster],mean) # calc cluster means\n    # STANDARDIZE Z: calc b for standardiwing z\n    a <- length(unique(data1[,cluster]))\n    if (var(clus.size)==0){\n     b <- unique(clus.size)\n   } else {\n    if (unbalclus==\"donner\"){\n     ifelse(warning,print(\"'average' clus size = Donner\"),NA)\n     n <- sum(clus.size)\n     b <- (n-(sum(clus.size^2)/n))/(a-1)\n    } else if (unbalclus==\"median\"){\n     ifelse(warning,print(\"'average' clus size = median\"),NA)\n     b <- median(clus.size)\n    } else if (unbalclus==\"mean\"){\n     ifelse(warning,print(\"'average' clus size = mean\"),NA)\n     b <- mean(clus.size)\n    } else {}\n   } # End of 'else'\n    # standardise z using cluster means (dfm = deviation from cluster mean)\n    cost.dfm <- data1[,TC_name]-rep(cost.x,times=clus.size)\n    qaly.dfm <- data1[,QALY_name]-rep(qaly.x,times=clus.size)\n    cost.z <- (cost.dfm)/sqrt(1-1/b)\n    qaly.z <- (qaly.dfm)/sqrt(1-1/b)\n    # SHRINKAGE: calc c for shrinking x\n    cost.ssw <- sum(cost.dfm^2)\n    qaly.ssw <- sum(qaly.dfm^2)\n    cost.ssb <- sum((cost.x-mean(cost.x))^2)\n    qaly.ssb <- sum((qaly.x-mean(qaly.x))^2)\n    cost.rhs <- a/(a-1) - cost.ssw/(b*(b-1)*cost.ssb)\n    qaly.rhs <- a/(a-1) - qaly.ssw/(b*(b-1)*qaly.ssb)\n    ifelse(cost.rhs<0, cost.c<-1, cost.c<-1-sqrt(cost.rhs))\n    ifelse(qaly.rhs<0, qaly.c<-1, qaly.c<-1-sqrt(qaly.rhs))\n    ## re-calc x\n    cost.x <- cost.c*mean(data1[,TC_name]) + (1-cost.c)*cost.x\n    qaly.x <- qaly.c*mean(data1[,QALY_name]) + (1-qaly.c)*qaly.x\n    # TWO-STAGE SAMPLING & RE-CONSTRUCT OBS WITH SHRUNKEN MEANS AND STANDARDISED RESIDUALS\n    # gen random clus (order) id with replacement\n    sampled.x.cid <- sample(1:length(unique(data1[,cluster])),replace=T)\n    sampled.z.iid <- sample(1:length(cost.z),sum(clus.size[sampled.x.cid]),replace=T) # chosen ind ids     for varying stratum sizes\n    sampled.cost <- rep(cost.x[sampled.x.cid],times=clus.size[sampled.x.cid])+cost.z[sampled.z.iid]\n    sampled.qaly <- rep(qaly.x[sampled.x.cid],times=clus.size[sampled.x.cid])+qaly.z[sampled.z.iid]\n    # bind data from multiple strata together\n    shrunk.data <- as.data.frame(rbind(shrunk.data,cbind(sampled.cost,sampled.qaly,\n    rep(unique(data1[,cluster])[sampled.x.cid],times=clus.size[sampled.x.cid]),\n    rep(unique(data[,trt_name_e])[count],times=sum(clus.size[sampled.x.cid])))))\n  } # end of while\n  #rename variables\n  colnames(shrunk.data) <- c(TC_name,QALY_name,cluster,trt_name_e)\n  #copy trt levels if factor\n  if(is_true(trt_fact)){ \n    shrunk.data[,trt_name_e] <- factor(shrunk.data[,trt_name_e], levels=sort(unique(shrunk.data[,trt_name_e])), labels = trt_lev)}\n    #create a dt object\n    dataset_tsb.dt <- data.table(shrunk.data)\n    #select and fit GLM based on distribution and link function (QALY)\n    if(QALY_dist==\"Beta\"){\n      glm_e <- betareg(QALYreg, data = dataset_tsb.dt, link = QALY_link)}\n    if(QALY_dist==\"NegBinomial\"){\n      glm_e <- glm.nb(QALYreg, data = dataset_tsb.dt, link = QALY_link)}\n    if(QALY_dist==\"Binomial\"){\n      glm_e <- glm(QALYreg, data = dataset_tsb.dt, family = binomial(link = QALY_link))}\n    if(QALY_dist==\"Gamma\"){\n      glm_e <- glm(QALYreg, data = dataset_tsb.dt, family = Gamma(link = QALY_link))}\n    if(QALY_dist==\"InvGaussian\"){\n      glm_e <- glm(QALYreg, data = dataset_tsb.dt, family = inverse.gaussian(link = QALY_link))}\n    if(QALY_dist==\"Poisson\"){\n      glm_e <- glm(QALYreg, data = dataset_tsb.dt, family = poisson(link = QALY_link))} \n    if(QALY_dist==\"Gaussian\"){\n      glm_e <- glm(QALYreg, data = dataset_tsb.dt, family = gaussian(link = QALY_link))}\n    #select and fit GLM based on distribution and link function (TC)\n    if(TC_dist==\"Beta\"){\n      glm_c <- betareg(TCreg, data = dataset_tsb.dt, link = TC_link)}\n    if(TC_dist==\"NegBinomial\"){\n      glm_c <- glm.nb(TCreg, data = dataset_tsb.dt, link = TC_link)}\n    if(TC_dist==\"Binomial\"){\n      glm_c <- glm(TCreg, data = dataset_tsb.dt, family = binomial(link = TC_link))}\n    if(TC_dist==\"Gamma\"){\n      glm_c <- glm(TCreg, data = dataset_tsb.dt, family = Gamma(link = TC_link))}\n    if(TC_dist==\"InvGaussian\"){\n      glm_c <- glm(TCreg, data = dataset_tsb.dt, family = inverse.gaussian(link = TC_link))}\n    if(TC_dist==\"Poisson\"){\n      glm_c <- glm(TCreg, data = dataset_tsb.dt, family = poisson(link = TC_link))} \n    if(TC_dist==\"Gaussian\"){\n      glm_c <- glm(TCreg, data = dataset_tsb.dt, family = gaussian(link = TC_link))}\n    #use emmeans function to get mean outcomes for each arm\n    glm_e.em <- emmeans(glm_e, trt_name_e, type = \"response\", data = dataset_tsb.dt)\n    glm_c.em <- emmeans(glm_c, trt_name_c, type = \"response\", data = dataset_tsb.dt)\n    em_e_ctr[i] <- summary(glm_e.em)[1,2]\n    em_e_int[i] <- summary(glm_e.em)[2,2]\n    em_c_ctr[i] <- summary(glm_c.em)[1,2]\n    em_c_int[i] <- summary(glm_c.em)[2,2]\n    #specify and compute mean differences between groups\n    coeff_e[i] <- em_e_int[i] - em_e_ctr[i]\n    coeff_c[i] <- em_c_int[i] - em_c_ctr[i]\n  }\n  #create list objects to store all results \n  res_e_tsb_list <-list(\"Delta_e\"=coeff_e,\"mu_e_ctr\"=em_e_ctr,\"mu_e_int\"=em_e_int)\n  res_c_tsb_list <-list(\"Delta_c\"=coeff_c,\"mu_c_ctr\"=em_c_ctr,\"mu_c_int\"=em_c_int)\n  input_list <- list(\"data\"=data, \"trt_pos\"=trt_pos, \"QALYreg\"=QALYreg,\n                     \"TCreg\"=TCreg,\"QALY_link\"=QALY_link,\"QALY_dist\"=QALY_dist,\n                     \"TC_dist\"=TC_dist,\"TC_link\"=TC_link,\"cluster\"=cluster, \"unbalclus\"=unbalclus)\n  #compute overall list and return it as output from the function\n  res_ec_tsb_list <- list(\"QALY_boot\"=res_e_tsb_list,\"TC_boot\"=res_c_tsb_list,\"inputs\"=input_list)\n  class(res_ec_tsb_list) <- \"tsbootCE_glm\"\n  return(res_ec_tsb_list)\n}\n\n#example on how to use function\n#set rng for reproducibility\nset.seed(2345)\n#apply function to dataset (here assume normal distributions since data simulated assuming normality)\ntsboot_res_glm <- tsboot_ec_glm(data = data.clus.df, QALYreg = QALY ~ trt, cluster = \"cluster\",\n                    TCreg = TC ~ trt, QALY_dist = \"Gaussian\", TC_dist = \"Gaussian\",\n                    QALY_link = \"identity\", TC_link = \"identity\", B=200)\n\n#apply CI function to TSB results for GLMs\ntsboot_ci_bca_glm <- boot_ci_glm(x = tsboot_res_glm, method = \"BCa\", confidence = 0.95)\n```\n:::\n\n\n## Summary\n\n**MLMs** have been advocated in the literature as an ideal approach to deal with clustering in general scenarios, with poor performance only whan having a small number of clusters and sample size [@gomes2012developing]. However, especially within a frequentist framework, the implementation of MLMs faces serious issues when deviating from standard methods assuming normality and, even under said assumptions, specification of a joint multivariate random effects model is often not trivial using standard software packages.\n\n**TSB** has been proposed as an alternative approach to handle clustering by correctly replicating the clustered structure at each bootstrap iteration in combination with standard statistical regression methods (eg OLS). Implementation of TSB without *shrinkage correction* is not recommended since the method tends to overestimate uncertainty due to the inherent double sampling procedure. However, even when the correction is applied and similarly to MLMs, it has been shown that TSB can perform poorly when dealing with a small number of clusters and sample sizes [@gomes2012developing]. In addition, as any non-parametric resampling method, different approaches can be used to obtain uncertainty measures for the bootstrapped quantities (eg confidence intervals), and a clear performance assessment of each alternative approach across all possible scenarios is lacking. \n\nWe note that, within a Bayesian framework, the possibility to rely on powerful and generic simulation methods such as **Mark Chain Monte Carlo** (MCMC) algorithms [@brooks2011handbook] allows to implement MLMs more easily and assuming different parametric distributions for CE outcomes, thus allowing to deal with high level of skewness even in small samples [@grieve2010bayesian;@ng2016multilevel].\n\n# Missing Data {#sec-missing}\n\nThe topic of missing data in trial-based CEA is huge and what I will attempt to cover here is merely a small piece of the overall picture from the literature, which I advice the interested reader to consult [@manca2005handling;@marshall2009can;@faria2014guide;@gomes2013multiple;@diaz2014handling;@gabrio2017handling;@gabrio2021joint;@mason2021flexible]. \n\nWhat is more or less certain is that some missing outcome values for some individuals is likely to occur at least at some measurment collection time during the trial, which impairs the computation of aggregated CE variables (eg QALY and TC) for those cases. If prevention strategies did not work well and a considerable amount of missing values occurs in the trial, then we have to accept that our analysis will be characterised by some inherent uncertainty. Indeed, although different approaches for handling missing data have been proposed in the CEA literature, any method relies on some **untestable** assumptions about the missing values that cannot be checked from the data at hand. \n\nThe best we can do is try to:\n\n  - choose and formulate our **assumptions** about the missing cases in the most transparent and plausible way given the trial and analysis context;\n  - select a **missingness method** that best matches those assumptions\n  - explore the **sensitivity of the results** under the stated assumptions to some plausible departures\n\nFor the first step, there is a general consensus in the literature of relying on the *Rubin's taxonomy* to formally define the assumptions about the missing data generating process, often referred to as **missing data mechanism**[@little2019statistical], which can be thought of as the mechanism responsible for the occurrence of missingness. According to the type of assumptions formulated, this mechanism can be broadly distinguished under three general classes:\n\n  - **Missing Completely At Random** (MCAR): missingness occurs randomly and is therefore not cause or associated with any other variables (eg lost records).\n  \n  - **Missing At Random** (MAR): missingness exclusively depends on some *observed* variable that is available (eg individuals have missing outcome data only because they are older and age is measured).\n  \n  - **Missing Not At Random** (MNAR): missingness (also) depends on some *unobserved* variable that is not fully available (eg individuals have missing outcome data because they are experiencing lower health/higher costs and at least some of these values are not measured).\n  \nChoice of the type of mechanism, and therefore missingness assumptions, for a specific analysis should be guided by both observed and external information available to the analysts, including missingness patterns, previous studies, clinical and expert knowledge. Once a decision is made about the \"most\" plausible mechanism, this should be set as the **base-case** assumption and a missingness method that provide vaòid results under the chosen assumption should be implemented for the analysis.\n\nIn general, methods that rely on quite restrictive assumptions (eg MCAR), such as **Complete Case Analysis** (CCA), should be avoided as they are likely to bias the results and mislead the CE conclusions. There are some specific situations in which these methods provide valid results, especially in the context of randomised trials [@white2005adjusting;@white2010bias], but their implementation should be clearly justified. Similarly, methods that replace the missing outcome data with a single value (eg mean or last value observed), referred to as **Single Imputation** (SI) approaches, should be avoided as they often make very restrictive and hard to interpret assumptions about the missingness mechanism that are unlikely to lead to plausible inferences.\n\nCurrently, the most recommended approach to deal with missing CE outcome data in trial-based analyses is **Multiple Imputation** (MI), often implemented under its version known as *multiple imputation by chained equations* or MICE [@van2012flexible]. A series of factors contributed to the success of MICE within the CEA community, which include: possibility to obtain inferences under different mechanisms' assumptions for each variable; correct representation of missingness uncertainty under the selected assumptions through the specification of an imputation model for each variable; handling of missing values in multiple non-normally distributed and correlated variables. Because of the popularity of MICE and its characteristics making it quite handy for implementation in trial-based CEA, in the next sections I will focus on this approach and show some examples on how it can be implemented in `R`.\n\n## Data generation\n\nThe following (folded) code is simply used to generate some artificial CEA data for exemplary purposes to demonstrate how MICE may be conducted in `R`. If not of interest, you may skip the folded code and jump to the actual implementation code in the next section. In this simulation I will make things easy and generate data assuming normal distributions and limited number of covariate data. However, the implementation of the methods shown in the following sections is general and can be applied to more realistic data too.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#simulate data and missing under different mechanisms\n#set rng for reproducibility\nset.seed(2345)\n#set up parameter values to generate data\nn_full <- 250\nmu_x1 <- 0.5\nsd_x1 <- 0.15\nx1_data <- rnorm(n_full, mu_x1, sd_x1) #simulate covariate values\nn_full <- 250\nmu_x2 <- 100\nsd_x2 <- 25\nx2_data <- rnorm(n_full, mu_x2, sd_x2) #simulate covariate values\np_trt <- 0.5\ntrt_data <- rbinom(n_full, p_trt, size = 1)#simulate trt indicator\nbeta0_data <- 0.2\nbeta1_data <- 0.15\nbeta2_data <- 1\n#simulate outcome values\ny1_data <- beta0_data + beta1_data*trt_data + beta2_data*x1_data + rnorm(n_full, 0, 0.1)\nalpha0_data <- 250\nalpha1_data <- 35\nalpha2_data <- 1\nalpha3_data <- 55\n#simulate outcome values\ny2_data <- alpha0_data + alpha1_data*trt_data + alpha2_data*x2_data + alpha3_data*y1_data + rnorm(n_full, 0, 50)\n#generate fully-observed datset and assign names to variables\nfull.df <- data.frame(y1_data,y2_data,trt_data,x1_data,x2_data)\nnames(full.df) <- c(\"QALY\",\"TC\",\"trt\",\"u\",\"c\")\nfull.df$trt <- ifelse(full.df$trt==0,\"old\",\"new\")\nfull.df$trt <- factor(full.df$trt, levels = c(\"old\",\"new\"))\n\n#introduce MAR missingness in QALY given u\nlibrary(boot)\n#set rng for reproducibility\nset.seed(2345)\n#set parameter values for mechanisms\neta0_mar <- -9\neta_1_mar <- 14.5\neta_2_mar <- 0\neta_3_mar <- 0\np_mar_e <- inv.logit(eta0_mar + eta_1_mar*full.df$u + eta_2_mar*full.df$QALY + eta_3_mar*as.numeric(full.df$trt))\niota0_mar <- -4\niota_1_mar <- 3\niota_2_mar <- 0\niota_3_mar <- 0\np_mar_c <- inv.logit(iota0_mar + iota_1_mar*full.df$c/100 + iota_2_mar*full.df$TC + iota_3_mar*as.numeric(full.df$trt))\n#generate missing data indicators (0=observed,1=missing)\nm_mar_e <- rbinom(n_full, p_mar_e, size = 1)\nm_mar_c <- rbinom(n_full, p_mar_c, size = 1)\nfull.mar <- full.df\n#introduce MAR missing QALY data and distinguish between a variable showing only the observed QALY values (QALY_obs) and another showing only the missing QALY values (QALY_mis)\nfull.mar$m_QALY <- m_mar_e\nfull.mar$m_TC <- m_mar_c\nfull.mar$QALY_obs <- ifelse(full.mar$m_QALY==0,full.mar$QALY,NA)\nfull.mar$TC_obs <- ifelse(full.mar$m_TC==0,full.mar$TC,NA)\n\n#randomly shuffle rows of the MAR dataset\nfull.mar <- full.mar[sample(1:nrow(full.mar)), ]\n#keep only relevant variables and rename them\ndataset.mis <- full.mar[,c(\"QALY_obs\",\"TC_obs\",\"trt\",\"u\",\"c\")]\nnames(dataset.mis) <- c(\"QALY\",\"TC\",\"trt\",\"u\",\"c\")\n```\n:::\n\n\nWe can inspect the first few rows of the generated data stored in the `R` object `full.mar` to see the variable showing the observed QALY values (`QALY`) and TC values (`TC`), which were generated under a MAR mechanism conditional on the fully observed baseline utilities (`u`) and costs (`c`).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nhead(dataset.mis, n=8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         QALY       TC trt         u         c\n34  0.6891580 378.6541 new 0.3518463  91.95314\n48  0.8021310       NA new 0.5254678  93.04074\n193 0.4477710 455.1781 new 0.2485247 126.63579\n188 0.8710712 471.2896 new 0.4307028 110.27430\n58  0.7926060       NA new 0.4134793 103.14698\n87         NA 458.9715 new 0.6311773 114.55390\n135 0.6656135 423.3912 new 0.3820836 132.51559\n107 0.6320513 320.8635 new 0.3455197 116.77745\n```\n\n\n:::\n:::\n\n\nWe can also manually compute the number of missing QALY and TC values generated in this dataset across treatment groups, corresponding to 61 (24 $\\%$) and 69 (28 $\\%$), respectively.\n\n## Method application\n\nAlthough a variety of missing data methods exist in the general statistical literature [@schafer2002missing], here I will exclusively focus on MI as the recommended approach for handling missing outcome CE data, with a focus on its most popular implementation version MICE. Without entering technical details, the general procedure behind MI methods can be summarised in three steps:\n\n  1. For each missing value, generate a set of $M$ imputed values according to some pre-specified *imputation model* to guide the imputation process. For each variable, its missing values are replaced with the $m=1\\ldots,M$ imputations to create a corresponding set of $M$ completed (observed and imputed) datasets. Elements that need to be defined within said model include: *method of imputation*; *number of imputed values* $M; *order of imputation* in case of multiple missing variables.\n  \n  2. In each imputed dataset $m$, the analysis model (ie the model used to derive the estimates of interest) is fitted and corresponding estimates $\\hat{\\beta}_m$ are derived for $m=1,\\ldots,M$. In CEA, typical examples of these models include OLS, SUR, GLM or MLMs. \n  \n  3. The so-called *Rubin's rules* [@little2019statistical] are used to combine the $M$ parameter estimates derived from fitting the analysis model to the completed datasets into single quantities $\\hat{\\beta}$. These rules ensure that the derivation of estimates and related uncertainty measures takes into account missing data uncertainty reflected by the uncertainty associated with the $M$ different imputations for each missing value. \n\nCompared to standard MI methods, the **MICE** version is particularly appealing when addressing missingness in CEA because it allows a separate imputation model specification for each missing variable in the dataset. This allows to specify imputations models that can directly address the data complexities of each variable (eg skewness, value ranges, association with other variables) within an univariate modelling approach (eg using regression methods). The price to pay for this flexibility is that the overall imputation model involving all missing variables is not theoretically well defined. However, simulation exercises have suggested that, provided the specification of the imputation model for each variable is correct, the derived results are valid under the specified missingness assumptions. \n\nWithin MICE, a popular imputation method is the non-parametric *predictive mean matching* (PMM) approach [@allison2015imputation], consisting in a specific type of resampling procedure that allows to generate imputations using predictive information from other variables and that are always consistent with the observed values for each variable. In addition, it is generally recommended that a sufficiently large number of imputations $M$ is chosen to ensure the validity of MI inferences. Despite different recommendations, in the literature a choice for $M$ closed to the so-called *fraction of missing information* for a given dataset is often advocated [@graham2007many;@carpenter2023multiple]. In many cases, this quantity is approximated by the overall proportion of missing values. As for the order of imputation of the missing variables, no generally accepted guidelines exist in the literature, with the exception of the general recommendation of applying MI methods separately to each treatment arm in a clinical trial setting. \n\nDepending on the assumed missingness mechanism, specification of the imputation model should change to reflect those assumptions. Often, MAR is suggested as the reference assumption to be taken in the base-case analysis by including as many variables likely associated with the missing data as possible in the imputation model to \"improve\" the robustness of the results, ie use all available information from the data to inform the imputation process. However, it is important to note that as the imputation model becomes more complex (ie includes more variables), the MICE algorithm (fundamentally a MCMC method) will likely encounter convergence problems and possibly lead to incorrect inferences, especially in the context of very sparse data and high missingness rates. Therefore, care should be used when selecting the variables to include in the imputation model as well as in the selection of which variables should be used to inform the imputation of a given missing variable to minimise chances of incurring in convergence problems. To this purpose, it is generally suggested to perform some preliminary analyses (eg using plots, test or regression methods) to check, for each missing variable, the association between missingness indicators and other variables in the dataset. The results from these analyses can then be used to guide the selection of the variables to include in the imputation model for each missing variable. \n\nIn `R`, the following code may be used to implement MICE (under a MAR assumption) to the generated data and summarise the output. The following MI specification is used: number of imputations $M=30$; PMM as imputation method for all missing variables (`QALY` and `TC`) using corresponding observed baseline values (`u` and `c`) as the only variables included in their respective imputation models.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(mice) #load package to implement MICE\n#split data by treatment group (Old and New)\ndataset.mis_Old <- dataset.mis[dataset.mis$trt==\"old\",]\ndataset.mis_New <- dataset.mis[dataset.mis$trt==\"new\",]\n#set up MICE inputs for old group\nmice_Old <- mice(dataset.mis_Old, print = FALSE, method = 'pmm', maxit = 0)\npM_Old <- mice_Old$predictorMatrix #extract default predictor matrix\n#customise pred matrix to your needs (row=variable to be imputed, column=variable used as predictor)\n#in the matrix an entry of 1 means that the corresponding column variable is used as predictor for the corresponding row variable\npM_Old[\"QALY\",c(\"TC\",\"u\")] <- 1 #require that QALY always imputed using TC and u\npM_Old[\"QALY\",c(\"c\")] <- 0 #require that QALY is never imputed using c\npM_Old[\"TC\",c(\"u\")] <- 0 #require that TC is never imputed using u\npM_Old[,c(\"trt\")] <- 0 #require that any variable is never imputed using trt\nmeth_Old <- mice_Old$method #extract default imputation methods\n#by default PMM assumed as imputation methods for numeric variables\n#set up MICE inputs for new group\nmice_New <- mice(dataset.mis_New, print = FALSE, method = 'pmm', maxit = 0)\npM_New <- mice_New$predictorMatrix #extract default predictor matrix\npM_New[\"QALY\",c(\"TC\",\"u\")] <- 1 #require that QALY always imputed using TC and u\npM_New[\"QALY\",c(\"c\")] <- 0 #require that QALY is never imputed using c\npM_New[\"TC\",c(\"u\")] <- 0 #require that TC is never imputed using u\npM_New[,c(\"trt\")] <- 0 #require that any variable is never imputed using trt\nmeth_New <- mice_New$method #extract default imputation methods\nM <- 30 #number of imputations\n#set rng for reproducibility\nset.seed(2345)\n#implement MICE to old and new group\nmice_Old_fit <- mice(dataset.mis_Old, predictorMatrix = pM_Old, method=meth_Old, m = M, print = FALSE)\nmice_New_fit <- mice(dataset.mis_New, predictorMatrix = pM_New, method=meth_New, m = M, print = FALSE)\n#combine the imputed datasets across groups\nmice_fit <- rbind(mice_Old_fit, mice_New_fit)\n#lme_mice_data <- with(mice_data, lm(QALY_obs ~ trt + u))\n```\n:::\n\n\nThe object `mice_fit` contains the results of the MICE imputation algorithm, which was implemented separately by treatment group for QALY and TC variables according to the imputation model specification given above. It is generally a good idea to check the results generated using the function `mice` to make sure that no general problems occurred in the algorithm convergence or no implausible imputations were generated for all variables. For example, we can check for possible convergence problems using the `plot` function which, when applied to a `mice` object, produces the following plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mice_fit, y=c(\"QALY\",\"TC\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=480}\n:::\n:::\n\n\nwhich shows the convergence pattern of mean and standard deviation statistics for QALY and TC variables with respect to each of the 30 imputed values (lines) across the algorithm iterations (default set to $5$ but can be modified within `mice` using the argument `maxit`). Overall, a random pattern like the one shown in this case suggests that no evident convergence problems seem to affect the imputed variables. We can also look at the density distribution of the imputed values and compare it to that of the observed data for each imputed variable using the function `densityplot` (usually helpful for numeric variables), which produces the following plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensityplot(mice_fit, ~ QALY+ TC|trt, layout=c(2, 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-43-1.png){width=480}\n:::\n:::\n\n\nThe argument `~ QALY+ TC|trt` is used to specify to display the results for the two outcome variables *conditional* on the treatment indicator values, i.e. within the two subgroups for the old and new arm. Many others diagnostic measures and plots are available in `mice` to check the generated imputations. For a general overview of how to implement these and other customisation options available for objects generated through `mice` I recommend checking out this [online and freely available book version](https://stefvanbuuren.name/fimd/) of the MICE developer.  \n\nAfter checking the results of MICE, we can now proceed to fit the analysis model to each completed dataset to derive $M$ estimates for the parameters of interest. In our case, we are interested in retrieving estimates for the mean QALY/TC in each treatment group and for the mean incrementals. The `mice` function is quite flexible and is compatible with many different `R` functions that fit different types of statistical methods, including OLS, SUR, GLM and MLM. In this simulated study, we fit standard OLS regression models to each outcome for demonstrative purposes. We achieve this by using the function `with` which allows to fit the desired models (specified as its second argument) within each completed dataset contained in the object `mice_fit` (specified as its first argument) \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#fit QALY and TC model in each completed dataset and store results\nlm_mice_e <- with(mice_fit, lm(QALY ~ trt + u))\nlm_mice_c <- with(mice_fit, lm(TC ~ trt + c))\n```\n:::\n\n\nThe generated objects `lm_mice_e` and `lm_mice_c` contain the $M$ different parameter estimates of the OLS regressions fitted to each completed dataset stored in `mice_fit`. Pooled estimates for these parameters across the $M$ values, obtained using Rubin's rules, can be generated by using the `pool` function. For example, for the QALY model, we can obtain and summarise the pooled point and uncertainty estimates for all model parameters by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n#pool regression results across imputations for QALY model\npool_lm_mice_e <- pool(lm_mice_e)\nsummary(pool_lm_mice_e, conf.int = TRUE, conf.level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         term  estimate  std.error statistic        df\n1 (Intercept) 0.1871130 0.02662999  7.026402 127.39204\n2      trtnew 0.1621742 0.01518626 10.679011 111.26219\n3           u 1.0105077 0.05340866 18.920298  94.03143\n                                     p.value     2.5 %    97.5 %  conf.low\n1 0.0000000001139214095370705727883371694009 0.1344186 0.2398074 0.1344186\n2 0.0000000000000000009451108771931596269286 0.1320824 0.1922660 0.1320824\n3 0.0000000000000000000000000000000008031831 0.9044640 1.1165514 0.9044640\n  conf.high\n1 0.2398074\n2 0.1922660\n3 1.1165514\n```\n\n\n:::\n:::\n\n\nGiven that in CEA we are interested in estimates for the mearginal means of each outcome and their difference between treatment groups, we can use the function `emmeans` from the package `emmean` to compute these quantities based on the MICE output in generated before. This can be done for each outcome by typing\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(emmeans) #load package to compute marginal means\n#obtain marginal means for QALY and TC across imputations\nem_lm_mice_e_ctr <- emmeans(lm_mice_e, ~ trt)\nem_lm_mice_c_ctr <- emmeans(lm_mice_c, ~ trt)\n\nem_lm_mice_e_ctr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n trt emmean      SE  df lower.CL upper.CL\n old  0.681 0.00973 140    0.662    0.701\n new  0.844 0.01140 103    0.821    0.866\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nem_lm_mice_c_ctr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n trt emmean   SE  df lower.CL upper.CL\n old    394 4.67 131      384      403\n new    430 5.00 150      420      439\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\n#take difference as - Old + New = New - Old\nnew_vs_old <- list(\"New vs Old\" = c(-1, 1))\n#compute linear combinations to get incremental estimates\nlm_mice_delta_e <- contrast(em_lm_mice_e_ctr, new_vs_old) \nlm_mice_delta_c <- contrast(em_lm_mice_c_ctr, new_vs_old) \n#obtain results in terms of confidence intervals\nconfint(lm_mice_delta_e, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast   estimate     SE  df lower.CL upper.CL\n New vs Old    0.162 0.0152 111    0.132    0.192\n\nConfidence level used: 0.95 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nconfint(lm_mice_delta_c, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n contrast   estimate   SE  df lower.CL upper.CL\n New vs Old     35.9 6.54 178       23     48.8\n\nConfidence level used: 0.95 \n```\n\n\n:::\n:::\n\n\nHere I stop myself from discussing possible extensions of MI to be integrated with other approaches that I showed before, eg combine MI with bootstrapping or with methods to handle clustering (perhaps this can be explored another time). One of the reasons is also because combination of MI with bootstrapping or other non-standard analysis methods can be very challenging and clear guidelines about the performance of the methods when combined together is still lacking [@schomaker2018bootstrap;@brand2019combining]. This is particularly problematic for CEA, where the additional complexities of the data should be taken into account while also being able to handle missingness uncertainty. \n\n\n## Summary\n\nIn CEA practice, simple but likely biased and inappropriate methods for handling missing data (eg CCA) have been historically implemented. In addition, many reviews highlighted a generally poor reporting practice about missing data information (eg amount, pattern, etc.) and, even more importantly, a lack of clarity about the methods adopted to generate the results and conclusions of the studies [@noble2012missing;@diaz2014handling;@leurent2018missing]. Only in recent years, a gradual movement towards more advanced and appropriate missing data methods (eg MI) seems to have occurred, although there is still a general lack of implementation details about these methods [@gabrio2017handling;@ling2022scoping].\n\nWhile the uptaking of more appropriate methods to handle missingness is encouraging, it is important to report in a transparent and clear way how these methods were used in a specific analysis since statistical and CE results can be quite sensitive to the specific modelling or imputation strategies chosen. As an example, MAR as the \"benchmark\" (and often only) assumption in CEA is routinely selected, which in many cases may be sensible and genuinely reflect the assumptions of the analysts about the missingness mechanism under consideration. However, the plausibility of any missingness assumption, including MAR, can never be checked from the data and therefore it is extremely important that some form of **sensitivity analyses** is conducted to assess the robustness of the study results to departures from MAR. Within an MI framework in CEA, different MNAR imputation strategies have developed to achieve this task [@leurent2018sensitivity;@leurent2020reference]. In the wider missing data CEA literature, alternative approaches have also been proposed to handle missingness using more flexible methods, such as *Bayesian models*, that naturally lend themselves to the incorporation of external evidence (eg expert opinion or informed guess) into the analysis and assess the robustness of the results to MNAR assumptions [@mason2018bayesian;@gabrio2019full].\n\nFinally, throughout this document I have exclusively focussed on implementing different statistical methods at the level of aggregated CE variables in a study, ie QALYs and TCs, which are usually computed based on the values of different components (ie utilities and costs) collected at different time points during the study period. An alternative analysis approach would be to focus on the disaggregated CE components and implement different statistical methods to analyse these variables instead, which has already been done in the literature [@mason2021flexible;@gabrio2020bayesian;@gabrio2021joint;@ben2023conducting]. While, in principle, shifting the focus of the analysis to aggregated variables is more convenient from a modelling perspective (ie less number of variables and their interactions to model), in practice, this is no longer true when some missingness occurs at the disaggregated level. Indeed, when some missing outcome values occur at one or multiple time points during the study, information about these partially-observed variables is lost when the objective of the analysis is moved at the more aggregated level. This is likely to be a major concern for those CE outcomes that are affected by so-called *item-level missingness*, where patients fill in only a part of the questionnaire answers, which is quite common for resource use instruments (used to compute costs). When this occurs, then implementing an analysis model at the most disaggregated data level (eg resource use answers or costs at each time point in the study) allows a more efficient use of the observed data collected, which would be instead at least partially lost by modelling at more aggregated data levels [@ben2023handling]. However, implementation of models at the level of disaggregated CE components is very challenging unless relying on standard normality and independence methods' assumptions. This is because: appropriate *missingness methods* need to be combined with adequate *methods to handle the typical CE data complexities* (correlation, skewness, clustering) within a more complex multivariate modelling framework while also generating correct bootstrapped estimates for the quantities of interest to produce standard CEA output.  \n\n# Conclusions\n\nHistorically, HTAs have been conducted with commercial software (eg SPSS) or spreadsheet software (eg Excel) which are sufficient for simple analyses **BUT** put constraints that limit credibility and relevance of the analysis. Conversely, modern programming languages (eg R) facilitate the development of models that are increasingly sophisticated and realistic, capable of quantifying decision **uncertainty**, tansparent, reproducible, reusable and adaptable [@incerti2019r]. I specifically insist on `R` as my default statistical software thanks also to the wide user/developer community, which is well suited for:  developing/implementing HTA models in a **single software environment**; catching up with **methodological advances**; and quickly spotting and correcting code errors via the **open-source** nature of packages.\n\nSo, what could be the reasons behind the slow adoption of modern programming lanagues among practitioners in HTA? In my opinion, a key reason is the still general lack of software experience in the HTA community. This is possibly due to an *insufficient* training in script-based programming software; and a *limited* guidance in the literature on how to implement standard models in such software. To improve the current situation, I believe it is critical that the next generation of health economists is trained in *state of the art* methods and *software* to implement them. In practice, this could be achieved by: developing university *courses* \\& *workshops*; writing *tutorial papers*; making code *freely available* on online repositories (eg *GitHub*); and *encouraging* the use of programming languages among researchers.\n\nSo, what do you think? did you find this material and code helpful for your own analyses? I really hope so. You can also find my code and examples available on my [GitHub repository](https://github.com/AnGabrio/Code/tree/master/RHTAmethods), where there is also a separate file containing well-wrapped functions to implement some of the methods illustrated in this post in a more user-friendly way (eg bootstrapping and TSB procedures). Thanks again for reading, and see you at my next update!\n\n\n# References\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}