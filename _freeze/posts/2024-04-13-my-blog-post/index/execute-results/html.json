{
  "hash": "7f1619cdd63046e1c9ed2ef8a293d075",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Two-Parameter Logistic Model\"\ndescription: \"\"\nauthor:\n  - name: Andrea Gabrio\n    url: https://angabrio.github.io/agabriosite2/\n    orcid: 0000-0002-7650-4534\n    affiliation: Maastricht University\n    affiliation-url: https://www.maastrichtuniversity.nl/research/methodology-and-statistics\ndate: 2024-04-13\ncategories: [Quarto, R, Academia, IRT] # self-defined categories\n#citation: \n#  url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/ \nimage: featured.png\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\n---\n\n\n![](featured.png){fig-align=\"center\"}\n\n\nHello folks, I hope everything is running smoothly for you. As for me, it has been quite a hectic period due to a mix of personal circumstances, teaching and grant application stuff overlapping. It seems I survived the first wave of tasks to do, and now look forward to finish off my remaining duties, mostly related to teaching in a new programme I will be coordinating these upcoming months. \n\nAs a way to take a break from my daily duties, today I would like to continue the topic from the last post about **IRT models** (check my last post out in case you are not sure what I am talking about!). In particular, I first introduced the theory behind the definition of the *Rasch Model* and its features and interpretation of its parameters in the IRT context. I then simulated some data from such model and showed how to estimate and assess the model to the data using standard `R` packages and functions. \n\nToday, I would like to move on and show alternative IRT models which try to address some of the limitations intrinsic to the Rasch model. For example, a quite strong assumptions underneath the Rasch model is that all items $k=1,\\ldots,K$ discriminate respondents in the same way (i.e. they are equally related to the individual ability parameter $\\theta_i$), but are only differentiated in terms of their difficulty parameter $b_k$. This assumption can be eased through the **two-parameter** logistic model\n\n$$\nP(Y_{ik}=1 | \\theta_i,a_k,b_k) = \\frac{\\text{exp}(a_k\\theta_i-b_k)}{1+\\text{exp}(a_k\\theta_i-b_k)}, \n$$\n\nwhere a item-level *discrimination* parameter $a_k$ is added, thus allowing items to discriminate between respondents given the same value of the latent ability $\\theta_i$. In terms of the item characteristic curve (ICC), this implies that the ICC has a slope parameter $a_k$ which varies depending on the item considered, there implying a different type of relationship between $\\theta_i$ and $P(Y_{ik}=1)$ based on the item selected. We can see by simulating data from the two-parameter logistic model ICC with different values for the discrimination parameter $a_k=\\{2,1,0.5\\}$ and same values for the difficulty parameter $b_k=0$ over the same range of the latent ability parameter $\\theta_i\\sim \\text{Uniform}(-4,4)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(768)\n\nicc_tplm <- function(theta,a,b){\n  p_resp <- (exp(a*theta - b)) / (1 + exp(a*theta - b))\nreturn(p_resp)\n}\n\ntheta_ex <- seq(from = -4, to = 4, by = 0.01)\nicc_ex1 <- icc_tplm(theta = theta_ex, a = 2, b = 0)\nicc_ex2 <- icc_tplm(theta = theta_ex, a = 1, b = 0)\nicc_ex3 <- icc_tplm(theta = theta_ex, a = 0.5, b = 0)\n```\n:::\n\n\nWe can then plot the generated ICCs under each scenario for the item-discrimination parameter $a_k$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot ICC\nlibrary(ggplot2)\nn <- length(icc_ex1)\ntplm_df <- data.frame(\n  prob = c(icc_ex1, icc_ex2, icc_ex3),\n  theta = c(theta_ex, theta_ex, theta_ex),\n  a = c(rep(\"a=2\", n), rep(\"a=1\", n), rep(\"a=0.5\", n))\n)\ntplm_df$a <- factor(tplm_df$a, levels = c(\"a=2\", \"a=1\", \"a=0.5\"))\n\nggplot(tplm_df, aes(x = theta, y = prob, color = a)) +\n  geom_point() +\n  xlim(-4,4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\"),\n        legend.title = element_blank(), legend.key = element_rect(fill = \"white\"),\n        axis.text = element_text(colour = 'black', face=\"bold\"), axis.ticks.x=element_blank(),\n        strip.background =element_rect(fill=\"white\"), strip.text = element_text(colour = 'black',face = \"bold\")) + labs(y = \"Probability of correct response\", x = \"latent ability\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe three ICCs have different discrimination parameters $a_k$:\n\n  . The ICC with the highest value $a_k=2$ is the one associated with the steepest slope. This implies that items with such a discrimination ability are able to easily distinguish respondents with latent ability values between $-1/1$ and associated these to a very high/low probability of correct response $P(Y_{ik}=1)$. \n  \n  . The ICC with the middle value $a_k=1$ instead is associated with the average slope. It is less able to discriminate between respondents with $\\theta_i$ values between $-2/2$ compared to the ICC with $a_k=2$.\n  \n  . The ICC with the lowest value $a_k=0.5$ is associated with the flattest slope. It is more able to discriminate between respondents with $\\theta_i$ values between $-2/2$ compared to the other two ICCs, but is less able to distinguish between respondents as the ability level is increased further. \n\nKeep in mind that such a distinction between ICCs is made for the same value of the difficulty parameter $b_k$, here assumed to be $0$. As I showed for the Rasch model, changing the value of $b_k$ will also affect the position of the ICC and thus how a given value of $\\theta_i$ is related to $P(Y_{ik})$. However, we saw that $b_k$ only affects ICCs by shifting its position on the plot, i.e. either more to the right (larger $b_k$) or to the left (smaller $b_k$), but will not affect their slope. Conversely, given the same level of difficulty $b_k$ for an item, its discrimination ability $a_k$ modifies the slope of the ICC and thus also affecting $P(Y_{ik})$, i.e. either making ICCs steeper (larger $a_k$) or flatter (smaller $a_k$).\n\nA probit version of the two-parameter model is also defined in the literature as the **normal ogive** model in which the ICC is based on a cumulative normal distribution\n\n$$\nP(Y_{ik}=1 | \\theta_i,a_k,b_k) = \\Phi(a_k\\theta_i - b_k)= \\int^{a_k\\theta_i-b_k}_{-\\infty}\\phi(z)d(z),\n$$\n\nwhere $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ are the cumulative and density normal distribution function. The two model formulations resemble each other when the logistic item parameter values are multiplied with a constant scaling factor $d=1.7$.\n\nUsing the example generated before, we can replicate the ICC graph using the same inputs for the normal ogive model as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(768)\n\nicc_ogive <- function(theta,a,b){\n  p_resp <- pnorm(a*theta - b)\nreturn(p_resp)\n}\n\ntheta_ex <- seq(from = -4, to = 4, by = 0.01)\nicc_ex1 <- icc_ogive(theta = theta_ex, a = 2, b = 0)\nicc_ex2 <- icc_ogive(theta = theta_ex, a = 1, b = 0)\nicc_ex3 <- icc_ogive(theta = theta_ex, a = 0.5, b = 0)\n\nogive_df <- data.frame(\n  prob = c(icc_ex1, icc_ex2, icc_ex3),\n  theta = c(theta_ex, theta_ex, theta_ex),\n  a = c(rep(\"a=2\", n), rep(\"a=1\", n), rep(\"a=0.5\", n))\n)\nogive_df$a <- factor(ogive_df$a, levels = c(\"a=2\", \"a=1\", \"a=0.5\"))\n\nggplot(ogive_df, aes(x = theta, y = prob, color = a)) +\n  geom_point() +\n  xlim(-4,4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\"),\n        legend.title = element_blank(), legend.key = element_rect(fill = \"white\"),\n        axis.text = element_text(colour = 'black', face=\"bold\"), axis.ticks.x=element_blank(),\n        strip.background =element_rect(fill=\"white\"), strip.text = element_text(colour = 'black',face = \"bold\")) + labs(y = \"Probability of correct response\", x = \"latent ability\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe term $a_k\\theta_i - b_k$ is often presented in the literature as $a_k(\\theta_i-b^\\star_k)$ to denote the fact that $b_k$ is defined on the same scale as $\\theta_i$, i.e. $b^\\star_k=\\frac{b_k}{a_k}$ represents the point on the ability scale where a respondent has the same chance of answering correctly or incorrectly on the item $k$ ($P(Y_{ik}=1)=0.5$). The metric of the ability parameters is known from item response data only up to a linear transformation. The metric can be identified by fixing a discrimination and difficulty parameter or by adding constraints that the sum of item difficulties and the product of item parameter values equals, for instance, zero and one, respectively, or by fixing the mean and variance of the population distribution of ability parameters. Note that the choice of the identifying restrictions can lead to numerical problems in the estimation of the parameters.\n\nGood! I think I am getting th gist of these introductory models, which is precisely why I think writing about them here can help me into learning or at least give me the motivation to learn such topic. I just hope that I can speed up the process a bit as I already want to learn more!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}