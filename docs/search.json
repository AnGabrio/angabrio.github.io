[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Item Response Theory Models (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nItem Response Theory Models (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralised Linear Mixed Models (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralised Linear Mixed Models (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralised Linear Models part II (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nFeb 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralised Linear Models (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit tests (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPartly Nested Anova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPartly Nested Anova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRandomised Complete Block Anova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNested Anova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nNested Anova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Autocorrelation (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal Autocorrelation (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVariance Heterogeneity (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nVariance Heterogeneity (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFactorial Analysis of Variance (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nFeb 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFactorial Analysis of Variance (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAncova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Factor Anova (JAGS))\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Factor Anova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAncova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRandomised Complete Block Anova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Two Populations (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Two Populations (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSuper basic introduction to JAGS\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSuper basic introduction to Stan\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using Stan via R …\n\n\n\nAndrea Gabrio\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html",
    "href": "tutorials/2020-03-21-irt-jags/index.html",
    "title": "Item Response Theory Models (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#load-the-data",
    "href": "tutorials/2020-03-21-irt-jags/index.html#load-the-data",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Load the data",
    "text": "Load the data\nI read in the data from the file wideformat.csv, which contains (simulated) data from \\(n=1000\\) individuals taking a \\(5\\)-item test. Items are coded \\(1\\) for correct and \\(0\\) for incorrect responses. When we get descriptives of the data, we see that the items differ in terms of the proportion of people who answered correctly, so we expect that we have some differences in item difficulty here.\n\ndata_dicho&lt;-read.csv(\"wideformat.csv\", sep = \",\")\nhead(data_dicho)\n\nNA        ID gender age Item.1 Item.2 Item.3 Item.4 Item.5\nNA 1 person1   Male  40      0      0      0      0      0\nNA 2 person2 Female  27      0      0      0      0      0\nNA 3 person3   Male  13      0      0      0      0      0\nNA 4 person4 Female  17      0      0      0      0      1\nNA 5 person5 Female  30      0      0      0      0      1\nNA 6 person6 Female  46      0      0      0      0      1\n\n#check proportion of correct responses by item\napply(data_dicho[,4:8], 2, sum)/nrow(data_dicho)\n\nNA Item.1 Item.2 Item.3 Item.4 Item.5 \nNA  0.924  0.709  0.553  0.763  0.870\n\n#summarise the data\nlibrary(psych)\ndescribe(data_dicho)\n\nNA         vars    n   mean     sd median trimmed    mad min  max range  skew\nNA ID*        1 1000 500.50 288.82  500.5  500.50 370.65   1 1000   999  0.00\nNA gender*    2 1000   1.50   0.50    2.0    1.51   0.00   1    2     1 -0.02\nNA age        3 1000  25.37  14.43   25.0   25.36  17.79   1   50    49  0.01\nNA Item.1     4 1000   0.92   0.27    1.0    1.00   0.00   0    1     1 -3.20\nNA Item.2     5 1000   0.71   0.45    1.0    0.76   0.00   0    1     1 -0.92\nNA Item.3     6 1000   0.55   0.50    1.0    0.57   0.00   0    1     1 -0.21\nNA Item.4     7 1000   0.76   0.43    1.0    0.83   0.00   0    1     1 -1.24\nNA Item.5     8 1000   0.87   0.34    1.0    0.96   0.00   0    1     1 -2.20\nNA         kurtosis   se\nNA ID*        -1.20 9.13\nNA gender*    -2.00 0.02\nNA age        -1.21 0.46\nNA Item.1      8.22 0.01\nNA Item.2     -1.16 0.01\nNA Item.3     -1.96 0.02\nNA Item.4     -0.48 0.01\nNA Item.5      2.83 0.01"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model",
    "href": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 1PLM to the data. First, I rename and preprocess the data to be passed to JAGS.\n\nY&lt;-data_dicho[,4:8]\nn&lt;-nrow(Y)\np&lt;-ncol(Y)\ndata_list&lt;-list(\"Y\",\"n\",\"p\")\n\nThen I specify the model using the following JAGS code.\n\nmodel1&lt;-\"\nmodel {\n for (i in 1:n){\n  for (j in 1:p){\n   Y[i , j] ~ dbern ( prob [i , j])\n   logit ( prob [i , j]) &lt;- theta [i] - delta [j]\n  }\n  theta [i] ~ dnorm (0.0 , 1.0)\n }\n\n for (j in 1:p){\n  delta [j] ~ dnorm (mu.delta , pr.delta )\n }\n  pr.delta &lt;- pow(s.delta , -2)\n  mu.delta ~ dnorm(0, 5)\n  s.delta ~ dunif(0, 10)\n  \n  for(i in 1:n){\n   for(j in 1:p){\n    Y.rep[i, j] ~ dbern(prob[i, j])\n    loglik_y[i, j]&lt;-logdensity.bern(Y[i,j], prob[i, j])\n   }\n  }\n}\n\"\n## write the model to a text file\nwriteLines(model1, con = \"model1PLM.txt\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"theta\", \"prob\",\"loglik_y\",\"Y.rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\nlibrary(R2jags)\nlibrary(coda)\nset.seed(3456)\nm1.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\n    model.file = \"model1PLM.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 5000\nNA    Unobserved stochastic nodes: 6007\nNA    Total graph size: 26016\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\n#see average value of item difficulty\ndiff&lt;-m1.r2jags$BUGSoutput$sims.list$delta\napply(diff,2,mean)\n\nNA [1] -2.8583972 -1.0620835 -0.2608953 -1.3853516 -2.2159276\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(m1.r2jags$BUGSoutput$sims.list$theta, 2, mean)\nprob&lt;-apply(m1.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nWe see that item \\(3\\) is the most difficult item (it’s curve is farthest to the right), and item \\(1\\) is the easiest (it’s curve is farthest to the left). The same conclusions can be drawn by checking the difficulty estimates above."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practically speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation&lt;-prob*neg_prob\nplot(theta,information[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.3))\nlines(theta,information[,1],col=\"red\")\nlines(theta,information[,2],col=\"blue\")\nlines(theta,information[,3],col=\"orange\")\nlines(theta,information[,4],col=\"green\")\nlines(theta,information[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nSimilar to the ICCs, we see that item \\(3\\) provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item \\(1\\) and \\(5\\) provides the most information about lower ability levels (the peak of its IIC is farthest to the left). We have seen that all ICCs and IICs for the items have the same shape in the 1PL model (i.e. all items are equally good at providing information about the latent trait). In the 2PL and 3PL models, we will see that this does not have to be the case.\nNext, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good covereage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\nlines(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.5472  0.5710  0.7669  0.7756  0.9305  1.0786\n\n\nWe see that this test provides the most information about low ability levels (the peak is around ability level \\(-1.5\\)), and less information about very high ability levels."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#assess-fit",
    "href": "tutorials/2020-03-21-irt-jags/index.html#assess-fit",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Assess fit",
    "text": "Assess fit\nWe perform posterior predictive checks to test whether individual items fit the 1PLM by comparing quantities computed from the predictions of the model with those from the observed data. If these match reasonably well, then there is indication that the model has a good fit.\n\nlibrary(bayesplot)\nlibrary(ggplot2)\nY.rep&lt;-m1.r2jags$BUGSoutput$sims.list$Y.rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nWe can conclude by summarising and plotting the latent ability scores of the participants\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \nNA -2.074587 -0.483828  0.078138  0.001189  0.692185  0.763989\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model-1",
    "href": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model-1",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 2PLM to the data using the following JAGS code.\n\nmodel2&lt;-\"\nmodel {\n for (i in 1:n){\n  for (j in 1:p){\n   Y[i , j] ~ dbern ( prob [i , j])\n   logit ( prob [i , j]) &lt;- alpha [j] * ( theta [i] - delta [j])\n  }\n  theta [i] ~ dnorm (0.0 , 1.0)\n }\n\n for (j in 1:p){\n  delta [j] ~ dnorm (m.delta , pr.delta )\n  alpha [j] ~ dnorm (m.alpha , pr.alpha ) T(0 , )\n }\n  pr.delta &lt;- pow(s.delta , -2)\n  pr.alpha &lt;- pow(s.alpha , -2)\n  \n  m.delta ~ dnorm(0,5)\n  m.alpha ~ dnorm(0,5)\n  s.delta ~ dunif(0,10)\n  s.alpha ~ dunif(0,10)\n  \n  for(i in 1:n){\n   for(j in 1:p){\n    Y.rep[i, j] ~ dbern(prob[i, j])\n    loglik_y[i, j]&lt;-logdensity.bern(Y[i,j], prob[i, j])\n   }\n  }\n}\n\"\n## write the model to a text file\nwriteLines(model2, con = \"model2PLM.txt\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"alpha\",\"theta\", \"prob\",\"loglik_y\",\"Y.rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\nm2.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\n    model.file = \"model2PLM.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 5000\nNA    Unobserved stochastic nodes: 6014\nNA    Total graph size: 31024\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves-1",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves-1",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\ndiscr&lt;-m2.r2jags$BUGSoutput$sims.list$alpha\ndiff&lt;-m2.r2jags$BUGSoutput$sims.list$delta\n#see average value of item difficulty\napply(diff,2,mean)\n\nNA [1] -3.4898494 -1.4275914 -0.3210807 -1.8517091 -3.0020341\n\n#see average value of item discriminability\napply(discr,2,mean)\n\nNA [1] 0.8300819 0.7204114 0.7766516 0.7243196 0.7142145\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(m2.r2jags$BUGSoutput$sims.list$theta, 2, mean)\nprob&lt;-apply(m2.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nUnlike the ICCs for the 1PLM, the ICCs for the 2PLM do not all have the same shape. Item curves which are more “spread out” indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Compare this to an item with high discriminability (steep slope): for this item, we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong. Because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item \\(3\\) is still the most difficult item (i.e. lowest probability of getting correct for most latent trait values, up until about \\(\\theta=0.2\\)). Items \\(1\\) and \\(5\\) are the easiest."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves-1",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves-1",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation&lt;-prob*neg_prob\ninformation2&lt;-information*(apply(discr,2,mean))^2\nplot(theta,information2[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.3))\nlines(theta,information2[,1],col=\"red\")\nlines(theta,information2[,2],col=\"blue\")\nlines(theta,information2[,3],col=\"orange\")\nlines(theta,information2[,4],col=\"green\")\nlines(theta,information2[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nThe item IICs demonstrate that some items provide more information about latent ability for different ability levels. The higher the item discriminability estimate, the more information an item provides about ability levels around the point where there is a \\(50\\%\\) chance of getting the item right (i.e. the steepest point in the ICC slope). For example, item \\(3\\) (orange) clearly provides the most information at high ability levels, around \\(\\theta=-0.5\\), but almost no information about low ability levels (\\(&lt; -1\\)) because the item is already too hard for those participants. In contrast, item \\(1\\) (red), which has low discriminability, doesn’t give very much information overall, but covers a wide range of ability levels.\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information2,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\nlines(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.3242  0.3973  0.4500  0.4541  0.5182  0.7528\n\n\nThe IIC for the whole test shows that the test provides the most information for slightly-lower-than average ability levels (about \\(\\theta=-1\\)), but does not provide much information about extremely high ability levels."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#assess-fit-1",
    "href": "tutorials/2020-03-21-irt-jags/index.html#assess-fit-1",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Assess fit",
    "text": "Assess fit\nNext, we check how well the 2PLM fits the data.\n\nY.rep&lt;-m2.r2jags$BUGSoutput$sims.list$Y.rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")\n\n\n\n\n\n\n\n\nWe can also compare the fit of the 1PLM and 2PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\nlibrary(loo)\n#extract log-likelihood\nloglik_m1&lt;-m1.r2jags$BUGSoutput$sims.list$loglik_y\nloglik_m2&lt;-m2.r2jags$BUGSoutput$sims.list$loglik_y\n\n#waic\nwaic_m1&lt;-waic(loglik_m1)\nwaic_m2&lt;-waic(loglik_m2)\n\n#looic\nlooic_m1&lt;-loo(loglik_m1)\nlooic_m2&lt;-loo(loglik_m2)\n\n#compare\ntable_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1])\ntable_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1])\nrownames(table_waic)&lt;-rownames(table_looic)&lt;-c(\"1PLM\",\"2PLM\")\nknitr::kable(cbind(table_waic,table_looic), \"pandoc\", align = \"c\")\n\n\n\n\n\np_waic\nwaic\np_loo\nlooic\n\n\n\n\n1PLM\n1.525000\n6.517631\n1.867805\n7.203242\n\n\n2PLM\n1.395955\n6.446221\n1.649366\n6.953044\n\n\n\n\n\nBoth criteria suggest that the 2PLM has a slightly better fit to the data."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores-1",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores-1",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nPlot the density curve of the estimated ability scores\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \nNA -1.917888 -0.419394  0.078240  0.001822  0.608518  0.677354\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model-2",
    "href": "tutorials/2020-03-21-irt-jags/index.html#fit-the-model-2",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 3PLM to the data using the following JAGS code.\n\nmodel3&lt;-\"\nmodel {\n for (i in 1:n){\n  for (j in 1:p){\n   Y[i , j] ~ dbern ( prob [i , j])\n   logit ( prob.star [i , j]) &lt;- alpha [j]*( theta [i] - delta [j])\n   prob [i , j] &lt;- eta[j ] + (1 - eta[j]) * prob.star [i , j]\n  }\n  theta [i] ~ dnorm (0.0 , 1.0)\n }\n\n for (j in 1:p){\n  delta [j] ~ dnorm (m.delta , pr.delta )\n  alpha [j] ~ dnorm (m.alpha , pr.alpha ) T(0 , )\n  eta[j] ~ dbeta (a.eta , b.eta)\n }\n  pr.delta &lt;- pow(s.delta , -2)\n  pr.alpha &lt;- pow(s.alpha , -2)\n  \n  m.delta ~ dnorm(0,5)\n  m.alpha ~ dnorm(0,5)\n  s.delta ~ dunif(0,10)\n  s.alpha ~ dunif(0,10)\n  a.eta ~ dunif(0,100)\n  b.eta ~ dunif(0,100)\n    \n  for(i in 1:n){\n   for(j in 1:p){\n    Y.rep[i, j] ~ dbern(prob[i, j])\n    loglik_y[i, j]&lt;-logdensity.bern(Y[i,j], prob[i, j])\n   }\n  }\n}\n\"\n## write the model to a text file\nwriteLines(model3, con = \"model3PLM.txt\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"alpha\", \"eta\",\"theta\", \"prob\",\"loglik_y\",\"Y.rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function.\n\nm3.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\n    model.file = \"model3PLM.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 5000\nNA    Unobserved stochastic nodes: 6021\nNA    Total graph size: 41037\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves-2",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-characteristic-curves-2",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\ndiscr&lt;-m3.r2jags$BUGSoutput$sims.list$alpha\ndiff&lt;-m3.r2jags$BUGSoutput$sims.list$delta\ngues&lt;-m3.r2jags$BUGSoutput$sims.list$eta\n#see average value of item difficulty\napply(diff,2,mean)\n\nNA [1] -2.8361776 -0.6672983  0.3323608 -1.0648926 -2.1758250\n\n#see average value of item discriminability\napply(discr,2,mean)\n\nNA [1] 0.9249227 1.1278880 1.4997368 0.8733855 0.8561385\n\n#see average value of item guessing\napply(gues,2,mean)\n\nNA [1] 0.2446947 0.2360422 0.2150625 0.2438348 0.2395731\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(m3.r2jags$BUGSoutput$sims.list$theta, 2, mean)\nprob&lt;-apply(m3.r2jags$BUGSoutput$sims.list$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nThe slopes of the ICCs look very similar to those of the 2PLM. We can see that all items have \\(y\\)-intercepts greater than zero, so that even at very low ability levels, there is some chance of getting these items correct (via guessing)."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves-2",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-the-item-information-curves-2",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\nHere I plot the IICs using points, rather than lines, to better display the patterns of the individuals, which vary substantially according to whether the item was correctly chosen due to chance or not.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation.p1&lt;-neg_prob/prob\ninformation.p2&lt;-(prob-apply(gues,2,mean))^2/(1-apply(gues,2,mean))^2\ninformation3&lt;-(apply(discr,2,mean))^2*(information.p2)*(information.p1)\nplot(theta,information3[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.7))\npoints(theta,information3[,1],col=\"red\")\npoints(theta,information3[,2],col=\"blue\")\npoints(theta,information3[,3],col=\"orange\")\npoints(theta,information3[,4],col=\"green\")\npoints(theta,information3[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information3,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\npoints(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.3793  0.4489  0.5062  0.6978  0.7956  1.5096"
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#assess-fit-2",
    "href": "tutorials/2020-03-21-irt-jags/index.html#assess-fit-2",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Assess fit",
    "text": "Assess fit\nNext, we check how well the 3PLM fits the data.\n\nY.rep&lt;-m3.r2jags$BUGSoutput$sims.list$Y.rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")\n\n\n\n\n\n\n\n\nWe can also compare the fit of the 1PLM, 2PLM and 3PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\n#extract log-likelihood\nloglik_m3&lt;-m3.r2jags$BUGSoutput$sims.list$loglik_y\n\n#waic\nwaic_m3&lt;-waic(loglik_m3)\n\n#looic\nlooic_m3&lt;-loo(loglik_m3)\n\n#compare\ntable_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1],waic_m3$estimates[2:3,1])\ntable_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1],looic_m3$estimates[2:3,1])\nrownames(table_waic)&lt;-rownames(table_looic)&lt;-c(\"1PLM\",\"2PLM\",\"3PLM\")\nknitr::kable(cbind(table_waic,table_looic), \"pandoc\", align = \"c\")\n\n\n\n\n\np_waic\nwaic\np_loo\nlooic\n\n\n\n\n1PLM\n1.525000\n6.517631\n1.867805\n7.203242\n\n\n2PLM\n1.395955\n6.446221\n1.649366\n6.953044\n\n\n3PLM\n1.368535\n6.395247\n1.654432\n6.967042\n\n\n\n\n\nBoth criteria suggest that both 1PLM and 2PLM have a better fit to the data than 3PLM."
  },
  {
    "objectID": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores-2",
    "href": "tutorials/2020-03-21-irt-jags/index.html#plot-ability-scores-2",
    "title": "Item Response Theory Models (JAGS)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nPlot the density curve of the estimated ability scores\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \nNA -1.7435119 -0.4170588  0.0471025  0.0006053  0.6298597  0.7189831\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  },
  {
    "objectID": "tutorials/2020-02-15-glmm-stan/index.html",
    "href": "tutorials/2020-02-15-glmm-stan/index.html",
    "title": "Generalised Linear Mixed Models (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-15-glmm-stan/index.html#data-check",
    "href": "tutorials/2020-02-15-glmm-stan/index.html#data-check",
    "title": "Generalised Linear Mixed Models (Stan)",
    "section": "Data check",
    "text": "Data check\nHow are the number of points for each team in a volleyball match distributed? Well, let’s start by assuming that both teams have many chances at making a point and that each team have the same probability of scoring each point chance. Given these assumptions the distribution of the number of points for each team should be well captured by a Poisson distribution. A quick and dirty comparison between the actual distribution of the number of scored goals and a Poisson distribution having the same mean number of scored goals support this notion.\n\npar(mfcol = c(2, 1), mar = rep(2.2, 4))\nhist(c(data$y2, data$y1), xlim = c(40, 120), breaks = 8,main = \"Distribution of the number of points\\nscored by a team in a match.\")\nmean_goals &lt;- mean(c(data$y2, data$y1))\nhist(rpois(9999, mean_goals), xlim = c(40, 120), breaks = 8 ,main = \"Random draw from a Poisson distribution with\\nthe same mean as the distribution above.\")"
  },
  {
    "objectID": "tutorials/2020-02-15-glmm-stan/index.html#model-fitting",
    "href": "tutorials/2020-02-15-glmm-stan/index.html#model-fitting",
    "title": "Generalised Linear Mixed Models (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nAll teams aren’t equally good and it will be assumed that all teams have a latent skill variable and the skill of a team minus the skill of the opposing team defines the predicted outcome of a game. As the number of goals are assumed to be Poisson distributed it is natural that the skills of the teams are on the log scale of the mean of the distribution. The distribution of the number of goals for the home team \\(i\\) when facing the away team \\(j\\) is then\n\\[\n\\text{Points} \\sim \\text{Pois}(\\lambda)\n\\]\nwhere \\(\\log(\\lambda)=\\mu + \\text{home} + \\text{skill}_i - \\text{skill}_j\\). \\(\\mu\\) is a constant, while home is the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The point outcome of a match between home team \\(i\\) and away team \\(j\\) is modeled as:\n\\[\n\\text{HomePoins}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{home},ij}),\n\\]\n\\[\n\\text{AwayPoints}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{away},ij}),\n\\]\nwhere\n\\[\n\\log(\\lambda_{\\text{home},ij}) = \\mu + \\text{home} + \\text{skill}_i - \\text{skill}_j,\n\\]\n\\[\n\\log(\\lambda_{\\text{away},ij}) = \\mu + \\text{skill}_j - \\text{skill}_i.\n\\]\nThe skill parameters for the home and away teams are specified as a function of a set of attack and defense skills, which in turn are a linear function of different in-game statistics including the number of attacks, digs, serves and blocks for each team:\n\\[\n\\text{skill}_i = \\alpha_{0i} + \\alpha_{1i}\\text{attacks} + \\alpha_{2i}\\text{serves} + ,\n\\]\n\\[\n\\text{skill}_j = \\beta_{0j} + \\beta_{1j}\\text{digs} + \\beta_{2j}\\text{blocks},\n\\]\nThe distribution of two indicator variables, related to whether or not the fifth set was played (\\(d^s\\)) and whether the home team was the winner (\\(d^g\\)) in each match, are also included in the model. These are modelled as:\n\\[\nd^s \\sim \\text{Bernoulli}(\\pi^s) \\;\\;\\; \\text{and} \\;\\;\\; d^g \\sim \\text{Bernoulli}(\\pi^g),\n\\]\nwhere the corresponding probabilities are specified as\n\\[\n\\text{logit}(\\pi^s) = \\gamma_{0} + \\gamma_1\\text{HomePoins}_{i} + \\gamma_2\\text{AwayPoins}_{j},\n\\]\n\\[\n\\text{logit}(\\pi^g) = \\delta_{0} + \\delta_1\\text{HomePoins}_{i} + \\delta_2\\text{AwayPoins}_{j} + \\delta_3d^g.\n\\]\nI set the prior distributions over \\(\\mu\\) and \\(\\text{home}\\) to:\n\\[\n\\text{home} \\sim N(0, 10000) \\;\\;\\; \\text{and} \\;\\;\\; \\mu \\sim N(0, 10000),\n\\]\nweakly informative priors on \\(\\boldsymbol \\gamma\\), \\(\\boldsymbol \\delta\\), and set the priors on the skill of all \\(n\\) teams using a hierarchical approach to :\n\\[\n\\text{skill}_{1,\\ldots,n} \\sim N(\\mu_{\\text{teams}}, \\sigma^2_{\\text{teams}}),\n\\]\nso that teams are assumed to have similar but not identical mean and variance parameters for thier skill parameters. Turning this into a Stan model requires some minor adjustments. I have to “anchor” the sum of team skills to a constant otherwise the mean skill can drift away freely (sum to zero constraint) and the model cannot be identified. Doing these adjustments results in the following model description:\n\nrstanString&lt;-\"\ndata{\nint&lt;lower=1&gt; nteams; // number of teams\nint&lt;lower=1&gt; ngames; // number of games\nint&lt;lower=1, upper=nteams&gt; home_team[ngames]; // home team ID (1, ..., 12)\nint&lt;lower=1, upper=nteams&gt; away_team[ngames]; // away team ID (1, ..., 12)\nvector [ngames] att_eff1; // in game statistics for number of attacks (home)\nvector [ngames] ser_eff1; // in game statistics for number of serves (home)\nvector [ngames] def_eff2; // in game statistics for number of digs (away)\nvector [ngames] blo_eff2; // in game statistics for number of blocks (away)\nvector [ngames] att_eff2; // in game statistics for number of attacks (away)\nvector [ngames] ser_eff2; // in game statistics for number of serves (away)\nvector [ngames] def_eff1; // in game statistics for number of digs (home)\nvector [ngames] blo_eff1; // in game statistics for number of blocks (home)\nint&lt;lower=0&gt; y1[ngames]; // number of points scored by home team\nint&lt;lower=0&gt; y2[ngames]; // number of points scored by away team\nint&lt;lower=0, upper=1&gt; ds[ngames]; // indicator for number of sets played (3/4 or 5)\nint&lt;lower=0, upper=1&gt; dg[ngames]; // indicator for winning of the match for home team\n}\nparameters{\nreal home;\nreal mu;\nreal mu0_att;\nreal mu0_def;\nreal&lt;lower=0&gt; sigma0_att;\nreal&lt;lower=0&gt; sigma0_def;\nreal mu1_att;\nreal mu1_def;\nreal&lt;lower=0&gt; sigma1_att;\nreal&lt;lower=0&gt; sigma1_def;\nreal mu1_ser;\nreal mu1_blo;\nreal&lt;lower=0&gt; sigma1_ser;\nreal&lt;lower=0&gt; sigma1_blo;\nvector [nteams] beta0_att_star;\nvector [nteams] beta1_att_star;\nvector [nteams] beta1_ser_star;\nvector [nteams] beta0_def_star;\nvector [nteams] beta1_def_star;\nvector [nteams] beta1_blo_star;\nreal gamma[3];\nreal delta[4];\n}\ntransformed parameters{\n//Trick to code the sum-to-zero constraint\nvector [nteams] beta0_att;\nvector [nteams] beta1_att;\nvector [nteams] beta1_ser;\nvector [nteams] beta0_def;\nvector [nteams] beta1_def;\nvector [nteams] beta1_blo;\nvector&lt;lower=0&gt;[ngames] theta1;\nvector&lt;lower=0&gt;[ngames] theta2;\nbeta0_att = beta0_att_star - mean(beta0_att_star);\nbeta1_att = beta1_att_star - mean(beta1_att_star);\nbeta1_ser = beta1_ser_star - mean(beta1_ser_star);\nbeta0_def = beta0_def_star - mean(beta0_def_star);\nbeta1_def = beta1_def_star - mean(beta1_def_star);\nbeta1_blo = beta1_blo_star - mean(beta1_blo_star);\n for (g in 1:ngames) {\n    theta1[g] = exp(home + mu + beta0_att[home_team[g]] + beta1_att[home_team[g]]*att_eff1[g] + beta1_ser[home_team[g]]*ser_eff1[g] + \n             beta0_def[away_team[g]] + beta1_def[away_team[g]]*def_eff2[g] + beta1_blo[away_team[g]]*blo_eff2[g]); \n    theta2[g] = exp(mu + beta0_att[away_team[g]] + beta1_att[away_team[g]]*att_eff2[g] + beta1_ser[away_team[g]]*ser_eff2[g] + \n             beta0_def[home_team[g]] + beta1_def[home_team[g]]*def_eff1[g] + beta1_blo[home_team[g]]*blo_eff1[g]); \n }\n}\nmodel {\n//priors\nhome ~ normal(0, 100);\nmu ~ normal(0, 100);\nmu0_att ~ normal(0, 100);\nmu0_def ~ normal(0, 100);\nsigma0_att ~ uniform(0, 100);\nsigma0_def ~ uniform(0, 100);\nmu1_att ~ normal(0, 100);\nmu1_def ~ normal(0, 100);\nsigma1_att ~ uniform(0, 100);\nsigma1_def ~ uniform(0, 100);\nmu1_ser ~ normal(0, 100);\nmu1_blo ~ normal(0, 100);\nsigma1_ser ~ uniform(0, 100);\nsigma1_blo ~ uniform(0, 100);\ngamma ~ normal(0, 100);\ndelta ~ normal(0, 100);\nbeta0_att_star ~ normal(mu0_att, sigma0_att);\nbeta0_def_star ~ normal(mu0_def, sigma0_def);\nbeta1_att_star ~ normal(mu1_att, sigma1_att);\nbeta1_def_star ~ normal(mu1_def, sigma1_def);\nbeta1_ser_star ~ normal(mu1_ser, sigma1_ser);\nbeta1_blo_star ~ normal(mu1_blo, sigma1_blo);\n// likelihood\n for (g in 1:ngames) {\n  y1[g] ~ poisson(theta1[g]);\n  y2[g] ~ poisson(theta2[g]);\n  ds[g] ~ bernoulli_logit(gamma[1] + gamma[2]*y1[g] + gamma[3]*y2[g]);\n  dg[g] ~ bernoulli_logit(delta[1] + delta[2]*y1[g] + delta[3]*y2[g] + delta[4]*ds[g]);\n }\n}\ngenerated quantities{\n// loglikelihood \nvector[ngames] loglik_y1;\nvector[ngames] loglik_y2;\nvector[ngames] loglik_ds;\nvector[ngames] loglik_dg;\n for (g in 1:ngames) {\n  loglik_y1[g] = poisson_lpmf(y1[g]| theta1[g]);\n  loglik_y2[g] = poisson_lpmf(y2[g]| theta2[g]);\n  loglik_ds[g] = bernoulli_logit_lpmf(ds[g]| gamma[1] + gamma[2]*y1[g] + gamma[3]*y2[g]);\n  loglik_dg[g] = bernoulli_logit_lpmf(dg[g]| delta[1] + delta[2]*y1[g] + delta[3]*y2[g] + delta[4]*ds[g]);\n }\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"Modelbasic.stan\")\n\nNext, we put the data into a list to be passed to Stan, define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\n#prepare data\ny1&lt;-data$y1\ny2&lt;-data$y2\nngames&lt;-max(data$Game)\nnteams&lt;-max(data$h)\nhome_team&lt;-data$h\naway_team&lt;-data$a\n\natt_eff1&lt;-data$atteff1\natt_eff2&lt;-data$atteff2\nser_eff1&lt;-data$sereff1\nser_eff2&lt;-data$sereff2\nblo_eff1&lt;-data$bloeff1\nblo_eff2&lt;-data$bloeff2\ndef_eff1&lt;-data$defeff1\ndef_eff2&lt;-data$defeff2\n\natt_eff1&lt;-atteff1.cen\natt_eff2&lt;-atteff2.cen\nser_eff1&lt;-sereff1.cen\nser_eff2&lt;-sereff2.cen\nblo_eff1&lt;-bloeff1.cen\nblo_eff2&lt;-bloeff2.cen\ndef_eff1&lt;-defeff1.cen\ndef_eff2&lt;-defeff2.cen\n\nds&lt;-ifelse(data$settot==5,1,0)\ndg&lt;-ifelse(data$set1&gt;data$set2,1,0)\n\n#pre-processing\ndatalist &lt;- list(y1=y1,y2=y2,ngames=ngames,nteams=nteams,home_team=home_team,away_team=away_team,att_eff1=att_eff1,att_eff2=att_eff2,def_eff1=def_eff1,def_eff2=def_eff2,ser_eff1=ser_eff1,ser_eff2=ser_eff2,blo_eff1=blo_eff1,blo_eff2=blo_eff2,ds=ds, dg=dg)\nparams &lt;- c(\"mu\",\"home\",\"gamma\",\"delta\",\"beta0_att\",\"beta0_def\",\n            \"beta1_att\",\"beta1_def\",\"beta1_ser\",\"beta1_blo\",\"theta1\",\n            \"theta2\",\"loglik_y1\",\"loglik_y2\",\"loglik_ds\",\"loglik_dg\")\nburnInSteps = 500\nnChains = 2\nnumSavedSteps = 2000\nthinSteps = 1\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Run the Stan code via the rstan package and the stan function.\n\nmodel_stan&lt;- stan(data = datalist, file = \"Modelbasic.stan\", \n                       chains = nChains, pars = params, iter = nIter, \n                       warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.000182 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.82 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)\nNA Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)\nNA Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)\nNA Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)\nNA Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)\nNA Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)\nNA Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)\nNA Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 62.452 seconds (Warm-up)\nNA Chain 1:                67.426 seconds (Sampling)\nNA Chain 1:                129.878 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.00014 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.4 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)\nNA Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)\nNA Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)\nNA Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)\nNA Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)\nNA Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)\nNA Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)\nNA Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 61.322 seconds (Warm-up)\nNA Chain 2:                75.791 seconds (Sampling)\nNA Chain 2:                137.113 seconds (Total)\nNA Chain 2:\n\nprint(model_stan, pars =c(\"mu\",\"home\",\"gamma\",\"delta\",\"beta0_att\",\n                          \"beta0_def\",\"beta1_att\",\"beta1_def\",\n                          \"beta1_ser\",\"beta1_blo\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1000; warmup=500; thin=1; \nNA post-warmup draws per chain=500, total post-warmup draws=1000.\nNA \nNA                  mean se_mean    sd    2.5%     25%     50%    75%  97.5% n_eff\nNA mu               4.44    0.00  0.01    4.42    4.43    4.44   4.45   4.47    75\nNA home             0.03    0.00  0.02    0.00    0.02    0.03   0.05   0.06    55\nNA gamma[1]      -111.64    8.40 37.84 -204.58 -129.36 -102.85 -85.91 -58.49    20\nNA gamma[2]         0.59    0.05  0.25    0.22    0.41    0.54   0.71   1.19    23\nNA gamma[3]         0.56    0.04  0.18    0.29    0.42    0.53   0.67   0.96    25\nNA delta[1]       -12.56    0.98  5.61  -23.44  -16.21  -12.60  -8.90  -1.70    33\nNA delta[2]         0.48    0.02  0.11    0.30    0.41    0.48   0.56   0.73    36\nNA delta[3]        -0.34    0.01  0.07   -0.49   -0.38   -0.33  -0.29  -0.21    61\nNA delta[4]        -3.28    0.24  1.49   -6.06   -4.34   -3.26  -2.27  -0.47    40\nNA beta0_att[1]    -0.01    0.00  0.02   -0.06   -0.03   -0.01   0.01   0.03   954\nNA beta0_att[2]     0.02    0.00  0.02   -0.02    0.00    0.02   0.03   0.06  1000\nNA beta0_att[3]     0.02    0.00  0.02   -0.02    0.01    0.02   0.04   0.06   931\nNA beta0_att[4]     0.00    0.00  0.02   -0.04   -0.01    0.00   0.02   0.05   949\nNA beta0_att[5]    -0.07    0.00  0.03   -0.13   -0.09   -0.07  -0.06  -0.02   682\nNA beta0_att[6]    -0.09    0.00  0.03   -0.14   -0.11   -0.09  -0.07  -0.04   717\nNA beta0_att[7]     0.03    0.00  0.02   -0.02    0.01    0.03   0.04   0.08   985\nNA beta0_att[8]     0.14    0.00  0.03    0.08    0.12    0.14   0.16   0.21   361\nNA beta0_att[9]    -0.05    0.00  0.02   -0.10   -0.07   -0.05  -0.04  -0.01   780\nNA beta0_att[10]    0.01    0.00  0.02   -0.04   -0.01    0.01   0.03   0.06   793\nNA beta0_att[11]   -0.01    0.00  0.03   -0.07   -0.03   -0.01   0.00   0.04  1008\nNA beta0_att[12]    0.02    0.00  0.03   -0.04    0.00    0.02   0.04   0.08   821\nNA beta0_def[1]     0.04    0.00  0.02    0.00    0.03    0.04   0.06   0.09   740\nNA beta0_def[2]     0.06    0.00  0.03    0.01    0.04    0.06   0.08   0.11   835\nNA beta0_def[3]     0.08    0.00  0.02    0.04    0.07    0.08   0.10   0.13   951\nNA beta0_def[4]    -0.08    0.00  0.02   -0.13   -0.10   -0.08  -0.07  -0.03   759\nNA beta0_def[5]     0.03    0.00  0.02   -0.02    0.01    0.03   0.05   0.08   668\nNA beta0_def[6]     0.01    0.00  0.02   -0.03    0.00    0.02   0.03   0.06  1086\nNA beta0_def[7]     0.05    0.00  0.03    0.00    0.03    0.05   0.07   0.10   782\nNA beta0_def[8]    -0.01    0.00  0.03   -0.06   -0.02   -0.01   0.01   0.05   745\nNA beta0_def[9]    -0.03    0.00  0.02   -0.08   -0.05   -0.03  -0.02   0.01   850\nNA beta0_def[10]    0.03    0.00  0.02   -0.02    0.02    0.03   0.05   0.08   909\nNA beta0_def[11]    0.01    0.00  0.03   -0.04   -0.01    0.01   0.03   0.06   995\nNA beta0_def[12]   -0.20    0.00  0.03   -0.27   -0.22   -0.20  -0.17  -0.13   216\nNA beta1_att[1]     0.39    0.01  0.38   -0.38    0.15    0.39   0.63   1.12   972\nNA beta1_att[2]    -0.87    0.01  0.38   -1.64   -1.13   -0.86  -0.62  -0.17  1045\nNA beta1_att[3]     0.32    0.01  0.35   -0.32    0.07    0.30   0.55   1.05   920\nNA beta1_att[4]     0.20    0.01  0.36   -0.47   -0.04    0.20   0.43   0.91   954\nNA beta1_att[5]    -0.17    0.02  0.39   -0.93   -0.42   -0.17   0.07   0.65   673\nNA beta1_att[6]     0.95    0.01  0.35    0.26    0.72    0.96   1.18   1.64   705\nNA beta1_att[7]    -0.54    0.02  0.49   -1.52   -0.88   -0.53  -0.22   0.50   895\nNA beta1_att[8]    -2.14    0.03  0.57   -3.33   -2.52   -2.11  -1.77  -1.10   330\nNA beta1_att[9]     0.79    0.01  0.32    0.15    0.57    0.79   1.02   1.41   939\nNA beta1_att[10]    0.05    0.02  0.42   -0.79   -0.22    0.04   0.34   0.89   768\nNA beta1_att[11]    1.17    0.01  0.36    0.46    0.92    1.17   1.42   1.83   897\nNA beta1_att[12]   -0.16    0.00  0.05   -0.25   -0.19   -0.15  -0.12  -0.06   729\nNA beta1_def[1]     0.12    0.01  0.17   -0.20    0.00    0.11   0.22   0.45   894\nNA beta1_def[2]     0.02    0.00  0.13   -0.24   -0.06    0.03   0.11   0.29   814\nNA beta1_def[3]     0.00    0.00  0.14   -0.27   -0.09    0.01   0.09   0.28   834\nNA beta1_def[4]     0.29    0.01  0.17   -0.06    0.17    0.29   0.40   0.64   317\nNA beta1_def[5]     0.07    0.01  0.17   -0.25   -0.04    0.06   0.18   0.41   466\nNA beta1_def[6]     0.04    0.01  0.17   -0.28   -0.07    0.03   0.15   0.35   990\nNA beta1_def[7]    -0.18    0.01  0.20   -0.60   -0.31   -0.17  -0.05   0.20   479\nNA beta1_def[8]    -0.71    0.02  0.17   -1.01   -0.83   -0.72  -0.61  -0.37    92\nNA beta1_def[9]    -0.07    0.01  0.15   -0.35   -0.18   -0.06   0.03   0.23   866\nNA beta1_def[10]    0.03    0.01  0.14   -0.26   -0.05    0.03   0.13   0.30   696\nNA beta1_def[11]   -0.01    0.01  0.14   -0.29   -0.11   -0.01   0.09   0.28   804\nNA beta1_def[12]    0.40    0.01  0.15    0.11    0.29    0.39   0.49   0.70   194\nNA beta1_ser[1]     0.13    0.02  0.50   -0.80   -0.20    0.12   0.43   1.15   956\nNA beta1_ser[2]     0.49    0.02  0.51   -0.48    0.15    0.46   0.82   1.53   815\nNA beta1_ser[3]     0.72    0.02  0.48   -0.20    0.39    0.71   1.03   1.67   474\nNA beta1_ser[4]    -0.65    0.01  0.43   -1.50   -0.92   -0.64  -0.36   0.20   884\nNA beta1_ser[5]     1.27    0.02  0.44    0.39    0.97    1.26   1.57   2.18   569\nNA beta1_ser[6]    -0.39    0.02  0.61   -1.57   -0.78   -0.42   0.00   0.83   883\nNA beta1_ser[7]    -0.88    0.03  0.60   -2.13   -1.27   -0.87  -0.45   0.24   432\nNA beta1_ser[8]     0.00    0.02  0.54   -1.06   -0.35   -0.01   0.35   1.06   879\nNA beta1_ser[9]    -0.05    0.02  0.47   -0.92   -0.37   -0.07   0.28   0.93   951\nNA beta1_ser[10]   -0.60    0.02  0.49   -1.61   -0.90   -0.58  -0.27   0.37   771\nNA beta1_ser[11]    0.99    0.02  0.52   -0.01    0.63    0.99   1.35   1.99   526\nNA beta1_ser[12]   -1.03    0.02  0.46   -1.99   -1.33   -1.02  -0.71  -0.17   685\nNA beta1_blo[1]    -0.02    0.00  0.09   -0.20   -0.08   -0.02   0.05   0.17   981\nNA beta1_blo[2]    -0.21    0.01  0.12   -0.45   -0.28   -0.20  -0.12   0.00   404\nNA beta1_blo[3]    -0.02    0.00  0.06   -0.14   -0.06   -0.02   0.02   0.10   856\nNA beta1_blo[4]    -0.04    0.00  0.06   -0.17   -0.08   -0.04   0.00   0.08  1037\nNA beta1_blo[5]     0.16    0.00  0.10   -0.02    0.09    0.15   0.23   0.36   586\nNA beta1_blo[6]     0.01    0.00  0.06   -0.10   -0.03    0.01   0.05   0.12  1100\nNA beta1_blo[7]     0.08    0.00  0.07   -0.05    0.04    0.08   0.13   0.23   734\nNA beta1_blo[8]    -0.09    0.00  0.11   -0.30   -0.16   -0.09  -0.01   0.11   922\nNA beta1_blo[9]    -0.15    0.00  0.08   -0.32   -0.21   -0.15  -0.10   0.00   773\nNA beta1_blo[10]   -0.03    0.00  0.05   -0.14   -0.07   -0.03   0.01   0.08   968\nNA beta1_blo[11]   -0.01    0.00  0.09   -0.19   -0.07   -0.01   0.05   0.16   993\nNA beta1_blo[12]    0.31    0.01  0.12    0.09    0.23    0.30   0.39   0.57   263\nNA               Rhat\nNA mu            1.02\nNA home          1.02\nNA gamma[1]      1.05\nNA gamma[2]      1.03\nNA gamma[3]      1.06\nNA delta[1]      1.01\nNA delta[2]      1.01\nNA delta[3]      1.03\nNA delta[4]      1.01\nNA beta0_att[1]  1.00\nNA beta0_att[2]  1.00\nNA beta0_att[3]  1.00\nNA beta0_att[4]  1.00\nNA beta0_att[5]  1.00\nNA beta0_att[6]  1.00\nNA beta0_att[7]  1.00\nNA beta0_att[8]  1.00\nNA beta0_att[9]  1.00\nNA beta0_att[10] 1.00\nNA beta0_att[11] 1.00\nNA beta0_att[12] 1.00\nNA beta0_def[1]  1.00\nNA beta0_def[2]  1.00\nNA beta0_def[3]  1.00\nNA beta0_def[4]  1.00\nNA beta0_def[5]  1.00\nNA beta0_def[6]  1.00\nNA beta0_def[7]  1.00\nNA beta0_def[8]  1.00\nNA beta0_def[9]  1.00\nNA beta0_def[10] 1.00\nNA beta0_def[11] 1.00\nNA beta0_def[12] 1.00\nNA beta1_att[1]  1.00\nNA beta1_att[2]  1.00\nNA beta1_att[3]  1.00\nNA beta1_att[4]  1.00\nNA beta1_att[5]  1.00\nNA beta1_att[6]  1.00\nNA beta1_att[7]  1.00\nNA beta1_att[8]  1.00\nNA beta1_att[9]  1.00\nNA beta1_att[10] 1.00\nNA beta1_att[11] 1.00\nNA beta1_att[12] 1.00\nNA beta1_def[1]  1.00\nNA beta1_def[2]  1.00\nNA beta1_def[3]  1.00\nNA beta1_def[4]  1.01\nNA beta1_def[5]  1.00\nNA beta1_def[6]  1.00\nNA beta1_def[7]  1.01\nNA beta1_def[8]  1.03\nNA beta1_def[9]  1.00\nNA beta1_def[10] 1.00\nNA beta1_def[11] 1.00\nNA beta1_def[12] 1.01\nNA beta1_ser[1]  1.00\nNA beta1_ser[2]  1.00\nNA beta1_ser[3]  1.00\nNA beta1_ser[4]  1.00\nNA beta1_ser[5]  1.01\nNA beta1_ser[6]  1.00\nNA beta1_ser[7]  1.00\nNA beta1_ser[8]  1.00\nNA beta1_ser[9]  1.00\nNA beta1_ser[10] 1.00\nNA beta1_ser[11] 1.01\nNA beta1_ser[12] 1.00\nNA beta1_blo[1]  1.00\nNA beta1_blo[2]  1.00\nNA beta1_blo[3]  1.00\nNA beta1_blo[4]  1.00\nNA beta1_blo[5]  1.00\nNA beta1_blo[6]  1.00\nNA beta1_blo[7]  1.00\nNA beta1_blo[8]  1.00\nNA beta1_blo[9]  1.00\nNA beta1_blo[10] 1.00\nNA beta1_blo[11] 1.00\nNA beta1_blo[12] 1.00\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 14:06:07 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n\nThe diagnostic results are not perfect, perhaps I should run the algorithm a bit longer to get better results as a total of \\(1000\\) iterations does not seem enough. Here I do not bother to save time. To get an idea about posterior results I can plot at the average marginal mean offensive and defensive skills for each of the \\(12\\) teams in the league using the following code.\n\n#plot att vs def effext by team\nmodel_stan_par&lt;-extract(model_stan)\n\nbeta0.att&lt;-apply(model_stan_par$beta0_att, 2, mean)\nbeta0.def&lt;-apply(model_stan_par$beta0_def, 2, mean)\nnames&lt;-unique(data.frame(data$home.team,data$h)[order(data$h),][,1])\n\nplot(beta0.att,beta0.def, main = \"\", type = \"n\", xlab = \"Mean attack effect\", ylab = \"Mean defence effect\", xlim=c(-0.13,0.08), ylim=c(-0.18,0.1))\npoints(beta0.att,beta0.def, pch=16, col=\"red\",cex = 1.2)\nabline(v=0)\nabline(h=0)\ntext(beta0.att,beta0.def,names,cex = 0.8, adj = c(0.4,1.3))\n\n\n\n\n\n\n\n\nDifferent clusters of teams can be easily detected and suggest a different performance of the teams based on their average offensive and defensive skills. Those associated with higher offensive (to the right) and lower defensive (to the bottom) effects are the ones with the best performance across the season."
  },
  {
    "objectID": "tutorials/2020-02-15-glmm-stan/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-15-glmm-stan/index.html#mcmc-diagnostics",
    "title": "Generalised Linear Mixed Models (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nUsing the generated MCMC samples I can now look at some diagnostic measures. For example, we can assess convergence of the chains using the function mcmc_combo in the package bayesplot which provides a summary of convergence diagnostics using different graphics. We consider the marginal skill parameters for four teams to give an example.\n\nmcmc_combo(model_stan, pars=c(\"beta0_att[1]\",\"beta0_att[2]\",\n                              \"beta0_att[3]\",\"beta0_att[4]\"))\n\n\n\n\n\n\n\nmcmc_combo(model_stan, pars=c(\"beta0_def[1]\",\"beta0_def[2]\",\n                              \"beta0_def[3]\",\"beta0_def[4]\"))"
  },
  {
    "objectID": "tutorials/2020-02-15-glmm-stan/index.html#model-validation",
    "href": "tutorials/2020-02-15-glmm-stan/index.html#model-validation",
    "title": "Generalised Linear Mixed Models (Stan)",
    "section": "Model validation",
    "text": "Model validation\nWe can finally assess the fit of the model to the data by computing posterior predictive checks, where we use the posterior values of the parameters to sample a large number of replications for the data. We then use these to generate different types of results and compare them with the actual results to detect possible misfits of the model.\n\n#obtain parameters to generate replications\ny1.pred&lt;-y2.pred&lt;-matrix(NA,length(model_stan_par$home),132)\nds.pred&lt;-dg.pred&lt;-matrix(NA,length(model_stan_par$home),132)\npi.s&lt;-pi.g&lt;-matrix(NA,length(model_stan_par$home),132)\ny1.mat&lt;-y2.mat&lt;-ds.mat&lt;-matrix(NA,length(model_stan_par$home),132)\nfor(i in 1:length(model_stan_par$home)){\n y1.mat[i,]&lt;-y1\n y2.mat[i,]&lt;-y2\n ds.mat[i,]&lt;-ds\n pi.s[i,]&lt;-inv.logit(model_stan_par$gamma[i,1] + model_stan_par$gamma[i,2]*y1.mat[i,] + model_stan_par$gamma[i,3]*y2.mat[i,])\n pi.g[i,]&lt;-inv.logit(model_stan_par$delta[i,1] + model_stan_par$delta[i,2]*y1.mat[i,] + model_stan_par$delta[i,3]*y2.mat[i,] + model_stan_par$delta[i,4]*ds.mat[i,])\n}\n\n#generate predictions\nset.seed(3456)\nfor(i in 1:length(model_stan_par$home)){\n  y1.pred[i,]&lt;-rpois(n=132,lambda = model_stan_par$theta1[i,])\n  y2.pred[i,]&lt;-rpois(n=132,lambda = model_stan_par$theta2[i,])\n  ds.pred[i,]&lt;-rbinom(n=132, size = 1,prob = pi.s[i,])\n  dg.pred[i,]&lt;-rbinom(n=132, size = 1,prob = pi.g[i,])\n}\n\n#compute prediction points\nresults&lt;-list()\nfor(i in 1:1000){\n  results[[i]]&lt;-data.frame(y1.pred[i,],y2.pred[i,],ds.pred[i,],dg.pred[i,],data$h,data$a) \n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==0 & results[[i]]$dg.pred.i...==1,3,0)\n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==1,2,results[[i]]$points.home)\n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==0,1,results[[i]]$points.home)\n  \n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==0 & results[[i]]$dg.pred.i...==0,3,0)\n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==0,2,results[[i]]$points.away)\n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==1,1,results[[i]]$points.away)\n}\n\n#compare results for scores by team\ntot.y1.list&lt;-tot.y2.list&lt;-list()\nfor(i in 1:1000){\n  tot.y1.list[[i]]&lt;-ddply(results[[i]], .(data.h), summarise, totscorehome=sum(y1.pred.i...))\n  tot.y2.list[[i]]&lt;-ddply(results[[i]], .(data.a), summarise, totscoreaway=sum(y2.pred.i...))\n}\ntot.y1.list.neg&lt;-tot.y2.list.neg&lt;-list()\nfor(i in 1:1000){\n  tot.y1.list.neg[[i]]&lt;-ddply(results[[i]], .(data.h), summarise, totscorehomeneg=sum(y2.pred.i...))\n  tot.y2.list.neg[[i]]&lt;-ddply(results[[i]], .(data.a), summarise, totscoreawayneg=sum(y1.pred.i...))\n}\n\n\ntot.y1.list.mat&lt;-tot.y2.list.mat&lt;-matrix(NA,1000,12)\ntot.y1.list.neg.mat&lt;-tot.y2.list.neg.mat&lt;-matrix(NA,1000,12)\nfor(i in 1:1000){\n  tot.y1.list.mat[i,]&lt;-tot.y1.list[[i]][,2]\n  tot.y2.list.mat[i,]&lt;-tot.y2.list[[i]][,2]\n  tot.y1.list.neg.mat[i,]&lt;-tot.y1.list.neg[[i]][,2]\n  tot.y2.list.neg.mat[i,]&lt;-tot.y2.list.neg[[i]][,2]\n}\ntot.y1.obs&lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y1))\ntot.y2.obs&lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y2))\ntot.y1.obs.neg&lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y2))\ntot.y2.obs.neg&lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y1))\n\n#compute prediction points\nresults&lt;-list()\nfor(i in 1:1000){\n  results[[i]]&lt;-data.frame(y1.pred[i,],y2.pred[i,],ds.pred[i,],dg.pred[i,],data$h,data$a) \n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==0 & results[[i]]$dg.pred.i...==1,3,0)\n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==1,2,results[[i]]$points.home)\n  results[[i]]$points.home&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==0,1,results[[i]]$points.home)\n  \n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==0 & results[[i]]$dg.pred.i...==0,3,0)\n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==0,2,results[[i]]$points.away)\n  results[[i]]$points.away&lt;-ifelse(results[[i]]$ds.pred.i...==1 & results[[i]]$dg.pred.i...==1,1,results[[i]]$points.away)\n}\n\n#compare results for scores by team\ntot.y1.list&lt;-tot.y2.list&lt;-list()\nfor(i in 1:1000){\n  tot.y1.list[[i]]&lt;-ddply(results[[i]], .(data.h), summarise, totscorehome=sum(y1.pred.i...))\n  tot.y2.list[[i]]&lt;-ddply(results[[i]], .(data.a), summarise, totscoreaway=sum(y2.pred.i...))\n}\ntot.y1.list.neg&lt;-tot.y2.list.neg&lt;-list()\nfor(i in 1:1000){\n  tot.y1.list.neg[[i]]&lt;-ddply(results[[i]], .(data.h), summarise, totscorehomeneg=sum(y2.pred.i...))\n  tot.y2.list.neg[[i]]&lt;-ddply(results[[i]], .(data.a), summarise, totscoreawayneg=sum(y1.pred.i...))\n}\n\n\ntot.y1.list.mat&lt;-tot.y2.list.mat&lt;-matrix(NA,1000,12)\ntot.y1.list.neg.mat&lt;-tot.y2.list.neg.mat&lt;-matrix(NA,1000,12)\nfor(i in 1:1000){\n  tot.y1.list.mat[i,]&lt;-tot.y1.list[[i]][,2]\n  tot.y2.list.mat[i,]&lt;-tot.y2.list[[i]][,2]\n  tot.y1.list.neg.mat[i,]&lt;-tot.y1.list.neg[[i]][,2]\n  tot.y2.list.neg.mat[i,]&lt;-tot.y2.list.neg[[i]][,2]\n}\ntot.y1.obs&lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y1))\ntot.y2.obs&lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y2))\ntot.y1.obs.neg&lt;-ddply(data, .(h), summarise, totscorehomeobs=sum(y2))\ntot.y2.obs.neg&lt;-ddply(data, .(a), summarise, totscorehomeobs=sum(y1))\n\n#scored\ntot.y.obs&lt;-tot.y1.obs[,2]+tot.y2.obs[,2]\ntot.y.pred&lt;-apply(tot.y1.list.mat,2,median)+apply(tot.y2.list.mat,2,median)\nres.y&lt;-cbind(tot.y.obs,tot.y.pred)\nrownames(res.y)&lt;-names\nres.y&lt;-round(res.y,digits = 0)\n\n#conceded\ntot.y.obs.neg&lt;-tot.y1.obs.neg[,2]+tot.y2.obs.neg[,2]\ntot.y.pred.neg&lt;-apply(tot.y1.list.neg.mat,2,median)+apply(tot.y2.list.neg.mat,2,median)\nres.y.neg&lt;-cbind(tot.y.obs.neg,tot.y.pred.neg)\nrownames(res.y.neg)&lt;-names\nres.y.neg&lt;-round(res.y.neg,digits = 0)\n\n#compare results for points\ndata.points.list&lt;-list()\nfor(i in 1:1000){\n  data.points.list[[i]]&lt;-data.frame(data$Game)\n  data.points.list[[i]]$Game&lt;-data$Game\n  data.points.list[[i]]$h&lt;-data$h\n  data.points.list[[i]]$a&lt;-data$a\n  data.points.list[[i]]$points.home&lt;-results[[i]]$points.home\n  data.points.list[[i]]$points.away&lt;-results[[i]]$points.away\n}\ntot.home.list&lt;-tot.away.list&lt;-tot.team.list&lt;-list()\nfor(i in 1:1000){\n  tot.home.list[[i]]&lt;-ddply(data.points.list[[i]], .(h), summarise, totpointhome=sum(points.home))\n  tot.away.list[[i]]&lt;-ddply(data.points.list[[i]], .(a), summarise, totpointaway=sum(points.away))\n}\n\nfor(i in 1:1000){\n  tot.team.list[[i]]&lt;-data.frame(levels(data$home.team), tot.home.list[[i]]$h)\n  tot.team.list[[i]]$tot.team&lt;-tot.home.list[[i]]$totpointhome + tot.away.list[[i]]$totpointaway\n  tot.team.list[[i]]$true.team&lt;-c(19, 39, 23, 50, 19, 11, 37, 51, 32, 33, 27, 50)\n  tot.team.list[[i]]&lt;-tot.team.list[[i]][order(tot.team.list[[i]]$tot.team, decreasing = TRUE),]\n  tot.team.list[[i]]$rank&lt;-rep(1:12)\n}\n\n#plot total scores by team\ntot.scores&lt;-matrix(NA,1000,12)\ncolnames(tot.scores)&lt;-names\nfor(i in 1:1000){\n  for(j in 1:12){\n    tot.scores[i,j]&lt;-tot.team.list[[i]]$tot.team[tot.team.list[[i]]$tot.home.list..i...h==j] \n  }\n}\ntot.scores.obs&lt;-c(19, 39, 23, 50, 19, 11, 37, 51, 32, 33, 27, 50)\ntot.scores.med&lt;-apply(tot.scores, 2, median)\ntot.scores.final&lt;-cbind(tot.scores.obs,tot.scores.med)\n\n#plot total wins \ntot.wins&lt;-matrix(NA,1000,12)\ncolnames(tot.wins)&lt;-names\nfor(i in 1:1000){\n  for(j in 1:12){\n    tot.wins[i,j]&lt;-length(data.points.list[[i]]$points.home[data.points.list[[i]]$points.home&gt;1 & \n                                                              data.points.list[[i]]$h==j]) + length(data.points.list[[i]]$points.away[data.points.list[[i]]$points.away&gt;1 & \n                                                                                                                                        data.points.list[[i]]$a==j])\n  }\n}\ntot.wins.obs&lt;-c(7,12,6,17,7,5,13,17,10,12,8,18)\ntot.wins.prop&lt;-tot.wins/22\ntot.wins.obs.prop&lt;-tot.wins.obs/22\ntot.wins.med&lt;-apply(tot.wins, 2, median)\ntot.wins.final&lt;-cbind(tot.wins.obs,tot.wins.med)\n\nHere, for each team, we compare the predicted and observed total number of points scored in the matches, the number of won/lost matches, and the league points scored based on a replicated and the original season results.\n\n#summarise pred results\nres.final.obs&lt;-cbind(res.y[,1],res.y.neg[,1],tot.wins.final[,1],tot.scores.final[,1])\nres.final.pred&lt;-cbind(res.y[,2],res.y.neg[,2],tot.wins.final[,2],tot.scores.final[,2])\nres.final&lt;-cbind(res.final.obs,res.final.pred)\ncolnames(res.final)&lt;-c(\"scored\",\"conc'd\",\"wins\",\"points\",\"scored\",\"conc'd\",\"wins\",\"points\")\nknitr::kable(res.final, \"pandoc\", align = \"c\")\n\n\n\n\n\nscored\nconc’d\nwins\npoints\nscored\nconc’d\nwins\npoints\n\n\n\n\nBergamo\n1848\n2025\n7\n19\n1846\n2019\n7\n20\n\n\nBusto Arsizio\n1999\n1927\n12\n39\n1996\n1918\n12\n37\n\n\nCasalmaggiore\n1922\n2051\n6\n23\n1914\n2035\n7\n23\n\n\nConegliano\n1960\n1696\n17\n50\n1958\n1704\n18\n50\n\n\nFilottrano\n1781\n1961\n7\n19\n1799\n1955\n6\n18\n\n\nLegnano\n1642\n1903\n5\n11\n1662\n1902\n4\n17\n\n\nMonza\n2003\n1943\n13\n37\n1999\n1936\n13\n38\n\n\nNovara\n1987\n1776\n17\n51\n1954\n1774\n17\n51\n\n\nPesaro\n1776\n1820\n10\n32\n1786\n1823\n11\n33\n\n\nPiacenza\n1888\n1939\n12\n33\n1887\n1934\n9\n30\n\n\nSan Casciano\n1807\n1881\n8\n27\n1808\n1878\n9\n29\n\n\nScandicci\n1865\n1556\n18\n50\n1862\n1583\n18\n51\n\n\n\n\n\nPredicted results (the last four columns in the table) are not so bad, especially when looking at the number of league points scored by the teams between the replicated and observed season which are quite similar. These results suggest that the model seems to predict in a reasonable way the league results. To further assess this aspect, we consider two plots. The first compares the ranking of the teams across a large number of replications of the season based on the number of wins, losses and league points gained for each team.\n\n#summarise pred results\nres.matrix&lt;-matrix(NA,length(tot.team.list),12)\ncolnames(res.matrix)&lt;-names\nfor(i in 1:1000){\n  res.matrix[i,1]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==1]\n  res.matrix[i,2]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==2]\n  res.matrix[i,3]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==3]\n  res.matrix[i,4]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==4]\n  res.matrix[i,5]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==5]\n  res.matrix[i,6]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==6]\n  res.matrix[i,7]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==7]\n  res.matrix[i,8]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==8]\n  res.matrix[i,9]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==9]\n  res.matrix[i,10]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==10]\n  res.matrix[i,11]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==11]\n  res.matrix[i,12]&lt;-tot.team.list[[i]]$rank[tot.team.list[[i]]$tot.home.list..i...h==12]\n}\n\n#create stacked barplot of results\ndata.barplot&lt;-data.frame(rep(1:c(1000*12)))\nnames(data.barplot)&lt;-c(\"Game\")\ndata.barplot$position&lt;-as.factor(c(res.matrix[,1],res.matrix[,2],res.matrix[,3],res.matrix[,4],\n                                   res.matrix[,5],res.matrix[,6],res.matrix[,7],res.matrix[,8],\n                                   res.matrix[,9],res.matrix[,10],res.matrix[,11],res.matrix[,12]))\ndata.barplot$team&lt;-rep(NA,1000*12)\ndata.barplot$team[1:1000]&lt;-rep(paste(names[1]),1000)\ndata.barplot$team[1001:2000]&lt;-rep(paste(names[2]),1000)\ndata.barplot$team[2001:3000]&lt;-rep(paste(names[3]),1000)\ndata.barplot$team[3001:4000]&lt;-rep(paste(names[4]),1000)\ndata.barplot$team[4001:5000]&lt;-rep(paste(names[5]),1000)\ndata.barplot$team[5001:6000]&lt;-rep(paste(names[6]),1000)\ndata.barplot$team[6001:7000]&lt;-rep(paste(names[7]),1000)\ndata.barplot$team[7001:8000]&lt;-rep(paste(names[8]),1000)\ndata.barplot$team[8001:9000]&lt;-rep(paste(names[9]),1000)\ndata.barplot$team[9001:10000]&lt;-rep(paste(names[10]),1000)\ndata.barplot$team[10001:11000]&lt;-rep(paste(names[11]),1000)\ndata.barplot$team[11001:12000]&lt;-rep(paste(names[12]),1000)\n#data.barplot$team&lt;-as.factor(data.barplot$team)\ndata.barplot$team &lt;-factor(data.barplot$team, levels = c(\"Novara\", \"Scandicci\",\"Conegliano\", \"Monza\",\"Busto Arsizio\",\n                                                         \"Pesaro\",\"Piacenza\", \"San Casciano\",\"Casalmaggiore\", \"Bergamo\",\n                                                         \"Filottrano\", \"Legnano\"))\n\n\n\ndata.barplot$match&lt;-c(rep(1,1000),rep(2,1000),rep(3,1000),rep(4,1000),rep(5,1000),rep(6,1000),\n                      rep(7,1000),rep(8,1000),rep(9,1000),rep(10,1000),rep(11,1000),rep(12,1000))\ndata.barplot$match&lt;-as.factor(data.barplot$match)\ndata.barplot$Game&lt;-rep(1,nrow(data.barplot))\ndata.barplot$area&lt;-ifelse(data.barplot$position==1|data.barplot$position==2|data.barplot$position==3,\"high\",\"middle\")\ndata.barplot$area&lt;-ifelse(data.barplot$position==12|data.barplot$position==11|data.barplot$position==10,\"low\",data.barplot$area)\ndata.barplot$area&lt;-as.factor(data.barplot$area)\ndata.barplot$area&lt;-ordered(data.barplot$area,levels=c(\"low\",\"middle\",\"high\"))\ndata.barplot$team&lt;-factor(data.barplot$team,levels = rev(levels(data.barplot$team)))\n\ndf.summary1&lt;-ddply(data.barplot,.(team,position),summarise,count=sum(Game), percent=sum(Game)/1000)\ndf.summary2&lt;-ddply(data.barplot,.(team,area),summarise,count=sum(Game), percent=sum(Game)/1000)\n\nggplot(df.summary1, aes(x=team, y=percent, fill=position)) +\n  geom_bar(stat=\"identity\", width = 0.7, colour=\"black\", lwd=0.1) +\n  geom_text(aes(label=ifelse(percent &gt;= 0.1, paste0(sprintf(\"%.0f\", percent*100),\"%\"),\"\")),\n            position=position_stack(vjust=0.5), colour=\"white\") +\n  coord_flip() + scale_y_continuous(labels = percent_format()) +\n  labs(y=\"\", x=\"\") + scale_fill_viridis(discrete = T) + \n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = \"white\"),\n        axis.line = element_line(colour = \"black\"))\n\n\n\n\n\n\n\n\nThe mean rankings of the teams are in line with those observed in the actual season, while also providing uncertainty about the chance of each team to end in a particular position in the league (only percentages above \\(10\\%\\) are shown for clarity).\nThe second plot compares the points trend throughout the season for each team with respect to the trend predicted by the model based on the replicated results for each match in the season.\n\n#########plot for cumlative points over simulated season\npoints.list&lt;-cum.points.list&lt;-list()\nfor(i in 1:nrow(res.matrix)){\n  points.list[[i]]&lt;-matrix(NA,22,12)\n  cum.points.list[[i]]&lt;-matrix(NA,22,12)\n  for(j in 1:12){\n    points.list[[i]][,j]&lt;-c(data.points.list[[i]]$points.home[data.points.list[[i]]$h==j],data.points.list[[i]]$points.away[data.points.list[[i]]$a==j])\n  }\n  colnames(points.list[[i]])&lt;-unique(levels(data$home.team))\n  rownames(points.list[[i]])&lt;-rep(1:22)\n  cum.points.list[[i]]&lt;-apply(points.list[[i]], 2, cumsum)\n}\n\n#########plot for cumlative points over simulated season\npoints.list&lt;-cum.points.list&lt;-list()\nfor(i in 1:nrow(res.matrix)){\n  points.list[[i]]&lt;-matrix(NA,22,12)\n  cum.points.list[[i]]&lt;-matrix(NA,22,12)\n  for(j in 1:12){\n    points.list[[i]][,j]&lt;-c(data.points.list[[i]]$points.home[data.points.list[[i]]$h==j],data.points.list[[i]]$points.away[data.points.list[[i]]$a==j])\n  }\n  colnames(points.list[[i]])&lt;-unique(levels(data$home.team))\n  rownames(points.list[[i]])&lt;-rep(1:22)\n  cum.points.list[[i]]&lt;-apply(points.list[[i]], 2, cumsum)\n}\n\n#plot cumulative points obs vs pred\nobs.cum.points&lt;-matrix(NA,22,12)\ncolnames(obs.cum.points)&lt;-unique(levels(data$home.team))\nrownames(obs.cum.points)&lt;-rep(1:22)\nobs.cum.points[1,]&lt;-c(0,1,0,3,3,3,0,2,0,0,3,3)\nobs.cum.points[2,]&lt;-c(0,3,0,3,0,0,3,3,0,3,0,3)\nobs.cum.points[3,]&lt;-c(0,2,3,3,0,0,0,3,3,1,0,3)\nobs.cum.points[4,]&lt;-c(0,3,0,3,0,0,3,3,1,2,0,3)\nobs.cum.points[5,]&lt;-c(0,3,0,3,0,3,0,3,3,0,0,3)\nobs.cum.points[6,]&lt;-c(0,3,1,3,0,0,0,3,1,3,2,2)\nobs.cum.points[7,]&lt;-c(2,1,3,2,0,1,1,3,2,2,1,0)\nobs.cum.points[8,]&lt;-c(0,2,1,1,0,0,3,2,3,2,1,3)\nobs.cum.points[9,]&lt;-c(3,3,1,3,0,2,2,1,0,0,3,0)\nobs.cum.points[10,]&lt;-c(1,2,1,2,1,2,1,1,2,2,1,2)\nobs.cum.points[11,]&lt;-c(0,1,0,3,0,0,3,3,0,3,3,2)\nobs.cum.points[12,]&lt;-c(3,0,0,3,0,0,3,3,3,0,0,3)\nobs.cum.points[13,]&lt;-c(0,1,3,3,2,0,3,2,1,3,0,0)\nobs.cum.points[14,]&lt;-c(2,3,1,3,0,0,0,3,3,0,0,3)\nobs.cum.points[15,]&lt;-c(2,1,0,3,2,0,3,1,0,3,0,3)\nobs.cum.points[16,]&lt;-c(0,3,0,3,0,0,0,3,3,0,3,3)\nobs.cum.points[17,]&lt;-c(2,0,3,0,0,3,3,1,0,3,0,3)\nobs.cum.points[18,]&lt;-c(0,0,0,3,3,1,2,3,1,2,3,0)\nobs.cum.points[19,]&lt;-c(3,3,3,2,0,0,3,0,0,1,0,3)\nobs.cum.points[20,]&lt;-c(0,0,2,0,3,1,0,3,3,0,3,3)\nobs.cum.points[21,]&lt;-c(0,3,0,1,3,0,2,2,0,1,3,3)\nobs.cum.points[22,]&lt;-c(1,1,1,0,2,0,2,3,3,2,1,2)\n\nobs.cum.points&lt;-apply(obs.cum.points, 2, cumsum)\n\npar(mar=c(2.1, 3.1, 3.1, 3.1))\npar(mfrow=c(4,3), mai = c(0.4, 0.4, 0.1, 0.2))\nfor(i in 1:12){\n  plot(rep(1:22),obs.cum.points[,i], axes = F, type = \"n\", xlab = \"games\", ylab = \"points\",xlim = c(0,23),ylim = c(0,54))\n  axis(1,at=c(0,5,10,15,20,25),labels = c(0,5,10,15,20,25))\n  axis(2,at=c(0,10,20,30,40,50),labels = c(0,10,20,30,40,50))\n  lines(rep(1:22),obs.cum.points[,i], lty=1,lwd=1,col=\"black\")\n  lines(rep(1:22), cum.points.list[[7]][,i], lty=1,lwd=1,col=\"red\")\n  text(5,50,unique(levels(names))[i],pos = 1, cex = 1)\n}\n\n\n\n\n\n\n\n\nThe observed trends (black lines) are pretty much matched by the trends predicted by the model (red lines) for most of the teams with few exceptions. This suggests that, although the model seems to have a good predictive ability, in some cases there are still margins of improvement. In my original paper I have improved the model using a different parameterisation where I tried to account for the dependence between the latent skill parameters using a scaled inverse Wishart distribution for the covariance matrix of the random effects. Here I do not consider this model here, which lead to some improvements compared with the basic Poisson model."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html",
    "href": "tutorials/2020-02-13-glm-jags/index.html",
    "title": "Generalised Linear Models (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#introduction",
    "href": "tutorials/2020-02-13-glm-jags/index.html#introduction",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nBefore discussing generalised linear models, we will first revise a couple of fundamental aspects of general linear models and in particular, how they restrict the usefulness of these models in clinical applications. General linear models provide a set of well adopted and recognised procedures for relating response variables to a linear combination of one or more continuous or categorical predictors (hence the “general”). Nevertheless, the reliability and applicability of such models are restricted by the degree to which the residuals conform to normality and the mean and variance are independent of one another. The general linear model essentially comprises three components.\n\\[\nE[Y] = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p + \\epsilon.\n\\]\n\nThe Random (Stochastic) component that specifies the conditional distribution (Normal or Gaussian distribution) of the response variable. Whilst the mean of the normal distribution is assumed to vary as a function of the linear predictors (Systematic component - the regression equation), the variance is assumed to remain constant. Denoted \\(\\epsilon\\) in the above equation, the random component is more formally defined as \\(Y_i \\sim N(0, \\sigma^2)\\). That is, each value of \\(Y\\) (the response) is assumed to be drawn from a normal distribution with different means (\\(\\mu_i\\)) yet fixed variance (\\(\\sigma^2\\)).\nThe Systematic component that represents the linear combination of predictors (which can be categorical, continuous, polynomial or other contrasts) for a linear predictor. The linear predictor describes (predict) the “expected” mean and variability of the response(s) (which are assumed to follow normal distribution(s)).\nThe Link function which links the expected values of the response (Random component) to the linear combination of predictors (systematic component). For the normal (Gaussian) distribution, the link function is a the “identity” link (\\(\\mu_i\\)). That is:\n\n\\[\n\\mu_i = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\n\\]\nThere are many real situations for which the assumptions imposed by the normal distribution are unlikely to be satisfied. For example, if the measured response to a predictor treatment (such as nest parasite load) can only be binary (such as abandoned or not), then the differences between the observed and expected values (residuals) are unlikely to follow a normal distribution. Instead, in this case, they should follow a binomial distribution.\nOften response variables have a restricted range. For example a species may be either present or not present and thus the response is restricted to either \\(1\\) (present) or absent (\\(0\\)). Values less than \\(0\\) or greater than \\(1\\) are not logical. Similarly, the abundance of a species in a quadrat is bounded by a minimum value of zero - it is not possible to have fewer than zero individuals. Proportional abundances are also restricted to between \\(0\\) and \\(1\\) (or \\(100\\)). The normal distribution however, is valid for the range between positive and negative infinity (ie not restricted) and thus expected values of the linear predictor can be outside of the restricted range that naturally operates on the response variable. Hence, the normal distribution might not always represent a sensible probability model as it can predict values outside the logical range of the data. Furthermore, the as a result of these range restrictions, variance can be tied to the mean in that expected probabilities towards the extremes of the restricted range tend to have lower variability (as the lower or upper bounds of the probabilities are trunctated)."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#data-types",
    "href": "tutorials/2020-02-13-glm-jags/index.html#data-types",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Data types",
    "text": "Data types\nResponse data can generally be classified into one of four levels\n\nNominal - responses are those that represent un-ordered categories For example, we could record the ‘preferred’ food choice of an animal as either “Fruit”, “Meat”, “Seeds” or “Leaves”. The spacing between categories is undetermined and responses are restricted to those options.\nOrdinal - responses are those that represent categories with sensible orders, yet undetermined spacing between categories. Likert scale questionnaire responses to questions such as “Rate the quality of your experience… on a scale of \\(1\\) to \\(5\\)” are a classic example. Categorized levels of a response (“High”, “Medium”,“Low”) would also be another example of an ordinal variable\nInterval - responses are those for which both the order and scale (spacing) are meaningful, yet multiplication is meaningless due to the arbitrary scale of the data (where zero does not refer to nothing). Temperature in degrees C is a good example of such a response (consider whether \\(-28\\) degrees \\(^\\star-1 = 28\\) degrees has a sensible interpretation).\nRatio - responses are those for which order, scale and zero are meaningful. For example a measurement scale such as length in millimeters or mass in grams."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#glms",
    "href": "tutorials/2020-02-13-glm-jags/index.html#glms",
    "title": "Generalised Linear Models (JAGS)",
    "section": "GLMs",
    "text": "GLMs\nGeneralized linear models (GLM’s) extend the application range of linear modelling by accommodating non-stable variances as well as alternative exponential residual distributions (such as the binomial and Poisson distributions). GLMs have the same three components as general linear models (of which the systematic component is identical), yet a broader range of Random components are accommodated and thus alternative Link functions must also be possible.\n\nRandom component defines the exponential distribution (Gaussian, Poisson, binomial, gamma, and inverse Gaussian distributions) from which the responses are assumed to be drawn. These distributions are characterised by some function of the mean (canonical or location parameter) and a function of the variance (dispersion parameter). Note that for binomial and Poisson distributions, the dispersion parameter is \\(1\\), whereas for the Guassian (normal) distribution the dispersion parameter is the error variance and is assumed to be independent of the mean. The negative binomial distribution can also be treated as an exponential distribution if the dispersion parameter is fixed as a constant.\nSystematic component again defines the linear combination of predictors\nLink function, \\(g(\\mu)\\) links the systematic and random components. Although there are many commonly employed link functions, typically the exact form of the link function depends on the nature of the random response distribution. Some of the canonical (natural choice) link functions and distribution pairings that are suitable for different forms of generalized linear models are listed in the following table. The only real restriction on a link function is that it must preserve the order of values such that larger values are always larger than smaller values (be monotonic) and must yield derivatives that are legal throughout the entire range of the data."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#link-functions",
    "href": "tutorials/2020-02-13-glm-jags/index.html#link-functions",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Link functions",
    "text": "Link functions\nIn contrast to fitting linear models to transformations of the raw data, the link functions transform the curve predicted by the systematic component into a scale approximating that of the response.\n\nLogit. Log odds-ratio The slope parameter represents the rate of change in log odds-ratio per unit increase in a predictor.\nProbit. The probit transformation is the inverse cumulative distribution for the standard normal distribution and is useful when the response is likely to be a categorization of an otherwise continuous scale. So whilst measurements might be recorded on a categorical scale (either for convenience or because that is how they manifest), these measurements are a proxy for an underlying variable (latent variable) that is actually continuous. So if the purpose of the linear modeling is to predict the underlying latent variable, then probit regression is likely to be appropriate. The slope parameter represents the rate of change in response probability per unit increase in a predictor.\nComplementary log-log. The log-log transformation is useful for extremely asymmetrical distributions (notably survival analyses)."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#estimation",
    "href": "tutorials/2020-02-13-glm-jags/index.html#estimation",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Estimation",
    "text": "Estimation\nThe generalized nature of GLM’s makes them incompatible with ordinary least squares model fitting procedures. Instead, parameter estimates and model fitting are typically achieved by maximum likelihood methods based on an iterative re-weighting algorithm (such as the Newton-Raphson algorithm). Essentially, the Newton-Raphson algorithm (also known as a scoring algorithm) fits a linear model to an adjusted response variable (transformed via the link function) using a set of weights and then iteratively re-fits the model with new sets of weights recalculated according to the fit of the previous iteration. For canonical link-distribution pairs (see the table above), the Newton-Raphson algorithm usually converges (arrives at a common outcome or equilibrium) very efficiently and reliably. The Newton-Raphson algorithm facilitates a unifying model fitting procedure across the family of exponential probability distributions thereby providing a means by which binary and count data can be incorporated into the suit of regular linear model designs. In fact, linear regression (including ANOVA, ANCOVA and other general linear models) can be considered a special form of GLM that features a normal distribution and identity link function and for which the maximum likelihood procedure has an exact solution. Notably, when variance is stable, both maximum likelihood and ordinary least squares yield very similar parameter estimates.\nTypical distributions used for GLMs include:\n\nGaussian.\nBinomial. Represents the number of successes out of \\(n\\) independent trials each with a set probability (typically \\(0.5\\))\nPoisson.\nNegative Binomial. Represents the number of failures out of a sequence of n independent trials before a success is obtained each with a set probability. Alternatively, a negative binomial can be defined in terms of its mean (\\(\\mu\\)) and dispersion parameter. The dispersion parameter can be used to adjust the variances independent of the mean and is therefore useful as an alternative to the Poisson distribution when there is evidence of overdispersion (dispersion parameter \\(&gt;1\\))."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#dispersion",
    "href": "tutorials/2020-02-13-glm-jags/index.html#dispersion",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Dispersion",
    "text": "Dispersion\nThe variance of binomial or Poisson distributions is assumed to be related to the sample size and mean respectively, and thus, there is not a variance parameter in their definitions. In fact, the variance (or dispersion) parameter is fixed to \\(1\\). As a result, logistic/probit regression as well as Poisson regression and log-linear modelling assume that sample variances conform to the respective distribution definitions. However, it is common for individual sampling units (e.g. individuals) to co-vary such that other, unmeasured influences, increase (or less commonly, decrease) variability. For example, although a population sex ratio might be 1:1, male to female ratios within a clutch might be highly skewed towards one or other sex. Positive correlations cause greater variance (overdispersion) and result in deflated standard errors (and thus exaggerated levels of precision and higher Type I errors). Additionally, count data (for example number of fish per transect) can be overdispersed as a result of an unexpectedly high number of zero’s (zero inflated). In this case, the zeros arise for two reasons.\n\nGenuine zero values - zero fish counted because there were non present.\nFalse zeros - there were fish present, yet not detected (and thus not recorded).\n\nThe dispersion parameter (degree of variance inflation or over-dispersion) can be estimated by dividing either the Pearsons \\(\\chi^2\\) or the Deviance by the degrees of freedom, where \\(n\\) is the number of observations in p parameters). As a general rule, dispersion parameters approaching \\(2\\) (or \\(0.5\\)) indicate possible violations of this assumption (although large overdispersion parameters can also be the result of a poorly specified model or outliers). Where over (or under) dispersion is suspected to be an issue, the following options are available:\n\nuse quasibinomial and quasipoisson families can be used as alternatives to model the dispersion. These quasi-likelihood models derive the dispersion parameter (function of the variance) from the observed data and are useful when overdispersion is suspected to be caused by positive correlations or other unobserved sources of variance. Rather than assuming that the variance is fixed, quasi- models assume that variance is a linear (multiplicative) function of the mean. Test statistics from such models should be based on F-tests rather than chi-squared tests.\nfor count data, use a negative binomial as an alternative to a Poisson distribution. The negative binomial distribution also estimates the dispersion parameter and assumes that the variance is a quadratic function of the mean.\nuse zero-inflated binomial (ZIB) and zero-inflated poisson (ZIP) when overdispersion is suspected to be caused by excessive numbers of zeros."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#binary-data---logistic-regression",
    "href": "tutorials/2020-02-13-glm-jags/index.html#binary-data---logistic-regression",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Binary data - logistic regression",
    "text": "Binary data - logistic regression\nLogistic regression is a form of GLM that employs the logit-binomial link distribution canonical pairing to model the effects of one or more continuous or categorical (with dummy coding) predictor variables on a binary (dead/alive, presence/absence, etc) response variable. For example, we could investigate the relationship between salinity levels (salt concentration) and mortality of frogs. Similarly, we could model the presence of a species of bird as a function of habitat patch size, or nest predation (predated or not) as a function of the distance from vegetative cover. Consider the fictitious data presented in the following figure. Clearly, a regular simple linear model is inappropriate for modelling the probability of presence. Note that at very low and high levels of \\(X\\), the predicted probabilities (probabilities or proportions of the population) are less than zero and greater than one respectively - logically impossible outcomes. Note also, that the residuals cannot be drawn from a normal distribution, since for any value of \\(X\\), there are only two possible outcomes (\\(1\\) or \\(0\\)).\nThe logistic model (Figure c above) relating the probability (\\(\\pi(x)\\)) that the response (\\(y_i\\)) equals one (present) for a given level of \\(x_i\\) (patch size) is defined as:\n\\[\n\\pi(x) = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}\n\\]\nAppropriately, since \\(e^{\\beta_0+\\beta_1x}\\) (the “natural constant” raised to a simple linear model) must evaluate to between 0 and infinity, the logistic model must asymptote towards (and is thus bounded by) zero and one. Alternatively (as described briefly above), the logit link function can be used to transform \\(\\pi(x)\\) such that the logistic model is expressed as the log odds (probability of one state relative to the alternative) against a familiar linear combination of the explanatory variables (as is linear regression).\n\\[\nln \\left(  \\frac{\\pi(x)}{1-\\pi(x)} \\right) = \\beta_0 + \\beta_1x_i\n\\]\nAlthough the \\(\\beta_0\\) (\\(y\\)-intercept) parameter is interpreted similar to that of linear regression (albeit of little clinical interest), this is not the case for the slope parameter (\\(\\beta_1\\)). Rather than representing the rate of change in the response for a given change in the predictor, in logistic regression, \\(\\beta_1\\) represents the rate of change in the odds ratio (ratio of odds of an event at two different levels of a predictor) for a given unit change in the predictor. The exponentiated slope represents the odds ratio (\\(\\theta=e^{\\beta_1}\\)), the proportional rate at which the predicted odds change for a given unit change of the predictor.\n\nNull hypotheses\nAs with linear regression, a separate \\(H_0\\) is tested for each of the estimated model parameters:\n\n\\(H_0:\\beta_1=0\\) (the population slope - proportional rate of change in odds ratio). This test examines whether the log odds of an occurrence are independent of the predictor variable and thus whether or not there is likely to be a relationship between the response and predictor.\n\\(H_0:\\beta_0=0\\) (the population intercept equals zero). As stated previously, this is typically of little clinical interest.\n\nSimilar to linear regression, there are two ways of testing the main null hypotheses:\n\nParameter estimation approach. Maximum likelihood estimates of the parameters and their asymptoticd standard errors (\\(S_{b1}\\)) are used to calculate the Wald \\(t\\) (or \\(t\\)-ratio) statistic \\(W=\\frac{b_1}{S_{b1}}\\), which approximately follows a standard \\(z\\) distribution when the null hypothesis is true. The reliability of Wald tests diminishes substantially with small sample sizes. For such cases, the second option is therefore more appropriate.\n(log)-likelihood ratio tests approach. This approach essentially involves comparing the fit of models with (full) and without (reduced) the term of interest:\n\n\\[\n\\text{logit}(\\pi) = \\beta_0 + \\beta_1x_1 \\;\\;\\; (\\text{full model})\n\\]\n\\[\n\\text{logit}(\\pi) = \\beta_0 \\;\\;\\; (\\text{reduced model})\n\\]\nThe fit of any given model is measured via log-likelihood and the differences between the fit of two models is described by a likelihood ratio statistic (G2 \\(= 2\\)(log-likelihood reduced model - log-likelihood full model)). The G2 quantity is also known as deviance and is analogous to the residual sums of squares in a linear model. When the null hypothesis is true, the G2 statistic approximately follows a \\(\\chi^2\\) distribution with one degree of freedom. An analogue of the linear model \\(r^2\\) measure can be calculated as:\n\\[\nr^2 = 1- \\frac{G^2_0}{G^2_1},\n\\]\nwhere \\(G^2_0\\) and \\(G^2_1\\) are the deviances due to the intercept and slope terms respectively. Analogous to the ANOVA table that partitions the total variation into components explained by each of the model terms (and the unexplained error), it is possible to construct a analysis of deviance table that partitions the deviance into components explained by each of the model terms."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#count-data---poisson-and-log-linear-models",
    "href": "tutorials/2020-02-13-glm-jags/index.html#count-data---poisson-and-log-linear-models",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Count data - Poisson and log-linear models",
    "text": "Count data - Poisson and log-linear models\nAnother form of data for which scale transformations are often unsuitable or unsuccessful are count data. Count data tend to follow a Poisson distribution (see here) and consequently, the mean and variance are usually related. Generalized linear models provide appropriate means to model count data according to two design contexts:\n\nas an alternative to linear regression for modeling count data against a linear combination of continuous and/or categorical predictor variables (Poisson regression)\nas an alternative to contingency tables in which the associations between categorical variables are explored (log-linear modelling)\n\nPoisson regression\nThe Poisson regression model is\n\\[\n\\log(\\mu)=\\beta_0 + \\beta_1x_1+ \\ldots + \\beta_px_p,\n\\]\nwhere \\(\\log(\\mu)\\) is the link function used to link the mean of the Poisson response variable to the linear combination of predictor variables. Poisson regression otherwise shares null hypotheses, parameter estimation, model fitting and selection with logistic regression.\nLog-linear modelling\nContingency tables were introduced along with caveats regarding the reliability and interoperability of such analyses (particularly when expected proportions are small or for multi-way tables). In contrast to logistic and Poisson regression, all variables in a log-linear model do not empirically distinguish between response and predictor variables. Nevertheless, as in contingency tables, causality can be implied when logical and justified by interpretation. The saturated (or full) log-linear model resembles a multiway ANOVA model. The full and reduced log-linear models for a two factor design are:\n\\[\n\\log(f_{ij}) = \\mu + \\gamma^A_i + \\gamma^B_j + \\gamma^{AB}_{ij} \\;\\;\\; (\\text{full model}),\n\\]\n\\[\n\\log(f_{ij}) = \\mu + \\gamma^A_i + \\gamma^B_j \\;\\;\\; (\\text{reduced model})\n\\]\nwhere \\(\\log(f_{ij}\\) is the log link function, \\(\\mu\\) is the mean of the (log) of expected frequencies (\\(f_{ij}\\)) and \\(\\gamma^A_i\\) is the effect of the ith category of the variable (A), \\(\\gamma^B_j\\) is the effect of the \\(j\\)-th category of B and \\(\\gamma^{AB}_{ij}\\) is the interactive effect of each category combination on the (log) expected frequencies. Reduced models differ from full models in the absence of all higher order interaction terms. Comparing the fit of full and reduced models therefore provides a means of assessing the effect of the interaction. Whilst two-way tables contain only a single interaction term (and thus a single full and reduced model), multiway tables have multiple interactions. For example, a three-way table has a three way interaction (ABC) as well as three two-way interactions (AB, AC, BC). Consequently, there are numerous full and reduced models, each appropriate for different interaction terms. The following table indicates the association between null hypothesis and fitted models.\n\nNull hypothese\nConsistent with contingency table analysis, log-linear models test the null hypothesis (\\(H_0\\)) that the categorical variables are independent of (not associated with) one another. Such null hypotheses are tested by comparing the fit (deviance, G2) of full and reduced models. The G2 is compared to a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in degrees of freedom of the full and reduced models. Thereafter, odds ratios are useful for interpreting any lack of independence. For multi-way tables, there are multiple full and reduced models:\n\nComplete dependence: \\(H_0: ABC = 0\\). No three way interaction. Either no association (conditional independence) between each pair of variables, or else the patterns of associations (conditional dependencies) are the same for each level of the third. If this null hypothesis is rejected (\\(ABC \\neq 0\\)), the causes of lack of independence can be explored by examining the residuals or odds ratios. Alternatively, main effects tests (testing the effects of two-way interactions separately at each level of the third) can be performed. If the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.\nConditional independence/dependence: if the three-way interaction is not rejected (no three-way association), lower order interactions can be explored.\n\n\\(H_0: AB=0\\) - A and B conditionally independent (not associated) within each level of C.\n\\(H_0: AC=0\\) - A and C conditionally independent (not associated) within each level of B.\n\\(H_0: BC=0\\) - B and C conditionally independent (not associated) within each level of A.\n\nMarginal independence:\n\n\\(H_0: AB=0\\) - no association between A and B pooling over C.\n\\(H_0: AC=0\\) - no association between A and C pooling over B.\n\\(H_0: BC=0\\) - no association between B and C pooling over A.\n\nComplete independence: If none of the two-way interactions are rejected (no two-way associations), complete independence (all two-way interactions equal zero) can be explored.\n\n\\(H_0: AB=AC=BC=0\\) - Each of the variables are completely independent of all the other variables.\n\n\nAnalysis of designs with more than three factors proceed similarly, starting with tests of higher order interactions and progressing to lower order interactions only in the absence of higher order interactions."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#assumptions",
    "href": "tutorials/2020-02-13-glm-jags/index.html#assumptions",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nCompared to general linear models, the requirements of generalised linear models are less stringent. In particular, neither normality nor homoscedasticity are assumed. Nevertheless, to maximize the reliability of null hypotheses tests, the following assumptions do apply:\n\nall observations should be independent to ensure that the samples provide an unbiased estimate of the intended population.\nit is important to establish that no observations are overly influential. Most linear model influence (and outlier) diagnostics extend to generalized linear models and are taken from the final iteration of the weighted least squares algorithm. Useful diagnoses include:\n\nResiduals - there are numerous forms of residuals that have been defined for generalized linear models, each essentially being a variant on the difference between observed and predicted (influence in \\(y\\)-space) theme. Note that the residuals from logistic regression are difficult to interpret.\nLeverage - a measure of outlyingness and influence in \\(x\\)-space.\nDfbeta - an analogue of Cook’s D statistic which provides a standardized measure of the overall influence of observations on the parameter estimates and model fit.\n\nalthough linearity between the response and predictors is not assumed, the relationship between each of the predictors and the link function is assumed to be linear. This linearity can be examined via the following:\n\ngoodness-of-fit. For log-linear models, \\(\\chi^2\\) contingency tables can be performed, however due to the low reliability of such tests with small sample sizes, this is not an option for logistic regression with continuous predictor(s) (since each combination is typically unique and thus the expected values are always \\(1\\)).\nHosmer-Lemeshow (\\(\\hat{C}\\)). Data are aggregated into \\(10\\) groups or bins (either by cutting the data according to the predictor range or equal frequencies in each group) such that goodness-of-fit test is more reliable. Nevertheless, the Hosmer-Lemeshow statistic has low power and relies on the somewhat arbitrary bin sizes.\nle Cessie-van Houwelingen-Copas omnibus test. This is a goodness-of-fit test for binary data based on the smoothing of residuals.\ncomponent + residual (partial residual) plots. Non-linearity is diagnosed as a substantial deviation from a linear trend.\n\n\nNon-linearity can be dealt with either by transformation (of the predictor variable(s), fitting polynomial terms or via splines/generalised additive modelling (GAM) depending on the degree and nature of the non-linearity.\n\n(over or under) dispersion."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-13-glm-jags/index.html#exploratory-data-analysis",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nSo lets explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\))\n\nhist(dat$x)\n\n\n\n\n\n\n\n#now for the scatterplot\nplot(y~x, dat)\nwith(dat, lines(lowess(y~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-13-glm-jags/index.html#model-fitting",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nEffects model\nNote that in order to prevent arithmetic overflows (particularly with the clog-log model, I am going to constrain the estimated linear predictor to between \\(-20\\) and \\(20\\). Values outside of this on a inverse-log scale are extremely small and huge respectively. I will demonstrate logistic regression with a range of possible link functions (each of which yield different parameter interpretations). Consider first the logit function:\n\\[\ny \\sim \\text{Bern}(\\pi),\n\\]\nwhere \\(\\text{logit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\nmodelString=\"\nmodel{\n  for (i in 1:N) {\n    y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- max(-20,min(20,beta0+beta1*x[i]))\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n}\n\"\nwriteLines(modelString, con='modellogit.txt')\n\ndat.list &lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\n\nparams &lt;- c('beta0','beta1')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nlibrary(R2jags)\ndat.logit.jags &lt;- jags(data=dat.list,model.file='modellogit.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 147\nNA \nNA Initializing model\n\n\nSecond, we consider the probit function:\n\\[\ny \\sim \\text{Bern}(\\pi),\n\\]\nwhere \\(\\text{probit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\nmodelString2=\"\nmodel{\n  for (i in 1:N) {\n    y[i] ~ dbern(p[i])\n    probit(p[i]) &lt;- max(-20,min(20,beta0+beta1*x[i]))\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n}\n\"\nwriteLines(modelString2, con='modelprobit.txt')\n\ndat.list &lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\n\nparams &lt;- c('beta0','beta1')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.probit.jags &lt;- jags(data=dat.list,model.file='modelprobit.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 147\nNA \nNA Initializing model\n\n\nFinally, the complementary log-log\n\\[\ny \\sim \\text{Bern}(\\pi),\n\\]\nwhere \\(\\text{probit}(\\pi)=\\beta_0+\\beta_1x_1\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\nmodelString3=\"\nmodel{\n  for (i in 1:N) {\n    y[i] ~ dbern(p[i])\n    cloglog(p[i]) &lt;- max(-20,min(20,beta0+beta1*x[i]))\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n}\n\"\nwriteLines(modelString3, con='modelcloglog.txt')\n\ndat.list &lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\n\nparams &lt;- c('beta0','beta1')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.cloglog.jags &lt;- jags(data=dat.list,model.file='modelcloglog.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 147\nNA \nNA Initializing model\n\n\nPrior to exploring the model parameters, it is prudent to confirm that the model did indeed fit the assumptions and was an appropriate fit to the data as well as that the MCMC sampling chain was adequately mixed and the retained samples independent. Whilst I will only demonstrate this for the logit model, the procedure would be identical for exploring the probit and clog-log models."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#model-evaluation",
    "href": "tutorials/2020-02-13-glm-jags/index.html#model-evaluation",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nlibrary(mcmcplots)\ndenplot(dat.logit.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\ntraplot(dat.logit.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.logit.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    50       54338 3746         14.50     \nNA  beta1    36       39555 3746         10.60     \nNA  deviance 4        4955  3746          1.32     \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    30       31743 3746          8.47     \nNA  beta1    40       52860 3746         14.10     \nNA  deviance 8        10336 3746          2.76\n\nautocorr.diag(as.mcmc(dat.logit.jags))\n\nNA            beta0     beta1  deviance\nNA Lag 0  1.0000000 1.0000000 1.0000000\nNA Lag 1  0.9816715 0.9811729 0.5841946\nNA Lag 5  0.9190319 0.9197111 0.4477029\nNA Lag 10 0.8458674 0.8477906 0.3948904\nNA Lag 50 0.4300407 0.4306464 0.2065881\n\n\nIt seems that the level of auto-correlation at the nominated lag of \\(10\\) is extremely high. Ideally, the level of auto-correlation should be less than \\(0.1\\). To achieve this, we need a lag of \\(1000\\). Consequently, we will resample at a lag of \\(1000\\) and obviously we are going to need more iterations to ensure that we retain a large enough sample from which to derive estimates. In order to support a thinning rate of \\(1000\\), the number of iterations is going to need to be very high. Hence, the following might take considerable time to run.\n\ndat.logit.jags &lt;- jags(data=dat.list,model.file='modellogit.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=100)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 147\nNA \nNA Initializing model\n\nprint(dat.logit.jags)\n\nNA Inference for Bugs model at \"modellogit.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100\nNA  n.sims = 100 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%    75%  97.5%  Rhat n.eff\nNA beta0     -16.51   9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040    58\nNA beta1       1.66   0.973   0.458   0.979   1.427  2.032  3.926 1.026   100\nNA deviance    9.94   2.764   7.457   8.161   8.942 10.678 16.848 1.024   100\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.8 and DIC = 13.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nautocorr.diag(as.mcmc(dat.logit.jags))\n\nNA               beta0      beta1   deviance\nNA Lag 0     1.0000000  1.0000000  1.0000000\nNA Lag 100   0.4435502  0.4390086  0.1529258\nNA Lag 500   0.1102886  0.1246140  0.1950554\nNA Lag 1000 -0.1091505 -0.1008427 -0.1582021\n\n\nConclusions: the samples are now less auto-correlated and the chains are arguably mixed better. We now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS.\n\nlibrary(boot)\ncoefs &lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]\nXmat &lt;- model.matrix(~x, data=dat)\neta&lt;-coefs %*% t(Xmat)\npi &lt;- inv.logit(eta)\n#sweep across rows and then divide by pi\nResid &lt;- -1*sweep(pi,2,dat$y,'-')/sqrt(pi*(1-pi))\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\n\n\n\n\n\n\n\n\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\nSSres&lt;-apply(Resid^2,1,sum)\n\n#generate a matrix of draws from a binomial distribution\n# the matrix is the same dimensions as pi and uses the probabilities of pi\nYNew &lt;- matrix(rbinom(length(pi),prob=pi,size=1),nrow=nrow(pi))\n\nResid1&lt;-(pi - YNew)/sqrt(pi*(1-pi))\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm = T)\n\nNA [1] 0.21875\n\n\nAlternatively, we could generate the new samples and calculate the sums squares of residuals etc all within JAGS.\n\ndat.list &lt;- with(dat, list(y=y, x=x, N=nrow(dat)))\nmodelString=\"\nmodel{\n  for (i in 1:N) {\n    y[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- max(-20,min(20,eta[i]))\n    eta[i] &lt;- beta0+beta1*x[i]\n    YNew[i] ~dbern(p[i])\n    varY[i] &lt;- p[i]*(1-p[i])\n    PRes[i] &lt;- (y[i] - p[i]) / sqrt(varY[i])\n    PResNew[i] &lt;- (YNew[i] - p[i]) / sqrt(varY[i])\n    D[i] &lt;- pow(PRes[i],2)\n    DNew[i] &lt;- pow(PResNew[i],2)\n  }\n  Fit &lt;- sum(D[1:N])\n  FitNew &lt;-sum(DNew[1:N]) \n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  pvalue &lt;- mean(FitNew&gt;Fit)\n}\n\"\nwriteLines(modelString, con='modellogit_v2.txt')\n\nparams &lt;- c('beta0','beta1','Fit','FitNew')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.logit.jags1 &lt;- jags(data=dat.list,model.file='modellogit_v2.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 22\nNA    Total graph size: 343\nNA \nNA Initializing model\n\nprint(dat.logit.jags1)\n\nNA Inference for Bugs model at \"modellogit_v2.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA Fit       38.470 339.283   6.122   8.237  12.604  24.120 186.621 1.013   540\nNA FitNew    15.837 230.080   0.395   2.025   3.780   8.372  63.411 1.001  3800\nNA beta0    -15.507   7.665 -35.657 -19.433 -13.831 -10.040  -4.873 1.024   660\nNA beta1      1.570   0.791   0.501   0.991   1.390   1.979   3.656 1.017 10000\nNA deviance   9.678   2.175   7.482   8.070   9.018  10.581  15.412 1.013   220\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 2.4 and DIC = 12.0\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nout &lt;- dat.logit.jags1$BUGSoutput\nmean(out$sims.list$FitNew &gt; out$sims.list$Fit)\n\nNA [1] 0.1947\n\n\nConclusions: although the Bayesian p-value is quite a bit lower than \\(0.5\\), suggesting that there is more variability in the data than should be expected from this simple logistic regression model, this value is not any closer to \\(0\\) (a value that would indicate that the model does not fit the data at all well. Thus we might conclude that whilst not ideal, the model is adequate."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#exploring-the-model-parameters",
    "href": "tutorials/2020-02-13-glm-jags/index.html#exploring-the-model-parameters",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Exploring the model parameters",
    "text": "Exploring the model parameters\nIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.\n\nlibrary(coda)\nprint(dat.logit.jags)\n\nNA Inference for Bugs model at \"modellogit.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded), n.thin = 100\nNA  n.sims = 100 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%    75%  97.5%  Rhat n.eff\nNA beta0     -16.51   9.652 -40.133 -20.118 -14.170 -9.587 -5.463 1.040    58\nNA beta1       1.66   0.973   0.458   0.979   1.427  2.032  3.926 1.026   100\nNA deviance    9.94   2.764   7.457   8.161   8.942 10.678 16.848 1.024   100\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.8 and DIC = 13.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nlibrary(plyr)\nadply(dat.logit.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1     Median       Mean       lower     upper    lower.1   upper.1\nNA 1 beta0 -14.169526 -16.510277 -38.4322729 -2.190571 -15.809670 -6.767604\nNA 2 beta1   1.427161   1.660376   0.3019023  3.728819   0.866335  1.791501\n\n\nConclusions: We would reject the null hypothesis (p\\(&lt;0.05\\)). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in log odds of y success. Every \\(1\\) unit increase in \\(x\\) results in a \\(0.86\\) unit increase in log odds-ratio. We usually express this in terms of odds-ratio rather than log odds-ratio, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.86}=2.36\\)) \\(2.36\\) unit increase in odds-ratio."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#explorations-of-the-trends",
    "href": "tutorials/2020-02-13-glm-jags/index.html#explorations-of-the-trends",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Explorations of the trends",
    "text": "Explorations of the trends\nWe might also be interested in the LD50 - the value of \\(x\\) where the probability switches from favoring \\(1\\) to favoring \\(0\\). LD50 is calculated as:\n\\[\nLD50 = - \\frac{\\text{intercept}}{\\text{slope}}\n\\]\n\nsummary(as.mcmc(-coefs[,1]/coefs[,2]))\n\nNA \nNA Iterations = 1:100\nNA Thinning interval = 1 \nNA Number of chains = 1 \nNA Sample size per chain = 100 \nNA \nNA 1. Empirical mean and standard deviation for each variable,\nNA    plus standard error of the mean:\nNA \nNA           Mean             SD       Naive SE Time-series SE \nNA        9.92488        0.84980        0.08498        0.06916 \nNA \nNA 2. Quantiles for each variable:\nNA \nNA   2.5%    25%    50%    75%  97.5% \nNA  7.737  9.460  9.894 10.448 11.538\n\n#OR\nLD50 &lt;- -coefs[,1]/coefs[,2]\ndata.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))\n\nNA        Median     Mean    lower    upper  lower.1  upper.1\nNA var1 9.894002 9.924877 7.930942 11.59808 9.547373 10.50285\n\n\nConclusions: the LD50 is \\(10.5\\). Finally, we will create a summary plot.\n\npar(mar = c(4, 5, 0, 0))\nplot(y ~ x, data = dat, type = \"n\", ann = F, axes = F)\npoints(y ~ x, data = dat, pch = 16)\nxs &lt;- seq(0, 20, l = 1000)\n\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nys &lt;- inv.logit(eta)\nlibrary(plyr)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\n\npoints(Median ~ x, data=data.tab,col = \"black\", type = \"l\")\nlines(lower ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\nlines(upper ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\naxis(1)\nmtext(\"X\", 1, cex = 1.5, line = 3)\naxis(2, las = 2)\nmtext(\"Y\", 2, cex = 1.5, line = 3)\nbox(bty = \"l\")"
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-13-glm-jags/index.html#exploratory-data-analysis-1",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nSo lets explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the either the number of successes (success) or the number of (failures) and the predictor (\\(x\\)). Note, that this will not account for the differences in trial size per group and so a scatterplot of the relationship between the number of successes (success) or the number of (failures) divided by the total number of trials against the predictor (\\(x\\)) might be more appropriate.\n\nhist(dat$x)\n\n\n\n\n\n\n\n#now for the scatterplot\nplot(success~x, dat)\nwith(dat, lines(lowess(success~x)))\n\n\n\n\n\n\n\n#scatterplot standardised for trial size\nplot(success/(success+failure)~x, dat)\nwith(dat, lines(lowess(success/(success+failure)~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness (although it is not all that uniform - random data) or other issues that might lead to non-linearity. The lowess smoother on either scatterplot does not display major deviations from a standard sigmoidal curve and thus linearity is likely to be satisfied. Violations of linearity could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-13-glm-jags/index.html#model-fitting-1",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nClearly the number of successes is also dependent on the number of trials. Larger numbers of trials might be expected to yeild higher numbers of successes.\n\ndat.list &lt;- with(dat, list(success=success, total=success+failure, x=x, N=nrow(dat)))\nmodelString=\"\nmodel{\n  for (i in 1:N) {\n    success[i] ~ dbin(p[i],total[i])\n    logit(p[i]) &lt;- max(-20,min(20,beta0+beta1*x[i]))\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n}\n\"\nwriteLines(modelString, con='modelgbin.txt')\n\nparams &lt;- c('beta0','beta1')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.logit.jags &lt;- jags(data=dat.list,model.file='modelgbin.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 10\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 87\nNA \nNA Initializing model\n\n\nAs with the logistic regression presented earlier, we could alternatively use probit or clog-log link functions."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#model-evaluation-1",
    "href": "tutorials/2020-02-13-glm-jags/index.html#model-evaluation-1",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndenplot(dat.logit.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\ntraplot(dat.logit.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.logit.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    46       50468 3746         13.50     \nNA  beta1    90       98698 3746         26.30     \nNA  deviance 6        8920  3746          2.38     \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                  \nNA           Burn-in  Total  Lower bound  Dependence\nNA           (M)      (N)    (Nmin)       factor (I)\nNA  beta0    84       103188 3746         27.50     \nNA  beta1    52       58312  3746         15.60     \nNA  deviance 8        9488   3746          2.53\n\nautocorr.diag(as.mcmc(dat.logit.jags))\n\nNA            beta0     beta1   deviance\nNA Lag 0  1.0000000 1.0000000 1.00000000\nNA Lag 1  0.9830416 0.9831425 0.56062724\nNA Lag 5  0.9248140 0.9256704 0.42678260\nNA Lag 10 0.8543024 0.8555131 0.36633408\nNA Lag 50 0.4631353 0.4636323 0.07250394\n\n\nLets explore the diagnostics - particularly the residuals.\n\ninv.logit &lt;- binomial()$linkinv\n#Calculate residuals\ncoefs &lt;- dat.logit.jags$BUGSoutput$sims.matrix[,1:2]\nXmat &lt;- model.matrix(~x, data=dat)\neta&lt;-coefs %*% t(Xmat)\npi &lt;- inv.logit(eta)\n#sweep across rows and then divide by pi\nResid &lt;- -1*sweep(pi,2,dat$success/(dat$success+dat$failure),'-')/sqrt(pi*(1-pi))\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\nlines(lowess(apply(Resid,2,mean)~apply(eta,2,mean)))\n\n\n\n\n\n\n\n\nConclusions: there is no obvious patterns in the residuals, or at least there are no obvious trends remaining that would be indicative of non-linearity.\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\nSSres&lt;-apply(Resid^2,1,sum)\n\n#generate a matrix of draws from a binomial distribution\n#the matrix is the same dimensions as pi and uses the probabilities of pi\nYNew &lt;- matrix(rbinom(length(pi),prob=pi,size=(dat$success+dat$failure)),nrow=nrow(pi))\nResid1 &lt;- 1*(pi-YNew/(dat$success+dat$failure))/sqrt(pi*(1-pi))\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm=T)\n\nNA [1] 0.4559\n\n\nConclusions: this Bayesian p-value is reasonably close to \\(0.5\\). Therefore we would conclude that there was no strong evidence for a lack of fit of the model."
  },
  {
    "objectID": "tutorials/2020-02-13-glm-jags/index.html#explorations-of-the-trends-1",
    "href": "tutorials/2020-02-13-glm-jags/index.html#explorations-of-the-trends-1",
    "title": "Generalised Linear Models (JAGS)",
    "section": "Explorations of the trends",
    "text": "Explorations of the trends\nWe might also be interested in the LD50 - the value of \\(x\\) where the probability switches from favoring \\(1\\) to favoring \\(0\\). LD50 is calculated as:\n\\[\nLD50 = - \\frac{\\text{intercept}}{\\text{slope}}\n\\]\n\nsummary(as.mcmc(-coefs[,1]/coefs[,2]))\n\nNA \nNA Iterations = 1:10000\nNA Thinning interval = 1 \nNA Number of chains = 1 \nNA Sample size per chain = 10000 \nNA \nNA 1. Empirical mean and standard deviation for each variable,\nNA    plus standard error of the mean:\nNA \nNA           Mean             SD       Naive SE Time-series SE \nNA       12.80838        6.30455        0.06305        0.05732 \nNA \nNA 2. Quantiles for each variable:\nNA \nNA  2.5%   25%   50%   75% 97.5% \nNA 10.09 12.45 13.08 13.58 14.41\n\n#OR\nLD50 &lt;- -coefs[,1]/coefs[,2]\ndata.frame(Median=median(LD50), Mean=mean(LD50), HPDinterval(as.mcmc(LD50)), HPDinterval(as.mcmc(LD50),p=0.5))\n\nNA        Median     Mean    lower    upper  lower.1  upper.1\nNA var1 13.08204 12.80838 10.76017 14.74202 12.71997 13.79013\n\n\nConclusions: the LD50 is \\(13.1\\). Finally, we will create a summary plot.\n\npar(mar = c(4, 5, 0, 0))\nplot(success/(success+failure) ~ x, data = dat, type = \"n\", ann = F, axes = F)\npoints(success/(success+failure) ~ x, data = dat, pch = 16)\nxs &lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)\n\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nys &lt;- inv.logit(eta)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\n\npoints(Median ~ x, data=data.tab,col = \"black\", type = \"l\")\nwith(data.tab,polygon(c(x,rev(x)),c(lower,rev(upper)), col=\"#0000ff60\", border=NA))\n#lines(lower ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\n#lines(upper ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\naxis(1)\nmtext(\"X\", 1, cex = 1.5, line = 3)\naxis(2, las = 2)\nmtext(\"Probability of success\", 2, cex = 1.5, line = 3)\nbox(bty = \"l\")\n\n\n\n\n\n\n\n#or via ggplot\n\nxs &lt;- seq(min(dat$x, na.rm=TRUE),max(dat$x, na.rm=TRUE), l = 1000)\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nlibrary(boot)\nys &lt;- inv.logit(eta)\nlibrary(plyr)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\n\nlibrary(ggplot2)\nlibrary(grid)\ndat$p &lt;- with(dat, success/(success+failure))\np1 &lt;- ggplot(data.tab,aes(y=Median, x=x)) + geom_point(data=dat,aes(y=p, x=x),color=\"gray40\")+\n             geom_smooth(aes(ymin=lower, ymax=upper), stat=\"identity\")+\n                         scale_x_continuous(\"X\")+scale_y_continuous(\"Probability of success\")\np1+theme(panel.grid.major=element_blank(),\n         panel.grid.minor=element_blank(),\n         panel.border=element_blank(),\n         panel.background=element_blank(),\n                 axis.title.y=element_text(size=15,vjust=0,angle=90),\n                 axis.title.x=element_text(size=15,vjust=-1),\n                 axis.text.y=element_text(size=12),\n                 axis.text.x=element_text(size=12),\n                 axis.line=element_line(),\n                 plot.margin=unit(c(0.5,0.5,2,2), \"lines\"))"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html",
    "title": "Partly Nested Anova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#introduction",
    "title": "Partly Nested Anova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nSplit-plot designs (plots refer to agricultural field plots for which these designs were originally devised) extend unreplicated factorial (randomised complete block and simple repeated measures) designs by incorporating an additional factor whose levels are applied to entire blocks. Similarly, complex repeated measures designs are repeated measures designs in which there are different types of subjects. Consider the example of a randomised complete block. Blocks of four treatments (representing leaf packs subject to different aquatic taxa) were secured in numerous locations throughout a potentially heterogeneous stream. If some of those blocks had been placed in riffles, some in runs and some in pool habitats of the stream, the design becomes a split-plot design incorporating a between block factor (stream region: runs, riffles or pools) and a within block factor (leaf pack exposure type: microbial, macro invertebrate or vertebrate). Furthermore, the design would enable us to investigate whether the roles that different organism scales play on the breakdown of leaf material in stream are consistent across each of the major regions of a stream (interaction between region and exposure type). Alternatively (or in addition), shading could be artificially applied to half of the blocks, thereby introducing a between block effect (whether the block is shaded or not). Extending the repeated measures examples from Tutorial 9.3a, there might have been different populations (such as different species or histories) of rats or sharks. Any single subject (such as an individual shark or rat) can only be of one of the populations types and thus this additional factor represents a between subject effect."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#linear-models",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#linear-models",
    "title": "Partly Nested Anova (Stan)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for three and four factor partly nested designs are:\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\gamma)_{ij} + (\\beta\\gamma)_{jk} + \\epsilon_{ijkl},\n\\]\n\\[\ny_{ijklm} = \\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij} + \\beta_k + \\delta_l + (\\alpha\\delta)_{il} + (\\gamma\\delta)_{jl} + (\\alpha\\gamma\\delta)_{ijl} + \\epsilon_{ijklm}, \\;\\;\\; \\text{(Model 2 additive - 2 between)}\n\\]\n\\[\ny_{ijklm} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_l +  (\\gamma\\delta)_{kl} + (\\alpha\\gamma)_{ik} + (\\alpha\\delta)_{il} + (\\alpha\\gamma\\delta)_{ikl} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2 additive - 1 between)}\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B and \\(\\epsilon\\) is the random unexplained or residual component."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#assumptions",
    "title": "Partly Nested Anova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAs partly nested designs share elements in common with each of nested, factorial and unreplicated factorial designs, they also share similar assumptions and implications to these other designs. Specifically, hypothesis tests assume that:\n\nthe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see Tables above) be used to explore normality. Scale transformations are often useful.\nthe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nthe appropriate residuals are independent of one another. Critically, experimental units within blocks/subjects should be adequately spaced temporally and spatially to restrict contamination or carryover effects. Non-independence resulting from the hierarchical design should be accounted for.\nthat the variance/covariance matrix displays sphericity (strickly, the variance-covariance matrix must display a very specific pattern of sphericity in which both variances and covariances are equal (compound symmetry), however, an F-ratio will still reliably follow an F distribution provided basic sphericity holds). This assumption is likely to be met only if the treatment levels within each block can be randomly ordered. This assumption can be managed by either adjusting the sensitivity of the affected F-ratios or employing linear mixed effects modelling to the design.\nthere are no block by within block interactions. Such interactions render non-significant within block effects difficult to interpret unless we assume that there are no block by within block interactions, non-significant within block effects could be due to either an absence of a treatment effect, or as a result of opposing effects within different blocks. As these block by within block interactions are unreplicated, they can neither be formally tested nor is it possible to perform main effects tests to diagnose non-significant within block effects."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#data-generation",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#data-generation",
    "title": "Partly Nested Anova (Stan)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnA &lt;- 3\nnC &lt;- 3\nnBlock &lt;- 36\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nC\nBlock &lt;- gl(nBlock, k=1)\nC &lt;- gl(nC,k=1)\n\n## Specify the cell means\nAC.means&lt;-(rbind(c(40,70,80),c(35,50,70),c(35,40,45)))\n## Convert these to effects\nX &lt;- model.matrix(~A*C,data=expand.grid(A=gl(3,k=1),C=gl(3,k=1)))\nAC &lt;- as.vector(AC.means)\nAC.effects &lt;- solve(X,AC)\n\nA &lt;- gl(nA,nBlock,n)\ndt &lt;- expand.grid(C=C,Block=Block)\ndt &lt;- data.frame(dt,A)\n\nXmat &lt;- cbind(model.matrix(~-1+Block, data=dt),model.matrix(~A*C, data=dt))\nblock.effects &lt;-  rnorm(n = nBlock, mean =0 , sd = sigma.block)\nall.effects &lt;- c(block.effects, AC.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.splt &lt;- data.frame(y=y, A=A,dt)\nhead(data.splt)  #print out the first six rows of the data set\n\nNA          y A C Block A.1\nNA 1 36.04388 1 1     1   1\nNA 2 62.96473 1 2     1   1\nNA 3 71.74448 1 3     1   1\nNA 4 35.33552 1 1     2   1\nNA 5 63.76434 1 2     2   1\nNA 6 76.19828 1 3     2   1\n\ntapply(data.splt$y,data.splt$A,mean)\n\nNA        1        2        3 \nNA 65.71431 49.43047 41.36212\n\ntapply(data.splt$y,data.splt$C,mean)\n\nNA        1        2        3 \nNA 38.41079 53.56792 64.52819\n\nreplications(y~A*C+Error(Block), data.splt)\n\nNA   A   C A:C \nNA  36  36  12\n\nlibrary(ggplot2)\nggplot(data.splt, aes(y=y, x=C, linetype=A, group=A)) + geom_line(stat='summary', fun.y=mean)\n\n\n\n\n\n\n\nggplot(data.splt, aes(y=y, x=C,color=A)) + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#exploratory-data-analysis",
    "title": "Partly Nested Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n# check between plot effects\nboxplot(y~A, ddply(data.splt,~A+Block, summarise,y=mean(y)))\n\n\n\n\n\n\n\n#OR\nggplot(ddply(data.splt,~A+Block, summarise,y=mean(y)), aes(y=y, x=A)) + geom_boxplot()\n\n\n\n\n\n\n\n# check within plot effects\nboxplot(y~A*C, data.splt)\n\n\n\n\n\n\n\n#OR \nggplot(data.splt, aes(y=y, x=C, fill=A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.splt, interaction.plot(C,Block,y))\n\n\n\n\n\n\n\n#OR with ggplot\nlibrary(ggplot2)\nggplot(data.splt, aes(y=y, x=C, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A*C, data.splt))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA C                                    \nNA Tukey test    1.4518           0.1466\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A*C, data.splt))\n\nNA      Test    Pvalue \nNA 1.4517644 0.1465671\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (C). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#model-fitting",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#model-fitting",
    "title": "Partly Nested Anova (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nWe will only explore the matrix parameterisation (random intercepts) of the model, where\n\\[\n\\text{number of lesions}_i = \\beta \\text{Site}_{j(i)} + \\epsilon_{i},\n\\]\nwhere \\(\\epsilon_i∼ N(0,\\sigma^2)\\) and we treat Distance as a factor.\n\nrstanString=\"\ndata{\n   int n;\n   int nZ;\n   int nX;\n   vector [n] y;\n   matrix [n,nX] X;\n   matrix [n,nZ] Z;\n   vector [nX] a0;\n   matrix [nX,nX] A0;\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nZ] gamma;\n  real&lt;lower=0&gt; sigma_Z;\n}\ntransformed parameters {\n   vector [n] mu;\n\n   mu = X*beta + Z*gamma; \n} \nmodel{\n    // Priors\n    beta ~ multi_normal(a0,A0);\n    gamma ~ normal( 0 , sigma_Z );\n    sigma_Z ~ cauchy(0,25);\n    sigma ~ cauchy(0,25);\n\n    y ~ normal( mu , sigma );\n}\ngenerated quantities {\n    vector [n] y_err;\n    real&lt;lower=0&gt; sd_Resid;\n\n    y_err = y - mu;\n    sd_Resid = sd(y_err);\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"matrixModel.stan\")\n\n\n#sort the data set so that the copper treatments are in a more logical order\nlibrary(dplyr)\ncopper$DIST &lt;- factor(copper$DIST)\ncopper$PLATE &lt;- factor(copper$PLATE)\ncopper.sort &lt;- arrange(copper,COPPER,PLATE,DIST)\n\nXmat &lt;- model.matrix(~COPPER*DIST, data=copper.sort)\nZmat &lt;- model.matrix(~-1+PLATE, data=copper.sort)\ncopper.list &lt;- list(y=copper.sort$WORMS,\n               X=Xmat, nX=ncol(Xmat),\n                           Z=Zmat, nZ=ncol(Zmat),\n                           n=nrow(copper.sort),\n                           a0=rep(0,ncol(Xmat)), A0=diag(100000,ncol(Xmat))\n                           )\nparams &lt;- c(\"beta\",\"gamma\",\"sigma\",\"sigma_Z\")\nburnInSteps = 500\nnChains = 2\nnumSavedSteps = 5000\nthinSteps = 1\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nlibrary(rstan)\nlibrary(coda)\n\ncopper.rstan.a &lt;- stan(data = copper.list, file = \"matrixModel.stan\", \n                       chains = nChains, pars = params, iter = nIter, \n                       warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  501 / 2500 [ 20%]  (Sampling)\nNA Chain 1: Iteration:  750 / 2500 [ 30%]  (Sampling)\nNA Chain 1: Iteration: 1000 / 2500 [ 40%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.112 seconds (Warm-up)\nNA Chain 1:                0.373 seconds (Sampling)\nNA Chain 1:                0.485 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  501 / 2500 [ 20%]  (Sampling)\nNA Chain 2: Iteration:  750 / 2500 [ 30%]  (Sampling)\nNA Chain 2: Iteration: 1000 / 2500 [ 40%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.109 seconds (Warm-up)\nNA Chain 2:                0.301 seconds (Sampling)\nNA Chain 2:                0.41 seconds (Total)\nNA Chain 2:\n\nprint(copper.rstan.a, par = c(\"beta\",\"gamma\",\"sigma\",\"sigma_Z\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2500; warmup=500; thin=1; \nNA post-warmup draws per chain=2000, total post-warmup draws=4000.\nNA \nNA             mean se_mean   sd   2.5%    25%    50%   75% 97.5% n_eff Rhat\nNA beta[1]    10.88    0.02 0.69   9.50  10.42  10.88 11.35 12.24  1187 1.00\nNA beta[2]    -3.65    0.03 0.98  -5.54  -4.30  -3.64 -2.99 -1.68  1440 1.00\nNA beta[3]   -10.62    0.03 0.99 -12.53 -11.30 -10.61 -9.95 -8.70  1235 1.00\nNA beta[4]     1.11    0.02 0.89  -0.66   0.52   1.09  1.67  2.87  1462 1.00\nNA beta[5]     1.54    0.02 0.87  -0.15   0.94   1.52  2.12  3.26  1487 1.00\nNA beta[6]     2.67    0.03 0.90   0.90   2.07   2.66  3.26  4.46  1290 1.00\nNA beta[7]     0.00    0.03 1.25  -2.49  -0.81   0.01  0.84  2.47  1834 1.00\nNA beta[8]     0.09    0.03 1.29  -2.51  -0.73   0.13  0.92  2.56  1509 1.00\nNA beta[9]    -0.29    0.03 1.22  -2.76  -1.08  -0.30  0.55  2.08  1764 1.00\nNA beta[10]    2.21    0.03 1.28  -0.30   1.40   2.21  3.05  4.74  1651 1.00\nNA beta[11]    0.08    0.03 1.27  -2.44  -0.78   0.06  0.95  2.56  1615 1.00\nNA beta[12]    4.92    0.03 1.30   2.34   4.07   4.90  5.81  7.43  1501 1.00\nNA gamma[1]    0.17    0.01 0.51  -0.75  -0.12   0.12  0.44  1.28  2056 1.00\nNA gamma[2]   -0.14    0.01 0.50  -1.25  -0.41  -0.09  0.14  0.82  2853 1.00\nNA gamma[3]    0.30    0.02 0.52  -0.57  -0.02   0.21  0.58  1.55  1045 1.00\nNA gamma[4]   -0.42    0.02 0.55  -1.68  -0.75  -0.32 -0.03  0.46   844 1.00\nNA gamma[5]   -0.35    0.02 0.53  -1.56  -0.66  -0.27  0.01  0.53  1132 1.00\nNA gamma[6]    0.76    0.04 0.67  -0.17   0.21   0.66  1.18  2.27   308 1.01\nNA gamma[7]   -0.16    0.01 0.51  -1.29  -0.43  -0.12  0.12  0.82  2640 1.00\nNA gamma[8]   -0.47    0.02 0.58  -1.80  -0.82  -0.36 -0.05  0.46   893 1.00\nNA gamma[9]    0.12    0.01 0.48  -0.84  -0.14   0.07  0.38  1.14  3089 1.00\nNA gamma[10]  -0.08    0.01 0.50  -1.14  -0.35  -0.06  0.19  0.87  3835 1.00\nNA gamma[11]  -0.13    0.01 0.48  -1.19  -0.38  -0.09  0.13  0.81  3242 1.00\nNA gamma[12]   0.20    0.01 0.50  -0.71  -0.08   0.14  0.46  1.33  1786 1.00\nNA gamma[13]   0.15    0.01 0.50  -0.79  -0.14   0.10  0.42  1.26  2182 1.00\nNA gamma[14]   0.10    0.01 0.48  -0.84  -0.16   0.06  0.35  1.11  3417 1.00\nNA gamma[15]  -0.08    0.01 0.49  -1.12  -0.35  -0.04  0.18  0.89  2643 1.00\nNA sigma       1.40    0.00 0.17   1.12   1.28   1.39  1.51  1.77  1788 1.00\nNA sigma_Z     0.59    0.02 0.33   0.08   0.34   0.56  0.78  1.33   196 1.01\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 13:49:26 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#mcmc-diagnostics",
    "title": "Partly Nested Anova (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nBefore fully exploring the parameters, it is prudent to examine the convergence and mixing diagnostics. Chose either any of the parameterizations (they should yield much the same).\n\nlibrary(mcmcplots)\nmcmc&lt;-As.mcmc.list(copper.rstan.a)\ndenplot(mcmc, parms = c(\"gamma\",\"beta\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"gamma\",\"beta\"))\n\n\n\n\n\n\n\nraftery.diag(mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\nautocorr.diag(mcmc)\n\nNA            beta[1]      beta[2]      beta[3]      beta[4]     beta[5]\nNA Lag 0   1.00000000  1.000000000  1.000000000  1.000000000  1.00000000\nNA Lag 1   0.41243540  0.339550873  0.400163967  0.277128766  0.26820878\nNA Lag 5   0.07102840  0.030234987  0.076252882  0.046470932  0.04059323\nNA Lag 10  0.01025273  0.011853811 -0.009715283  0.003697684  0.02096189\nNA Lag 50 -0.01935325 -0.008677298 -0.020469817 -0.021681255 -0.02427059\nNA            beta[6]      beta[7]      beta[8]     beta[9]    beta[10]\nNA Lag 0  1.000000000  1.000000000  1.000000000  1.00000000  1.00000000\nNA Lag 1  0.304121166  0.171032611  0.270680721  0.19255934  0.22372525\nNA Lag 5  0.062616531  0.027598718  0.049050651  0.04072616  0.04169052\nNA Lag 10 0.008920267  0.008779169 -0.009819737  0.01071783  0.02903601\nNA Lag 50 0.007623398 -0.005112117 -0.010928777 -0.01614899 -0.01044049\nNA            beta[11]     beta[12]     gamma[1]      gamma[2]     gamma[3]\nNA Lag 0   1.000000000  1.000000000  1.000000000  1.0000000000  1.000000000\nNA Lag 1   0.233241517  0.238614222  0.043510725  0.0009244863  0.072822630\nNA Lag 5   0.034862905  0.042742374 -0.001918630  0.0241914956  0.054090901\nNA Lag 10 -0.003071861 -0.001409673  0.035940499  0.0307487918  0.043888093\nNA Lag 50  0.010507726 -0.020517084  0.009830037 -0.0160805051 -0.008810545\nNA            gamma[4]   gamma[5]   gamma[6]    gamma[7]   gamma[8]     gamma[9]\nNA Lag 0   1.000000000 1.00000000 1.00000000  1.00000000 1.00000000  1.000000000\nNA Lag 1   0.087957279 0.10474543 0.26272701  0.06220393 0.11883475 -0.026530632\nNA Lag 5   0.100739861 0.09079565 0.20002903  0.02354125 0.09171889  0.003297873\nNA Lag 10  0.053674794 0.03524323 0.11875428  0.02491723 0.08026158 -0.005151384\nNA Lag 50 -0.005883612 0.01517194 0.02817319 -0.01743733 0.02240176  0.002685263\nNA           gamma[10]    gamma[11]    gamma[12]   gamma[13]     gamma[14]\nNA Lag 0   1.000000000  1.000000000 1.0000000000 1.000000000  1.0000000000\nNA Lag 1  -0.083471258 -0.021778094 0.0026519129 0.035619521 -0.0068898873\nNA Lag 5  -0.025783628 -0.001113343 0.0358077449 0.003617877 -0.0356908223\nNA Lag 10  0.007973235  0.007520300 0.0397866864 0.016260498 -0.0008975898\nNA Lag 50 -0.001931942  0.006841088 0.0006658607 0.003163153 -0.0062199336\nNA          gamma[15]        sigma   sigma_Z       lp__\nNA Lag 0   1.00000000  1.000000000 1.0000000 1.00000000\nNA Lag 1   0.02463886  0.076223538 0.6566621 0.84167987\nNA Lag 5  -0.01506360  0.046194982 0.3651672 0.56805415\nNA Lag 10  0.02910642  0.020878887 0.2362394 0.41770858\nNA Lag 50 -0.03103926 -0.009786308 0.0630208 0.04551303"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#parameter-estimates",
    "href": "tutorials/2020-02-11-partly-nested-anova-stan/index.html#parameter-estimates",
    "title": "Partly Nested Anova (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\nprint(copper.rstan.a)\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2500; warmup=500; thin=1; \nNA post-warmup draws per chain=2000, total post-warmup draws=4000.\nNA \nNA             mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]    10.88    0.02 0.69   9.50  10.42  10.88  11.35  12.24  1187 1.00\nNA beta[2]    -3.65    0.03 0.98  -5.54  -4.30  -3.64  -2.99  -1.68  1440 1.00\nNA beta[3]   -10.62    0.03 0.99 -12.53 -11.30 -10.61  -9.95  -8.70  1235 1.00\nNA beta[4]     1.11    0.02 0.89  -0.66   0.52   1.09   1.67   2.87  1462 1.00\nNA beta[5]     1.54    0.02 0.87  -0.15   0.94   1.52   2.12   3.26  1487 1.00\nNA beta[6]     2.67    0.03 0.90   0.90   2.07   2.66   3.26   4.46  1290 1.00\nNA beta[7]     0.00    0.03 1.25  -2.49  -0.81   0.01   0.84   2.47  1834 1.00\nNA beta[8]     0.09    0.03 1.29  -2.51  -0.73   0.13   0.92   2.56  1509 1.00\nNA beta[9]    -0.29    0.03 1.22  -2.76  -1.08  -0.30   0.55   2.08  1764 1.00\nNA beta[10]    2.21    0.03 1.28  -0.30   1.40   2.21   3.05   4.74  1651 1.00\nNA beta[11]    0.08    0.03 1.27  -2.44  -0.78   0.06   0.95   2.56  1615 1.00\nNA beta[12]    4.92    0.03 1.30   2.34   4.07   4.90   5.81   7.43  1501 1.00\nNA gamma[1]    0.17    0.01 0.51  -0.75  -0.12   0.12   0.44   1.28  2056 1.00\nNA gamma[2]   -0.14    0.01 0.50  -1.25  -0.41  -0.09   0.14   0.82  2853 1.00\nNA gamma[3]    0.30    0.02 0.52  -0.57  -0.02   0.21   0.58   1.55  1045 1.00\nNA gamma[4]   -0.42    0.02 0.55  -1.68  -0.75  -0.32  -0.03   0.46   844 1.00\nNA gamma[5]   -0.35    0.02 0.53  -1.56  -0.66  -0.27   0.01   0.53  1132 1.00\nNA gamma[6]    0.76    0.04 0.67  -0.17   0.21   0.66   1.18   2.27   308 1.01\nNA gamma[7]   -0.16    0.01 0.51  -1.29  -0.43  -0.12   0.12   0.82  2640 1.00\nNA gamma[8]   -0.47    0.02 0.58  -1.80  -0.82  -0.36  -0.05   0.46   893 1.00\nNA gamma[9]    0.12    0.01 0.48  -0.84  -0.14   0.07   0.38   1.14  3089 1.00\nNA gamma[10]  -0.08    0.01 0.50  -1.14  -0.35  -0.06   0.19   0.87  3835 1.00\nNA gamma[11]  -0.13    0.01 0.48  -1.19  -0.38  -0.09   0.13   0.81  3242 1.00\nNA gamma[12]   0.20    0.01 0.50  -0.71  -0.08   0.14   0.46   1.33  1786 1.00\nNA gamma[13]   0.15    0.01 0.50  -0.79  -0.14   0.10   0.42   1.26  2182 1.00\nNA gamma[14]   0.10    0.01 0.48  -0.84  -0.16   0.06   0.35   1.11  3417 1.00\nNA gamma[15]  -0.08    0.01 0.49  -1.12  -0.35  -0.04   0.18   0.89  2643 1.00\nNA sigma       1.40    0.00 0.17   1.12   1.28   1.39   1.51   1.77  1788 1.00\nNA sigma_Z     0.59    0.02 0.33   0.08   0.34   0.56   0.78   1.33   196 1.01\nNA lp__      -45.84    0.72 8.72 -60.26 -51.47 -47.10 -41.74 -23.59   146 1.01\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 13:49:26 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#introduction",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#linear-models",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#linear-models",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for two and three factor nested design are:\n\\[\ny_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#assumptions",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnTreat &lt;- 3\nnBlock &lt;- 35\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nTreat\nBlock &lt;- gl(nBlock, k=1)\nA &lt;- gl(nTreat,k=1)\ndt &lt;- expand.grid(A=A,Block=Block)\n#Xmat &lt;- model.matrix(~Block + A + Block:A, data=dt)\nXmat &lt;- model.matrix(~-1+Block + A, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\nA.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\n## Sum to zero block effects\nblock.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\nA.effects &lt;- c(40,70,80)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.rcb &lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\nhead(data.rcb)  #print out the first six rows of the data set\n\nNA          y A Block\nNA 1 45.80853 1     1\nNA 2 66.71784 2     1\nNA 3 93.29238 3     1\nNA 4 43.10101 1     2\nNA 5 73.20697 2     2\nNA 6 91.77487 3     2"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~A, data.rcb)\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.rcb, interaction.plot(A,Block,y))\n\n#OR with ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A, data.rcb))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA Tukey test   -1.4163           0.1567\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\n\nNA       Test     Pvalue \nNA -1.4163343  0.1566776\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nlibrary(asbio)\nwith(data.rcb,tukey.add.test(y,A,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 2.0060029   Denom df = 67    p-value = 0.1613102\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nFull parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[\n\\boldsymbol \\mu =\n  \\begin{bmatrix} 0  \\\\ 0  \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim   \n  \\begin{bmatrix}\n   1000000 & 0 & 0 \\\\\n   0 & 1000000 & 0 \\\\\n   0 & 0 & 1000000\n   \\end{bmatrix}.\n\\]\nHierarchical parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#full-means-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#full-means-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Full means parameterisation",
    "text": "Full means parameterisation\n\nrstanString=\"\ndata{\n   int n;\n   int nA;\n   int nB;\n   vector [n] y;\n   int A[n];\n   int B[n];\n}\n\nparameters{\n  real alpha[nA];\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha ~ normal( 0 , 100 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha[A[i]] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"fullModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.rcb.list &lt;- with(data.rcb, list(y=y, A=as.numeric(A), B=as.numeric(Block),\n  n=nrow(data.rcb), nB=length(levels(Block)),nA=length(levels(A))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\nlibrary(rstan)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.c &lt;- stan(data = data.rcb.list, file = \"fullModel.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.7e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.243 seconds (Warm-up)\nNA Chain 1:                0.111 seconds (Sampling)\nNA Chain 1:                0.354 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 7e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.237 seconds (Warm-up)\nNA Chain 2:                0.105 seconds (Sampling)\nNA Chain 2:                0.342 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.c, par = c(\"alpha\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha[1] 41.55    0.10 2.13 37.40 40.15 41.54 42.94 45.68   441 1.01\nNA alpha[2] 69.50    0.11 2.16 65.31 68.08 69.50 70.86 73.85   419 1.01\nNA alpha[3] 81.85    0.10 2.12 77.71 80.45 81.80 83.21 85.95   425 1.01\nNA sigma     5.07    0.01 0.44  4.29  4.75  5.05  5.35  6.00  2176 1.00\nNA sigma_B  11.71    0.03 1.60  9.10 10.58 11.55 12.63 15.35  3757 1.00\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:08:01 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.c.df &lt;-as.data.frame(extract(data.rcb.rstan.c))\nhead(data.rcb.rstan.c.df)\n\nNA    alpha.1  alpha.2  alpha.3    sigma  sigma_B      lp__\nNA 1 40.92583 67.75321 79.80043 4.655019 10.81956 -319.5226\nNA 2 41.66058 72.22219 83.44578 4.325701 15.14545 -323.5076\nNA 3 41.23380 69.83747 82.08849 5.818799 10.75345 -329.9066\nNA 4 40.46224 68.34375 80.72886 4.690904 11.07078 -317.4154\nNA 5 38.51851 66.61851 79.33672 4.498886 11.73147 -318.8312\nNA 6 41.81091 69.41454 82.56791 6.062546  9.78244 -322.8856\n\ndata.rcb.mcmc.c&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.c)\n\nlibrary(coda)\nMCMCsum &lt;- function(x) {\n   data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n}\n\nplyr:::adply(as.matrix(data.rcb.rstan.c.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1 alpha.1   41.535980   32.655709   40.149101   41.535980   42.944616\nNA 2 alpha.2   69.496977   61.677410   68.079227   69.496977   70.861836\nNA 3 alpha.3   81.799857   73.717659   80.452339   81.799857   83.205086\nNA 4   sigma    5.048656    3.677651    4.748179    5.048656    5.349604\nNA 5 sigma_B   11.554897    7.586198   10.576991   11.554897   12.629219\nNA 6    lp__ -321.666229 -342.185020 -325.626894 -321.666229 -318.292350\nNA         X100.       lower       upper     lower.1    upper.1\nNA 1   50.527411   37.476978   45.726479   40.282467   43.04881\nNA 2   78.798149   65.297631   73.829659   68.437537   71.17360\nNA 3   90.863749   77.678551   85.948313   80.381917   83.10695\nNA 4    6.931274    4.212814    5.902948    4.673778    5.25575\nNA 5   19.103937    8.800251   14.887844   10.404933   12.39880\nNA 6 -307.824839 -333.210469 -312.359491 -325.688234 -318.38516"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#full-effect-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nrstan2String=\"\ndata{\n   int n;\n   int nB;\n   vector [n] y;\n   int A2[n];\n   int A3[n];\n   int B[n];\n}\n\nparameters{\n  real alpha0;\n  real alpha2;\n  real alpha3;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha0 ~ normal( 0 , 1000 );\n    alpha2 ~ normal( 0 , 1000 );\n    alpha3 ~ normal( 0 , 1000 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha0 + alpha2*A2[i] + \n               alpha3*A3[i] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstan2String, con = \"full2Model.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA2 &lt;- ifelse(data.rcb$A=='2',1,0)\nA3 &lt;- ifelse(data.rcb$A=='3',1,0)\ndata.rcb.list &lt;- with(data.rcb, list(y=y, A2=A2, A3=A3, B=as.numeric(Block),\n   n=nrow(data.rcb), nB=length(levels(Block))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha0\",\"alpha2\",\"alpha3\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.f &lt;- stan(data = data.rcb.list, file = \"full2Model.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.2e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.421 seconds (Warm-up)\nNA Chain 1:                0.165 seconds (Sampling)\nNA Chain 1:                0.586 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.445 seconds (Warm-up)\nNA Chain 2:                0.175 seconds (Sampling)\nNA Chain 2:                0.62 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.f, par = c(\"alpha0\", \"alpha2\", \"alpha3\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha0  41.73    0.14 2.17 37.48 40.25 41.70 43.24 46.17   253    1\nNA alpha2  27.91    0.03 1.23 25.49 27.10 27.93 28.70 30.30  1991    1\nNA alpha3  40.24    0.03 1.19 37.85 39.44 40.26 41.04 42.52  2033    1\nNA sigma    5.08    0.01 0.46  4.28  4.76  5.05  5.37  6.07  1685    1\nNA sigma_B 11.73    0.03 1.57  9.15 10.63 11.57 12.63 15.27  2206    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:08:33 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.f.df &lt;-as.data.frame(extract(data.rcb.rstan.f))\nhead(data.rcb.rstan.f.df)\n\nNA     alpha0   alpha2   alpha3    sigma  sigma_B      lp__\nNA 1 39.95904 28.82693 40.37188 5.244175 11.47116 -318.3557\nNA 2 40.26174 27.82831 41.60322 5.111554 12.84181 -311.5651\nNA 3 39.07137 28.27831 38.76523 4.700475 12.79742 -315.8146\nNA 4 41.14351 30.10751 40.30998 4.690441 13.88115 -325.3576\nNA 5 42.56938 26.98799 39.34253 5.384412 10.29316 -321.9314\nNA 6 40.45307 29.22397 41.05599 5.021199 12.07345 -314.9324\n\ndata.rcb.mcmc.f&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.f)\n\nplyr:::adply(as.matrix(data.rcb.rstan.f.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1  alpha0   41.698390   34.473108   40.246925   41.698390   43.235888\nNA 2  alpha2   27.929974   23.249223   27.096648   27.929974   28.701302\nNA 3  alpha3   40.264280   35.080170   39.437859   40.264280   41.043282\nNA 4   sigma    5.052888    3.786100    4.757611    5.052888    5.366813\nNA 5 sigma_B   11.567793    7.661363   10.634201   11.567793   12.631537\nNA 6    lp__ -321.145807 -342.911147 -325.003745 -321.145807 -317.615781\nNA         X100.       lower       upper     lower.1     upper.1\nNA 1   48.769516   37.757085   46.360518   40.444880   43.392733\nNA 2   32.665468   25.650646   30.418299   27.152255   28.743241\nNA 3   44.938585   37.833235   42.486534   39.485525   41.085516\nNA 4    7.054157    4.214457    5.954861    4.688346    5.269776\nNA 5   19.027742    9.063056   15.130256   10.347573   12.291484\nNA 6 -306.630367 -332.364491 -310.979729 -323.592133 -316.356397"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n}\ntransformed parameters {\n  vector[n] mu;    \n  \n  mu = X*beta;\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat &lt;- model.matrix(~A, data=data.rcb)\ndata.rcb.list &lt;- with(data.rcb, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rcb), nB=length(levels(Block))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.d &lt;- stan(data = data.rcb.list, file = \"matrixModel.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.9e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.343 seconds (Warm-up)\nNA Chain 1:                0.128 seconds (Sampling)\nNA Chain 1:                0.471 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.338 seconds (Warm-up)\nNA Chain 2:                0.127 seconds (Sampling)\nNA Chain 2:                0.465 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.d, par = c(\"beta\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 41.70    0.12 2.19 37.16 40.31 41.76 43.18 45.87   311 1.01\nNA beta[2] 27.93    0.02 1.20 25.55 27.14 27.93 28.71 30.22  2830 1.00\nNA beta[3] 40.25    0.02 1.25 37.78 39.41 40.26 41.09 42.71  2510 1.00\nNA sigma    5.06    0.01 0.45  4.28  4.74  5.04  5.33  6.03  1684 1.00\nNA sigma_B 11.71    0.03 1.52  9.13 10.61 11.58 12.64 15.06  2905 1.00\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:06 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.d.df &lt;-as.data.frame(extract(data.rcb.rstan.d))\nhead(data.rcb.rstan.d.df)\n\nNA     beta.1   beta.2   beta.3    sigma   sigma_B      lp__\nNA 1 43.53750 26.19984 38.88439 4.383973 11.909139 -311.6449\nNA 2 40.00707 29.45451 40.75455 5.187455 12.559968 -318.1385\nNA 3 41.04678 27.38811 40.37273 4.133845 12.234813 -313.0129\nNA 4 40.53589 28.77436 41.33340 4.601260  9.623768 -316.7945\nNA 5 39.19210 28.78798 41.84681 4.408715 12.655509 -313.2638\nNA 6 41.09020 28.31770 40.18192 4.504707 11.065015 -311.0851\n\ndata.rcb.mcmc.d&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.d)\n\nplyr:::adply(as.matrix(data.rcb.rstan.d.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1  beta.1   41.758141   33.279797   40.306435   41.758141   43.178469\nNA 2  beta.2   27.931300   23.108350   27.136590   27.931300   28.712865\nNA 3  beta.3   40.260645   34.766550   39.412227   40.260645   41.086043\nNA 4   sigma    5.035827    3.837230    4.738249    5.035827    5.330248\nNA 5 sigma_B   11.582596    7.470131   10.613220   11.582596   12.637936\nNA 6    lp__ -320.912804 -347.661595 -324.937268 -320.912804 -317.463916\nNA        X100.       lower       upper     lower.1     upper.1\nNA 1   51.15476   37.008800   45.713133   40.384918   43.218462\nNA 2   32.92478   25.618576   30.287537   27.029125   28.593104\nNA 3   44.64738   37.866252   42.759044   39.451019   41.116210\nNA 4    7.73581    4.288374    6.031137    4.719787    5.304595\nNA 5   19.08792    8.929760   14.729667   10.377374   12.370289\nNA 6 -306.91630 -332.294151 -311.520232 -324.167208 -316.967283"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Data generation",
    "text": "Data generation\nImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nslope &lt;- 30\nintercept &lt;- 200\nnBlock &lt;- 35\nnTime &lt;- 10\nsigma &lt;- 50\nsigma.block &lt;- 30\nn &lt;- nBlock*nTime\nBlock &lt;- gl(nBlock, k=1)\nTime &lt;- 1:10\nrho &lt;- 0.8\ndt &lt;- expand.grid(Time=Time,Block=Block)\nXmat &lt;- model.matrix(~-1+Block + Time, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\n#A.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,slope)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\n## Sum to zero block effects\n##block.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\n###A.effects &lt;- c(40,70,80)\n##all.effects &lt;- c(block.effects,intercept,slope)\n##lin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\neps &lt;- NULL\neps[1] &lt;- 0\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] #residuals\n}\ny &lt;- rnorm(n,lin.pred,sigma)+eps\n\n#OR\neps &lt;- NULL\n# first value cant be autocorrelated\neps[1] &lt;- rnorm(1,0,sigma)\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma)  #residuals\n}\ny &lt;- lin.pred + eps\ndata.rm &lt;- data.frame(y=y, dt)\nhead(data.rm)  #print out the first six rows of the data set\n\nNA          y Time Block\nNA 1 282.1142    1     1\nNA 2 321.1404    2     1\nNA 3 278.7700    3     1\nNA 4 285.8709    4     1\nNA 5 336.6390    5     1\nNA 6 333.5961    6     1\n\nggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method='lm') + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~Time, data.rm)\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nwith(data.rm, interaction.plot(Time,Block,y))\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+Time, data.rm))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA Time         -0.7274           0.4675\nNA Tukey test   -0.9809           0.3267\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\n\nNA       Test     Pvalue \nNA -0.9808606  0.3266615\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nwith(data.rm,tukey.add.test(y,Time,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 0.3997341   Denom df = 305    p-value = 0.5277003\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\n\nSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\nlibrary(nlme)\ndata.rm.lme &lt;- lme(y~Time, random=~1|Block, data=data.rm)\nacf(resid(data.rm.lme), lag=10)\n\n\n\n\n\n\n\n\nConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Model fitting",
    "text": "Model fitting"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n}\ntransformed parameters {\n  vector[n] mu;    \n  \n  mu = X*beta;\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel2.stan\")\n\nXmat &lt;- model.matrix(~Time, data=data.rm)\ndata.rm.list &lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rm), nB=length(levels(Block))))\n\nparams &lt;- c('beta','sigma','sigma_B')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.rstan.d  &lt;- stan(data = data.rm.list, file = \"matrixModel2.stan\", \n                            chains = nChains, pars = params, iter = nIter, \n                            warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.194 seconds (Warm-up)\nNA Chain 1:                0.285 seconds (Sampling)\nNA Chain 1:                1.479 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.6e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 1.21 seconds (Warm-up)\nNA Chain 2:                0.283 seconds (Sampling)\nNA Chain 2:                1.493 seconds (Total)\nNA Chain 2:\n\nprint(data.rm.rstan.d , par = c('beta','sigma','sigma_B'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1] 186.70    0.72 11.96 162.54 178.89 186.74 194.88 209.68   272    1\nNA beta[2]  30.79    0.02  1.02  28.82  30.10  30.77  31.48  32.75  2076    1\nNA sigma    55.90    0.04  2.21  51.64  54.35  55.83  57.39  60.29  2729    1\nNA sigma_B  64.52    0.19  8.76  50.39  58.35  63.49  69.44  83.96  2044    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:12 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n\nGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\nrstanString3=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n   vector [n] tgroup;\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n  real ar;\n}\ntransformed parameters {\n  vector[n] mu;    \n  vector[n] E;\n  vector[n] res;\n\n  mu = X*beta;\n  for (i in 1:n) {\n     E[i] = 0;\n  }\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n    res[i] = y[i] - mu[i];\n    if(i&gt;0 && i &lt; n && tgroup[i+1] == tgroup[i]) {\n      E[i+1] = res[i];\n    }\n    mu[i] = mu[i] + (E[i] * ar);\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString3, con = \"matrixModel3.stan\")\n\nXmat &lt;- model.matrix(~Time, data=data.rm)\ndata.rm.list &lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rm), nB=length(levels(Block)),\n  tgroup=as.numeric(Block)))\n\nparams &lt;- c('beta','sigma','sigma_B','ar')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.rstan.d  &lt;- stan(data = data.rm.list, file = \"matrixModel3.stan\", \n                            chains = nChains, pars = params, iter = nIter, \n                            warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 5.7e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.57 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.438 seconds (Warm-up)\nNA Chain 1:                0.502 seconds (Sampling)\nNA Chain 1:                1.94 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 2.7e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 2.343 seconds (Warm-up)\nNA Chain 2:                0.483 seconds (Sampling)\nNA Chain 2:                2.826 seconds (Total)\nNA Chain 2:\n\nprint(data.rm.rstan.d , par = c('beta','sigma','sigma_B','ar'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1] 179.17    0.24 12.76 153.91 170.91 179.06 187.48 204.24  2816    1\nNA beta[2]  31.29    0.02  1.70  27.91  30.17  31.29  32.43  34.72  5406    1\nNA sigma    48.73    0.03  1.99  45.11  47.33  48.63  50.06  52.72  5115    1\nNA sigma_B  49.78    0.26 10.61  30.48  42.28  49.07  56.56  73.10  1604    1\nNA ar        0.78    0.00  0.05   0.68   0.75   0.78   0.81   0.88  2786    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:49 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html",
    "title": "Nested Anova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#introduction",
    "title": "Nested Anova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nWhen single sampling units are selected amongst highly heterogeneous conditions, it is unlikely that these single units will adequately represent the populations and repeated sampling is likely to yield very different outcomes. For example, if we were investigating the impacts of fuel reduction burning across a highly heterogeneous landscape, our ability to replicate adequately might be limited by the number of burn sites available.\nAlternatively, sub-replicates within each of the sampling units (e.g. sites) can be collected (and averaged) so as to provided better representatives for each of the units and ultimately reduce the unexplained variability of the test of treatments. In essence, the sub-replicates are the replicates of an additional nested factor whose levels are nested within the main treatment factor. A nested factor refers to a factor whose levels are unique within each level of the factor it is nested within and each level is only represented once. For example, the fuel reduction burn study design could consist of three burnt sites and three un-burnt (control) sites each containing four quadrats (replicates of site and sub-replicates of the burn treatment). Each site represents a unique level of a random factor (any given site cannot be both burnt and un-burnt) that is nested within the fire treatment (burned or not).\nA nested design can be thought of as a hierarchical arrangement of factors (hence the alternative name hierarchical designs) whereby a treatment is progressively sub-replicated. As an additional example, imagine an experiment designed to comparing the leaf toughness of a number of tree species. Working down the hierarchy, five individual trees were randomly selected within (nested within) each species, three branches were randomly selected within each tree, two leaves were randomly selected within each branch and the force required to shear the leaf material in half (transversely) was measured in four random locations along the leaf. Clearly any given leaf can only be from a single branch, tree and species. Each level of sub-replication is introduced to further reduce the amount of unexplained variation and thereby increasing the power of the test for the main treatment effect. Additionally, it is possible to investigate which scale has the greatest (or least, etc) degree of variability - the level of the species, individual tree, branch, leaf, leaf region etc.\n\nNested factors are typically random factors, of which the levels are randomly selected to represent all possible levels (e.g. sites). When the main treatment effect (often referred to as Factor A) is a fixed factor, such designs are referred to as a mixed model nested ANOVA, whereas when Factor A is random, the design is referred to as a Model II nested ANOVA.\nFixed nested factors are also possible. For example, specific dates (corresponding to particular times during a season) could be nested within season. When all factors are fixed, the design is referred to as a Model I mixed model.\nFully nested designs (the topic of this chapter) differ from other multi-factor designs in that all factors within (below) the main treatment factor are nested and thus interactions are un-replicated and cannot be tested. Indeed, interaction effects (interaction between Factor A and site) are assumed to be zero."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-models-frequentist",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-models-frequentist",
    "title": "Nested Anova (JAGS)",
    "section": "Linear models (frequentist)",
    "text": "Linear models (frequentist)\nThe linear models for two and three factor nested design are:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk},\n\\]\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + gamma_{k(j(i))}  + \\epsilon_{ijkl},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-models-bayesian",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-models-bayesian",
    "title": "Nested Anova (JAGS)",
    "section": "Linear models (Bayesian)",
    "text": "Linear models (Bayesian)\nSo called “random effects” are modelled differently from “fixed effects” in that rather than estimate their individual effects, we instead estimate the variability due to these “random effects”. Since technically all variables in a Bayesian framework are random, some prefer to use the terms ‘fixed effects’ and ‘varying effects’. As random factors typically represent “random” selections of levels (such as a set of randomly selected sites), incorporated in order to account for the dependency structure (observations within sites are more likely to be correlated to one another - not strickly independent) to the data, we are not overly interested in the individual differences between levels of the ‘varying’ (random) factor. Instead (in addition to imposing a separate correlation structure within each nest), we want to know how much variability is attributed to this level of the design. The linear models for two and three factor nested design are:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}, \\;\\;\\; \\epsilon_{ijk} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B})\n\\]\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + \\gamma_{k(j(i))} + \\epsilon_{ijkl}, \\;\\;\\; \\epsilon_{ijkl} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\;\\;\\; \\gamma_{k(j(i))} \\sim N(0, \\sigma^2_C)\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the variability of Factor B (nested within Factor A), \\(\\gamma\\) is the variability of Factor C (nested within Factor B) and \\(\\epsilon\\) is the random unexplained or residual component that is assumed to be normally distributed with a mean of zero and a constant amount of standard deviation (\\(\\sigma^2\\)). The subscripts are iterators. For example, the \\(i\\) represents the number of effects to be estimated for Factor A. Thus the first formula can be read as the \\(k\\)-th observation of \\(y\\) is drawn from a normal distribution (with a specific level of variability) and mean proposed to be determined by a base mean (\\(\\mu\\) - mean of the first treatment across all nests) plus the effect of the \\(i\\)-th treatment effect plus the variabilitythe model proposes that, given a base mean (\\(\\mu\\)) and knowing the effect of the \\(i\\)-th treatment (factor A) and which of the \\(j\\)-th nests within the treatment the \\(k\\)-th observation from Block \\(j\\) (factor B) within treatment effect."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#null-hypotheses",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#null-hypotheses",
    "title": "Nested Anova (JAGS)",
    "section": "Null hypotheses",
    "text": "Null hypotheses\nSeparate null hypotheses are associated with each of the factors, however, nested factors are typically only added to absorb some of the unexplained variability and thus, specific hypotheses tests associated with nested factors are of lesser biological importance. Hence, rather than estimate the effects of random effects, we instead estimate how much variability they contribute.\nFactor A: the main treatment effect (fixed)\n\n\\(H_0(A): \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\\(H_0(A) : \\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\nFactor A: the main treatment effect (random)\n\n\\(H_0(A) : \\sigma^2_{\\alpha}=0\\) (population variance equals zero). There is no added variance due to all possible levels of A.\n\nFactor B: the nested effect (random)\n\n\\(H_0(B) : \\sigma^2_{\\beta}=0\\) (population variance equals zero). There is no added variance due to all possible levels of B within the (set or all possible) levels of A.\n\nFactor B: the nested effect (fixed)\n\n\\(H_0(B): \\mu_{1(1)}=\\mu_{2(1)}=\\ldots=\\mu_{j(i)}=\\mu\\) (the population group means of B (within A) are all equal).\n\\(H_0(B): \\beta_{1(1)}=\\beta_{2(1)}=\\ldots=\\beta_{j(i)}=0\\) (the effect of each chosen B group equals zero)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#analysis-of-variance",
    "title": "Nested Anova (JAGS)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nAnalysis of variance sequentially partitions the total variability in the response variable into components explained by each of the factors (starting with the factors lowest down in the hierarchy - the most deeply nested) and the components unexplained by each factor. Explained variability is calculated by subtracting the amount unexplained by the factor from the amount unexplained by a reduced model that does not contain the factor. When the null hypothesis for a factor is true (no effect or added variability), the ratio of explained and unexplained components for that factor (F-ratio) should follow a theoretical F-distribution with an expected value less than 1. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different combinations of fixed and random factors in a nested linear model (see Table below).\n\nfact_anova_table\n\nThe corresponding R syntax is given below.\n\n#A fixed/random, B random (balanced)\nsummary(aov(y~A+Error(B), data))\nVarCorr(lme(y~A,random=1|B, data))\n\n#A fixed/random, B random (unbalanced)\nanova(lme(y~A,random=1|B, data), type='marginal')\n\n#A fixed/random, B fixed(balanced)\nsummary(aov(y~A+B, data))\n\n#A fixed/random, B fixed (unbalanced)\ncontrasts(data$B) &lt;- contr.sum\nAnova(aov(y~A/B, data), type='III')"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#variance-components",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#variance-components",
    "title": "Nested Anova (JAGS)",
    "section": "Variance components",
    "text": "Variance components\nAs previously alluded to, it can often be useful to determine the relative contribution (to explaining the unexplained variability) of each of the factors as this provides insights into the variability at each different scales. These contributions are known as Variance components and are estimates of the added variances due to each of the factors. For consistency with leading texts on this topic, I have included estimated variance components for various balanced nested ANOVA designs in the above table. However, variance components based on a modified version of the maximum likelihood iterative model fitting procedure (REML) is generally recommended as this accommodates both balanced and unbalanced designs. While there are no numerical differences in the calculations of variance components for fixed and random factors, fixed factors are interpreted very differently and arguably have little clinical meaning (other to infer relative contribution). For fixed factors, variance components estimate the variance between the means of the specific populations that are represented by the selected levels of the factor and therefore represent somewhat arbitrary and artificial populations. For random factors, variance components estimate the variance between means of all possible populations that could have been selected and thus represents the true population variance."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#assumptions",
    "title": "Nested Anova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAn F-distribution represents the relative frequencies of all the possible F-ratio’s when a given null hypothesis is true and certain assumptions about the residuals (denominator in the F-ratio calculation) hold. Consequently, it is also important that diagnostics associated with a particular hypothesis test reflect the denominator for the appropriate F-ratio. For example, when testing the null hypothesis that there is no effect of Factor A (\\(H_0(A):\\alpha_i=0\\)) in a mixed nested ANOVA, the means of each level of Factor B are used as the replicates of Factor A. As with single factor anova, hypothesis testing for nested ANOVA assumes the residuals are:\n\nnormally distributed. Factors higher up in the hierarchy of a nested model are based on means (or means of means) of lower factors and thus the Central Limit Theory would predict that normality will usually be satisfied for the higher level factors. Nevertheless, boxplots using the appropriate scale of replication should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another - this requires special consideration so as to ensure that the scale at which sub-replicates are measured is still great enough to enable observations to be independent."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#unbalanced-nested-designs",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#unbalanced-nested-designs",
    "title": "Nested Anova (JAGS)",
    "section": "Unbalanced nested designs",
    "text": "Unbalanced nested designs\nDesigns that incorporate fixed and random factors (either nested or factorial), involve F-ratio calculations in which the denominators are themselves random factors other than the overall residuals. Many statisticians argue that when such denominators are themselves not statistically significant (at the \\(0.25\\) level), there are substantial power benefits from pooling together successive non-significant denominator terms. Thus an F-ratio for a particular factor might be recalculated after pooling together its original denominator with its denominators denominator and so on. The conservative \\(0.25\\) is used instead of the usual 0.05 to reduce further the likelihood of Type II errors (falsely concluding an effect is non-significant - that might result from insufficient power).\nFor a simple completely balanced nested ANOVA, it is possible to pool together (calculate their mean) each of the sub-replicates within each nest (site) and then perform single factor ANOVA on those aggregates. Indeed, for a balanced design, the estimates and hypothesis for Factor A will be identical to that produced via nested ANOVA. However, if there are an unequal number of sub-replicates within each nest, then the single factor ANOVA will be less powerful that a proper nested ANOVA. Unbalanced designs are those designs in which sample (subsample) sizes for each level of one or more factors differ. These situations are relatively common in biological research, however such imbalance has some important implications for nested designs.\nFirstly, hypothesis tests are more robust to the assumptions of normality and equal variance when the design is balanced. Secondly (and arguably, more importantly), the model contrasts are not orthogonal (independent) and the sums of squares component attributed to each of the model terms cannot be calculated by simple additive partitioning of the total sums of squares. In such situations, exact F-ratios cannot be constructed (at least in theory), variance components calculations are more complicated and significance tests cannot be computed. The denominator MS in an F-ratio is determined by examining the expected value of the mean squares of each term in a model. Unequal sample sizes result in expected means squares for which there are no obvious logical comparators that enable the impact of an individual model term to be isolated. The severity of this issue depends on which scale of the sub-sampling hierarchy the unbalance(s) occurs as well whether the unbalance occurs in the replication of a fixed or random factor. For example, whilst unequal levels of the first nesting factor (e.g. unequal number of burn vs un-burnt sites) has no effect on F-ratio construction or hypothesis testing for the top level factor (irrespective of whether either of the factors are fixed or random), unequal sub-sampling (replication) at the level of a random (but not fixed) nesting factor will impact on the ability to construct F-ratios and variance components of all terms above it in the hierarchy. There are a number of alternative ways of dealing with unbalanced nested designs. All alternatives assume that the imbalance is not a direct result of the treatments themselves. Such outcomes are more appropriately analysed by modelling the counts of surviving observations via frequency analysis.\n\nSplit the analysis up into separate smaller simple ANOVA’s each using the means of the nesting factor to reflect the appropriate scale of replication. As the resulting sums of squares components are thereby based on an aggregated dataset the analyses then inherit the procedures and requirements of single ANOVA.\nAdopt mixed-modelling techniques.\n\nWe note that, in a Bayesian framework, issues of design balance essentially evaporate."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-mixed-effects-models",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#linear-mixed-effects-models",
    "title": "Nested Anova (JAGS)",
    "section": "Linear mixed effects models",
    "text": "Linear mixed effects models\nAlthough the term “mixed-effects” can be used to refer to any design that incorporates both fixed and random predictors, its use is more commonly restricted to designs in which factors are nested or grouped within other factors. Typical examples include nested, longitudinal (measurements repeated over time) data, repeated measures and blocking designs. Furthermore, rather than basing parameter estimations on observed and expected mean squares or error strata (as outline above), mixed-effects models estimate parameters via maximum likelihood (ML) or residual maximum likelihood (REML). In so doing, mixed-effects models more appropriately handle estimation of parameters, effects and variance components of unbalanced designs (particularly for random effects). Resulting fitted (or expected) values of each level of a factor (for example, the expected population site means) are referred to as Best Linear Unbiased Predictors (BLUP’s). As an acknowledgement that most estimated site means will be more extreme than the underlying true population means they estimate (based on the principle that smaller sample sizes result in greater chances of more extreme observations and that nested sub-replicates are also likely to be highly intercorrelated), BLUP’s are less spread from the overall mean than are simple site means. In addition, mixed-effects models naturally model the “within-block” correlation structure that complicates many longitudinal designs.\nWhilst the basic concepts of mixed-effects models have been around for a long time, recent computing advances and adoptions have greatly boosted the popularity of these procedures. Linear mixed effects models are currently at the forefront of statistical development, and as such, are very much a work in progress - both in theory and in practice. Recent developments have seen a further shift away from the traditional practices associated with degrees of freedom, probability distribution and p-value calculations. The traditional approach to inference testing is to compare the fit of an alternative (full) model to a null (reduced) model (via an F-ratio). When assumptions of normality and homogeneity of variance apply, the degrees of freedom are easily computed and the F-ratio has an exact F-distribution to which it can be compared. However, this approach introduces two additional problematic assumptions when estimating fixed effects in a mixed effects model. Firstly, when estimating the effects of one factor, the parameter estimates associated with other factor(s) are assumed to be the true values of those parameters (not estimates). Whilst this assumption is reasonable when all factors are fixed, as random factors are selected such that they represent one possible set of levels drawn from an entire population of possible levels for the random factor, it is unlikely that the associated parameter estimates accurately reflect the true values. Consequently, there is not necessarily an appropriate F-distribution. Furthermore, determining the appropriate degrees of freedom (nominally, the number of independent observations on which estimates are based) for models that incorporate a hierarchical structure is only possible under very specific circumstances (such as completely balanced designs). Degrees of freedom is a somewhat arbitrary defined concept used primarily to select a theoretical probability distribution on which a statistic can be compared. Arguably, however, it is a concept that is overly simplistic for complex hierarchical designs. Most statistical applications continue to provide the “approximate” solutions (as did earlier versions within R). However, R linear mixed effects development leaders argue strenuously that given the above shortcomings, such approximations are variably inappropriate and are thus omitted.\nMarkov chain Monte Carlo (MCMC) sampling methods provide a Bayesian-like alternative for inference testing. Markov chains use the mixed model parameter estimates to generate posterior probability distributions of each parameter from which Monte Carlo sampling methods draw a large set of parameter samples. These parameter samples can then be used to calculate highest posterior density (HPD) intervals (also known as Bayesian credible intervals). Such intervals indicate the interval in which there is a specified probability (typically \\(95\\)%) that the true population parameter lies. Furthermore, whilst technically against the spirit of the Bayesian philosophy, it is also possible to generate P values on which to base inferences."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#exploratory-data-analysis",
    "title": "Nested Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n#Effects of treatment\nboxplot(y~A, ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Site effects\nboxplot(y~Sites, ddply(data.nest, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n## with ggplot2\nggplot(ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)), aes(y=y, x=A)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#full-effect-parameterisation",
    "title": "Nested Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- alpha0 + alpha[A[i]] + beta[site[i]]\n   }\n   \n   #Priors\n   alpha0 ~ dnorm(0, 1.0E-6)\n   alpha[1] &lt;- 0\n   for (i in 2:nA) {\n     alpha[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   for (i in 1:nSite) {\n     beta[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;-z/sqrt(chSq.B)\n   z.B ~ dnorm(0, .0016)I(0,)\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.nest.list &lt;- with(data.nest,\n        list(y=y,\n                 site=as.numeric(Sites),\n         A=as.numeric(A),\n         n=nrow(data.nest),\n         nSite=length(levels(Sites)),\n                 nA = length(levels(A))\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha0\",\"alpha\",\"sigma\",\"sigma.B\")\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.nest.r2jags.f &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 22\nNA    Total graph size: 502\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.f)\n\nNA Inference for Bugs model at \"fullModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA alpha[1]   0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1\nNA alpha[2]  27.388   7.149  13.085  22.881  27.312  31.980  41.230 1.001  3000\nNA alpha[3]  40.839   7.083  26.936  36.251  40.800  45.412  55.107 1.002  3000\nNA alpha0    42.325   4.978  32.452  39.136  42.215  45.422  52.310 1.002  3000\nNA sigma      5.069   0.307   4.530   4.851   5.051   5.265   5.722 1.002  3000\nNA sigma.B   10.990   2.527   7.168   9.260  10.656  12.306  17.136 1.009   190\nNA deviance 909.635   5.937 899.898 905.400 908.952 913.145 923.175 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 17.6 and DIC = 927.3\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#matrix-parameterisation",
    "title": "Nested Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(alpha[],X[i,]) + inprod(beta[], Z[i,])\n   } \n   \n   #Priors\n   alpha ~ dmnorm(a0,A0)\n   for (i in 1:nZ) {\n     beta[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;-z/sqrt(chSq.B)\n   z.B ~ dnorm(0, .0016)I(0,)\n   chSq.B ~ dgamma(0.5, 0.5)\n\n}\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA.Xmat &lt;- model.matrix(~A,data.nest)\nZmat &lt;- model.matrix(~-1+Sites, data.nest)\ndata.nest.list &lt;- with(data.nest,\n        list(y=y,\n         X=A.Xmat,\n         n=nrow(data.nest),\n         Z=Zmat, nZ=ncol(Zmat),\n         a0=rep(0,3), A0=diag(3)\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma.B\",'beta')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.nest.r2jags.m &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 20\nNA    Total graph size: 3231\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.m)\n\nNA Inference for Bugs model at \"matrixModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA alpha[1]   0.201   1.016  -1.750  -0.474   0.215   0.872   2.161 1.001  3000\nNA alpha[2]   0.082   0.972  -1.835  -0.585   0.092   0.730   1.954 1.003  2000\nNA alpha[3]   0.077   1.005  -1.867  -0.608   0.075   0.771   2.055 1.001  3000\nNA beta[1]   31.532   1.871  27.942  30.237  31.536  32.794  35.248 1.001  3000\nNA beta[2]   38.069   1.911  34.289  36.788  38.125  39.343  41.817 1.001  3000\nNA beta[3]   59.346   1.872  55.692  58.089  59.346  60.579  63.088 1.001  3000\nNA beta[4]   40.644   1.936  36.885  39.378  40.659  41.960  44.321 1.002  1400\nNA beta[5]   40.506   1.855  36.802  39.248  40.492  41.750  44.199 1.001  3000\nNA beta[6]   90.495   2.131  86.451  89.013  90.489  91.970  94.602 1.001  3000\nNA beta[7]   75.252   2.114  71.007  73.850  75.238  76.681  79.322 1.002  1200\nNA beta[8]   57.061   2.180  52.888  55.574  57.032  58.568  61.289 1.001  2400\nNA beta[9]   61.336   2.171  57.214  59.855  61.372  62.822  65.415 1.001  3000\nNA beta[10]  62.816   2.159  58.580  61.353  62.774  64.268  67.144 1.001  3000\nNA beta[11]  93.379   2.134  89.192  91.945  93.374  94.750  97.533 1.001  3000\nNA beta[12]  83.011   2.161  78.822  81.508  83.024  84.486  87.245 1.001  3000\nNA beta[13]  82.765   2.202  78.398  81.292  82.774  84.252  87.054 1.001  3000\nNA beta[14]  81.140   2.165  76.775  79.675  81.185  82.598  85.236 1.001  3000\nNA beta[15]  74.041   2.119  70.008  72.616  74.027  75.493  78.245 1.001  3000\nNA sigma      5.058   0.306   4.499   4.844   5.049   5.255   5.710 1.002  1200\nNA sigma.B   68.791  13.133  48.825  59.338  66.869  75.995  98.963 1.002  3000\nNA deviance 909.431   6.235 899.560 905.043 908.621 913.008 923.865 1.003   810\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 19.4 and DIC = 928.8\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#hierarchical-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#hierarchical-parameterisation",
    "title": "Nested Anova (JAGS)",
    "section": "Hierarchical parameterisation",
    "text": "Hierarchical parameterisation\n\nmodelString3=\"\nmodel {\n   #Likelihood (esimating site means (gamma.site)\n   for (i in 1:n) {\n      y[i]~dnorm(quad.means[i],tau)\n      quad.means[i] &lt;- gamma.site[site[i]]\n   }\n   for (i in 1:s) {\n      gamma.site[i] ~ dnorm(site.means[i], tau.site)\n      site.means[i] &lt;- inprod(beta[],A.Xmat[i,])\n   }\n   #Priors\n   for (i in 1:a) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;-z/sqrt(chSq.B)\n   z.B ~ dnorm(0, .0016)I(0,)\n   chSq.B ~ dgamma(0.5, 0.5)\n\n   tau.site &lt;- pow(sigma.site,-2)\n   sigma.site &lt;-z/sqrt(chSq.site)\n   z.site ~ dnorm(0, .0016)I(0,)\n   chSq.site ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString3, con = \"hierarchicalModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA.Xmat &lt;- model.matrix(~A,ddply(data.nest,~Sites,catcolwise(unique)))\ndata.nest.list &lt;- with(data.nest,\n        list(y=y,\n                 site=Sites,\n         A.Xmat= A.Xmat,\n         n=nrow(data.nest),\n         s=length(levels(Sites)),\n                 a = ncol(A.Xmat)\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",\"sigma\",\"sigma.site\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.nest.r2jags.h &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"hierarchicalModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 24\nNA    Total graph size: 406\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.h)\n\nNA Inference for Bugs model at \"hierarchicalModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     42.139   4.991  32.186  38.913  42.226  45.346  51.751 1.001  3000\nNA beta[2]     27.611   6.859  13.692  23.437  27.617  31.993  41.118 1.001  3000\nNA beta[3]     41.048   7.032  26.813  36.805  41.067  45.316  55.566 1.002  1200\nNA sigma        5.058   0.315   4.483   4.841   5.036   5.257   5.763 1.001  3000\nNA sigma.site  10.889   2.386   7.235   9.269  10.578  12.125  16.695 1.005  3000\nNA deviance   909.557   6.168 899.915 905.154 908.708 913.153 923.686 1.001  1900\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 19.0 and DIC = 928.6\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nIf you want to include finite-population standard deviations in the model you can use the following code.\n\nmodelString4=\"\nmodel {\n   #Likelihood (esimating site means (gamma.site)\n   for (i in 1:n) {\n      y[i]~dnorm(quad.means[i],tau)\n      quad.means[i] &lt;- gamma.site[site[i]]\n      y.err[i]&lt;- quad.means[i]-y[i]\n   }\n   for (i in 1:s) {\n      gamma.site[i] ~ dnorm(site.means[i], tau.site)\n      site.means[i] &lt;- inprod(beta[],A.Xmat[i,])\n      site.err[i] &lt;- site.means[i] - gamma.site[i]\n   }\n   #Priors\n   for (i in 1:a) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.site &lt;- pow(sigma.site,-2)\n   sigma.site &lt;-z/sqrt(chSq.site)\n   z.site ~ dnorm(0, .0016)I(0,)\n   chSq.site ~ dgamma(0.5, 0.5)\n   \n   sd.y &lt;- sd(y.err)\n   sd.site &lt;- sd(site.err)\n   sd.A &lt;- sd(beta)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString4, con = \"SDModel.txt\")\n\n#data list\nA.Xmat &lt;- model.matrix(~A,ddply(data.nest,~Sites,catcolwise(unique)))\ndata.nest.list &lt;- with(data.nest,\n        list(y=y,\n                 site=Sites,\n         A.Xmat= A.Xmat,\n         n=nrow(data.nest),\n         s=length(levels(Sites)),\n                 a = ncol(A.Xmat)\n         )\n)\n\n#parameters and chain details\nparams &lt;- c(\"beta\",\"sigma\",\"sd.y\",'sd.site','sd.A','sigma.site')\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest.r2jags.SD &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"SDModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 22\nNA    Total graph size: 571\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.SD)\n\nNA Inference for Bugs model at \"SDModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     42.336   5.027  32.564  39.187  42.338  45.373  52.570 1.004   420\nNA beta[2]     27.417   7.290  12.457  22.904  27.308  31.955  42.039 1.001  2100\nNA beta[3]     40.862   7.164  26.163  36.386  40.920  45.444  55.173 1.007   770\nNA sd.A        10.042   4.276   2.657   7.162   9.646  12.369  19.900 1.001  2200\nNA sd.site     10.592   1.057   9.214   9.909  10.354  11.029  13.276 1.010   280\nNA sd.y         4.999   0.095   4.852   4.929   4.987   5.058   5.219 1.003   770\nNA sigma        5.047   0.309   4.489   4.830   5.029   5.257   5.705 1.005   310\nNA sigma.site  11.003   2.465   7.419   9.295  10.610  12.282  16.704 1.004   480\nNA deviance   909.411   6.011 899.576 904.938 908.750 913.034 922.925 1.003   630\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 18.0 and DIC = 927.5\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCalculate \\(R^2\\) from the posterior of the model.\n\ndata.nest.mcmc.listSD &lt;- as.mcmc(data.nest.r2jags.SD)\n\nXmat &lt;- model.matrix(~A, data.nest)\ncoefs &lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[['beta']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nZ.var &lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[['sd.site']]^2\nR.var &lt;- data.nest.r2jags.SD$BUGSoutput$sims.list[['sd.y']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.site &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.site &lt;- data.frame(Mean=mean(R2.site), Median=median(R2.site), HPDinterval(as.mcmc(R2.site)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\nrbind(R2.site=R2.site, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\n\nNA                      Mean    Median      lower      upper\nNA R2.site        0.26437322 0.2428822 0.16881028 0.41958555\nNA R2.marginal    0.67674004 0.6992418 0.49930501 0.78437310\nNA R2.res         0.05888674 0.0584191 0.03459529 0.08514432\nNA R2.conditional 0.94111326 0.9415809 0.91485568 0.96540471"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#graphical-summaries",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#graphical-summaries",
    "title": "Nested Anova (JAGS)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nnewdata &lt;- with(data.nest, data.frame(A=levels(A)))\nXmat &lt;- model.matrix(~A, newdata)\ncoefs &lt;- data.nest.r2jags.m$BUGSoutput$sims.list[['alpha']]\nfit &lt;- coefs %*% t(Xmat)\nnewdata &lt;- cbind(newdata,\n   adply(fit, 2, function(x) {\n          data.frame(Mean=mean(x), Median=median(x), HPDinterval(as.mcmc(x)),\n             HPDinterval(as.mcmc(x), p=0.68))\n   })\n)\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(grid)\np1 &lt;- ggplot(newdata, aes(y=Median, x=A)) +\n  geom_errorbar(aes(ymin=lower, ymax=upper), width=0.01, size=1) +\n  geom_errorbar(aes(ymin=lower.1, ymax=upper.1), width=0, size=2) +\n  geom_point(size=4, shape=21, fill='white')+\n  scale_y_continuous('Y')+\n  scale_x_discrete('X')+\n  theme_classic()+\n  theme(axis.title.y=element_text(vjust=2, size=rel(1.25)),\n        axis.title.x=element_text(vjust=-2, size=rel(1.25)),\n        plot.margin=unit(c(0.5,0.5,2,2), 'lines')\n  )\n\np1"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#exploratory-data-analysis-1",
    "title": "Nested Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n#Effects of treatment\nboxplot(y~A, ddply(data.nest1, ~A+Sites,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Site effects\nboxplot(y~Sites, ddply(data.nest1, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Quadrat effects\nboxplot(y~Quads, ddply(data.nest1, ~A+Sites+Quads+Pits,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity.\nit is a little difficult to assess normality/homogeneity of variance of quadrats since there are only two pits per quadrat. Nevertheless, there is no suggestion that variance increases with increasing mean.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#frequentist-for-comparison",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#frequentist-for-comparison",
    "title": "Nested Anova (JAGS)",
    "section": "Frequentist for comparison",
    "text": "Frequentist for comparison\n\nlibrary(nlme)\nd.lme &lt;- lme(y ~ A, random=~1|Sites/Quads,data=data.nest1)\nsummary(d.lme)\n\nNA Linear mixed-effects model fit by REML\nNA   Data: data.nest1 \nNA        AIC      BIC   logLik\nNA   1137.994 1155.937 -562.997\nNA \nNA Random effects:\nNA  Formula: ~1 | Sites\nNA         (Intercept)\nNA StdDev:    10.38249\nNA \nNA  Formula: ~1 | Quads %in% Sites\nNA         (Intercept) Residual\nNA StdDev:    8.441617 7.161177\nNA \nNA Fixed effects:  y ~ A \nNA                Value Std.Error DF  t-value p-value\nNA (Intercept) 41.38646  5.043341 75 8.206159  0.0000\nNA Aa2         21.36271  7.132361 12 2.995180  0.0112\nNA Aa3         39.14584  7.132361 12 5.488482  0.0001\nNA  Correlation: \nNA     (Intr) Aa2   \nNA Aa2 -0.707       \nNA Aa3 -0.707  0.500\nNA \nNA Standardized Within-Group Residuals:\nNA         Min          Q1         Med          Q3         Max \nNA -2.11852502 -0.54600753 -0.03428581  0.53382436  2.26256392 \nNA \nNA Number of Observations: 150\nNA Number of Groups: \nNA            Sites Quads %in% Sites \nNA               15               75\n\nanova(d.lme)\n\nNA             numDF denDF  F-value p-value\nNA (Intercept)     1    75 446.9150  &lt;.0001\nNA A               2    12  15.1037   5e-04"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#full-effect-parameterisation-1",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#full-effect-parameterisation-1",
    "title": "Nested Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- alpha0 + alpha[A[i]] + beta.site[site[i]] + beta.quad[quad[i]]\n   }\n   \n   #Priors\n   alpha0 ~ dnorm(0, 1.0E-6)\n   alpha[1] &lt;- 0\n   for (i in 2:nA) {\n     alpha[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   for (i in 1:nSite) {\n     beta.site[i] ~ dnorm(0, tau.Bs) #prior\n   }\n   for (i in 1:nQuad) {\n     beta.quad[i] ~ dnorm(0, tau.Bq) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.Bs &lt;- pow(sigma.Bs,-2)\n   sigma.Bs &lt;-z/sqrt(chSq.Bs)\n   z.Bs ~ dnorm(0, .0016)I(0,)\n   chSq.Bs ~ dgamma(0.5, 0.5)\n\n   tau.Bq &lt;- pow(sigma.Bq,-2)\n   sigma.Bq &lt;-z/sqrt(chSq.Bq)\n   z.Bq ~ dnorm(0, .0016)I(0,)\n   chSq.Bq ~ dgamma(0.5, 0.5)\n\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel2.txt\")\n\ndata.nest.list &lt;- with(data.nest1,\n        list(y=y,\n                 site=as.numeric(Sites),\n         A=as.numeric(A),\n         n=nrow(data.nest1),\n         nSite=length(levels(Sites)),\n                 nA = length(levels(A)),\n         nQuad=length(levels(Quads)),\n                 quad = as.numeric(Quads)\n         )\n)\n\nparams &lt;- c(\"alpha0\",\"alpha\",\"sigma\",\"sigma.Bs\",\"sigma.Bq\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest.r2jags.f2 &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 99\nNA    Total graph size: 793\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.f2)\n\nNA Inference for Bugs model at \"fullModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%      25%      50%      75%    97.5%  Rhat\nNA alpha[1]    0.000   0.000   0.000    0.000    0.000    0.000    0.000 1.000\nNA alpha[2]   21.147   7.532   6.252   16.137   21.140   25.968   35.890 1.001\nNA alpha[3]   38.985   7.635  23.341   34.130   39.120   43.879   53.757 1.001\nNA alpha0     41.541   5.460  30.677   37.967   41.659   45.032   52.383 1.001\nNA sigma       7.294   0.604   6.238    6.870    7.264    7.664    8.580 1.003\nNA sigma.Bq    8.433   1.132   6.355    7.650    8.378    9.175   10.757 1.005\nNA sigma.Bs   10.779   2.673   6.704    8.951   10.409   12.219   17.127 1.017\nNA deviance 1020.495  17.724 988.898 1007.948 1019.500 1032.389 1056.708 1.005\nNA          n.eff\nNA alpha[1]     1\nNA alpha[2]  3000\nNA alpha[3]  3000\nNA alpha0    3000\nNA sigma      970\nNA sigma.Bq   420\nNA sigma.Bs   100\nNA deviance   510\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 156.8 and DIC = 1177.3\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-jags/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-09-nested-anova-jags/index.html#matrix-parameterisation-1",
    "title": "Nested Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(alpha[], X[i,]) + inprod(beta.site[],Z.site[i,]) + inprod(beta.quad[],Z.quad[i,])\n      y.err[i] &lt;- y[i]-mu[i]\n   }\n   \n   #Priors\n   for (i in 1:nX) {\n     alpha[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   for (i in 1:nSite) {\n     beta.site[i] ~ dnorm(0, tau.Bs) #prior\n   }\n   for (i in 1:nQuad) {\n     beta.quad[i] ~ dnorm(0, tau.Bq) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;-z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.Bs &lt;- pow(sigma.Bs,-2)\n   sigma.Bs &lt;-z/sqrt(chSq.Bs)\n   z.Bs ~ dnorm(0, .0016)I(0,)\n   chSq.Bs ~ dgamma(0.5, 0.5)\n\n   tau.Bq &lt;- pow(sigma.Bq,-2)\n   sigma.Bq &lt;-z/sqrt(chSq.Bq)\n   z.Bq ~ dnorm(0, .0016)I(0,)\n   chSq.Bq ~ dgamma(0.5, 0.5)\n\n   sd.res &lt;- sd(y.err[])\n   sd.site &lt;- sd(beta.site[])\n   sd.quad &lt;- sd(beta.quad[])   \n }\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel2.txt\")\n\nXmat &lt;- model.matrix(~A, data=data.nest1)\nZsite &lt;- model.matrix(~-1+Sites, data=data.nest1)\nZquad &lt;- model.matrix(~-1+Quads, data=data.nest1)\n\ndata.nest.list &lt;- with(data.nest1,\n        list(y=y,\n         n=nrow(data.nest1),\n                 X=Xmat, nX=ncol(Xmat),\n         Z.site=Zsite, nSite=ncol(Zsite),\n                 Z.quad=Zquad, nQuad=ncol(Zquad)\n         )\n)\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma.Bs\",\"sigma.Bq\",'sd.res','sd.site','sd.quad')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest.r2jags.m2 &lt;- jags(data = data.nest.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 150\nNA    Unobserved stochastic nodes: 99\nNA    Total graph size: 14993\nNA \nNA Initializing model\n\nprint(data.nest.r2jags.m2)\n\nNA Inference for Bugs model at \"matrixModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%      25%      50%      75%    97.5%  Rhat\nNA alpha[1]   41.247   5.438  30.494   37.721   41.227   44.692   52.262 1.002\nNA alpha[2]   21.535   7.824   6.537   16.541   21.439   26.473   37.416 1.003\nNA alpha[3]   39.276   7.723  24.165   34.357   39.319   44.191   54.637 1.001\nNA sd.quad     8.427   0.828   6.866    7.889    8.420    8.956   10.131 1.001\nNA sd.res      7.221   0.420   6.500    6.924    7.186    7.486    8.137 1.010\nNA sd.site    10.263   1.703   7.202    9.180   10.187   11.240   13.917 1.002\nNA sigma       7.261   0.598   6.189    6.845    7.209    7.631    8.540 1.010\nNA sigma.Bq    8.514   1.064   6.557    7.776    8.454    9.189   10.801 1.001\nNA sigma.Bs   10.703   2.802   6.379    8.805   10.283   12.108   17.304 1.001\nNA deviance 1019.366  17.429 987.783 1007.166 1018.196 1030.618 1056.340 1.010\nNA          n.eff\nNA alpha[1]  3000\nNA alpha[2]  3000\nNA alpha[3]  2500\nNA sd.quad   3000\nNA sd.res     150\nNA sd.site   3000\nNA sigma      160\nNA sigma.Bq  3000\nNA sigma.Bs  3000\nNA deviance   160\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 151.0 and DIC = 1170.4\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nIf we use the JAGS matrix parameterisation model from above, the JAGS model is already complete (as we defined the sd components in that model already).\n\ndata.nest1.mcmc.listSD &lt;- as.mcmc(data.nest.r2jags.m2)\n\nXmat &lt;- model.matrix(~A, data.nest1)\ncoefs &lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[['alpha']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nZ.var &lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[['sd.site']]^2\nR.var &lt;- data.nest.r2jags.m2$BUGSoutput$sims.list[['sd.res']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.site &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.site &lt;- data.frame(Mean=mean(R2.site), Median=median(R2.site), HPDinterval(as.mcmc(R2.site)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\nrbind(R2.site=R2.site, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\n\nNA                     Mean    Median     lower     upper\nNA R2.site        0.2537842 0.2373232 0.1145934 0.4450797\nNA R2.marginal    0.6199972 0.6408875 0.4077973 0.7873383\nNA R2.res         0.1262186 0.1233096 0.0646023 0.1907540\nNA R2.conditional 0.8737814 0.8766904 0.8092460 0.9353977"
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html",
    "href": "tutorials/2020-02-08-acf-jags/index.html",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#introduction",
    "href": "tutorials/2020-02-08-acf-jags/index.html#introduction",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\n\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\nIn order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:\n\\[\ny_i \\sim Dist(\\mu_i),\n\\]\nwhere \\(\\mu_i=\\boldsymbol X \\boldsymbol \\beta + \\boldsymbol Z \\boldsymbol \\gamma\\), with \\(\\boldsymbol X\\) and \\(\\boldsymbol \\beta\\) representing the fixed data structure and fixed effects, respectively, while with \\(\\boldsymbol Z\\) and \\(\\boldsymbol \\gamma\\) represent the varying data structure and varying effects, respectively. In simple regression, there are no “varying” effects, and thus:\n\\[\n\\boldsymbol \\gamma \\sim MVN(\\boldsymbol 0, \\boldsymbol \\Sigma),\n\\]\nwhere \\(\\boldsymbol \\Sigma\\) is a variance-covariance matrix of the form\n\\[\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho^{\\phi_{1,2}} & \\ldots & \\rho^{\\phi_{1,n}} \\\\\n   \\rho^{\\phi_{2,1}} & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\phi_{n,1}} & \\ldots & \\ldots & 1\n   \\end{bmatrix}.\n\\]\nNotice that this introduces a very large number of additional parameters that require estimating: \\(\\sigma^2\\) (error variance), \\(\\rho\\) (base autocorrelation) and each of the individual covariances (\\(\\rho^{\\phi_{n,n}}\\)). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.\n\nWhen we assume that all residuals are independent (regular regression), i.e. \\(\\rho=0\\), \\(\\boldsymbol \\Sigma\\) is essentially equal to \\(\\sigma^2 \\boldsymbol I\\) and we simply use:\n\n\\[\n\\boldsymbol \\gamma \\sim N( 0,\\sigma^2).\n\\]\n\nWe could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a first order autoregressive (AR1) structure in which exponent on the correlation declines linearly according to the time lag (\\(\\mid t - s\\mid\\)).\n\n\\[\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho & \\ldots & \\rho^{\\mid t-s \\mid} \\\\\n   \\rho & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\mid t-s \\mid } & \\ldots & \\ldots & 1\n   \\end{bmatrix}.\n\\]\nNote, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag \\(2\\) pairs will have the same correlation and so on."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-08-acf-jags/index.html#model-fitting",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\nmodelString = \" \n  model {\n  #Likelihood\n  for (i in 1:n) {\n  fit[i] &lt;- inprod(beta[],X[i,])\n  y[i] ~ dnorm(mu[i],tau.cor)\n  }\n  e[1] &lt;- (y[1] - fit[1])\n  mu[1] &lt;- fit[1]\n  for (i in 2:n) {\n  e[i] &lt;- (y[i] - fit[i]) #- phi*e[i-1]\n  mu[i] &lt;- fit[i] + phi * e[i-1]\n  }\n  #Priors\n  phi ~ dunif(-1,1)\n  for (i in 1:nX) {\n  beta[i] ~ dnorm(0,1.0E-6)\n  }\n  sigma &lt;- z/sqrt(chSq)    # prior for sigma; cauchy = normal/sqrt(chi^2)\n  z ~ dnorm(0, 0.04)I(0,)\n  chSq ~ dgamma(0.5, 0.5)  # chi^2 with 1 d.f.\n  tau &lt;- pow(sigma, -2)\n  tau.cor &lt;- tau #* (1- phi*phi)\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString, con = \"tempModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list &lt;- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 10000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10000\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.temporalCor.r2jags &lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,\n    model.file = \"tempModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 50\nNA    Unobserved stochastic nodes: 5\nNA    Total graph size: 413\nNA \nNA Initializing model\n\nprint(data.temporalCor.r2jags)\n\nNA Inference for Bugs model at \"tempModel.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   30.841  11.858   8.852  22.556  30.505  38.559  55.177 1.001 10000\nNA beta[2]    0.225   0.100   0.028   0.159   0.225   0.292   0.422 1.001  3800\nNA phi        0.913   0.054   0.793   0.879   0.919   0.954   0.994 1.001  3400\nNA sigma     12.133   1.253   9.967  11.253  12.034  12.902  14.828 1.001  7300\nNA deviance 391.602   2.641 388.354 389.656 390.985 392.927 398.180 1.001  9200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.5 and DIC = 395.1\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-08-acf-jags/index.html#mcmc-diagnostics",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nlibrary(mcmcplots)\ndenplot(data.temporalCor.r2jags, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\ntraplot(data.temporalCor.r2jags, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\n\n\ndata.mcmc = as.mcmc(data.temporalCor.r2jags)\n#Raftery diagnostic\nraftery.diag(data.mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  2        3930  3746         1.05      \nNA  beta[2]  2        3866  3746         1.03      \nNA  deviance 2        3866  3746         1.03      \nNA  phi      7        7397  3746         1.97      \nNA  sigma    4        4636  3746         1.24      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  3        4062  3746         1.080     \nNA  beta[2]  2        3620  3746         0.966     \nNA  deviance 2        3803  3746         1.020     \nNA  phi      6        6878  3746         1.840     \nNA  sigma    4        4713  3746         1.260\n\n\n\n#Autocorrelation diagnostic\nautocorr.diag(data.mcmc)\n\nNA            beta[1]      beta[2]     deviance          phi        sigma\nNA Lag 0  1.000000000  1.000000000  1.000000000  1.000000000  1.000000000\nNA Lag 1  0.174857318 -0.006205038  0.164212015  0.398270011  0.166634323\nNA Lag 5  0.017823932  0.002140092 -0.016470982  0.017851360  0.011892997\nNA Lag 10 0.004107514  0.010910488  0.020001216 -0.005693854  0.007020861\nNA Lag 50 0.002176470  0.016102607  0.008360988  0.002061169 -0.007663541\n\n\nAll diagnostics seem fine."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#model-validation",
    "href": "tutorials/2020-02-08-acf-jags/index.html#model-validation",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Model validation",
    "text": "Model validation\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[\nRes_i = Y_i - \\mu_i\n\\]\n\\[\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n\\]\n\\[\nRes_i = \\frac{Res_i}{\\sigma}\n\\]\n\nmcmc = data.temporalCor.r2jags$BUGSoutput$sims.matrix\n# generate a model matrix\nnewdata = data.temporalCor\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\nwch = grep(\"beta\", colnames(mcmc))\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nlibrary(ggplot2)\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n\n\n\n\n\n\n\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\")\n\n\n\n\n\n\n\nplot(acf(resid, lag = 40))\n\n\n\n\n\n\n\n\nNo obvious autocorrelation or other issues with residuals remaining."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#parameter-estimates",
    "href": "tutorials/2020-02-08-acf-jags/index.html#parameter-estimates",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nExplore parameter estimates.\n\nlibrary(broom)\nlibrary(broom.mixed)\ntidyMCMC(as.mcmc(data.temporalCor.r2jags), conf.int = TRUE, conf.method = \"HPDinterval\")\n\nNA # A tibble: 4 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   30.8     11.9      7.36      53.5  \nNA 2 beta[2]    0.225    0.100    0.0321     0.425\nNA 3 phi        0.913    0.0537   0.813      1.00 \nNA 4 sigma     12.1      1.25     9.91      14.7"
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-08-acf-jags/index.html#model-fitting-1",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\nmodelString2 = \"\n  model {\n  #Likelihood\n  for (i in 1:n) {\n  mu[i] &lt;- inprod(beta[],X[i,])\n  }\n  y[1:n] ~ dmnorm(mu[1:n],Omega)\n  for (i in 1:n) {\n  for (j in 1:n) {\n  Sigma[i,j] &lt;- sigma2*(equals(i,j) + (1-equals(i,j))*pow(phi,abs(i-j))) \n  }\n  }\n  Omega &lt;- inverse(Sigma)\n  \n  #Priors\n  phi ~ dunif(-1,1)\n  for (i in 1:nX) {\n  beta[i] ~ dnorm(0,1.0E-6)\n  }\n  sigma &lt;- z/sqrt(chSq)    # prior for sigma; cauchy = normal/sqrt(chi^2)\n  z ~ dnorm(0, 0.04)I(0,)\n  chSq ~ dgamma(0.5, 0.5)  # chi^2 with 1 d.f.\n  sigma2 = pow(sigma,2)\n  #tau.cor &lt;- tau #* (1- phi*phi)\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"tempModel2.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list &lt;- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 10000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10000\n\n\nNow run the JAGS code via the R2jags interface.\n\ndata.temporalCor2.r2jags &lt;- jags(data = data.temporalCor.list, inits = NULL, parameters.to.save = params,\n    model.file = \"tempModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 1\nNA    Unobserved stochastic nodes: 5\nNA    Total graph size: 5566\nNA \nNA Initializing model\n\nprint(data.temporalCor2.r2jags)\n\nNA Inference for Bugs model at \"tempModel2.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   19.926  24.597 -19.141   9.722  18.990  29.365  64.348 1.014 10000\nNA beta[2]    0.225   0.100   0.028   0.159   0.227   0.291   0.421 1.001 10000\nNA phi        0.890   0.055   0.773   0.854   0.895   0.930   0.980 1.011   160\nNA sigma     30.352  15.780  18.171  22.799  26.810  32.951  61.419 1.010   410\nNA deviance 392.642   2.706 389.232 390.628 392.029 394.019 399.490 1.001  2900\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.7 and DIC = 396.3\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-08-acf-jags/index.html#mcmc-diagnostics-1",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\ndenplot(data.temporalCor2.r2jags, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\ntraplot(data.temporalCor2.r2jags, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\n\n\ndata.mcmc = as.mcmc(data.temporalCor2.r2jags)\n#Raftery diagnostic\nraftery.diag(data.mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  15       14982 3746         4.00      \nNA  beta[2]  2        3866  3746         1.03      \nNA  deviance 2        3995  3746         1.07      \nNA  phi      9        9308  3746         2.48      \nNA  sigma    8        10294 3746         2.75      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  4        4955  3746         1.320     \nNA  beta[2]  2        3620  3746         0.966     \nNA  deviance 2        3930  3746         1.050     \nNA  phi      12       12162 3746         3.250     \nNA  sigma    8        10644 3746         2.840\n\n\n\n#Autocorrelation diagnostic\nautocorr.diag(data.mcmc)\n\nNA             beta[1]      beta[2]   deviance       phi      sigma\nNA Lag 0   1.000000000  1.000000000 1.00000000 1.0000000 1.00000000\nNA Lag 1   0.023745389 -0.007088969 0.19477040 0.8775299 0.95206712\nNA Lag 5   0.019171996  0.008569178 0.08589717 0.5774327 0.80961727\nNA Lag 10 -0.009155805  0.008682983 0.06468974 0.3677587 0.64495814\nNA Lag 50  0.012167974  0.014954099 0.01686647 0.0317406 0.04466731\n\n\nAll diagnostics seem fine."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#model-validation-1",
    "href": "tutorials/2020-02-08-acf-jags/index.html#model-validation-1",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Model validation",
    "text": "Model validation\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[\nRes_i = Y_i - \\mu_i\n\\]\n\\[\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n\\]\n\\[\nRes_i = \\frac{Res_i}{\\sigma}\n\\]\n\nmcmc = data.temporalCor2.r2jags$BUGSoutput$sims.matrix\n# generate a model matrix\nnewdata = data.temporalCor\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\nwch = grep(\"beta\", colnames(mcmc))\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n\n\n\n\n\n\n\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\")\n\n\n\n\n\n\n\nplot(acf(resid, lag = 40))\n\n\n\n\n\n\n\n\nNo obvious autocorrelation or other issues with residuals remaining"
  },
  {
    "objectID": "tutorials/2020-02-08-acf-jags/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-08-acf-jags/index.html#parameter-estimates-1",
    "title": "Temporal Autocorrelation (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nExplore parameter estimates.\n\ntidyMCMC(as.mcmc(data.temporalCor2.r2jags), conf.int = TRUE, conf.method = \"HPDinterval\")\n\nNA # A tibble: 4 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   19.9     24.6    -16.6       66.3  \nNA 2 beta[2]    0.225    0.0997   0.0313     0.423\nNA 3 phi        0.890    0.0546   0.780      0.984\nNA 4 sigma     30.4     15.8     16.2       51.2"
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html#introduction",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html#introduction",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\n\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html#model-fitting-1",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\nmodelString2 = \"\n  model {\n  #Likelihood\n  for (i in 1:n) {\n  y[i]~dnorm(mu[i],tau[x[i]])\n  mu[i] &lt;- inprod(beta[],X[i,])\n  }\n  \n  #Priors and derivatives\n  for (i in 1:ngroups) {\n  beta[i] ~ dnorm(0,1.0E-6)\n  \n  sigma[i] &lt;- z[i]/sqrt(chSq[i])\n  z[i] ~ dnorm(0, 0.04)I(0,)\n  chSq[i] ~ dgamma(0.5, 0.5)\n  tau[i] &lt;- pow(sigma[i], -2)\n  }\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"heteroskModel2.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX = model.matrix(~x, data.het1)\ndata.het1.list &lt;- with(data.het1, list(y = y, x = as.numeric(x), X = X,\n    n = nrow(data.het1), ngroups = ncol(X)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\ndata.het1.r2jags &lt;- jags(data = data.het1.list, inits = NULL, parameters.to.save = params,\n    model.file = \"heteroskModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 50\nNA    Unobserved stochastic nodes: 15\nNA    Total graph size: 444\nNA \nNA Initializing model\n\nprint(data.het1.r2jags)\n\nNA Inference for Bugs model at \"heteroskModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   40.282   1.227  37.861  39.518  40.278  41.044  42.731 1.001 11000\nNA beta[2]    4.088   1.508   1.063   3.115   4.095   5.059   7.063 1.001  5000\nNA beta[3]   14.553   1.336  11.874  13.714  14.566  15.402  17.177 1.001  5600\nNA beta[4]   -0.655   1.242  -3.118  -1.425  -0.656   0.118   1.804 1.001 11000\nNA beta[5]  -10.364   1.286 -12.875 -11.173 -10.353  -9.550  -7.830 1.001 12000\nNA sigma[1]   3.748   0.971   2.378   3.062   3.583   4.231   6.071 1.001 13000\nNA sigma[2]   2.647   0.729   1.640   2.143   2.504   2.995   4.461 1.001  5400\nNA sigma[3]   1.629   0.456   1.001   1.314   1.541   1.846   2.767 1.001  4000\nNA sigma[4]   0.570   0.169   0.346   0.454   0.537   0.647   1.001 1.001  3500\nNA sigma[5]   1.181   0.336   0.727   0.950   1.118   1.342   2.021 1.001  7100\nNA deviance 182.822   5.288 174.824 178.961 182.076 185.810 195.061 1.001 11000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 14.0 and DIC = 196.8\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html#mcmc-diagnostics-1",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nlibrary(mcmcplots)\ndenplot(data.het1.r2jags, parms = c(\"beta\", \"sigma\"))\n\n\n\n\n\n\n\ntraplot(data.het1.r2jags, parms = c(\"beta\", \"sigma\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html#parameter-estimates-1",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the model.\n\nprint(data.het1.r2jags)\n\nNA Inference for Bugs model at \"heteroskModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   40.282   1.227  37.861  39.518  40.278  41.044  42.731 1.001 11000\nNA beta[2]    4.088   1.508   1.063   3.115   4.095   5.059   7.063 1.001  5000\nNA beta[3]   14.553   1.336  11.874  13.714  14.566  15.402  17.177 1.001  5600\nNA beta[4]   -0.655   1.242  -3.118  -1.425  -0.656   0.118   1.804 1.001 11000\nNA beta[5]  -10.364   1.286 -12.875 -11.173 -10.353  -9.550  -7.830 1.001 12000\nNA sigma[1]   3.748   0.971   2.378   3.062   3.583   4.231   6.071 1.001 13000\nNA sigma[2]   2.647   0.729   1.640   2.143   2.504   2.995   4.461 1.001  5400\nNA sigma[3]   1.629   0.456   1.001   1.314   1.541   1.846   2.767 1.001  4000\nNA sigma[4]   0.570   0.169   0.346   0.454   0.537   0.647   1.001 1.001  3500\nNA sigma[5]   1.181   0.336   0.727   0.950   1.118   1.342   2.021 1.001  7100\nNA deviance 182.822   5.288 174.824 178.961 182.076 185.810 195.061 1.001 11000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 14.0 and DIC = 196.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n# OR\ntidyMCMC(as.mcmc(data.het1.r2jags), conf.int = TRUE, conf.method = \"HPDinterval\")\n\nNA # A tibble: 10 × 5\nNA    term     estimate std.error conf.low conf.high\nNA    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA  1 beta[1]    40.3       1.23    37.9      42.7  \nNA  2 beta[2]     4.09      1.51     0.980     6.97 \nNA  3 beta[3]    14.6       1.34    12.0      17.3  \nNA  4 beta[4]    -0.655     1.24    -3.15      1.76 \nNA  5 beta[5]   -10.4       1.29   -12.9      -7.86 \nNA  6 sigma[1]    3.75      0.971    2.23      5.72 \nNA  7 sigma[2]    2.65      0.729    1.53      4.12 \nNA  8 sigma[3]    1.63      0.456    0.906     2.54 \nNA  9 sigma[4]    0.570     0.169    0.313     0.905\nNA 10 sigma[5]    1.18      0.336    0.656     1.82\n\n\nConclusions\n\nthe mean of the first group (A) is \\(40.3\\)\nthe mean of the second group (B) is \\(4.12\\) units greater than (A)\nthe mean of the third group (C) is \\(14.6\\) units greater than (A)\nthe mean of the forth group (D) is \\(-0.637\\) units greater (i.e. less) than (A)\nthe mean of the fifth group (E) is \\(-10.3\\) units greater (i.e. less) than (A)\n\nThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\nmcmcpvalue &lt;- function(samp) {\n    ## elementary version that creates an empirical p-value for the\n    ## hypothesis that the columns of samp have mean zero versus a general\n    ## multivariate distribution with elliptical contours.\n\n    ## differences from the mean standardized by the observed\n    ## variance-covariance factor\n\n    ## Note, I put in the bit for single terms\n    if (length(dim(samp)) == 0) {\n        std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\n            transpose = TRUE)\n        sqdist &lt;- colSums(std * std)\n        sum(sqdist[-1] &gt; sqdist[1])/length(samp)\n    } else {\n        std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\n            transpose = TRUE)\n        sqdist &lt;- colSums(std * std)\n        sum(sqdist[-1] &gt; sqdist[1])/nrow(samp)\n    }\n\n}\n## since values are less than zero\nmcmc = data.het1.r2jags$BUGSoutput$sims.matrix\nfor (i in grep(\"beta\", colnames(mcmc), value = TRUE)) print(paste(i, mcmcpvalue(mcmc[,i])))\n\nNA [1] \"beta[1] 0\"\nNA [1] \"beta[2] 0.0116\"\nNA [1] \"beta[3] 0\"\nNA [1] \"beta[4] 0.567133333333333\"\nNA [1] \"beta[5] 0\"\n\nmcmcpvalue(mcmc[, grep(\"beta\", colnames(mcmc))])\n\nNA [1] 0\n\n\nWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-jags/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-07-heterogeneity-jags/index.html#graphical-summaries-1",
    "title": "Variance Heterogeneity (JAGS)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = data.het1.r2jags$BUGSoutput$sims.matrix\n## Calculate the fitted values\nnewdata = data.frame(x = levels(data.het1$x))\nXmat = model.matrix(~x, newdata)\nwch = grep(\"beta\", colnames(mcmc))\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nnewdata = newdata %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\nggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\n    ymax = conf.high)) + scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het1,\n    aes(y = y, x = x), color = \"gray\") + geom_pointrange(aes(ymin = conf.low,\n    ymax = conf.high)) + scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\n## Calculate partial residuals fitted values\nfdata = rdata = data.het1\nfMat = rMat = model.matrix(~x, fdata)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data.het1$y - apply(coefs, 2, median) %*% t(rMat))\nrdata = rdata %&gt;% mutate(partial.resid = resid + fit)\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\n    color = \"gray\") + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n    scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#introduction",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nFactorial designs are an extension of single factor ANOVA designs in which additional factors are added such that each level of one factor is applied to all levels of the other factor(s) and these combinations are replicated. For example, we might design an experiment in which the effects of temperature (high vs low) and fertiliser (added vs not added) on the growth rate of seedlings are investigated by growing seedlings under the different temperature and fertilizer combinations. In addition to investigating the impacts of the main factors, factorial designs allow us to investigate whether the effects of one factor are consistent across levels of another factor. For example, is the effect of temperature on growth rate the same for both fertilised and unfertilized seedlings and similarly, does the impact of fertiliser treatment depend on the temperature under which the seedlings are grown?\nArguably, these interactions give more sophisticated insights into the dynamics of the system we are investigating. Hence, we could add additional main effects, such as soil pH, amount of water, etc, along with all the two way (temp:fert, temp:pH, temp:water, etc), three-way (temp:fert:pH, temp:pH:water), four-way (and so on) interactions in order to explore how these various factors interact with one another to effect the response. However, the more interactions, the more complex the model becomes to specify, compute and interpret - not to mention the rate at which the number of required observations increases. Factorial designs can consist:\n\nentirely of crossed fixed factors (Model I ANOVA - most common) in which conclusions are restricted to the specific combinations of levels selected for the experiment.\nentirely of crossed random factors (Model II ANOVA).\na mixture of crossed fixed and random factors (Model III ANOVA).\n\nThe latter are useful for investigating the generality of a main treatment effect (fixed) over broad spatial, temporal or clinical levels of organisation. That is whether the observed effects of temperature and/or fertiliser (for example) are observed across the entire genera or country."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#linear-model",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#linear-model",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Linear model",
    "text": "Linear model\nAs with single factor ANOVA, the linear model could be constructed as either effects or means parameterisation, although only effects parameterisation will be considered here. The linear models for two and three factor design are\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\]\nand\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkl},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component. Note that although the linear models for Model I, Model II and Model III designs are identical, the interpretation of terms (and thus null hypothesis) differ. Recall from the tutorial on single factor ANOVA, that categorical variables in linear models are actually re-parameterised dummy codes - and thus the \\(\\alpha\\) term above, actually represents one or more dummy codes. Thus, if we actually had two levels of Factor A (A1 and A2) and three levels of Factor B (B1, B2, B3), then the fully parameterised linear model would be:\n\\[\ny=\\beta_{A1B1}+\\beta_{A2B1−A1B1}+\\beta_{A1B2−A1B1}+\\beta_{A1B3−A1B1}+\\beta_{A2B2−A1B2−A2B1−A1B1}+\\beta_{A2B3−A1B3−A2B1−A1B1}.\n\\]\nThus, such a model would have six parameters to estimate (in addition to the variance)."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#null-hypothesis",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#null-hypothesis",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nThere are separate null hypothesis associated with each of the main effects and the interaction terms.\n\nModel 1 - fixed effects\nFactor A\n\n\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal. The mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is rejected indicating that the treatment has been found to affect the response variable. Note, as with multiple regression models, these “effects” represent partial effects. In the above, the effect of Factor A is actually the effect of Factor A at the first level of the Factor(s).\nFactor B\n\n\\(H_0(B):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal - at the first level of Factor A. Equivalent interpretation to Factor A above.\nFactor AB: interaction\n\n\\(H_0(AB):\\mu_{ij}=\\mu_i+\\mu_j-\\mu\\)\n\nThe population group means are all equal. For any given combination of factor levels, the population group mean will be equal to the difference between the overall population mean and the simple additive effects of the individual factor group means. That is, the effects of the main treatment factors are purely additive and independent of one another. This is equivalent to \\(H_0(AB): \\alpha\\beta_{ij}=0\\), no interaction between Factor A and Factor B.\n\n\nModel 2 - random effects\nFactor A\n\n\\(H_0(A):\\sigma^2_{\\alpha}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of A.\nFactor B\n\n\\(H_0(B):\\sigma^2_{\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\n\n\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\n\nThere is no added variance due to all possible interactions between all possible levels of A and B.\n\n\nModel 3 - mixed effects\nFixed factor - e.g. A\n\n\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal. The mean of population \\(1\\) (pooled over all levels of the random factor) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean pooling over all possible levels of the random factor. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\n\nNo effect of any level of this factor pooled over all possible levels of the random factor.\nRandom factor - e.g. B\n\n\\(H_0(B):\\sigma^2_{\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\nThe interaction of a fixed and random factor is always considered a random factor.\n\n\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible interactions between all possible levels of A and B."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#analysis-of-variance",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nWhen fixed factorial designs are balanced, the total variance in the response variable can be sequentially partitioned into what is explained by each of the model terms (factors and their interactions) and what is left unexplained. For each of the specific null hypotheses, the overall unexplained variability is used as the denominator in F-ratio calculations, and when a null hypothesis is true, an F-ratio should follow an F distribution with an expected value less than \\(1\\). Random factors are added to provide greater generality of conclusions. That is, to enable us to make conclusions about the effect of one factor (such as whether or not fertiliser is added) over all possible levels (not just those sampled) of a random factor (such as all possible locations, seasons, varieties, etc). In order to expand our conclusions beyond the specific levels used in the design, the hypothesis tests (and thus F-ratios) must reflect this extra generality by being more conservative.\nThe appropriate F-ratios for fixed, random and mixed factorial designs are presented below. Generally, once the terms (factors and interactions) have been ordered into a hierarchy (single factors at the top, highest level interactions at the bottom and terms of same order given equivalent positions in the hierarchy), the denominator for any term is selected as the next appropriate random term (an interaction that includes the term to be tested) encountered lower in the hierarchy. Interaction terms that contain one or more random factors are considered themselves to be random terms, as is the overall residual term (as all observations are assumed to be random representations of the entire population(s)). Note, when designs include a mixture of fixed and random crossed effects, exact denominator degrees of freedoms for certain F-ratios are undefined and traditional approaches adopt rather inexact estimated approximate or “Quasi” F-ratios. Pooling of non-significant F-ratio denominator terms, in which lower random terms are added to the denominator (provided \\(\\alpha &gt; 0.25\\)), may also be useful. For random factors within mixed models, selecting F-ratio denominators that are appropriate for the intended hypothesis tests is a particularly complex and controversial issue. Traditionally, there are two alternative approaches and whilst the statistical resumes of each are complicated, essentially they differ in whether or not the interaction term is constrained for the test of the random factor.\nThe constrained or restricted method (Model I), stipulates that for the calculation of a random factor F-ratio (which investigates the added variance added due to the random factor), the overall effect of the interaction is treated as zero. Consequently, the random factor is tested against the residual term. The unconstrained or unrestrained method (Model II) however, does not set the interaction effect to zero and therefore the interaction term is used as the random factor F-ratio denominator. This method assumes that the interaction terms for each level of the random factor are completely independent (correlations between the fixed factor must be consistent across all levels of the random factor). Some statisticians maintain that the independence of the interaction term is difficult to assess for clinical data and therefore, the restricted approach is more appropriate. However, others have suggested that the restricted method is only appropriate for balanced designs."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#quasi-f-ratios",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#quasi-f-ratios",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Quasi F-ratios",
    "text": "Quasi F-ratios\nAn additional complication for three or more factor models that contain two or more random factors, is that there may not be a single appropriate interaction term to use as the denominator for many of the main effects F-ratios. For example, if Factors A and B are random and C is fixed, then there are two random interaction terms of equivalent level under Factor C (\\(A^\\prime \\times C\\) and \\(B^\\prime \\times C\\)). As a result, the value of the of the Mean Squares (MS) expected when the null hypothesis is true cannot be easily defined. The solutions for dealing with such situations (quasi F-ratios) involve adding (and subtracting) terms together to create approximate estimates of F-ratio denominators. Alternatively, for random factors, variance components with confidence intervals can be used. These solutions are sufficiently unsatisfying as to lead many statisticians to recommend that factorial designs with two or more random factors should avoided if possible. Arguably however, linear mixed effects models offer more appropriate solutions to the above issues as they are more robust for unbalanced designs, accommodate covariates and provide a more comprehensive treatment and overview of all the underlying data structures.\n\nfact_anova_table\n\nNA     df           MS       A,B fixed          A,B random       \nNA A   \"a-1\"        \"MS A\"   \"(MS A)/(MS res)\"  \"(MS A)/(MS AB)\" \nNA B   \"b-1\"        \"MS B\"   \"(MS B)/(MS res)\"  \"(MS B)/(MS AB)\" \nNA AB  \"(b-1)(a-1)\" \"MS AB\"  \"(MS AB)/(MS res)\" \"(MS AB)/(MS AB)\"\nNA Res \"(n-1)ba\"    \"MS res\" \"\"                 \"\"               \nNA     A fixed B random (model I) A fixed B random (model II)\nNA A   \"(MS A)/(MS AB)\"           \"(MS A)/(MS AB)\"           \nNA B   \"(MS B)/(MS res)\"          \"(MS B)/(MS AB)\"           \nNA AB  \"(MS AB)/(MS res)\"         \"(MS AB)/(MS res)\"         \nNA Res \"\"                         \"\"\n\n\nThe corresponding R syntax is given below.\n\n#Type I SS (Balanced)\nanova(lm(y ~ A * B, data))\n\n#Type II SS (Unbalanced)\nAnova(lm(y ~ A * B, data), type = \"II\")\n\n#Type III SS (Unbalanced)\nAnova(lm(y ~ A * B, data), type = \"III\")\n\n#Variance components\nsummary(lmer(y ~ 1 + (1 | A) + (1 | B) + (1 | A:B), data))\n\nNote that for fixed factor models, when null hypotheses of interactions are rejected, the null hypothesis of the individual constituent factors are unlikely to represent the true nature of the effects and thus are of little value. The nature of such interactions are further explored by fitting simpler linear models (containing at least one less factor) separately for each of the levels of the other removed factor(s). Such Main effects tests are based on a subset of the data, and therefore estimates of the overall residual (unexplained) variability are unlikely to be as precise as the estimates based on the global model. Consequently, F-ratios involving MSResid should use the estimate of MSResid from the global model rather than that based on the smaller, theoretically less precise subset of data. For random and mixed models, since the objective is to generalise the effect of one factor over and above any interactions with other factors, the main factor effects can be interpreted even in the presence of significant interactions. Nevertheless, it should be noted that when a significant interaction is present in a mixed model, the power of the main fixed effects will be reduced (since the amount of variability explained by the interaction term will be relatively high, and this term is used as the denominator for the F-ratio calculation)."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#assumptions",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nHypothesis tests assume that the residuals are:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see table above) should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another.\n\nPlanned and unplanned comparisons\nAs with single factor analysis of variance, planned and unplanned multiple comparisons (such as Tukey’s test) can be incorporated into or follow the linear model respectively so as to further investigate any patterns or trends within the main factors and/or the interactions. As with single factor analysis of variance, the contrasts must be defined prior to fitting the linear model, and no more than \\(p−1\\) (where \\(p\\) is the number of levels of the factor) contrasts can be defined for a factor.\nUnbalanced designs\nA factorial design can be thought of as a table made up of rows (representing the levels of one factor), columns (levels of another factor), and cells (the individual combinations of the set of factors). Whilst the middle left table does not have equal sample sizes in each cell, the sample sizes are in proportion and as such, does not present the issues discussed below for unbalanced designs.\nIn addition to impacting on normality and homogeneity of variance, unequal sample sizes in factorial designs have major implications for the partitioning of the total sums of squares into each of the model components. For balanced designs, the total sums of squares (SSTotal) is equal to the additive sums of squares of each of the components (including the residual). For example, in a two factor balanced design, SSTotal=SSA+SSB+SSAB+SSResid. This can be represented diagrammatically by a Venn Diagram in which each of the SS for the term components butt against one another and are surrounded by the SSResid. However, in unbalanced designs, the sums of squares will be non-orthogonal and the sum of the individual components does not add up to the total sums of squares. Diagrammatically, the SS of the terms intersect or are separated.\nIn regular sequential sums of squares (Type I SS), the sum of the individual sums of squares must be equal to the total sums of squares, the sums of squares of the last factor to be estimated will be calculated as the difference between the total sums of squares and what has already been accounted for by other components. Consequently, the order in which factors are specified in the model (and thus estimated) will alter their sums of squares and therefore their F-ratios. To overcome this problem, traditionally there are two other alternative methods of calculating sums of squares.\n\nType II (hierarchical) SS estimate the sums of squares of each term as the improvement it contributes upon addition of that term to a model of greater complexity and lower in the hierarchy (recall that the hierarchical structure descends from the simplest model down to the fully populated model). The SS for the interaction as well as the first factor to be estimated are the same as for Type I SS. Type II SS estimate the contribution of a factor over and above the contributions of other factors of equal or lower complexity but not above the contributions of the interaction terms or terms nested within the factor. However, these sums of squares are weighted by the sample sizes of each level and therefore are biased towards the trends produced by the groups (levels) that have higher sample sizes. As a result of the weightings, Type II SS actually test hypotheses about really quite complex combinations of factor levels. Rather than test a hypothesis that \\(\\mu_{High}=\\mu_{Medium}=\\mu_{Low}\\), Type II SS might be testing that \\(4\\times\\mu_{High}=1\\times\\mu_{Medium}=0.25\\times\\mu_{Low}\\).\nType III (marginal or orthogonal) SS estimate the sums of squares of each term as the improvement based on a comparison of models with and without the term and are unweighted by sample sizes. Type III SS essentially measure just the unique contribution of each factor over and above the contributions of the other factors and interactions. For unbalanced designs,Type III SS essentially test equivalent hypotheses to balanced Type I SS and are therefore arguably more appropriate for unbalanced factorial designs than Type II SS. Importantly, Type III SS are only interpretable if they are based on orthogonal contrasts (such as sum or helmert contrasts and not treatment contrasts).\n\nThe choice between Type II and III SS clearly depends on the nature of the question. For example, if we had measured the growth rate of seedlings subjected to two factors (temperature and fertiliser), Type II SS could address whether there was an effect of temperature across the level of fertiliser treatment, whereas Type III SS could assess whether there was an effect of temperature within each level of the fertiliser treatment.\nWhen an entire combination, or cell, is missing (perhaps due to unforeseen circumstances) it is not possible to test all the main effects and/or interactions. The bottom right table above depicts such as situation. One solution is to fit a large single factor ANOVA with as many levels as there are cells (this is known as a cell means model) and investigate various factor and interaction effects via specific contrasts (see the following tables). Difficulties in establishing appropriate error terms, makes missing cells in random and mixed factor designs substantially more complex."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#assumptions-1",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#assumptions-1",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions are:\n\nAll of the observations are independent - this must be addressed at the design and collection stages. Importantly, to be considered independent replicates, the replicates must be made at the same scale at which the treatment is applied. For example, if the experiment involves subjecting organisms housed in tanks to different water temperatures, then the unit of replication is the individual tanks not the individual organisms in the tanks. The individuals in a tank are strictly not independent with respect to the treatment.\nThe response variable (and thus the residuals) should be normally distributed for each sampled populations (combination of factors). Boxplots of each treatment combination are useful for diagnosing major issues with normality.\nThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately) for each combination of treatments. Again, boxplots are useful."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-06-factorial-anova-jags/index.html#exploratory-data-analysis",
    "title": "Factorial Analysis of Variance (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y ~ A * B, data)\n\n# OR via ggplot2\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = y, x = A, fill = B)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html",
    "title": "Single Factor Anova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#introduction",
    "title": "Single Factor Anova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nSingle factor Analysis of Variance (ANOVA), also known as single factor classification, is used to investigate the effect of a single factor comprising two or more groups (treatment levels) from a completely randomised design. Completely randomised refers to the absence of restrictions on the random allocation of experimental or sampling units to factor levels.\nFor example, consider a situation in which three types of treatments (A, B and C) are applied to replicate sampling units across the sampling domain. Importantly, the treatments are applied at the scale of the sampling units and the treatments applied to each sampling unit do not extend to any other neighbouring sampling units. Another possible situation is where the scale of a treatment is far larger than that of a sampling unit. This design features two treatments, each replicated three times. Note that additional sampling units within each Site (the scale at which the treatment occurs) would NOT constitute additional replication. Rather, these would be sub-replicates. That is, they would be replicates of the Sites, not the treatments (since the treatments occur at the level of whole sites). In order to genuinely increase the number of replicates, it is necessary to have more Sites. The random allocation of sampling units within the sampling domain (such as population) is appropriate provided either the underlying response is reasonably homogenous throughout the domain, or else, there is a large number of sampling units. If the conditions are relatively hetrogenous, then the exact location of the sampling units is likely to be highly influential and may mask any detectable effects of treatments."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#fixed-and-random-effects",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#fixed-and-random-effects",
    "title": "Single Factor Anova (Stan)",
    "section": "Fixed and random effects",
    "text": "Fixed and random effects\nFrom a frequentist perspective, fixed factors are factors whose levels represent the specific populations of interest. For example, a factor that comprises “high”, “medium” and “low” temperature treatments is a fixed factor - we are only interested in comparing those three populations. Conclusions about the effects of a fixed factor are restricted to the specific treatment levels investigated and for any subsequent experiments to be comparable, the same specific treatments of the factor would need to be used. By contrast, random factors are factors whose levels are randomly chosen from all the possible levels of populations and are used as random representatives of the populations. For example, five random temperature treatments could be used to represent a full spectrum of temperature treatments. In this case, conclusions are extrapolated to all the possible treatment (temperature) levels and for subsequent experiments, a new random set of treatments of the factor would be selected.\nOther common examples of random factors include sites and subjects - factors for which we are attempting to generalise over. Furthermore, the nature of random factors means that we have no indication of how a new level of that factor (such as another subject or site) are likely to respond and thus it is not possible to predict new observations from random factors. These differences between fixed and random factors are reflected in the way their respective null hypotheses are formulated and interpreted. Whilst fixed factors contrast the effects of the different levels of the factor, random factors are modelled as the amount of additional variability they introduce. Random factors are modelled with a mean of \\(0\\) and their variance is estimated as the effect coefficient."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#linear-model",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#linear-model",
    "title": "Single Factor Anova (Stan)",
    "section": "Linear model",
    "text": "Linear model\nThe linear model for single factor classification is similar to that of multiple linear regression. The linear model can thus be represented by either:\n\nMeans parameterisation - in which the regression slopes represent the means of each treatment group and the intercept is removed (to prevent over-parameterisation).\n\n\\[\ny_{ij} = \\beta_1(\\text{level}_1)_{ij} + \\beta_2(\\text{level}_2)_{ij} + \\ldots + \\epsilon_{ij},\n\\]\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) respectively represent the means response of treatment level \\(1\\) and \\(2\\). This is often simplified to \\(y_{ij}=\\alpha_i + \\epsilon_{ij}\\).\n\nEffects parameterisation - the intercept represents a property such as the mean of one of the treatment groups (treatment contrasts) or the overall mean (sum contrasts), and the slope parameters represent effects (differences between each other group and the reference mean for example).\n\n\\[\ny_{ij} = \\mu + \\beta_2(\\text{level}_2)_{ij} + \\beta_3(\\text{level}_3)_{ij} + \\ldots + \\epsilon_{ij},\n\\]\nwhere \\(\\mu\\) is the mean of the first treatment group, \\(\\beta_2\\) and \\(\\beta_3\\) respectively represent the effects (change from level \\(1\\)) of level \\(2\\) and \\(3\\) on the mean response. This is often simplified to: \\(y_{ij}=\\mu + \\alpha_i + \\epsilon_{ij}\\), with \\(\\alpha_1=0\\).\nSince we are traditionally interested in investigating effects (differences) rather than treatment means, effects parameterisation is far more common (particularly when coupled with hypothesis testing). In a Bayesian framework, it does not really matter whether models are fit with means or effects parameterisation since the posterior likelihood can be querried in any way and repeatedly - thus enabling us to explore any specific effects after the model has been fit. Nevertheless, to ease comparisons with frequentist approaches, we will stick with effects paramterisation."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#null-hypothesis-fixed-factor",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#null-hypothesis-fixed-factor",
    "title": "Single Factor Anova (Stan)",
    "section": "Null hypothesis: fixed factor",
    "text": "Null hypothesis: fixed factor\nWe can associate a null hypothesis test with each estimated parameter. For example, in a cell for each estimated mean in a means model we could test a null hypothesis that the population mean is equal to zero (e.g. \\(H_0\\): \\(\\alpha_1=0\\), \\(H_0\\): \\(\\alpha_2=0\\), \\(\\ldots\\)). However, this rarely would be of much interest. By contrast, individual null hypotheses associated with each parameter of the effects model can be used to investigate the differences between each group and a reference group (for example). In addition to the individual null hypothesis tests, a single fixed factor ANOVA tests the collective \\(H_0\\) that there are no differences between the population group means:\n\n\\(H_0: \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\\(H_0 : \\alpha_2=\\alpha_3=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#null-hypothesis-random-factor",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#null-hypothesis-random-factor",
    "title": "Single Factor Anova (Stan)",
    "section": "Null hypothesis: random factor",
    "text": "Null hypothesis: random factor\nThe collective \\(H_0\\) for a random factor is that the variance between all possible treatment groups equals zero:\n\n\\(H_0 : \\sigma^2_{\\alpha}=0\\) (added variance due to this factor equals zero).\n\nNote that whilst the null hypotheses for fixed and random factors are different (fixed: population group means all equal, random: variances between populations all equal zero), the linear model fitted for fixed and random factors in single factor ANOVA models is identical. For more complex multi-factor ANOVA models however, the distinction between fixed and random factors has important consequences for building and interpreting statistical models and null hypotheses."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#analysis-of-variance",
    "title": "Single Factor Anova (Stan)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nWhen the null hypothesis is true (and the populations are identical), the amount of variation among observations within groups should be similar to the amount of variation in observations between groups. However, when the null hypothesis is false (and some means are different from other means), the amount of variation among observations might be expected to be less than the amount of variation within groups. Analysis of variance, or ANOVA, partitions the total variance in the response (dependent) variable into a component of the variance that is explained by combinations of one or more categorical predictor variables (called factors) and a component of the variance that cannot be explained (residual). The variance ratio (F-ratio) from this partitioning can then be used to test the null hypothesis (\\(H_0\\)) that the population group or treatment means are all equal. Ttotal variation can be decomposed into components explained by the groups (\\(MS_{groups}\\)) and and unexplained (\\(MS_{residual}\\)) by the groups. The gray arrows in b) depict the relative amounts explained by the groups. The proposed groupings generally explain why the first few points are higher on the y-axis than the last three points. The probability of collecting our sample, and thus generating the sample ratio of explained to unexplained variation (or one more extreme), when the null hypothesis is true (and population means are equal) is the area under the F-distribution beyond our sample ratio (\\(\\text{F-ratio}=\\frac{MS_{groups}}{MS_{residual}}\\)).\nWhen the null hypothesis is true (and the test assumptions have not been violated), the ratio (F-ratio) of explained to unexplained variance follows a theoretical probability distribution (F-distribution). When the null hypothesis is true, and there is no effect of the treatment on the response variable, the ratio of explained variability to unexplained variability is expected to be \\(\\leq 1\\). Since the denominator should represent the expected numerator in the absence of an effect. Importantly, the denominator in an F-ratio calculation essentially represents what we would expect the numerator to be in the absence of a treatment effect. For simple analyses, identifying what these expected values are is relatively straightforward (equivalent to the degree of within group variability). However, in more complex designs (particularly involving random factors and hierarchical treatment levels), the logical “groups” can be more difficult (and in some cases impossible) to identify. In such cases, nominating the appropriate F-ratio denominator for estimating an specific effect requires careful consideration. The following table depicts the anatomy of the single factor ANOVA table\n\nanova_table\n\nNA          df       MS       F-ratio          \nNA Factor A \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"\nNA Residual \"(n-1)a\" \"MS res\" \"\"\n\n\nand corresponding R syntax.\n\nanova(lm(DV ~ A, dataset))\n# OR\nanova(aov(DV ~ A, dataset))\n\nAn F-ratio substantially greater than \\(1\\) suggests that the model relating the response variable to the categorical variable explains substantially more variability than is left unexplained. In turn, this implies that the linear model does represent the data well and that differences between observations can be explained largely by differences in treatment levels rather than purely the result of random variation. If the probability of getting the observed (sample) F-ratio or one more extreme is less than some predefined critical value (typically \\(5\\)% or \\(0.05\\)), we conclude that it is highly unlikely that the observed samples could have been collected from populations in which the treatment has no effect and therefore we would reject the null hypothesis."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#assumptions",
    "title": "Single Factor Anova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAn F-ratio from real data can only reliably relate to a theoretical F-distribution when the data conform to certain assumptions. Hypothesis testing for a single factor ANOVA model assumes that the residuals (and therefore the response variable for each of the treatment levels) are all:\n\nnormally distributed - although ANOVA is robust to non-normality provided sample sizes and variances are equal. Boxplots should be used to explore normality, skewness, bimodality and outliers. In the event of homogeneity of variance issues (see below), a Q-Q normal plot can also be useful for exploring normality (as this might be the cause of non-homogeneity). Scale transformations are often useful.\nequally varied - provided sample sizes are equal and the largest to smallest variance ratio does not exceed 3:1 (9:1 for sd), ANOVA is reasonably robust to this assumption, however, relationships between variance and mean and/or sample size are of particular concern as they elevate the Type I error rate. Boxplots and plots of means against variance should be used to explore the spread of values. Residual plots should reveal no patterns. Since unequal variances are often the result of non-normality, transformations that improve normality will also improve variance homogeneity.\nindependent of one another - this assumption must be addressed at the design and collection stages and cannot be compensated for later (unless a model is used that specifically accounts for particular types of non-independent data, such as that introduced with hierarchical designs or autocorrelation)\n\nViolations of these assumptions reduce the reliability of the analysis."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-04-single-factor-anova-stan/index.html#exploratory-data-analysis",
    "title": "Single Factor Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nNormality and Homogeneity of variance\n\n\nboxplot(y ~ x, data)\n\n# OR via ggplot2\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = y, x = x)) + geom_boxplot() + theme_classic()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity. Obvious violations could be addressed either by, for example, transforming the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#introduction",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#introduction",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nMultiple regression is an extension of simple linear regression whereby a response variable is modelled against a linear combination of two or more simultaneously measured predictor variables. There are two main purposes of multiple linear regression:\n\nTo develop a better predictive model (equation) than is possible from models based on single independent variables.\nTo investigate the relative individual effects of each of the multiple independent variables above and beyond (standardised across) the effects of the other variables.\n\nAlthough the relationship between response variable and the additive effect of all the predictor variables is represented overall by a single multidimensional plane (surface), the individual effects of each of the predictor variables on the response variable (standardised across the other variables) can be depicted by single partial regression lines. The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable. In essence, it is the effect of one predictor variable at one specific level (the means) of all the other predictor variables (i.e. when each of the other predictors are set to their averages).\nMultiple regression models can be constructed additively (containing only the predictor variables themselves) or in a multiplicative design (which incorporate interactions between predictor variables in addition to the predictor variables themselves). Multiplicative models are used primarily for testing inferences about the effects of various predictor variables and their interactions on the response variable. Additive models by contrast are used for generating predictive models and estimating the relative importance of individual predictor variables more so than hypothesis testing."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#additive-model",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#additive-model",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Additive Model",
    "text": "Additive Model\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} +  \\beta_2x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\n\\]\nwhere \\(\\beta_0\\) is the population \\(y\\)-intercept (value of \\(y\\) when all partial slopes equal zero), \\(\\beta_1,\\beta_2,\\ldots,\\beta_{J}\\) are the partial population slopes of \\(Y\\) on \\(X_1,X_2,\\ldots,X_J\\) respectively holding the other \\(X\\) constant. \\(\\epsilon_i\\) is the random unexplained error or residual component. The additive model assumes that the effect of one predictor variable (partial slope) is independent of the levels of the other predictor variables."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#multiplicative-model",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#multiplicative-model",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Multiplicative Model",
    "text": "Multiplicative Model\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} +  \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\n\\]\nwhere \\(\\beta_3x_{i1}x_{i2}\\) is the interactive effect of \\(X_1\\) and \\(X_2\\) on \\(Y\\) and it examines the degree to which the effect of one of the predictor variables depends on the levels of the other predictor variable(s)."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#data-generation",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#data-generation",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nLets say we had set up a natural experiment in which we measured a response (\\(y\\)) from each of \\(20\\) sampling units (\\(n=20\\)) across a landscape. At the same time, we also measured two other continuous covariates (\\(x_1\\) and \\(x_2\\)) from each of the sampling units. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nn = 100\nintercept = 5\ntemp = runif(n)\nnitro = runif(n) + 0.8 * temp\nint.eff = 2\ntemp.eff &lt;- 0.85\nnitro.eff &lt;- 0.5\nres = rnorm(n, 0, 1)\ncoef &lt;- c(int.eff, temp.eff, nitro.eff, int.eff)\nmm &lt;- model.matrix(~temp * nitro)\n\ny &lt;- t(coef %*% t(mm)) + res\ndata &lt;- data.frame(y, x1 = temp, x2 = nitro, cx1 = scale(temp,\n    scale = F), cx2 = scale(nitro, scale = F))\nhead(data)\n\nNA          y        x1        x2         cx1         cx2\nNA 1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\nNA 2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557\nNA 3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\nNA 4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568\nNA 5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415\nNA 6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335\n\n\nWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the components linear predictor (continuous predictors). We could model the relationship via either:\n\nAn additive model in which the effects of each predictor contribute in an additive way to the response - we do not allow for an interaction as we consider an interaction either not of great importance or likely to be absent.\nA multiplicative model in which the effects of each predictor and their interaction contribute to the response - we allow for the impact of one predictor to vary across the range of the other predictor."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#centering-the-data",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#centering-the-data",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Centering the data",
    "text": "Centering the data\nWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\n\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\nIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\nFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\nNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function, which centers and scales (divides by standard deviation) the data. We only really need to center the data, so we provide the argument scale=FALSE. Also note that the scale function attaches the pre-centered mean (and standard deviation if scaling is performed) as attributes to the scaled data in order to facilitate back-scaling to the original scale. While these attributes are often convenient, they do cause issues for some of the Bayesian routines and so we will strip these attributes using the as.numeric function. Instead, we will create separate scalar variables to store the pre-scaled means.\n\ndata &lt;- within(data, {\n    cx1 &lt;- as.numeric(scale(x1, scale = FALSE))\n    cx2 &lt;- as.numeric(scale(x2, scale = FALSE))\n})\nhead(data)\n\nNA          y        x1        x2         cx1         cx2\nNA 1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\nNA 2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557\nNA 3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\nNA 4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568\nNA 5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415\nNA 6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335\n\nmean.x1 = mean(data$x1)\nmean.x2 = mean(data$x2)"
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#additive-model-1",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#additive-model-1",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Additive Model",
    "text": "Additive Model\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX = model.matrix(~cx1 + cx2, data = data)\ndata.list &lt;- with(data, list(y = y, X = X[, -1], nX = ncol(X) -\n    1, n = nrow(data)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta0\", \"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nNow run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\ndata.r2jags.add &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\n    model.file = \"ttestModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 4\nNA    Total graph size: 614\nNA \nNA Initializing model\n\nprint(data.r2jags.add)\n\nNA Inference for Bugs model at \"ttestModel.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]    2.830   0.442   1.964   2.538   2.830   3.125   3.694 1.001  7400\nNA beta[2]    1.582   0.380   0.833   1.327   1.581   1.834   2.319 1.001 14000\nNA beta0      3.799   0.100   3.603   3.733   3.797   3.865   3.997 1.001 15000\nNA sigma      0.996   0.074   0.864   0.944   0.992   1.043   1.154 1.001 15000\nNA deviance 281.420   2.961 277.779 279.260 280.727 282.888 288.827 1.001 15000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 4.4 and DIC = 285.8\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#multiplicative-model-1",
    "href": "tutorials/2020-02-03-multiple-linear-regression-jags/index.html#multiplicative-model-1",
    "title": "Multiple Linear Regression (JAGS)",
    "section": "Multiplicative Model",
    "text": "Multiplicative Model\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX = model.matrix(~cx1 * cx2, data = data)\ndata.list &lt;- with(data, list(y = y, X = X[, -1], nX = ncol(X) - 1, n = nrow(data)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta0\", \"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\ndata.r2jags.mult &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\n    model.file = \"ttestModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 5\nNA    Total graph size: 715\nNA \nNA Initializing model\n\nprint(data.r2jags.mult)\n\nNA Inference for Bugs model at \"ttestModel.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]    2.800   0.451   1.914   2.500   2.801   3.104   3.680 1.001 15000\nNA beta[2]    1.504   0.389   0.744   1.237   1.505   1.766   2.267 1.001 15000\nNA beta[3]    1.451   1.210  -0.933   0.643   1.456   2.238   3.849 1.001 15000\nNA beta0      3.715   0.122   3.475   3.633   3.715   3.797   3.957 1.001  6000\nNA sigma      0.994   0.073   0.863   0.944   0.989   1.039   1.151 1.001 15000\nNA deviance 280.964   3.307 276.617 278.541 280.281 282.649 289.157 1.001 15000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 5.5 and DIC = 286.4\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html",
    "title": "Simple Linear Regression (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#introduction",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#introduction",
    "title": "Simple Linear Regression (Stan)",
    "section": "Introduction",
    "text": "Introduction\nMany clinicians get a little twitchy and nervous around mathematical and statistical formulae and nomenclature. Whilst it is possible to perform basic statistics without too much regard for the actual equation (model) being employed, as the complexity of the analysis increases, the need to understand the underlying model becomes increasingly important. Moreover, model specification in BUGS/JAGS/Stan (the language used to program Bayesian modelling) aligns very closely to the underlying formulae. Hence a good understanding of the underlying model is vital to be able to create a sensible Bayesian model. Consequently, I will always present the linear model formulae along with the analysis.\nTo introduce the philosophical and mathematical differences between classical (frequentist) and Bayesian statistics, based on previous works, we present a provocative yet compelling trend analysis of two hypothetical populations (A vs B). The temporal trend of population A shows very little variability from a very subtle linear decline (\\(n=10\\), \\(\\text{slope}=-0.10\\), \\(\\text{p-value}=0.048\\)). By contrast, the B population appears to decline more dramatically, yet has substantially more variability (\\(n=10\\), \\(\\text{slope}=-10.23\\), \\(\\text{p-value}=0.058\\)). From a traditional frequentist perspective, we would conclude that there is a “significant” relationship in Population A (\\(p&lt;0.05\\)), yet not in Population B (\\(p&gt;0.05\\)). However, if we consider a third population C which is exactly the same as populstion B but with a higher number of observations, then we may end up with a completely different conclusion compared with that based on population B (\\(n=100\\), \\(\\text{slope}=-10.47\\), \\(\\text{p-value}&lt;0.001\\)).\nThe above illustrates a couple of things:\n\nstatistical significance does not necessarily translate into clinical importance. Indeed, population B is declining at nearly \\(10\\) times the rate of population A. That sounds rather important, yet on the basis of the hypothesis test, we would dismiss the decline in population B.\nthat a p-value is just the probability of detecting an effect or relationship - what is the probability that the sample size is large enough to pick up a difference.\n\nLet us now look at it from a Bayesian perspective, with a focus on population A and B. We would conclude that:\n\nthe mean (plus or minus CI) slopes for Population A and B are \\(-0.1 (-0.21,0)\\) and \\(-10.08 (-20.32,0.57)\\) respectively\nthe Bayesian approach allows us to query the posterior distribution is many other ways in order to ask sensible clinical questions. For example, we might consider that a rate of change of \\(5\\)% or greater represents an important biological impact. For population A and B, the probability that the rate is \\(5\\)% or greater is \\(0\\) and \\(0.85\\) respectively."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#linear-regression",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#linear-regression",
    "title": "Simple Linear Regression (Stan)",
    "section": "Linear regression",
    "text": "Linear regression\nSimple linear regression is a linear modelling process that models a continuous response against a single continuous predictor. The linear model is expressed as:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma),\n\\]\nwhere \\(y_i\\) is the response variable for each of the \\(i=1\\ldots,n\\) observations, \\(\\beta_0\\) is the intercept (value when \\(x=0\\)), \\(\\beta_1\\) is the slope (rate of change in \\(y\\) per unit change in \\(x\\)), \\(x_i\\) is the predictor variable, \\(\\epsilon_i\\) is the residual value (difference between the observed value and the value expected by the model). The parameters of the trendline \\(\\boldsymbol \\beta=(\\beta_0,\\beta_1)\\) are determined by Ordinary Least Squares (OLS) in which the sum of the squared residuals is minimized. A non-zero population slope is indicative of a relationship."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#centering-the-data",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#centering-the-data",
    "title": "Simple Linear Regression (Stan)",
    "section": "Centering the data",
    "text": "Centering the data\nWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\n\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\nIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\nFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\nNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function.\n\ndata &lt;- within(data, {\n    cx &lt;- as.numeric(scale(x, scale = FALSE))\n})\nhead(data)\n\nNA          y x   cx\nNA 1 35.69762 1 -7.5\nNA 2 35.84911 2 -6.5\nNA 3 43.29354 3 -5.5\nNA 4 34.35254 4 -4.5\nNA 5 33.14644 5 -3.5\nNA 6 39.57532 6 -2.5"
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#normality",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#normality",
    "title": "Simple Linear Regression (Stan)",
    "section": "Normality",
    "text": "Normality\nEstimation and inference testing in linear regression assumes that the response is normally distributed in each of the populations. In this case, the populations are all possible measurements that could be collected at each level of \\(x\\) - hence there are \\(16\\) populations. Typically however, we only collect a single observation from each population (as is also the case here). How then can be evaluate whether each of these populations are likely to have been normal? For a given response, the population distributions should follow much the same distribution shapes. Therefore provided the single samples from each population are unbiased representations of those populations, a boxplot of all observations should reflect the population distributions."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#homogeneity-of-variance",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#homogeneity-of-variance",
    "title": "Simple Linear Regression (Stan)",
    "section": "Homogeneity of variance",
    "text": "Homogeneity of variance\nSimple linear regression also assumes that each of the populations are equally varied. Actually, it is prospect of a relationship between the mean and variance of \\(y\\)-values across x-values that is of the greatest concern. Strictly the assumption is that the distribution of \\(y\\) values at each \\(x\\) value are equally varied and that there is no relationship between mean and variance. However, as we only have a single \\(y\\)-value for each \\(x\\)-value, it is difficult to directly determine whether the assumption of homogeneity of variance is likely to have been violated (mean of one value is meaningless and variability can’t be assessed from a single value). If we then plot the residuals (difference between observed values and those predicted by the trendline) against the predict values and observe a definite presence of a pattern, then it is indicative of issues with the assumption of homogeneity of variance.\nHence looking at the spread of values around a trendline on a scatterplot of \\(y\\) against \\(x\\) is a useful way of identifying gross violations of homogeneity of variance. Residual plots provide an even better diagnostic. The presence of a wedge shape is indicative that the population mean and variance are related."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#linearity",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#linearity",
    "title": "Simple Linear Regression (Stan)",
    "section": "Linearity",
    "text": "Linearity\nLinear regression fits a straight (linear) line through the data. Therefore, prior to fitting such a model, it is necessary to establish whether this really is the most sensible way of describing the relationship. That is, does the relationship appear to be linearly related or could some other non-linear function describe the relationship better. Scatterplots and residual plots are useful diagnostics."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#model-assumptions",
    "href": "tutorials/2020-02-02-simple-linear-regression-stan/index.html#model-assumptions",
    "title": "Simple Linear Regression (Stan)",
    "section": "Model assumptions",
    "text": "Model assumptions\nThe typical assumptions which need to be checked when fitting a standard linear regression model are:\n\nAll of the observations are independent - this must be addressed at the design and collection stages\nThe response variable (and thus the residuals) should be normally distributed\nThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately)\nThe relationship between the linear predictor (right hand side of the regression formula) and the link function should be linear. A scatterplot with smoother can be useful for identifying possible non-linearity.\n\nSo lets explore normality, homogeneity of variances and linearity by constructing a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\)). We will also include a range of smoothers (linear and lowess) and marginal boxplots on the scatterplot to assist in exploring linearity and normality respectively.\n\n# scatterplot\nlibrary(car)\nscatterplot(y ~ x, data)\n\n\n\n\n\n\n\n\nConclusions:\nThere is no evidence that the response variable is non-normal. The spread of values around the trendline seems fairly even (hence it there is no evidence of non-homogeneity). The data seems well represented by the linear trendline. Furthermore, the lowess smoother does not appear to have a consistent shift trajectory. Obvious violations could be addressed either by:\n\nConsider a non-linear linear predictor (such as a polynomial, spline or other non-linear function)\nTransform the scale of the response variables (e.g. to address normality)"
  },
  {
    "objectID": "tutorials/2020-02-01-comparing-two-pop-stan/index.html",
    "href": "tutorials/2020-02-01-comparing-two-pop-stan/index.html",
    "title": "Comparing Two Populations (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThe BUGS/JAGS/Stan languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-comparing-two-pop-stan/index.html#fitting-the-model-in-stan",
    "href": "tutorials/2020-02-01-comparing-two-pop-stan/index.html#fitting-the-model-in-stan",
    "title": "Comparing Two Populations (Stan)",
    "section": "Fitting the model in Stan",
    "text": "Fitting the model in Stan\nBroadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (Means parameterisation) or we can estimate the mean of one group and the difference between this group and the other group(s) (Effects parameterisation). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).\n\nEffects parameterisation\n\n\\[\ny_i = \\beta_0 + \\beta_{j}x_i + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n\\]\nEach \\(y_i\\) is modelled by an intercept \\(\\beta_0\\) (mean of group A) plus a difference parameter \\(\\beta_j\\) (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (\\(x_i\\)), plus a residual drawn from a normal distribution with mean \\(0\\) and standard deviation \\(\\sigma\\). Actually, there are as many \\(\\beta_j\\) parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\).\n\nMeans parameterisation\n\n\\[\ny_i = \\beta_{j} + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n\\]\nEach \\(y_i\\) is modelled as the mean \\(\\beta_j\\) of each group (\\(j=1,2\\)) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of \\(\\sigma\\). Actually, \\(\\boldsymbol \\beta\\) is a set of \\(j\\) coefficients corresponding to the \\(j\\) dummy coded factor levels. Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\).\nWhilst the Stan language broadly resembles BUGS/JAGS, there are numerous important differences. Some of these differences are to support translation to c++ for compilation (such as declaring variables). Others reflect leveraging of vectorization to speed up run time. Here are some important notes about Stan:\n\nAll variables must be declared\nVariables declared in the parameters block will be collected\nAnything in the transformed block will be collected as samples. Also, checks will be made every loop\n\nNow I will demonstrate fitting the models with Stan. Note, I am using the refresh=0 option so as to suppress the larger regular output in the interest of keeping output to what is necessary for this tutorial. When running outside of a tutorial context, the regular verbose output is useful as it provides a way to gauge progress.\nEffects Parameterisation\n\nstanString = \" \ndata {\n int n;\n vector [n] y;\n vector [n] x;\n }\n parameters {\n real &lt;lower=0, upper=100&gt; sigma;\n real beta0;\n real beta;\n }\n transformed parameters {\n }\n model {\n vector [n] mu;\n \n //Priors\n beta0 ~ normal(0,1000);\n beta ~ normal(0,1000);\n sigma ~ cauchy(0,25);\n \n mu = beta0 + beta*x;\n //Likelihood\n y ~ normal(mu, sigma);\n }\n generated quantities {\n vector [2] Group_means;\n real CohensD;\n //Other Derived parameters \n //# Group means (note, beta is a vector)\n Group_means[1] = beta0;\n Group_means[2] = beta0+beta;\n \n CohensD = beta /sigma;  \n }\n \n \"\n## write the model to a text file\nwriteLines(stanString, con = \"ttestModel.stan\")\n\nMeans Parameterisation\n\nstanString.means = \"  \n data {\n int n;\n int nX;\n vector [n] y;\n matrix [n,nX] x;\n }\n parameters {\n real &lt;lower=0, upper=100&gt; sigma;\n vector [nX] beta;\n }\n transformed parameters {\n }\n model {\n vector [n] mu;\n \n //Priors\n beta ~ normal(0,1000);\n sigma ~ cauchy(0,25);\n \n mu = x*beta;\n //Likelihood\n y ~ normal(mu, sigma);\n }\n generated quantities {\n vector [2] Group_means;\n real CohensD;\n \n //Other Derived parameters \n Group_means[1] = beta[1];\n Group_means[2] = beta[1]+beta[2];\n \n CohensD = beta[2] /sigma;  \n }\n \n \"\n## write the model to a text file\nwriteLines(stanString.means, con = \"ttestModelMeans.stan\")\n\nArrange the data as a list (as required by Stan).\n\ndata.list &lt;- with(data, list(y = y, x = (xn - 1), n = nrow(data)))\nX &lt;- model.matrix(~x, data)\ndata.list.means = with(data, list(y = y, x = X, n = nrow(data), nX = ncol(X)))\n\nDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\ninits &lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,\n    data$x, mean))), sigma = sd(data$y/2))\ninits.means &lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))\n\nDefine the nodes (parameters and derivatives) to monitor.\n\nparams &lt;- c(\"beta0\", \"beta\", \"sigma\", \"Group_means\", \"CohensD\")\nparams.means &lt;- c(\"beta\", \"sigma\", \"Group_means\",\"CohensD\")\n\nDefine the chain parameters.\n\nburnInSteps = 500  # the number of initial samples to discard\nnChains = 2  # the number of independed sampling chains to perform \nthinSteps = 1  # the thinning rate\nnIter = 2000\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\nlibrary(rstan)\n\nWhen using the stan function (rtsan package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\nEffects Parameterisation\n\ndata.stan = stan(file = \"ttestModel.stan\", \n  data = data.list, \n  pars = params,\n  iter = nIter,\n  warmup = burnInSteps, \n  chains = nChains, \n  thin = thinSteps, \n  init = \"random\", #or inits=list(inits,inits)\n  refresh = 0)\n\n#print results\nprint(data.stan)\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                   mean se_mean   sd    2.5%     25%     50%     75%   97.5%\nNA beta0           105.20    0.01 0.35  104.51  104.97  105.20  105.43  105.89\nNA beta            -27.31    0.01 0.55  -28.37  -27.67  -27.32  -26.95  -26.22\nNA sigma             2.79    0.00 0.20    2.43    2.64    2.78    2.91    3.20\nNA Group_means[1]  105.20    0.01 0.35  104.51  104.97  105.20  105.43  105.89\nNA Group_means[2]   77.89    0.01 0.44   77.03   77.60   77.89   78.18   78.75\nNA CohensD          -9.86    0.02 0.73  -11.31  -10.35   -9.83   -9.35   -8.45\nNA lp__           -150.69    0.04 1.23 -153.81 -151.23 -150.35 -149.81 -149.31\nNA                n_eff Rhat\nNA beta0           1726    1\nNA beta            1543    1\nNA sigma           1752    1\nNA Group_means[1]  1726    1\nNA Group_means[2]  3066    1\nNA CohensD         1795    1\nNA lp__            1188    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:16:01 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n\nMeans Parameterisation\n\ndata.stan.means = stan(file = \"ttestModelMeans.stan\", \n  data = data.list.means, \n  pars = params.means,\n  iter = nIter,\n  warmup = burnInSteps, \n  chains = nChains, \n  thin = thinSteps, \n  init = \"random\", #or inits=list(inits.means,inits.means)\n  refresh = 0)\n\n#print results\nprint(data.stan.means)\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                   mean se_mean   sd    2.5%     25%     50%     75%   97.5%\nNA beta[1]         105.21    0.01 0.35  104.51  104.97  105.21  105.44  105.91\nNA beta[2]         -27.33    0.01 0.56  -28.43  -27.69  -27.33  -26.96  -26.22\nNA sigma             2.78    0.00 0.20    2.41    2.64    2.77    2.91    3.20\nNA Group_means[1]  105.21    0.01 0.35  104.51  104.97  105.21  105.44  105.91\nNA Group_means[2]   77.88    0.01 0.43   77.05   77.58   77.88   78.18   78.75\nNA CohensD          -9.88    0.02 0.73  -11.32  -10.39   -9.86   -9.37   -8.51\nNA lp__           -150.71    0.03 1.20 -153.75 -151.26 -150.40 -149.83 -149.35\nNA                n_eff Rhat\nNA beta[1]         1986    1\nNA beta[2]         2007    1\nNA sigma           2144    1\nNA Group_means[1]  1986    1\nNA Group_means[2]  3201    1\nNA CohensD         2202    1\nNA lp__            1429    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:16:32 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n\nNotes\n\nIf inits=\"random\" the stan function will randomly generate initial values between \\(-2\\) and \\(2\\) on the unconstrained support. The optional additional parameter init_r can be set to some value other than \\(2\\) to change the range of the randomly generated inits. Other available options include: set inits=\"0\" to initialize all parameters to zero on the unconstrained support; set inital values by providing a list equal in length to the number of chains; set initial values by providing a function that returns a list for specifying the initial values of parameters for a chain.\nIn addition to the mean and quantiles of each of the sample nodes, the stan function will calculate.\n\nThe effective sample size for each sample - if n.eff for a node is substantially less than the number of iterations, then it suggests poor mixing.\nThe Potential scale reduction factor or Rhat values for each sample - these are a convergence diagnostic (values of \\(1\\) indicate full convergence, values greater than \\(1.01\\) are indicative of non-convergence.\n\n\nThe total number samples collected is \\(3000\\). That is, there are \\(3000\\) samples collected from the multidimensional posterior distribution and thus, \\(3000\\) samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#introduction",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#linear-models",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#linear-models",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for two and three factor nested design are:\n\\[\ny_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#assumptions",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnTreat &lt;- 3\nnBlock &lt;- 35\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nTreat\nBlock &lt;- gl(nBlock, k=1)\nA &lt;- gl(nTreat,k=1)\ndt &lt;- expand.grid(A=A,Block=Block)\n#Xmat &lt;- model.matrix(~Block + A + Block:A, data=dt)\nXmat &lt;- model.matrix(~-1+Block + A, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\nA.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\n## Sum to zero block effects\nblock.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\nA.effects &lt;- c(40,70,80)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.rcb &lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\nhead(data.rcb)  #print out the first six rows of the data set\n\nNA          y A Block\nNA 1 45.80853 1     1\nNA 2 66.71784 2     1\nNA 3 93.29238 3     1\nNA 4 43.10101 1     2\nNA 5 73.20697 2     2\nNA 6 91.77487 3     2"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~A, data.rcb)\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.rcb, interaction.plot(A,Block,y))\n\n#OR with ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A, data.rcb))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA Tukey test   -1.4163           0.1567\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\n\nNA       Test     Pvalue \nNA -1.4163343  0.1566776\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nlibrary(asbio)\nwith(data.rcb,tukey.add.test(y,A,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 2.0060029   Denom df = 67    p-value = 0.1613102\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nFull parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[\n\\boldsymbol \\mu =\n  \\begin{bmatrix} 0  \\\\ 0  \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim   \n  \\begin{bmatrix}\n   1000000 & 0 & 0 \\\\\n   0 & 1000000 & 0 \\\\\n   0 & 0 & 1000000\n   \\end{bmatrix}.\n\\]\nHierarchical parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- beta0 + beta[A[i]] + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   }\n   \n   #Priors\n   beta0 ~ dnorm(0, 1.0E-6)\n   beta[1] &lt;- 0\n   for (i in 2:nA) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=as.numeric(Block),\n         A=as.numeric(A),\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n                 nA = length(levels(A))\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta0\",\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.rcb.r2jags.f &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 42\nNA    Total graph size: 582\nNA \nNA Initializing model\n\nprint(data.rcb.r2jags.f)\n\nNA Inference for Bugs model at \"fullModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1\nNA beta[2]    27.923   1.236  25.587  27.086  27.918  28.736  30.363 1.001  3000\nNA beta[3]    40.263   1.229  37.821  39.463  40.271  41.070  42.706 1.001  3000\nNA beta0      41.834   2.154  37.519  40.406  41.833  43.338  46.002 1.001  3000\nNA gamma[1]    3.731   3.394  -2.865   1.418   3.748   6.043  10.452 1.001  3000\nNA gamma[2]    4.534   3.439  -2.033   2.182   4.508   6.865  11.317 1.003   690\nNA gamma[3]   -3.951   3.464 -10.690  -6.324  -3.884  -1.600   2.829 1.001  3000\nNA gamma[4]   -4.129   3.454 -10.758  -6.477  -4.200  -1.784   2.630 1.003   780\nNA gamma[5]   -5.314   3.480 -12.143  -7.633  -5.325  -2.947   1.594 1.001  3000\nNA gamma[6]   -6.050   3.377 -12.486  -8.331  -6.071  -3.700   0.524 1.001  3000\nNA gamma[7]   -0.709   3.373  -7.083  -3.017  -0.728   1.585   6.032 1.001  3000\nNA gamma[8]  -15.033   3.446 -21.689 -17.322 -15.065 -12.741  -7.939 1.001  3000\nNA gamma[9]   27.856   3.444  20.996  25.498  27.927  30.226  34.525 1.001  3000\nNA gamma[10]  12.830   3.591   5.798  10.453  12.809  15.249  20.065 1.001  3000\nNA gamma[11] -14.936   3.427 -21.825 -17.228 -14.945 -12.635  -8.165 1.001  3000\nNA gamma[12]  -7.878   3.427 -14.571 -10.161  -7.929  -5.551  -1.275 1.001  3000\nNA gamma[13] -10.865   3.430 -17.569 -13.203 -10.852  -8.555  -4.076 1.001  2100\nNA gamma[14]   9.153   3.466   2.557   6.679   9.161  11.473  15.941 1.002  1100\nNA gamma[15]  -3.897   3.495 -10.811  -6.227  -3.882  -1.565   2.866 1.001  3000\nNA gamma[16]   1.321   3.437  -5.486  -0.927   1.328   3.539   8.066 1.001  3000\nNA gamma[17]  -4.137   3.445 -11.102  -6.451  -4.046  -1.802   2.384 1.001  2300\nNA gamma[18]  -4.257   3.449 -10.970  -6.607  -4.280  -1.978   2.695 1.001  2900\nNA gamma[19]  16.435   3.468   9.749  14.031  16.449  18.830  23.059 1.002  1600\nNA gamma[20]  -5.108   3.439 -11.784  -7.392  -5.100  -2.851   1.708 1.001  3000\nNA gamma[21]  18.935   3.517  12.023  16.632  18.857  21.210  25.944 1.001  3000\nNA gamma[22] -20.654   3.459 -27.290 -23.041 -20.593 -18.341 -14.063 1.001  3000\nNA gamma[23]   7.325   3.570   0.265   4.959   7.346   9.652  14.226 1.001  2500\nNA gamma[24]  -1.300   3.500  -8.248  -3.680  -1.256   1.043   5.590 1.001  3000\nNA gamma[25]  -6.114   3.419 -12.672  -8.442  -6.078  -3.725   0.554 1.001  2500\nNA gamma[26]   1.038   3.451  -5.502  -1.351   1.018   3.415   7.820 1.001  3000\nNA gamma[27]  -4.346   3.400 -10.942  -6.604  -4.351  -1.980   2.254 1.001  3000\nNA gamma[28]  -4.721   3.368 -11.281  -6.939  -4.724  -2.572   2.057 1.001  3000\nNA gamma[29] -12.328   3.513 -19.166 -14.660 -12.295 -10.096  -5.361 1.001  3000\nNA gamma[30] -12.858   3.534 -19.927 -15.216 -12.782 -10.455  -5.999 1.001  3000\nNA gamma[31]   0.272   3.457  -6.677  -2.020   0.279   2.562   7.061 1.002  1700\nNA gamma[32]   8.682   3.389   1.905   6.358   8.769  10.986  15.304 1.002  1100\nNA gamma[33]   0.315   3.433  -6.393  -2.013   0.292   2.624   7.101 1.001  3000\nNA gamma[34]   9.586   3.491   2.775   7.232   9.621  11.954  16.438 1.002  1600\nNA gamma[35]  23.906   3.429  17.230  21.620  23.976  26.208  30.709 1.004   660\nNA res[1]      0.243   2.906  -5.276  -1.686   0.233   2.121   5.940 1.001  3000\nNA res[2]     -6.771   2.869 -12.345  -8.647  -6.778  -4.904  -1.088 1.001  3000\nNA res[3]      7.465   2.940   1.587   5.472   7.482   9.382  13.252 1.001  3000\nNA res[4]     -3.267   2.927  -8.857  -5.249  -3.282  -1.245   2.325 1.003   710\nNA res[5]     -1.085   2.946  -6.792  -3.130  -1.110   0.894   4.685 1.003   580\nNA res[6]      5.144   2.913  -0.568   3.244   5.128   7.112  11.094 1.003   600\nNA res[7]     -0.049   2.992  -5.806  -2.023  -0.081   1.917   5.915 1.002  1400\nNA res[8]     -2.652   3.020  -8.617  -4.629  -2.690  -0.658   3.254 1.001  2100\nNA res[9]      2.018   2.992  -3.965  -0.008   1.989   4.022   7.816 1.001  1900\nNA res[10]    -2.071   2.894  -7.884  -4.016  -2.075  -0.143   3.626 1.003   790\nNA res[11]     0.729   2.960  -5.122  -1.274   0.754   2.666   6.590 1.003   660\nNA res[12]     0.287   2.974  -5.811  -1.700   0.396   2.232   5.977 1.003   700\nNA res[13]    -2.939   2.956  -8.894  -4.938  -2.989  -0.959   2.831 1.001  3000\nNA res[14]     4.213   2.943  -1.578   2.254   4.220   6.162  10.013 1.002  3000\nNA res[15]    -2.451   2.949  -8.246  -4.449  -2.478  -0.503   3.535 1.001  3000\nNA res[16]    -2.462   2.911  -8.169  -4.429  -2.422  -0.560   3.167 1.001  3000\nNA res[17]     3.440   2.912  -2.164   1.559   3.394   5.390   9.051 1.001  3000\nNA res[18]    -2.208   2.908  -7.897  -4.117  -2.183  -0.279   3.505 1.001  3000\nNA res[19]    -5.249   2.902 -10.958  -7.149  -5.263  -3.289   0.456 1.001  2800\nNA res[20]     4.201   2.886  -1.484   2.238   4.217   6.100   9.930 1.001  3000\nNA res[21]     1.085   2.889  -4.579  -0.852   1.104   2.988   6.981 1.001  3000\nNA res[22]     0.756   2.958  -5.385  -1.108   0.712   2.747   6.541 1.001  3000\nNA res[23]     1.284   2.979  -4.882  -0.642   1.295   3.319   6.941 1.001  3000\nNA res[24]    -5.388   2.941 -11.391  -7.320  -5.410  -3.425   0.336 1.001  3000\nNA res[25]     3.141   2.979  -2.786   1.153   3.135   5.090   9.051 1.001  3000\nNA res[26]    -4.587   2.978 -10.372  -6.597  -4.605  -2.589   1.268 1.001  3000\nNA res[27]     7.011   2.963   1.359   4.983   6.994   8.965  12.803 1.001  3000\nNA res[28]     7.495   3.018   1.549   5.443   7.508   9.538  13.331 1.002  1600\nNA res[29]     0.730   3.013  -5.133  -1.311   0.723   2.759   6.603 1.001  2200\nNA res[30]    -5.563   3.054 -11.597  -7.683  -5.577  -3.452   0.255 1.001  2100\nNA res[31]    -3.927   2.962  -9.656  -5.903  -3.900  -1.965   1.925 1.001  3000\nNA res[32]     2.986   2.928  -2.794   1.134   2.985   4.946   8.735 1.001  3000\nNA res[33]    -1.871   2.951  -7.734  -3.824  -1.864   0.076   4.020 1.001  3000\nNA res[34]    -0.528   2.951  -6.322  -2.505  -0.516   1.395   5.205 1.001  3000\nNA res[35]    -1.472   2.949  -7.285  -3.350  -1.439   0.455   4.220 1.001  3000\nNA res[36]     0.722   2.938  -4.922  -1.223   0.716   2.663   6.487 1.001  3000\nNA res[37]    -0.493   2.928  -6.329  -2.503  -0.515   1.546   5.111 1.002   940\nNA res[38]    -2.832   2.938  -8.610  -4.822  -2.827  -0.812   2.778 1.002  1300\nNA res[39]     1.268   2.931  -4.339  -0.752   1.281   3.287   6.896 1.002  1200\nNA res[40]     2.968   2.956  -2.952   0.972   2.987   4.925   8.768 1.003   540\nNA res[41]    -2.427   2.942  -8.293  -4.383  -2.363  -0.534   3.629 1.003   660\nNA res[42]     1.150   2.922  -4.453  -0.813   1.176   3.054   7.197 1.003   620\nNA res[43]    -7.026   2.953 -12.859  -8.930  -7.097  -5.036  -1.170 1.001  3000\nNA res[44]     2.862   2.915  -2.600   0.922   2.711   4.854   8.438 1.001  3000\nNA res[45]     3.397   2.955  -2.273   1.404   3.364   5.361   9.127 1.001  3000\nNA res[46]     1.390   2.972  -4.597  -0.607   1.321   3.366   7.167 1.001  2000\nNA res[47]     2.490   2.991  -3.362   0.502   2.495   4.531   8.375 1.001  3000\nNA res[48]    -3.582   2.963  -9.351  -5.573  -3.608  -1.606   2.286 1.001  2700\nNA res[49]    -2.288   2.916  -7.729  -4.298  -2.366  -0.291   3.588 1.003  1000\nNA res[50]    -1.083   2.906  -6.569  -3.067  -1.105   0.863   4.716 1.002  1300\nNA res[51]     2.286   2.912  -3.216   0.331   2.244   4.222   8.075 1.002  1200\nNA res[52]    -2.829   2.903  -8.540  -4.796  -2.804  -0.891   2.706 1.001  3000\nNA res[53]     1.533   2.928  -4.178  -0.380   1.544   3.452   7.214 1.001  2700\nNA res[54]     0.366   2.907  -5.326  -1.576   0.391   2.274   6.016 1.001  3000\nNA res[55]     7.373   2.957   1.529   5.353   7.414   9.358  13.176 1.002  1300\nNA res[56]    -3.029   2.984  -9.041  -4.985  -2.963  -1.047   2.888 1.002  1000\nNA res[57]    -0.932   2.961  -6.889  -2.936  -0.824   0.995   4.741 1.002  1100\nNA res[58]     0.955   2.941  -4.752  -1.089   0.981   2.907   6.650 1.001  3000\nNA res[59]    -2.168   2.938  -8.052  -4.106  -2.164  -0.262   3.668 1.001  3000\nNA res[60]    -0.054   2.957  -5.874  -2.011   0.024   1.879   5.755 1.001  3000\nNA res[61]     4.652   3.013  -1.277   2.681   4.595   6.649  10.447 1.001  3000\nNA res[62]     1.763   3.015  -4.198  -0.283   1.800   3.811   7.690 1.001  3000\nNA res[63]    -2.627   2.975  -8.515  -4.603  -2.640  -0.650   3.124 1.001  3000\nNA res[64]    -1.878   3.019  -7.923  -3.808  -1.879   0.207   3.832 1.001  3000\nNA res[65]    -7.955   2.986 -14.124  -9.906  -7.908  -5.914  -2.185 1.001  3000\nNA res[66]     5.629   3.022  -0.370   3.651   5.702   7.692  11.441 1.001  3000\nNA res[67]    -9.447   2.997 -15.297 -11.427  -9.472  -7.515  -3.360 1.002  1100\nNA res[68]     3.633   3.011  -2.241   1.639   3.558   5.638   9.647 1.002  1400\nNA res[69]     7.139   3.005   1.189   5.081   7.118   9.082  13.136 1.002  1300\nNA res[70]    -6.267   2.959 -11.956  -8.323  -6.249  -4.253  -0.493 1.001  3000\nNA res[71]     6.538   2.978   0.563   4.494   6.525   8.512  12.391 1.001  3000\nNA res[72]    -0.621   2.967  -6.432  -2.609  -0.683   1.364   5.125 1.001  3000\nNA res[73]    -0.989   2.943  -6.875  -2.968  -0.975   0.949   4.951 1.001  3000\nNA res[74]     1.375   2.946  -4.265  -0.614   1.336   3.269   7.187 1.001  2400\nNA res[75]    -1.399   2.934  -7.135  -3.371  -1.401   0.618   4.478 1.001  2700\nNA res[76]    -0.971   2.970  -6.912  -2.914  -1.004   1.027   4.841 1.001  3000\nNA res[77]    -3.549   2.958  -9.315  -5.540  -3.567  -1.572   2.143 1.001  3000\nNA res[78]     4.860   2.940  -0.988   2.887   4.805   6.835  10.606 1.001  3000\nNA res[79]     6.984   2.964   1.461   4.967   6.920   9.004  12.824 1.001  3000\nNA res[80]    -7.875   3.011 -13.687  -9.926  -7.936  -5.815  -1.954 1.001  3000\nNA res[81]     0.160   2.947  -5.484  -1.824   0.127   2.155   6.047 1.001  3000\nNA res[82]     2.734   2.915  -3.183   0.820   2.794   4.729   8.263 1.001  3000\nNA res[83]     2.626   2.932  -3.161   0.748   2.624   4.515   8.266 1.001  3000\nNA res[84]    -6.416   2.954 -12.297  -8.315  -6.382  -4.439  -0.826 1.001  3000\nNA res[85]    -2.326   3.020  -8.242  -4.293  -2.308  -0.307   3.564 1.001  3000\nNA res[86]    -1.054   3.030  -7.047  -3.116  -1.048   0.971   4.956 1.001  3000\nNA res[87]     0.823   3.034  -5.036  -1.236   0.832   2.846   6.820 1.001  3000\nNA res[88]    -3.700   3.021  -9.662  -5.732  -3.744  -1.656   2.141 1.001  3000\nNA res[89]     5.124   3.000  -0.700   3.125   5.166   7.053  10.967 1.001  3000\nNA res[90]    -3.973   3.026  -9.965  -5.967  -3.979  -1.965   2.004 1.001  3000\nNA res[91]     6.800   2.936   1.114   4.888   6.726   8.792  12.683 1.001  2100\nNA res[92]    -1.633   2.932  -7.382  -3.615  -1.645   0.303   4.342 1.002  1500\nNA res[93]    -5.027   2.937 -10.801  -6.975  -5.090  -3.114   0.895 1.002  1600\nNA res[94]    11.068   2.894   5.415   9.144  11.037  12.950  16.844 1.017   920\nNA res[95]    -5.145   2.871 -10.758  -7.025  -5.209  -3.257   0.653 1.002   920\nNA res[96]    -3.909   2.889  -9.670  -5.821  -3.975  -2.083   2.104 1.002  1000\nNA res[97]     1.670   2.927  -4.070  -0.286   1.655   3.647   7.528 1.001  2600\nNA res[98]    -1.855   2.902  -7.533  -3.814  -1.846   0.085   3.932 1.001  3000\nNA res[99]     0.809   2.905  -4.984  -1.113   0.807   2.832   6.509 1.001  3000\nNA res[100]    1.492   2.952  -4.294  -0.488   1.451   3.471   7.192 1.001  2000\nNA res[101]    0.647   2.987  -5.287  -1.341   0.649   2.697   6.465 1.002  1500\nNA res[102]   -0.289   2.974  -6.080  -2.262  -0.296   1.781   5.395 1.002  1600\nNA res[103]   -1.309   2.976  -7.063  -3.273  -1.394   0.728   4.790 1.004   440\nNA res[104]   11.580   2.959   5.725   9.638  11.567  13.515  17.434 1.003   770\nNA res[105]   -5.108   2.939 -10.788  -7.110  -5.108  -3.165   0.707 1.006   490\nNA sigma       5.090   0.453   4.294   4.775   5.059   5.360   6.091 1.002   980\nNA sigma.B    11.494   1.491   8.926  10.487  11.365  12.348  14.912 1.002   920\nNA deviance  637.702  11.556 618.449 629.190 636.668 645.140 663.417 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 66.8 and DIC = 704.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   \n   #Priors\n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA.Xmat &lt;- model.matrix(~A,data.rcb)\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=A.Xmat,\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n         a0=rep(0,3), A0=diag(3)\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.rcb.r2jags.m &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 910\nNA \nNA Initializing model\n\nprint(data.rcb.r2jags.m)\n\nNA Inference for Bugs model at \"matrixModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     0.550   1.010  -1.396  -0.163   0.543   1.212   2.618 1.001  3000\nNA beta[2]     0.624   0.969  -1.267  -0.021   0.609   1.267   2.494 1.002   830\nNA beta[3]     1.556   1.005  -0.458   0.886   1.564   2.248   3.495 1.001  3000\nNA gamma[1]   64.957  12.115  41.593  56.722  64.856  72.937  89.019 1.001  3000\nNA gamma[2]   65.491  12.061  40.343  57.573  65.450  73.443  89.230 1.001  3000\nNA gamma[3]   57.107  11.937  34.004  49.204  56.969  65.191  79.840 1.001  3000\nNA gamma[4]   56.660  11.705  32.676  48.922  56.796  64.418  79.244 1.002   980\nNA gamma[5]   55.496  12.269  30.868  47.463  55.494  63.576  79.632 1.001  3000\nNA gamma[6]   54.801  11.877  31.954  46.951  54.385  62.674  78.056 1.002  3000\nNA gamma[7]   60.234  11.740  36.788  52.823  60.472  67.916  83.053 1.001  3000\nNA gamma[8]   45.171  11.789  21.628  37.438  45.279  53.104  69.031 1.002  3000\nNA gamma[9]   89.740  11.870  66.877  81.784  89.985  97.893 112.876 1.001  3000\nNA gamma[10]  74.392  11.959  51.486  65.937  74.289  82.703  98.386 1.002  1500\nNA gamma[11]  45.824  11.994  22.631  37.864  45.861  53.737  69.411 1.002  1100\nNA gamma[12]  52.874  11.847  29.777  44.934  53.044  61.009  75.635 1.002  1800\nNA gamma[13]  49.828  12.010  26.670  41.934  49.809  57.903  73.611 1.001  3000\nNA gamma[14]  70.252  11.879  46.842  62.471  70.259  78.152  93.618 1.001  3000\nNA gamma[15]  56.956  11.790  34.318  48.968  56.782  65.102  79.771 1.001  3000\nNA gamma[16]  62.229  12.088  39.561  53.779  62.120  70.801  85.754 1.002  1500\nNA gamma[17]  56.154  11.923  32.379  48.260  56.301  64.248  79.156 1.004   840\nNA gamma[18]  56.302  11.789  32.839  48.585  56.477  64.399  79.221 1.001  3000\nNA gamma[19]  78.011  12.127  53.246  69.932  78.041  86.393 100.917 1.001  3000\nNA gamma[20]  55.445  11.822  32.414  47.480  55.477  63.566  78.456 1.002  3000\nNA gamma[21]  79.975  11.935  56.573  72.184  80.069  87.728 103.091 1.001  3000\nNA gamma[22]  39.667  11.800  16.288  31.836  39.723  47.341  62.882 1.001  3000\nNA gamma[23]  68.605  11.860  45.831  60.540  68.721  76.702  91.376 1.001  3000\nNA gamma[24]  59.858  12.057  36.038  51.799  59.900  67.940  83.312 1.001  2200\nNA gamma[25]  54.974  11.970  31.161  47.135  55.153  62.882  78.327 1.002  1400\nNA gamma[26]  62.397  11.915  38.745  54.113  62.260  70.437  86.239 1.002  3000\nNA gamma[27]  56.526  11.968  32.943  48.610  56.627  64.491  80.067 1.003  1800\nNA gamma[28]  56.062  12.002  33.315  48.254  56.158  64.016  79.264 1.001  3000\nNA gamma[29]  47.976  11.787  24.940  39.984  47.840  55.752  71.140 1.001  3000\nNA gamma[30]  47.866  11.894  24.408  40.161  47.877  55.955  70.787 1.001  3000\nNA gamma[31]  61.528  12.021  37.815  53.617  61.620  69.800  84.596 1.001  3000\nNA gamma[32]  70.047  11.805  46.872  62.230  70.224  78.030  93.180 1.001  3000\nNA gamma[33]  61.830  11.934  38.654  53.718  61.828  69.563  85.450 1.002  1800\nNA gamma[34]  70.909  12.075  47.408  62.788  70.805  78.794  95.338 1.002  3000\nNA gamma[35]  85.532  12.138  61.716  77.422  85.479  93.434 109.336 1.001  3000\nNA res[1]    -19.698  12.037 -43.684 -27.635 -19.684 -11.694   3.838 1.001  3000\nNA res[2]      0.587  12.062 -23.374  -7.403   0.579   8.638  24.005 1.001  2500\nNA res[3]     26.230  12.050   2.155  18.315  26.057  34.356  50.024 1.001  3000\nNA res[4]    -22.940  11.997 -46.569 -30.787 -23.060 -15.111   2.226 1.001  3000\nNA res[5]      6.542  11.989 -16.855  -1.311   6.542  14.381  31.699 1.001  3000\nNA res[6]     24.179  11.977   0.862  16.176  24.040  31.902  49.149 1.001  3000\nNA res[7]    -19.824  11.904 -42.484 -27.853 -19.648 -11.889   3.474 1.001  3000\nNA res[8]      4.872  11.923 -17.801  -3.160   4.933  12.798  28.368 1.001  3000\nNA res[9]     20.951  11.893  -1.904  12.827  21.045  28.785  44.008 1.001  3000\nNA res[10]   -21.576  11.660 -44.079 -29.235 -21.708 -13.770   2.548 1.002   920\nNA res[11]     8.523  11.658 -14.011   0.830   8.435  16.332  32.472 1.002  1100\nNA res[12]    19.489  11.657  -2.829  11.774  19.266  27.343  43.385 1.002   910\nNA res[13]   -22.465  12.167 -46.486 -30.516 -22.434 -14.565   2.114 1.001  3000\nNA res[14]    11.986  12.183 -11.847   3.842  12.016  20.032  36.714 1.001  3000\nNA res[15]    16.730  12.218  -7.281   8.522  16.737  24.706  41.406 1.001  3000\nNA res[16]   -22.029  11.845 -45.221 -29.925 -21.621 -14.244   0.444 1.001  3000\nNA res[17]    11.172  11.874 -12.338   3.326  11.433  18.964  33.977 1.001  3000\nNA res[18]    16.933  11.886  -6.385   8.941  17.228  24.796  39.651 1.001  3000\nNA res[19]   -24.908  11.723 -47.784 -32.552 -25.170 -17.491  -1.875 1.001  3000\nNA res[20]    11.841  11.745 -11.265   4.268  11.662  19.369  35.065 1.001  3000\nNA res[21]    20.133  11.744  -2.743  12.424  19.956  27.740  43.426 1.001  3000\nNA res[22]   -18.164  11.729 -41.484 -26.023 -18.365 -10.358   5.425 1.001  3000\nNA res[23]     9.664  11.740 -13.656   1.865   9.491  17.451  33.025 1.001  3000\nNA res[24]    14.399  11.768  -8.920   6.550  14.224  22.160  38.381 1.001  3000\nNA res[25]   -17.459  11.814 -40.240 -25.508 -17.731  -9.597   5.503 1.001  3000\nNA res[26]     2.112  11.834 -20.695  -5.859   1.791   9.903  25.291 1.001  3000\nNA res[27]    25.119  11.869   2.237  17.073  24.856  33.014  48.190 1.001  3000\nNA res[28]   -12.783  11.927 -36.486 -21.081 -12.685  -4.484  10.101 1.002  1600\nNA res[29]     7.751  11.951 -16.443  -0.606   7.820  16.063  30.397 1.001  2100\nNA res[30]    12.866  11.971 -10.922   4.637  12.799  21.163  36.114 1.002  1600\nNA res[31]   -23.404  11.959 -47.054 -31.381 -23.430 -15.471  -0.221 1.002  1200\nNA res[32]    10.809  11.960 -12.765   2.757  10.883  18.657  33.990 1.002  1400\nNA res[33]    17.359  11.972  -6.134   9.343  17.364  25.293  40.617 1.002  1100\nNA res[34]   -19.997  11.800 -42.736 -28.106 -20.202 -12.124   3.343 1.002  2500\nNA res[35]     6.359  11.812 -16.232  -1.893   6.081  14.115  29.804 1.002  1900\nNA res[36]    19.960  11.807  -2.519  11.753  19.730  27.900  43.458 1.002  2600\nNA res[37]   -19.902  11.980 -43.600 -27.871 -19.843 -12.058   3.587 1.001  3000\nNA res[38]     5.059  12.005 -18.429  -3.025   5.164  12.756  28.837 1.001  3000\nNA res[39]    20.566  11.996  -3.070  12.608  20.634  28.366  44.009 1.001  3000\nNA res[40]   -16.847  11.831 -39.884 -24.766 -17.012  -9.239   6.596 1.001  3000\nNA res[41]     5.057  11.855 -18.074  -2.883   4.927  12.687  28.394 1.001  3000\nNA res[42]    20.042  11.830  -3.241  12.206  20.003  27.718  43.766 1.001  3000\nNA res[43]   -26.596  11.746 -49.680 -34.580 -26.349 -18.627  -3.862 1.001  3000\nNA res[44]    10.592  11.735 -12.047   2.531  10.659  18.547  33.288 1.001  3000\nNA res[45]    22.535  11.759  -0.427  14.411  22.577  30.481  45.274 1.001  3000\nNA res[46]   -18.234  12.031 -41.669 -26.696 -18.076  -9.978   4.443 1.002  1200\nNA res[47]    10.165  12.061 -13.282   1.662  10.365  18.474  33.054 1.002  1500\nNA res[48]    15.501  12.060  -7.938   7.051  15.690  23.578  38.325 1.002  1200\nNA res[49]   -21.296  11.891 -44.247 -29.499 -21.509 -13.441   2.004 1.002  1000\nNA res[50]     7.208  11.902 -15.618  -0.939   6.981  15.152  30.441 1.002   860\nNA res[51]    21.986  11.895  -0.592  13.867  21.799  29.853  45.640 1.002  1000\nNA res[52]   -22.104  11.729 -44.941 -29.987 -22.312 -14.241   1.133 1.001  3000\nNA res[53]     9.556  11.751 -13.024   1.751   9.257  17.457  33.011 1.001  3000\nNA res[54]    19.797  11.734  -2.966  11.838  19.629  27.542  43.295 1.001  3000\nNA res[55]   -12.918  12.097 -35.949 -21.363 -12.982  -4.866  11.578 1.001  3000\nNA res[56]     3.979  12.106 -19.166  -4.374   3.889  12.063  28.610 1.001  3000\nNA res[57]    17.484  12.122  -5.688   8.981  17.300  25.557  42.114 1.001  3000\nNA res[58]   -18.315  11.797 -41.135 -26.620 -18.209 -10.520   4.471 1.001  3000\nNA res[59]     5.862  11.815 -16.900  -2.341   6.146  13.803  29.023 1.001  3000\nNA res[60]    19.383  11.783  -3.464  11.144  19.447  27.072  42.559 1.001  3000\nNA res[61]   -15.105  11.900 -38.125 -22.888 -15.284  -7.276   8.233 1.001  3000\nNA res[62]     9.306  11.923 -13.585   1.507   9.128  17.140  32.775 1.001  3000\nNA res[63]    16.323  11.956  -6.322   8.357  16.116  24.316  39.779 1.001  3000\nNA res[64]   -20.915  11.793 -44.085 -28.598 -21.048 -13.147   2.444 1.001  3000\nNA res[65]     0.307  11.820 -22.750  -7.598   0.334   7.975  23.864 1.001  3000\nNA res[66]    25.298  11.813   2.156  17.551  25.247  33.151  48.893 1.001  3000\nNA res[67]   -29.443  11.830 -52.213 -37.430 -29.692 -21.434  -6.575 1.001  3000\nNA res[68]    10.936  11.848 -11.706   2.810  10.811  18.885  33.881 1.001  3000\nNA res[69]    25.850  11.873   2.835  17.722  25.645  33.821  48.845 1.001  3000\nNA res[70]   -26.142  11.991 -49.697 -34.072 -26.243 -18.049  -2.711 1.001  2300\nNA res[71]    13.963  11.984  -9.101   6.160  13.990  22.029  37.049 1.001  3000\nNA res[72]    18.211  12.015  -4.983  10.314  18.104  26.258  41.685 1.001  2300\nNA res[73]   -20.794  11.932 -43.963 -28.725 -20.938 -12.958   3.106 1.002  1600\nNA res[74]     8.870  11.937 -14.459   0.895   8.787  16.905  32.822 1.001  2000\nNA res[75]    17.504  11.929  -5.274   9.402  17.317  25.457  41.765 1.002  1600\nNA res[76]   -21.046  11.851 -44.910 -28.922 -21.131 -12.843   2.003 1.002  3000\nNA res[77]     3.675  11.857 -19.985  -4.059   3.613  11.706  27.196 1.003  3000\nNA res[78]    23.492  11.881  -0.028  15.552  23.399  31.576  46.776 1.003  3000\nNA res[79]   -12.603  11.937 -35.961 -20.361 -12.719  -4.579  10.948 1.002  2900\nNA res[80]    -0.163  11.955 -23.303  -8.120  -0.343   7.789  23.405 1.002  2200\nNA res[81]    19.279  11.963  -4.001  11.319  19.160  27.293  42.755 1.002  3000\nNA res[82]   -16.766  11.961 -39.955 -24.721 -16.887  -9.030   6.016 1.001  3000\nNA res[83]    10.426  11.958 -13.000   2.504  10.510  18.221  32.859 1.001  3000\nNA res[84]    12.792  11.939 -10.192   4.873  12.720  20.537  35.661 1.001  3000\nNA res[85]   -21.347  11.725 -44.125 -28.923 -21.164 -13.609   1.848 1.001  3000\nNA res[86]     7.224  11.690 -15.758  -0.362   7.382  15.079  30.544 1.001  3000\nNA res[87]    20.510  11.739  -2.412  12.708  20.647  28.327  43.283 1.001  3000\nNA res[88]   -23.140  11.838 -46.005 -31.274 -23.099 -15.502  -0.066 1.001  3000\nNA res[89]    12.983  11.858  -9.972   4.729  13.009  20.680  36.860 1.001  3000\nNA res[90]    15.293  11.892  -7.122   6.928  15.405  23.032  38.696 1.001  3000\nNA res[91]   -13.173  11.935 -36.126 -21.376 -13.267  -5.416  10.293 1.001  3000\nNA res[92]     5.694  11.935 -17.082  -2.485   5.640  13.450  29.247 1.001  3000\nNA res[93]    13.708  11.936  -9.020   5.488  13.624  21.589  37.402 1.001  3000\nNA res[94]    -9.013  11.766 -31.685 -16.863  -9.233  -1.091  14.418 1.001  3000\nNA res[95]     2.073  11.780 -20.569  -5.756   1.871   9.830  25.639 1.001  3000\nNA res[96]    14.717  11.782  -8.181   6.737  14.422  22.602  37.767 1.001  3000\nNA res[97]   -18.561  11.906 -41.914 -26.360 -18.779 -10.492   4.073 1.002  1300\nNA res[98]     5.213  11.908 -18.272  -2.559   5.068  13.190  28.414 1.002  1100\nNA res[99]    19.285  11.939  -4.469  11.483  19.203  27.469  42.441 1.002  1300\nNA res[100]  -18.547  12.018 -42.274 -26.484 -18.430 -10.631   5.083 1.001  3000\nNA res[101]    7.907  12.020 -16.041  -0.084   8.072  15.733  31.471 1.001  3000\nNA res[102]   18.379  12.034  -5.679  10.333  18.514  26.429  41.930 1.001  3000\nNA res[103]  -21.652  12.104 -45.310 -29.603 -21.605 -13.692   1.709 1.001  2500\nNA res[104]   18.537  12.095  -5.105  10.797  18.645  26.404  42.117 1.001  1900\nNA res[105]   13.256  12.137 -10.949   5.489  13.253  21.341  36.897 1.001  2600\nNA sigma      20.838   1.809  17.597  19.556  20.736  21.936  24.751 1.002  1000\nNA sigma.B    63.500   7.812  50.201  58.005  62.978  68.138  80.806 1.001  3000\nNA deviance  934.350  11.459 914.767 926.442 933.465 941.520 959.367 1.004   460\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 65.5 and DIC = 999.9\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nFor a simple model with only two hierarchical levels, the model is the same as above. If you want to include finite-population standard deviations in the model you can use the following code.\n\nmodelString3=\"\nmodel {\n   #Likelihood (esimating site means (gamma.site)\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- gamma[Block[i]] + inprod(beta[], X[i,]) \n      y.err[i]&lt;- mu[i]-y[i]\n   }\n   for (i in 1:nBlock) {\n      gamma[i] ~ dnorm(0, tau.block)\n   }\n   #Priors\n   for (i in 1:nX) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   sigma ~ dunif(0, 100)\n   tau &lt;- 1 / (sigma * sigma)\n   sigma.block ~ dunif(0, 100)\n   tau.block &lt;- 1 / (sigma.block * sigma.block)\n\n   sd.y &lt;- sd(y.err)\n   sd.block &lt;- sd(gamma)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString3, con = \"SDModel.txt\")\n\n#data list\nA.Xmat &lt;- model.matrix(~A,ddply(data.rcb,~Block,catcolwise(unique)))\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=Block,\n         X= A.Xmat,\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n                 nX = ncol(A.Xmat)\n         )\n)\n\n#parameters and chain details\nparams &lt;- c(\"beta\",\"sigma\",\"sd.y\",'sd.block','sigma.block')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rcb.r2jagsSD &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"SDModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 899\nNA \nNA Initializing model\n\nprint(data.rcb.r2jagsSD)\n\nNA Inference for Bugs model at \"SDModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA             mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]      41.715   2.196  37.449  40.231  41.710  43.183  45.995 1.001  3000\nNA beta[2]      27.928   1.209  25.537  27.146  27.918  28.713  30.317 1.002   980\nNA beta[3]      40.272   1.210  37.832  39.461  40.267  41.096  42.658 1.001  2300\nNA sd.block     11.358   0.519  10.353  11.029  11.345  11.706  12.370 1.001  3000\nNA sd.y          5.014   0.260   4.592   4.827   4.986   5.172   5.609 1.002  1300\nNA sigma         5.074   0.443   4.322   4.752   5.045   5.350   6.036 1.001  3000\nNA sigma.block  11.692   1.546   9.114  10.589  11.586  12.612  15.118 1.002  3000\nNA deviance    637.262  10.949 618.392 629.413 636.321 644.252 660.930 1.001  2200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 59.9 and DIC = 697.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCalculate \\(R^2\\) from the posterior of the model.\n\ndata.rcb.mcmc.listSD &lt;- as.mcmc(data.rcb.r2jagsSD)\n\nXmat &lt;- model.matrix(~A, data.rcb)\ncoefs &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['beta']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nZ.var &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['sd.block']]^2\nR.var &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['sd.y']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.block &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.block &lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\nrbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\n\nNA                     Mean     Median      lower      upper\nNA R2.block       0.2927774 0.29248056 0.24902731 0.33605200\nNA R2.marginal    0.6500204 0.65101312 0.60509352 0.68965593\nNA R2.res         0.0572022 0.05628758 0.04596228 0.07055798\nNA R2.conditional 0.9427978 0.94371242 0.92944202 0.95403772"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#planned-comparisonsand-pairwise-tests",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#planned-comparisonsand-pairwise-tests",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Planned comparisonsand pairwise tests",
    "text": "Planned comparisonsand pairwise tests\nSince there are no restrictions on the type and number of comparisons derived from the posteriors, Bayesian analyses provide a natural framework for exploring additional contrasts and comparisons. For example, to compare all possible levels:\n\ncoefs &lt;- data.rcb.r2jags.m$BUGSoutput$sims.list[[c('beta')]]\nhead(coefs)\n\nNA            [,1]        [,2]       [,3]\nNA [1,] -1.0697767 -0.46647636  0.4808020\nNA [2,]  0.6186153  1.46210386  2.3592529\nNA [3,] -1.5100302  0.09180824  1.1835869\nNA [4,] -0.3127107  0.66392714 -0.5681012\nNA [5,]  1.5552936  1.06785499  2.6443403\nNA [6,]  0.7282182  0.59829747  2.8548669\n\nnewdata &lt;- data.frame(A=levels(data.rcb$A))\n# A Tukeys contrast matrix\nlibrary(multcomp)\ntuk.mat &lt;- contrMat(n=table(newdata$A), type=\"Tukey\")\nXmat &lt;- model.matrix(~A, data=newdata)\npairwise.mat &lt;- tuk.mat %*% Xmat\npairwise.mat\n\nNA       (Intercept) A2 A3\nNA 2 - 1           0  1  0\nNA 3 - 1           0  0  1\nNA 3 - 2           0 -1  1\n\ncomps &lt;- coefs %*% t(pairwise.mat)\n\nMCMCsum &lt;- function(x) {\n   data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n}\n\n(comps &lt;-plyr:::adply(comps,2,MCMCsum))\n\nNA      X1    Median       X0.         X25.      X50.     X75.    X100.      lower\nNA 1 2 - 1 0.6093838 -2.556240 -0.020575421 0.6093838 1.267051 4.786166 -1.2766747\nNA 2 3 - 1 1.5638199 -1.833977  0.886430287 1.5638199 2.248195 4.835948 -0.4024791\nNA 3 3 - 2 0.9310770 -4.672228 -0.003765539 0.9310770 1.864871 5.592247 -1.5970184\nNA      upper     lower.1  upper.1\nNA 1 2.479999  0.03512204 1.316762\nNA 2 3.539364  0.92897200 2.273364\nNA 3 3.728297 -0.03345687 1.823124"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nslope &lt;- 30\nintercept &lt;- 200\nnBlock &lt;- 35\nnTime &lt;- 10\nsigma &lt;- 50\nsigma.block &lt;- 30\nn &lt;- nBlock*nTime\nBlock &lt;- gl(nBlock, k=1)\nTime &lt;- 1:10\nrho &lt;- 0.8\ndt &lt;- expand.grid(Time=Time,Block=Block)\nXmat &lt;- model.matrix(~-1+Block + Time, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\n#A.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,slope)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\n## Sum to zero block effects\n##block.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\n###A.effects &lt;- c(40,70,80)\n##all.effects &lt;- c(block.effects,intercept,slope)\n##lin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\neps &lt;- NULL\neps[1] &lt;- 0\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] #residuals\n}\ny &lt;- rnorm(n,lin.pred,sigma)+eps\n\n#OR\neps &lt;- NULL\n# first value cant be autocorrelated\neps[1] &lt;- rnorm(1,0,sigma)\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma)  #residuals\n}\ny &lt;- lin.pred + eps\ndata.rm &lt;- data.frame(y=y, dt)\nhead(data.rm)  #print out the first six rows of the data set\n\nNA          y Time Block\nNA 1 282.1142    1     1\nNA 2 321.1404    2     1\nNA 3 278.7700    3     1\nNA 4 285.8709    4     1\nNA 5 336.6390    5     1\nNA 6 333.5961    6     1\n\nggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method='lm') + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~Time, data.rm)\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nwith(data.rm, interaction.plot(Time,Block,y))\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+Time, data.rm))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA Time         -0.7274           0.4675\nNA Tukey test   -0.9809           0.3267\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\n\nNA       Test     Pvalue \nNA -0.9808606  0.3266615\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nwith(data.rm,tukey.add.test(y,Time,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 0.3997341   Denom df = 305    p-value = 0.5277003\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\n\nSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\nlibrary(nlme)\ndata.rm.lme &lt;- lme(y~Time, random=~1|Block, data=data.rm)\nacf(resid(data.rm.lme), lag=10)\n\n\n\n\n\n\n\n\nConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- beta0 + beta*Time[i] + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   }\n   \n   #Priors\n   beta0 ~ dnorm(0, 1.0E-6)\n   beta ~ dnorm(0, 1.0E-6) #prior\n   \n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel2.txt\")\n\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         Time=Time,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block))\n             )\n)\n\nparams &lt;- c(\"beta0\",\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.f &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 41\nNA    Total graph size: 1815\nNA \nNA Initializing model\n\nprint(data.rm.r2jags.f)\n\nNA Inference for Bugs model at \"fullModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\nNA beta        30.689   1.047   28.609   30.017   30.687   31.401   32.705 1.001\nNA beta0      189.009  12.648  164.589  180.318  189.054  197.523  213.976 1.001\nNA gamma[1]   -35.015  20.219  -74.991  -47.706  -35.021  -21.382    3.826 1.002\nNA gamma[2]   -52.026  20.114  -91.663  -65.012  -52.008  -38.575  -12.685 1.001\nNA gamma[3]    20.417  19.878  -19.313    7.462   20.587   33.579   60.228 1.001\nNA gamma[4]     0.671  20.295  -38.799  -12.839    0.882   14.490   39.807 1.001\nNA gamma[5]    67.812  19.967   29.683   54.189   67.514   81.150  109.392 1.001\nNA gamma[6]    36.338  19.760   -2.575   22.780   36.203   49.381   76.772 1.001\nNA gamma[7]    24.072  20.155  -14.701   10.740   24.009   36.728   63.397 1.001\nNA gamma[8]   -31.199  20.016  -70.564  -44.687  -31.149  -17.691    7.011 1.001\nNA gamma[9]    73.971  20.034   35.053   60.309   73.846   87.726  113.132 1.003\nNA gamma[10]   58.034  19.900   19.397   44.730   58.085   71.380   97.283 1.001\nNA gamma[11]  141.644  20.387  101.956  127.950  141.240  154.897  181.751 1.001\nNA gamma[12]    5.655  20.094  -32.833   -7.787    5.349   19.065   47.017 1.001\nNA gamma[13]  -44.187  20.168  -84.778  -57.576  -44.641  -30.571   -5.599 1.001\nNA gamma[14]  -23.866  19.908  -63.673  -37.311  -23.653  -10.578   14.435 1.001\nNA gamma[15]   30.407  20.239   -8.109   16.928   30.379   43.830   70.587 1.001\nNA gamma[16]  103.433  20.123   64.608   90.052  103.087  116.736  143.495 1.002\nNA gamma[17]   91.556  20.060   53.115   77.561   91.725  104.473  131.814 1.002\nNA gamma[18]  -63.563  20.127 -102.913  -77.195  -63.210  -50.190  -24.916 1.002\nNA gamma[19]   16.404  19.820  -21.892    2.880   16.232   29.420   55.497 1.001\nNA gamma[20]  -26.858  19.837  -66.283  -39.890  -26.676  -13.651   12.760 1.002\nNA gamma[21] -104.771  19.743 -143.174 -117.701 -105.347  -92.448  -64.620 1.001\nNA gamma[22]  -14.307  19.903  -54.617  -27.704  -14.041   -0.901   23.918 1.001\nNA gamma[23]  -81.493  19.863 -121.932  -94.860  -81.367  -67.618  -43.350 1.001\nNA gamma[24]  -86.520  20.067 -125.826 -100.133  -86.297  -73.003  -47.481 1.001\nNA gamma[25]  -47.166  20.417  -86.549  -61.568  -47.155  -33.479   -6.020 1.001\nNA gamma[26]  -92.375  19.497 -130.380 -105.540  -92.310  -79.043  -54.017 1.002\nNA gamma[27]   20.875  20.031  -18.328    6.625   20.873   34.275   60.661 1.002\nNA gamma[28]   74.464  19.909   36.179   61.091   74.369   87.433  114.480 1.001\nNA gamma[29]   -0.792  19.771  -39.202  -14.402   -0.589   12.677   36.999 1.001\nNA gamma[30]  -75.855  20.350 -116.077  -89.286  -76.179  -62.465  -35.911 1.001\nNA gamma[31]  -58.457  20.104  -97.479  -72.394  -58.390  -44.695  -19.492 1.001\nNA gamma[32]  -53.274  20.080  -91.482  -66.984  -53.201  -40.309  -13.057 1.001\nNA gamma[33]   15.405  20.131  -22.770    1.695   15.048   28.831   55.869 1.001\nNA gamma[34]   59.338  20.334   19.750   45.810   59.517   72.790   99.194 1.001\nNA gamma[35]   56.933  20.301   15.197   43.906   57.388   70.588   94.583 1.001\nNA res[1]      97.432  17.921   62.629   85.359   97.337  109.111  134.580 1.002\nNA res[2]     105.769  17.698   71.182   93.805  105.826  117.408  142.144 1.002\nNA res[3]      32.710  17.534   -1.148   20.909   32.522   44.178   68.460 1.002\nNA res[4]       9.121  17.431  -23.865   -2.637    8.871   20.532   44.239 1.002\nNA res[5]      29.201  17.391   -4.021   17.641   28.835   40.540   64.071 1.002\nNA res[6]      -4.531  17.414  -38.094  -16.092   -4.863    6.832   30.205 1.002\nNA res[7]    -107.185  17.500 -140.651 -118.950 -107.516  -95.741  -72.125 1.002\nNA res[8]     -37.350  17.647  -71.015  -49.217  -37.556  -25.708   -1.746 1.002\nNA res[9]     -67.307  17.855 -101.635  -79.246  -67.326  -55.699  -31.035 1.002\nNA res[10]    -78.614  18.121 -113.985  -90.735  -78.654  -66.764  -42.063 1.002\nNA res[11]     49.884  18.193   13.728   37.834   50.102   61.789   84.845 1.001\nNA res[12]     11.595  17.930  -23.183   -0.446   11.692   23.269   46.321 1.001\nNA res[13]     61.820  17.726   27.566   49.860   61.924   73.375   96.299 1.001\nNA res[14]     -3.458  17.582  -37.401  -15.192   -3.465    8.046   30.969 1.001\nNA res[15]    -10.511  17.499  -44.587  -21.767  -10.459    1.089   24.112 1.001\nNA res[16]     -2.243  17.479  -36.572  -13.604   -2.049    9.337   32.255 1.001\nNA res[17]    -50.520  17.522  -85.179  -61.867  -50.373  -38.782  -16.278 1.001\nNA res[18]    -62.585  17.627  -97.953  -73.969  -62.501  -50.564  -28.305 1.001\nNA res[19]    -42.079  17.792  -77.762  -53.482  -42.166  -30.142   -7.621 1.001\nNA res[20]      9.165  18.018  -26.988   -2.455    9.147   21.271   43.778 1.001\nNA res[21]    -77.926  17.501 -113.047  -89.469  -78.006  -66.052  -43.688 1.002\nNA res[22]    -73.189  17.257 -108.057  -84.536  -73.411  -61.442  -39.850 1.002\nNA res[23]    -14.228  17.075  -48.379  -25.553  -14.413   -2.773   18.665 1.002\nNA res[24]    -31.958  16.955  -66.594  -43.263  -32.141  -20.655    0.554 1.002\nNA res[25]     -7.975  16.900  -42.439  -19.028   -8.192    3.408   24.762 1.002\nNA res[26]     24.320  16.909   -9.262   13.308   24.202   35.576   57.313 1.002\nNA res[27]     38.799  16.983    5.216   27.563   38.571   50.247   72.421 1.002\nNA res[28]     69.516  17.120   35.582   58.330   69.291   80.872  103.259 1.002\nNA res[29]     55.153  17.321   20.895   43.887   54.848   66.615   89.335 1.002\nNA res[30]     28.976  17.581   -5.726   17.342   28.565   40.577   63.635 1.002\nNA res[31]   -121.587  17.872 -156.152 -133.567 -121.526 -109.634  -87.382 1.001\nNA res[32]   -100.256  17.618 -133.884 -111.966 -100.063  -88.659  -66.409 1.001\nNA res[33]    -57.168  17.423  -90.542  -68.902  -57.110  -45.627  -22.964 1.001\nNA res[34]    -17.580  17.290  -51.232  -29.462  -17.603   -6.075   16.449 1.001\nNA res[35]    -40.581  17.219  -74.430  -52.498  -40.599  -29.045   -6.738 1.001\nNA res[36]     57.619  17.212   23.794   45.812   57.457   69.114   91.210 1.001\nNA res[37]     61.388  17.269   27.008   49.782   61.277   72.884   95.367 1.004\nNA res[38]     56.259  17.389   21.473   44.753   56.294   67.702   90.416 1.001\nNA res[39]    109.316  17.570   73.992   97.577  109.390  120.643  143.528 1.001\nNA res[40]     52.088  17.811   16.206   39.944   52.135   63.655   86.709 1.001\nNA res[41]    -38.914  17.643  -73.656  -50.658  -38.634  -26.860   -5.046 1.001\nNA res[42]     77.326  17.420   42.616   65.720   77.575   89.215  111.120 1.001\nNA res[43]     50.865  17.258   16.915   39.497   51.222   62.617   84.564 1.001\nNA res[44]    110.679  17.159   76.469   99.239  111.186  122.203  144.204 1.001\nNA res[45]      4.790  17.122  -29.711   -6.616    5.343   16.301   38.330 1.001\nNA res[46]    -17.661  17.150  -52.491  -29.227  -17.113   -5.957   16.017 1.001\nNA res[47]     -7.311  17.242  -42.494  -18.933   -6.757    4.462   26.166 1.001\nNA res[48]     -3.089  17.396  -38.865  -14.616   -2.473    8.846   31.113 1.001\nNA res[49]    -65.133  17.611 -101.697  -76.777  -64.519  -53.174  -30.398 1.001\nNA res[50]    -63.661  17.886 -100.707  -75.693  -63.048  -51.501  -28.264 1.001\nNA res[51]    -31.518  17.144  -65.465  -43.209  -31.355  -19.712    1.672 1.001\nNA res[52]     14.815  16.893  -18.897    3.470   14.776   26.305   47.890 1.001\nNA res[53]     70.347  16.704   36.921   58.939   70.092   81.766  102.877 1.001\nNA res[54]    -50.853  16.579  -83.834  -62.055  -51.073  -39.619  -18.942 1.001\nNA res[55]     25.083  16.520   -8.057   14.027   24.935   36.357   56.754 1.001\nNA res[56]    -38.143  16.527  -71.222  -49.423  -38.357  -26.836   -6.450 1.001\nNA res[57]     -4.070  16.600  -36.763  -15.317   -4.236    7.197   27.724 1.001\nNA res[58]     33.306  16.738    0.513   22.191   33.191   44.522   65.598 1.001\nNA res[59]     20.080  16.940  -12.937    8.828   19.836   31.550   52.764 1.001\nNA res[60]    -12.900  17.204  -46.161  -24.276  -13.088   -1.463   20.472 1.001\nNA res[61]    -17.368  17.885  -51.348  -29.241  -17.425   -5.291   18.072 1.001\nNA res[62]      7.369  17.652  -26.193   -4.253    7.234   19.236   42.448 1.001\nNA res[63]     49.245  17.479   15.639   37.647   49.046   60.880   83.698 1.001\nNA res[64]    -64.174  17.368  -97.742  -75.711  -64.357  -52.534  -30.120 1.001\nNA res[65]   -134.249  17.320 -167.266 -146.107 -134.437 -122.586 -100.604 1.001\nNA res[66]    -37.107  17.334  -70.680  -48.991  -37.270  -25.543   -3.298 1.001\nNA res[67]     21.279  17.412  -12.492    9.532   21.267   33.096   55.105 1.001\nNA res[68]     37.284  17.552    3.245   25.564   37.265   49.184   71.420 1.001\nNA res[69]     63.944  17.753   29.095   52.068   63.864   75.938   98.781 1.001\nNA res[70]     95.234  18.013   60.203   83.123   95.270  107.455  130.632 1.001\nNA res[71]    -48.396  17.620  -83.102  -60.471  -48.230  -36.426  -13.894 1.001\nNA res[72]     16.818  17.403  -17.131    4.932   16.936   28.621   50.566 1.001\nNA res[73]    -10.912  17.247  -44.871  -22.609  -10.795    0.606   22.307 1.001\nNA res[74]      2.547  17.154  -31.372   -8.982    2.664   14.012   35.509 1.001\nNA res[75]    -13.113  17.125  -47.212  -24.603  -12.951   -1.578   20.054 1.001\nNA res[76]     32.577  17.159   -1.807   20.989   32.596   44.219   65.805 1.001\nNA res[77]      7.970  17.257  -27.008   -3.791    7.911   19.682   41.186 1.001\nNA res[78]     31.495  17.418   -3.602   19.914   31.400   43.395   65.506 1.001\nNA res[79]      4.718  17.639  -30.550   -6.860    4.806   16.702   39.109 1.001\nNA res[80]    -51.946  17.919  -87.119  -63.726  -51.922  -39.897  -17.025 1.001\nNA res[81]    -63.210  17.647  -97.791  -75.690  -63.212  -51.190  -29.440 1.002\nNA res[82]    -31.067  17.390  -65.386  -43.236  -31.079  -19.273    2.374 1.002\nNA res[83]     43.678  17.193    9.558   31.772   43.540   55.495   76.981 1.002\nNA res[84]     20.380  17.058  -13.497    8.805   20.360   32.176   53.934 1.002\nNA res[85]     54.597  16.986   21.096   43.276   54.749   66.249   87.971 1.002\nNA res[86]    124.353  16.979   90.938  113.063  124.583  135.902  157.927 1.002\nNA res[87]     67.176  17.037   33.903   55.809   67.439   78.691  101.104 1.002\nNA res[88]    -30.778  17.158  -64.046  -42.370  -30.407  -19.292    2.913 1.002\nNA res[89]    -55.098  17.342  -88.751  -66.851  -54.716  -43.392  -21.208 1.002\nNA res[90]    -73.427  17.586 -107.590  -85.203  -73.116  -61.615  -38.693 1.002\nNA res[91]    -39.879  17.759  -75.389  -51.772  -39.985  -28.068   -4.896 1.001\nNA res[92]     40.803  17.486    5.829   29.249   40.623   52.320   75.462 1.001\nNA res[93]      3.288  17.272  -30.880   -8.346    3.017   14.813   37.091 1.001\nNA res[94]      8.096  17.120  -26.071   -3.487    7.840   19.496   41.421 1.001\nNA res[95]    -18.230  17.031  -51.683  -29.609  -18.372   -6.678   14.819 1.001\nNA res[96]    -27.022  17.006  -60.187  -38.426  -27.134  -15.863    6.296 1.001\nNA res[97]    -19.513  17.045  -52.825  -30.880  -19.632   -8.423   13.668 1.001\nNA res[98]     37.064  17.149    3.831   25.570   37.124   48.265   70.659 1.001\nNA res[99]     21.843  17.315  -12.203   10.180   21.878   33.093   55.675 1.001\nNA res[100]    39.105  17.542    5.025   27.487   38.948   50.788   73.056 1.001\nNA res[101]    29.449  17.991   -5.205   17.155   29.781   41.807   63.442 1.001\nNA res[102]    49.685  17.766   15.218   37.673   50.072   61.814   82.819 1.001\nNA res[103]    -8.723  17.601  -43.029  -20.739   -8.303    3.244   24.590 1.001\nNA res[104]    54.477  17.498   20.833   42.653   54.759   66.407   87.773 1.001\nNA res[105]     4.508  17.456  -29.390   -7.159    4.817   16.421   37.676 1.001\nNA res[106]   -21.847  17.477  -55.720  -33.395  -21.535   -9.860   11.772 1.001\nNA res[107]    32.423  17.561   -2.084   20.791   32.811   44.448   66.578 1.001\nNA res[108]    70.203  17.706   35.619   58.406   70.311   82.156  104.571 1.001\nNA res[109]   -18.915  17.912  -54.616  -30.740  -18.911   -6.868   15.481 1.001\nNA res[110]   -79.501  18.176 -115.834  -91.501  -79.578  -67.080  -44.590 1.001\nNA res[111]   -35.407  17.785  -70.692  -46.976  -35.584  -23.160   -0.930 1.001\nNA res[112]   -16.834  17.533  -51.394  -28.356  -17.011   -4.647   17.012 1.001\nNA res[113]    -2.964  17.342  -37.311  -14.283   -3.253    9.068   30.829 1.001\nNA res[114]    17.958  17.211  -16.244    6.888   17.697   29.740   51.566 1.001\nNA res[115]    43.960  17.144    9.743   32.909   43.779   55.749   77.773 1.002\nNA res[116]     6.922  17.141  -27.302   -4.089    6.746   18.522   41.245 1.002\nNA res[117]   -42.437  17.202  -76.634  -53.389  -42.592  -31.097   -7.471 1.002\nNA res[118]    18.962  17.325  -15.305    7.723   18.882   30.432   53.655 1.003\nNA res[119]    54.157  17.511   19.505   42.669   54.007   65.841   88.924 1.003\nNA res[120]   -30.836  17.757  -66.091  -42.472  -31.031  -18.943    4.597 1.003\nNA res[121]    29.694  17.913   -5.583   17.603   29.789   41.163   65.302 1.001\nNA res[122]    -8.428  17.668  -42.894  -20.446   -8.260    2.790   26.838 1.001\nNA res[123]   -97.805  17.483 -132.040 -109.658  -97.722  -86.688  -63.003 1.001\nNA res[124]   -58.400  17.360  -92.551  -70.268  -58.013  -47.184  -24.118 1.001\nNA res[125]   -38.480  17.298  -72.858  -50.159  -38.072  -27.272   -4.162 1.001\nNA res[126]   -23.590  17.301  -58.378  -35.298  -23.103  -12.266   10.906 1.001\nNA res[127]     3.860  17.366  -31.068   -7.655    4.299   15.220   38.206 1.001\nNA res[128]    58.998  17.494   23.758   47.395   59.472   70.391   93.062 1.001\nNA res[129]    69.127  17.683   33.354   57.321   69.673   80.505  103.147 1.001\nNA res[130]    35.991  17.931   -0.747   24.279   36.423   47.825   70.397 1.001\nNA res[131]   -18.707  17.753  -53.599  -30.549  -18.607   -6.714   16.819 1.003\nNA res[132]   -14.747  17.516  -48.954  -26.537  -14.555   -2.845   20.061 1.003\nNA res[133]    10.374  17.340  -23.749   -1.370   10.597   22.133   44.664 1.003\nNA res[134]   -37.152  17.225  -70.611  -48.839  -37.026  -25.712   -3.136 1.002\nNA res[135]   -32.541  17.174  -65.860  -44.108  -32.536  -21.077    1.265 1.002\nNA res[136]    28.588  17.186   -4.848   17.066   28.625   39.992   62.044 1.002\nNA res[137]    23.576  17.262  -10.078   11.987   23.698   35.092   57.117 1.002\nNA res[138]   -10.078  17.401  -43.670  -21.709   -9.920    1.610   23.933 1.001\nNA res[139]   -16.016  17.601  -49.961  -27.799  -15.770   -4.358   18.774 1.001\nNA res[140]    48.626  17.861   14.303   36.410   48.905   60.610   83.764 1.001\nNA res[141]     4.593  17.780  -29.625   -7.191    4.424   16.505   38.788 1.001\nNA res[142]    57.463  17.544   23.749   45.968   57.368   69.300   91.358 1.001\nNA res[143]    44.743  17.368   11.543   33.225   44.780   56.414   78.495 1.001\nNA res[144]    47.987  17.253   14.659   36.564   48.140   59.625   81.776 1.001\nNA res[145]     2.009  17.202  -31.053   -9.596    2.094   13.679   35.963 1.001\nNA res[146]    23.279  17.214   -9.893   11.605   23.253   34.820   57.175 1.001\nNA res[147]   -15.427  17.290  -49.433  -27.118  -15.449   -4.007   19.070 1.001\nNA res[148]   -92.242  17.428 -126.419 -104.123  -92.223  -80.822  -57.405 1.001\nNA res[149]   -76.403  17.628 -110.319  -88.293  -76.440  -64.957  -41.280 1.001\nNA res[150]    27.022  17.887   -7.788   14.991   26.934   38.901   62.553 1.001\nNA res[151]    56.524  17.435   23.497   44.650   56.376   68.008   91.937 1.002\nNA res[152]    94.888  17.193   62.473   83.265   94.748  106.107  129.551 1.001\nNA res[153]    85.122  17.012   53.069   73.707   84.983   96.084  119.032 1.001\nNA res[154]    28.800  16.893   -3.079   17.754   28.668   39.844   62.380 1.001\nNA res[155]     3.921  16.839  -28.102   -7.248    3.904   15.053   37.301 1.001\nNA res[156]   -19.671  16.850  -51.711  -30.814  -19.649   -8.511   13.270 1.001\nNA res[157]   -48.454  16.926  -81.115  -59.563  -48.405  -37.301  -15.678 1.001\nNA res[158]   -12.976  17.067  -46.088  -24.257  -12.925   -1.706   20.012 1.001\nNA res[159]   -79.807  17.269 -113.216  -91.035  -79.682  -68.569  -46.865 1.001\nNA res[160]   -30.224  17.532  -64.395  -41.789  -29.847  -18.816    2.893 1.001\nNA res[161]   -10.710  17.984  -46.042  -22.979  -10.276    1.572   24.298 1.002\nNA res[162]   -82.452  17.740 -118.119  -94.601  -82.059  -70.317  -47.916 1.002\nNA res[163]   -48.077  17.555  -83.627  -59.974  -47.795  -35.994  -13.951 1.002\nNA res[164]    68.821  17.431   33.752   56.976   68.995   80.691  102.233 1.005\nNA res[165]    12.830  17.369  -22.123    1.019   12.897   24.719   45.628 1.002\nNA res[166]    38.006  17.370    3.280   26.085   37.983   49.937   71.003 1.002\nNA res[167]   -23.347  17.435  -58.240  -35.377  -23.237  -11.414    9.865 1.002\nNA res[168]    22.078  17.561  -13.256    9.887   22.185   34.157   55.218 1.001\nNA res[169]    15.237  17.749  -20.525    3.105   15.325   27.296   48.552 1.001\nNA res[170]    79.730  17.996   43.542   67.295   79.674   91.820  113.635 1.002\nNA res[171]    63.717  17.877   28.767   51.218   63.625   75.992   98.346 1.002\nNA res[172]    50.693  17.611   16.356   38.398   50.624   62.696   84.475 1.001\nNA res[173]    16.355  17.405  -17.386    4.388   16.412   28.087   49.899 1.001\nNA res[174]     5.229  17.259  -28.246   -6.690    5.281   16.848   38.937 1.001\nNA res[175]   -25.424  17.176  -58.537  -37.419  -25.426  -13.955    7.970 1.001\nNA res[176]   -60.299  17.157  -93.240  -72.218  -60.299  -48.754  -26.651 1.001\nNA res[177]   -17.707  17.202  -50.497  -29.605  -17.777   -6.222   15.477 1.001\nNA res[178]   -67.087  17.310  -99.859  -79.145  -67.067  -55.435  -33.398 1.001\nNA res[179]    21.851  17.480  -10.981    9.770   21.871   33.713   56.070 1.001\nNA res[180]   -40.647  17.711  -73.957  -52.771  -40.513  -28.613   -5.841 1.001\nNA res[181]   -19.458  17.542  -54.333  -31.085  -19.422   -7.722   15.640 1.001\nNA res[182]    13.382  17.297  -20.657    2.031   13.446   24.888   47.985 1.001\nNA res[183]    42.203  17.113    8.488   30.978   42.207   53.606   75.709 1.001\nNA res[184]    20.699  16.991  -12.648    9.734   20.572   32.070   53.897 1.001\nNA res[185]    22.419  16.933  -10.796   11.505   22.206   33.885   55.335 1.001\nNA res[186]    67.746  16.941   34.410   57.004   67.445   79.392  100.423 1.001\nNA res[187]   -17.017  17.012  -50.239  -27.920  -17.298   -5.427   15.763 1.001\nNA res[188]   -51.228  17.148  -84.441  -62.328  -51.548  -39.688  -18.069 1.001\nNA res[189]   -23.628  17.345  -57.805  -34.767  -23.760  -11.997   10.684 1.001\nNA res[190]   -39.945  17.603  -74.634  -51.215  -40.154  -28.208   -5.214 1.001\nNA res[191]    52.530  17.512   18.626   41.139   52.206   64.088   87.150 1.006\nNA res[192]    79.593  17.246   46.248   68.448   79.388   91.100  113.049 1.017\nNA res[193]    71.051  17.040   37.868   59.980   70.793   82.571  104.153 1.021\nNA res[194]   -14.917  16.897  -48.034  -25.902  -15.296   -3.462   18.041 1.005\nNA res[195]    -7.135  16.817  -39.943  -18.047   -7.544    4.215   25.897 1.005\nNA res[196]   -18.174  16.803  -51.133  -29.182  -18.623   -6.895   14.827 1.005\nNA res[197]   -16.439  16.854  -48.969  -27.463  -16.762   -5.147   16.557 1.004\nNA res[198]   -69.150  16.970 -102.173  -80.328  -69.361  -57.572  -35.980 1.004\nNA res[199]   -27.445  17.149  -60.854  -38.683  -27.690  -15.800    6.299 1.003\nNA res[200]   -71.100  17.389 -104.684  -82.727  -71.394  -59.189  -37.691 1.003\nNA res[201]     1.426  17.309  -34.407   -9.571    1.332   12.962   35.011 1.001\nNA res[202]    36.131  17.046    0.771   25.266   36.227   47.347   69.464 1.001\nNA res[203]     5.510  16.844  -28.967   -5.374    5.685   16.535   38.753 1.001\nNA res[204]    49.200  16.705   15.453   38.505   49.435   60.187   82.079 1.001\nNA res[205]   -10.960  16.631  -44.546  -21.713  -10.492   -0.147   21.703 1.001\nNA res[206]  -133.889  16.623 -167.539 -144.693 -133.301 -123.036 -101.431 1.001\nNA res[207]   -68.634  16.681 -102.616  -79.683  -67.975  -57.769  -36.182 1.001\nNA res[208]     2.212  16.804  -31.897   -8.838    2.830   13.283   34.752 1.001\nNA res[209]     2.431  16.991  -31.627   -8.767    3.000   13.550   35.269 1.001\nNA res[210]    41.968  17.240    6.811   30.539   42.440   53.467   75.061 1.001\nNA res[211]   -67.622  17.508 -102.075  -79.236  -67.541  -56.136  -32.937 1.001\nNA res[212]   -57.530  17.266  -91.092  -68.962  -57.500  -46.025  -22.873 1.001\nNA res[213]  -140.313  17.084 -173.718 -151.700 -140.155 -128.926 -105.859 1.002\nNA res[214]   -50.542  16.964  -83.840  -61.841  -50.386  -39.139  -16.241 1.002\nNA res[215]    55.074  16.909   22.122   43.726   55.352   66.374   89.195 1.002\nNA res[216]   100.133  16.919   67.118   88.860  100.592  111.422  133.899 1.002\nNA res[217]    80.975  16.993   48.141   69.704   81.338   92.108  115.080 1.002\nNA res[218]    65.212  17.132   32.285   53.430   65.470   76.536  100.152 1.001\nNA res[219]   -21.674  17.332  -55.118  -33.640  -21.456  -10.022   13.856 1.002\nNA res[220]    24.003  17.593   -9.530   11.811   24.101   35.907   59.852 1.002\nNA res[221]    60.185  17.783   25.820   48.457   59.813   72.436   96.171 1.001\nNA res[222]    26.825  17.539   -6.907   15.267   26.510   38.780   62.320 1.001\nNA res[223]   -37.765  17.355  -71.270  -49.250  -38.173  -25.946   -2.142 1.001\nNA res[224]   -33.962  17.232  -67.496  -45.390  -34.499  -22.046    1.130 1.001\nNA res[225]   -58.522  17.173  -91.829  -70.044  -59.083  -46.732  -23.657 1.001\nNA res[226]   -55.706  17.177  -88.390  -67.397  -56.263  -44.042  -21.182 1.001\nNA res[227]   -94.620  17.245 -127.457 -106.411  -95.119  -82.969  -60.749 1.001\nNA res[228]    19.371  17.375  -13.948    7.607   19.044   31.010   53.459 1.001\nNA res[229]    25.246  17.568   -8.529   13.626   24.828   37.075   59.410 1.001\nNA res[230]    84.355  17.820   50.467   72.405   83.982   96.570  118.968 1.001\nNA res[231]   -31.497  18.030  -66.696  -43.447  -31.785  -19.738    3.853 1.001\nNA res[232]   -33.555  17.750  -68.255  -45.064  -33.695  -22.067    1.221 1.001\nNA res[233]   -46.454  17.529  -80.525  -58.085  -46.585  -35.121  -11.905 1.001\nNA res[234]   -84.283  17.368 -118.128  -95.729  -84.398  -73.094  -50.480 1.001\nNA res[235]    23.792  17.270   -9.429   12.004   23.713   34.911   57.625 1.001\nNA res[236]   -37.979  17.234  -71.525  -49.693  -38.039  -26.826   -3.726 1.001\nNA res[237]    -0.851  17.262  -34.217  -12.666   -0.823   10.179   33.380 1.001\nNA res[238]    55.116  17.354   21.132   43.276   55.278   66.313   89.932 1.001\nNA res[239]    66.340  17.507   31.871   54.397   66.435   77.816  102.154 1.001\nNA res[240]    22.509  17.721  -12.331   10.444   22.467   34.188   58.690 1.001\nNA res[241]    48.779  17.985   13.144   36.603   49.101   61.358   82.744 1.001\nNA res[242]    54.607  17.728   19.723   42.634   54.853   67.014   88.097 1.001\nNA res[243]    -2.573  17.531  -36.920  -14.310   -2.250    9.522   31.153 1.001\nNA res[244]   -64.682  17.394  -98.704  -76.398  -64.353  -52.693  -31.136 1.001\nNA res[245]    59.232  17.320   25.143   47.518   59.597   70.983   92.873 1.001\nNA res[246]    19.963  17.309  -14.316    8.145   20.310   31.886   53.371 1.001\nNA res[247]   -70.443  17.361 -105.373  -82.320  -70.049  -58.397  -36.717 1.001\nNA res[248]   -23.463  17.476  -58.679  -35.360  -23.248  -11.587   10.598 1.001\nNA res[249]     2.831  17.652  -32.482   -8.951    3.087   14.786   37.287 1.001\nNA res[250]   -59.475  17.888  -95.014  -71.176  -59.315  -47.543  -24.802 1.001\nNA res[251]  -118.664  17.326 -151.645 -130.208 -118.894 -107.309  -83.446 1.002\nNA res[252]   -91.020  17.066 -123.512 -102.427  -91.199  -79.836  -57.008 1.001\nNA res[253]    -6.258  16.868  -38.555  -17.830   -6.464    4.709   27.751 1.001\nNA res[254]    36.251  16.734    4.248   24.639   35.981   47.226   70.217 1.001\nNA res[255]    13.667  16.664  -18.023    2.118   13.400   24.523   47.428 1.001\nNA res[256]   -21.601  16.659  -53.042  -33.062  -21.878  -10.646   12.267 1.001\nNA res[257]     5.310  16.721  -26.340   -6.021    5.033   16.411   39.015 1.001\nNA res[258]    21.015  16.847  -10.852    9.662   20.790   32.193   55.145 1.001\nNA res[259]    57.059  17.037   24.518   45.611   56.831   68.241   92.442 1.002\nNA res[260]    34.481  17.288    1.254   23.094   34.411   45.908   70.086 1.001\nNA res[261]    50.413  17.715   15.780   38.279   50.907   62.488   84.294 1.002\nNA res[262]     1.013  17.462  -32.559  -11.003    1.463   13.068   33.840 1.002\nNA res[263]   -13.632  17.268  -46.927  -25.466  -13.251   -1.826   18.917 1.002\nNA res[264]    28.083  17.137   -5.178   16.694   28.340   39.623   60.566 1.003\nNA res[265]    73.774  17.069   40.522   62.477   73.840   85.174  106.109 1.003\nNA res[266]   -36.234  17.065  -69.304  -47.505  -35.963  -24.747   -3.793 1.003\nNA res[267]   -22.093  17.125  -55.763  -33.330  -21.828  -10.616   10.070 1.003\nNA res[268]    14.160  17.249  -19.827    3.041   14.507   25.776   46.182 1.003\nNA res[269]   -59.954  17.435  -93.778  -71.235  -59.514  -48.101  -27.315 1.003\nNA res[270]   -22.812  17.681  -57.121  -34.293  -22.324  -10.851   10.918 1.003\nNA res[271]  -125.907  17.458 -160.933 -137.188 -125.802 -114.461  -92.053 1.001\nNA res[272]   -62.314  17.229  -97.496  -73.416  -62.473  -50.835  -29.085 1.001\nNA res[273]   -35.666  17.061  -70.145  -46.545  -35.800  -24.427   -3.232 1.001\nNA res[274]    -2.957  16.956  -37.392  -13.883   -2.813    8.075   29.213 1.001\nNA res[275]    -9.344  16.916  -43.467  -20.207   -9.343    1.681   22.885 1.001\nNA res[276]    22.554  16.940  -11.628   11.868   22.593   33.645   54.629 1.001\nNA res[277]    73.779  17.029   39.473   63.039   73.780   84.850  106.184 1.001\nNA res[278]   143.908  17.181  109.484  132.820  143.915  155.171  176.723 1.001\nNA res[279]   100.141  17.395   65.573   88.965  100.141  111.576  133.506 1.001\nNA res[280]   -46.044  17.669  -80.667  -57.400  -45.955  -34.590  -12.407 1.001\nNA res[281]    -5.699  17.595  -39.468  -17.381   -5.770    5.983   28.301 1.001\nNA res[282]     0.419  17.343  -33.174  -10.923    0.288   11.974   34.262 1.001\nNA res[283]   -12.869  17.152  -46.211  -24.317  -13.049   -1.401   20.398 1.001\nNA res[284]    12.545  17.023  -20.331    1.056   12.458   24.113   45.686 1.001\nNA res[285]    54.857  16.958   22.320   43.310   54.741   66.049   88.406 1.001\nNA res[286]    12.136  16.958  -20.207    0.644   11.947   23.530   45.348 1.001\nNA res[287]   -10.984  17.022  -43.192  -22.616  -10.977    0.294   22.275 1.001\nNA res[288]     4.979  17.150  -27.724   -6.762    5.048   16.454   38.007 1.001\nNA res[289]   -29.791  17.340  -62.952  -41.651  -29.779  -18.299    3.958 1.001\nNA res[290]   -25.671  17.591  -59.797  -37.708  -25.457  -14.066    8.558 1.001\nNA res[291]    28.550  17.963   -7.697   16.728   28.845   40.812   62.212 1.001\nNA res[292]    -9.089  17.712  -45.024  -20.553   -8.933    2.924   24.333 1.001\nNA res[293]   -49.732  17.521  -85.124  -61.052  -49.636  -38.060  -16.508 1.001\nNA res[294]   -58.677  17.390  -93.259  -69.572  -58.567  -47.134  -25.799 1.001\nNA res[295]   -57.955  17.322  -92.707  -68.933  -57.842  -46.344  -25.049 1.001\nNA res[296]    -3.734  17.317  -38.668  -14.858   -3.653    7.873   29.033 1.001\nNA res[297]    69.494  17.375   34.053   58.293   69.643   81.139  102.719 1.001\nNA res[298]    42.465  17.496    6.356   31.206   42.578   54.056   75.774 1.001\nNA res[299]     7.231  17.678  -29.387   -4.288    7.395   19.167   40.819 1.001\nNA res[300]   -23.337  17.919  -60.558  -34.994  -23.228  -11.302   10.882 1.001\nNA res[301]   -51.903  17.982  -88.248  -63.876  -52.309  -39.714  -17.303 1.001\nNA res[302]   -37.852  17.747  -73.574  -49.599  -38.337  -25.637   -3.466 1.001\nNA res[303]     9.383  17.571  -25.880   -2.168    9.060   21.337   43.272 1.001\nNA res[304]     6.786  17.457  -27.982   -4.692    6.466   18.794   40.534 1.001\nNA res[305]   -83.287  17.404 -117.576  -94.688  -83.511  -71.384  -50.132 1.001\nNA res[306]   -56.131  17.415  -90.605  -67.604  -56.382  -44.092  -22.768 1.001\nNA res[307]    29.387  17.488   -5.255   17.630   29.168   41.485   62.770 1.001\nNA res[308]    97.884  17.624   63.110   86.054   97.739  110.020  131.499 1.001\nNA res[309]    53.516  17.820   18.505   41.582   53.468   65.807   87.676 1.001\nNA res[310]   -20.057  18.074  -55.492  -32.169  -20.032   -7.574   14.442 1.001\nNA res[311]   101.300  17.549   67.220   89.168  101.137  113.302  135.233 1.001\nNA res[312]    83.175  17.312   49.506   71.360   82.808   95.098  116.774 1.001\nNA res[313]    71.785  17.135   37.980   60.064   71.677   83.600  105.227 1.001\nNA res[314]    88.437  17.022   55.339   76.901   88.506  100.014  122.116 1.001\nNA res[315]    -0.110  16.972  -33.336  -11.341    0.071   11.200   33.192 1.001\nNA res[316]   -26.794  16.987  -60.411  -38.159  -26.771  -15.599    6.418 1.001\nNA res[317]   -88.891  17.067 -122.585 -100.389  -88.824  -77.593  -55.404 1.001\nNA res[318]   -96.339  17.209 -130.866 -107.673  -96.328  -84.910  -62.716 1.001\nNA res[319]   -61.837  17.414  -96.679  -73.288  -61.840  -50.019  -28.051 1.001\nNA res[320]  -108.552  17.679 -143.474 -120.010 -108.694  -96.508  -74.344 1.001\nNA res[321]   -74.410  17.625 -109.575  -85.816  -74.485  -62.663  -39.753 1.001\nNA res[322]   -41.400  17.380  -75.871  -52.760  -41.519  -29.797   -7.178 1.001\nNA res[323]   -74.807  17.196 -108.901  -86.261  -74.760  -63.308  -41.163 1.001\nNA res[324]   -45.144  17.074  -79.209  -56.502  -45.026  -33.729  -12.103 1.001\nNA res[325]     4.537  17.016  -29.468   -6.710    4.542   15.856   37.618 1.001\nNA res[326]    59.794  17.022   26.116   48.375   59.758   71.282   93.240 1.001\nNA res[327]    40.165  17.092    6.814   28.767   40.355   51.655   73.479 1.001\nNA res[328]    30.285  17.226   -3.464   18.692   30.679   41.872   63.673 1.001\nNA res[329]    22.589  17.422  -10.860   10.756   23.053   34.110   56.454 1.001\nNA res[330]    92.703  17.678   58.616   80.766   93.284  104.245  126.965 1.001\nNA res[331]    95.283  17.977   59.650   83.167   95.192  107.412  130.221 1.001\nNA res[332]   112.719  17.720   77.424  100.669  112.724  124.693  147.222 1.001\nNA res[333]    70.443  17.522   35.436   58.422   70.438   82.385  104.268 1.001\nNA res[334]    69.514  17.384   34.563   57.587   69.353   81.410  102.556 1.001\nNA res[335]    70.134  17.309   35.643   58.219   69.855   81.817  103.521 1.001\nNA res[336]    -1.755  17.297  -35.726  -13.635   -1.976   10.018   31.941 1.001\nNA res[337]   -93.736  17.349 -127.604 -105.548  -93.768  -81.835  -60.000 1.001\nNA res[338]   -48.951  17.463  -82.779  -60.869  -49.029  -36.839  -14.994 1.001\nNA res[339]  -121.819  17.638 -156.071 -133.910 -122.015 -109.591  -87.286 1.001\nNA res[340]  -103.701  17.874 -138.279 -115.672 -103.713  -91.305  -68.978 1.001\nNA res[341]   -69.317  18.059 -103.485  -81.639  -69.415  -57.406  -33.457 1.001\nNA res[342]   -32.346  17.788  -65.785  -44.511  -32.362  -20.467    2.839 1.001\nNA res[343]   -21.629  17.575  -54.850  -33.597  -21.808   -9.882   13.449 1.001\nNA res[344]   -59.307  17.422  -92.307  -71.080  -59.415  -47.483  -24.425 1.001\nNA res[345]    -3.627  17.331  -36.621  -15.370   -3.984    8.092   31.060 1.001\nNA res[346]    78.394  17.304   46.202   66.537   78.021   90.261  112.620 1.001\nNA res[347]   100.999  17.339   68.667   89.254  100.770  113.032  135.290 1.001\nNA res[348]   -22.296  17.438  -55.281  -34.095  -22.507  -10.328   12.264 1.001\nNA res[349]    46.092  17.598   12.847   34.241   46.001   58.157   80.422 1.001\nNA res[350]    27.883  17.819   -5.915   15.648   27.735   40.121   63.077 1.001\nNA sigma       55.917   2.244   51.705   54.351   55.829   57.468   60.369 1.003\nNA sigma.B     64.474   8.406   50.251   58.466   63.695   69.675   83.144 1.006\nNA deviance  3809.753   9.145 3794.047 3803.114 3809.077 3815.608 3829.461 1.003\nNA           n.eff\nNA beta       3000\nNA beta0      3000\nNA gamma[1]   1800\nNA gamma[2]   3000\nNA gamma[3]   2100\nNA gamma[4]   3000\nNA gamma[5]   3000\nNA gamma[6]   3000\nNA gamma[7]   3000\nNA gamma[8]   3000\nNA gamma[9]   2800\nNA gamma[10]  3000\nNA gamma[11]  2100\nNA gamma[12]  3000\nNA gamma[13]  3000\nNA gamma[14]  3000\nNA gamma[15]  2500\nNA gamma[16]  1700\nNA gamma[17]  1700\nNA gamma[18]  1800\nNA gamma[19]  3000\nNA gamma[20]  1500\nNA gamma[21]  3000\nNA gamma[22]  3000\nNA gamma[23]  3000\nNA gamma[24]  3000\nNA gamma[25]  3000\nNA gamma[26]  1700\nNA gamma[27]  1500\nNA gamma[28]  3000\nNA gamma[29]  2300\nNA gamma[30]  3000\nNA gamma[31]  3000\nNA gamma[32]  3000\nNA gamma[33]  2500\nNA gamma[34]  3000\nNA gamma[35]  3000\nNA res[1]     1100\nNA res[2]     1000\nNA res[3]      990\nNA res[4]      940\nNA res[5]      910\nNA res[6]      880\nNA res[7]      860\nNA res[8]      850\nNA res[9]      840\nNA res[10]     840\nNA res[11]    3000\nNA res[12]    3000\nNA res[13]    3000\nNA res[14]    3000\nNA res[15]    3000\nNA res[16]    3000\nNA res[17]    3000\nNA res[18]    3000\nNA res[19]    3000\nNA res[20]    3000\nNA res[21]    1200\nNA res[22]    1200\nNA res[23]    1100\nNA res[24]    1000\nNA res[25]    1000\nNA res[26]     970\nNA res[27]     950\nNA res[28]    1200\nNA res[29]     920\nNA res[30]     920\nNA res[31]    3000\nNA res[32]    3000\nNA res[33]    3000\nNA res[34]    3000\nNA res[35]    3000\nNA res[36]    3000\nNA res[37]    3000\nNA res[38]    3000\nNA res[39]    3000\nNA res[40]    3000\nNA res[41]    3000\nNA res[42]    3000\nNA res[43]    3000\nNA res[44]    3000\nNA res[45]    3000\nNA res[46]    3000\nNA res[47]    3000\nNA res[48]    3000\nNA res[49]    3000\nNA res[50]    3000\nNA res[51]    3000\nNA res[52]    3000\nNA res[53]    3000\nNA res[54]    3000\nNA res[55]    3000\nNA res[56]    3000\nNA res[57]    3000\nNA res[58]    3000\nNA res[59]    3000\nNA res[60]    3000\nNA res[61]    3000\nNA res[62]    3000\nNA res[63]    3000\nNA res[64]    3000\nNA res[65]    3000\nNA res[66]    3000\nNA res[67]    3000\nNA res[68]    3000\nNA res[69]    3000\nNA res[70]    3000\nNA res[71]    3000\nNA res[72]    3000\nNA res[73]    3000\nNA res[74]    3000\nNA res[75]    3000\nNA res[76]    3000\nNA res[77]    3000\nNA res[78]    3000\nNA res[79]    3000\nNA res[80]    3000\nNA res[81]    1100\nNA res[82]    1100\nNA res[83]    1000\nNA res[84]     970\nNA res[85]     930\nNA res[86]     930\nNA res[87]    1100\nNA res[88]     860\nNA res[89]     850\nNA res[90]     850\nNA res[91]    3000\nNA res[92]    3000\nNA res[93]    3000\nNA res[94]    3000\nNA res[95]    3000\nNA res[96]    3000\nNA res[97]    3000\nNA res[98]    3000\nNA res[99]    3000\nNA res[100]   3000\nNA res[101]   2700\nNA res[102]   2700\nNA res[103]   2800\nNA res[104]   3000\nNA res[105]   3000\nNA res[106]   3000\nNA res[107]   3000\nNA res[108]   3000\nNA res[109]   3000\nNA res[110]   3000\nNA res[111]   3000\nNA res[112]   3000\nNA res[113]   3000\nNA res[114]   3000\nNA res[115]   3000\nNA res[116]   3000\nNA res[117]   3000\nNA res[118]   3000\nNA res[119]   3000\nNA res[120]   3000\nNA res[121]   3000\nNA res[122]   3000\nNA res[123]   3000\nNA res[124]   3000\nNA res[125]   3000\nNA res[126]   3000\nNA res[127]   3000\nNA res[128]   3000\nNA res[129]   3000\nNA res[130]   3000\nNA res[131]   3000\nNA res[132]   3000\nNA res[133]   3000\nNA res[134]   3000\nNA res[135]   3000\nNA res[136]   3000\nNA res[137]   3000\nNA res[138]   3000\nNA res[139]   3000\nNA res[140]   3000\nNA res[141]   2700\nNA res[142]   2700\nNA res[143]   2800\nNA res[144]   3000\nNA res[145]   3000\nNA res[146]   3000\nNA res[147]   3000\nNA res[148]   3000\nNA res[149]   3000\nNA res[150]   3000\nNA res[151]   1700\nNA res[152]   2200\nNA res[153]   2300\nNA res[154]   1900\nNA res[155]   1900\nNA res[156]   2000\nNA res[157]   2200\nNA res[158]   2300\nNA res[159]   2500\nNA res[160]   2700\nNA res[161]   1500\nNA res[162]   1500\nNA res[163]   1500\nNA res[164]   1000\nNA res[165]   1600\nNA res[166]   1700\nNA res[167]   1800\nNA res[168]   1900\nNA res[169]   2000\nNA res[170]   1600\nNA res[171]   3000\nNA res[172]   1900\nNA res[173]   1900\nNA res[174]   2000\nNA res[175]   2000\nNA res[176]   2100\nNA res[177]   2300\nNA res[178]   2400\nNA res[179]   2600\nNA res[180]   2800\nNA res[181]   3000\nNA res[182]   3000\nNA res[183]   3000\nNA res[184]   3000\nNA res[185]   3000\nNA res[186]   3000\nNA res[187]   3000\nNA res[188]   3000\nNA res[189]   3000\nNA res[190]   3000\nNA res[191]   1500\nNA res[192]    630\nNA res[193]    570\nNA res[194]   1600\nNA res[195]   1700\nNA res[196]   1800\nNA res[197]   1900\nNA res[198]   2000\nNA res[199]   2100\nNA res[200]   2300\nNA res[201]   3000\nNA res[202]   3000\nNA res[203]   3000\nNA res[204]   3000\nNA res[205]   3000\nNA res[206]   3000\nNA res[207]   3000\nNA res[208]   3000\nNA res[209]   3000\nNA res[210]   3000\nNA res[211]   2000\nNA res[212]   1900\nNA res[213]   1700\nNA res[214]   1600\nNA res[215]   1600\nNA res[216]   1700\nNA res[217]   1700\nNA res[218]   1900\nNA res[219]   1400\nNA res[220]   1400\nNA res[221]   3000\nNA res[222]   3000\nNA res[223]   3000\nNA res[224]   3000\nNA res[225]   3000\nNA res[226]   3000\nNA res[227]   3000\nNA res[228]   3000\nNA res[229]   3000\nNA res[230]   3000\nNA res[231]   3000\nNA res[232]   3000\nNA res[233]   3000\nNA res[234]   3000\nNA res[235]   3000\nNA res[236]   3000\nNA res[237]   3000\nNA res[238]   3000\nNA res[239]   3000\nNA res[240]   3000\nNA res[241]   3000\nNA res[242]   3000\nNA res[243]   3000\nNA res[244]   3000\nNA res[245]   3000\nNA res[246]   3000\nNA res[247]   3000\nNA res[248]   3000\nNA res[249]   3000\nNA res[250]   3000\nNA res[251]   1800\nNA res[252]   1800\nNA res[253]   1900\nNA res[254]   1900\nNA res[255]   2000\nNA res[256]   2100\nNA res[257]   2200\nNA res[258]   2400\nNA res[259]   3000\nNA res[260]   2800\nNA res[261]    930\nNA res[262]    880\nNA res[263]    830\nNA res[264]    790\nNA res[265]    750\nNA res[266]    740\nNA res[267]    720\nNA res[268]    710\nNA res[269]    710\nNA res[270]    710\nNA res[271]   3000\nNA res[272]   3000\nNA res[273]   3000\nNA res[274]   3000\nNA res[275]   3000\nNA res[276]   3000\nNA res[277]   3000\nNA res[278]   3000\nNA res[279]   3000\nNA res[280]   3000\nNA res[281]   2600\nNA res[282]   2600\nNA res[283]   2700\nNA res[284]   2800\nNA res[285]   3000\nNA res[286]   3000\nNA res[287]   3000\nNA res[288]   3000\nNA res[289]   3000\nNA res[290]   3000\nNA res[291]   3000\nNA res[292]   3000\nNA res[293]   3000\nNA res[294]   3000\nNA res[295]   3000\nNA res[296]   3000\nNA res[297]   3000\nNA res[298]   3000\nNA res[299]   3000\nNA res[300]   3000\nNA res[301]   3000\nNA res[302]   3000\nNA res[303]   3000\nNA res[304]   3000\nNA res[305]   3000\nNA res[306]   3000\nNA res[307]   3000\nNA res[308]   3000\nNA res[309]   3000\nNA res[310]   3000\nNA res[311]   3000\nNA res[312]   3000\nNA res[313]   3000\nNA res[314]   3000\nNA res[315]   3000\nNA res[316]   3000\nNA res[317]   3000\nNA res[318]   3000\nNA res[319]   3000\nNA res[320]   3000\nNA res[321]   2700\nNA res[322]   2800\nNA res[323]   2900\nNA res[324]   3000\nNA res[325]   3000\nNA res[326]   3000\nNA res[327]   3000\nNA res[328]   3000\nNA res[329]   3000\nNA res[330]   3000\nNA res[331]   3000\nNA res[332]   3000\nNA res[333]   3000\nNA res[334]   3000\nNA res[335]   3000\nNA res[336]   3000\nNA res[337]   3000\nNA res[338]   3000\nNA res[339]   3000\nNA res[340]   3000\nNA res[341]   3000\nNA res[342]   3000\nNA res[343]   3000\nNA res[344]   3000\nNA res[345]   3000\nNA res[346]   3000\nNA res[347]   3000\nNA res[348]   3000\nNA res[349]   3000\nNA res[350]   3000\nNA sigma       700\nNA sigma.B    1000\nNA deviance    720\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 41.8 and DIC = 3851.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   \n   #Priors\n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel2.txt\")\n\nXmat &lt;- model.matrix(~Time,data.rm)\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=Xmat,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block)),\n         a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\n         )\n)\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.m &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 2521\nNA \nNA Initializing model\n\nprint(data.rm.r2jags.m)\n\nNA Inference for Bugs model at \"matrixModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\nNA beta[1]      0.118   0.993   -1.825   -0.560    0.127    0.772    2.136 1.002\nNA beta[2]      9.067   1.041    6.989    8.384    9.085    9.754   11.096 1.009\nNA gamma[1]   268.496  28.003  214.165  249.963  268.515  287.365  322.199 1.002\nNA gamma[2]   249.357  27.207  196.523  230.541  249.374  268.093  303.009 1.001\nNA gamma[3]   326.337  28.472  271.718  306.769  325.694  345.668  383.423 1.002\nNA gamma[4]   305.015  28.782  249.411  285.199  305.040  324.243  361.356 1.001\nNA gamma[5]   377.709  27.831  324.335  358.940  377.037  396.616  432.150 1.001\nNA gamma[6]   343.430  27.578  288.864  325.843  342.939  361.444  396.819 1.001\nNA gamma[7]   332.166  27.783  278.248  313.377  331.664  351.388  385.777 1.002\nNA gamma[8]   271.616  27.163  217.184  253.588  271.956  289.628  322.963 1.001\nNA gamma[9]   384.589  27.386  332.007  366.136  384.502  402.093  440.209 1.001\nNA gamma[10]  367.756  27.587  314.137  349.123  368.026  386.501  421.707 1.001\nNA gamma[11]  457.715  27.438  405.652  439.504  457.268  476.098  513.423 1.001\nNA gamma[12]  312.867  27.216  262.216  293.798  313.154  331.284  366.377 1.001\nNA gamma[13]  258.804  27.818  201.324  240.475  259.644  276.264  313.330 1.002\nNA gamma[14]  279.449  28.042  224.175  260.014  279.067  298.411  335.404 1.001\nNA gamma[15]  337.363  28.099  284.866  317.715  336.998  356.109  394.348 1.001\nNA gamma[16]  416.383  27.354  363.925  397.826  416.595  434.011  473.215 1.002\nNA gamma[17]  403.080  27.501  349.003  384.921  403.079  421.627  456.974 1.002\nNA gamma[18]  237.418  27.946  183.713  218.161  237.435  256.648  290.420 1.001\nNA gamma[19]  323.886  27.728  270.777  305.233  323.114  342.641  377.533 1.001\nNA gamma[20]  275.462  28.013  220.684  255.987  275.838  294.317  330.957 1.001\nNA gamma[21]  194.059  27.398  141.539  175.756  194.167  211.807  248.181 1.002\nNA gamma[22]  289.821  27.693  236.092  271.210  289.707  307.332  344.403 1.001\nNA gamma[23]  217.663  28.050  164.441  197.887  217.932  235.812  274.184 1.002\nNA gamma[24]  212.748  27.923  159.816  193.937  211.567  231.708  268.892 1.001\nNA gamma[25]  255.843  26.839  204.539  237.181  255.916  274.542  309.265 1.003\nNA gamma[26]  206.396  28.055  152.547  187.160  206.227  225.611  261.170 1.001\nNA gamma[27]  327.559  27.486  273.350  309.633  327.509  345.364  380.375 1.002\nNA gamma[28]  384.714  27.705  330.300  365.884  384.552  403.155  437.847 1.003\nNA gamma[29]  304.783  28.077  251.111  284.961  304.255  323.437  360.045 1.001\nNA gamma[30]  225.251  28.017  168.742  206.532  226.000  243.944  279.897 1.001\nNA gamma[31]  242.036  28.513  184.632  223.534  242.114  260.552  297.438 1.001\nNA gamma[32]  250.059  27.603  197.965  231.432  249.373  268.506  305.275 1.003\nNA gamma[33]  321.386  27.863  266.714  302.647  321.462  340.165  377.552 1.003\nNA gamma[34]  368.704  27.907  314.047  350.229  368.434  387.452  423.293 1.001\nNA gamma[35]  365.968  27.738  311.338  347.444  365.930  385.018  420.415 1.005\nNA res[1]       4.433  27.784  -48.877  -14.717    4.562   22.705   58.869 1.002\nNA res[2]      34.393  27.628  -18.119   15.474   34.478   52.583   88.788 1.002\nNA res[3]     -17.044  27.510  -69.264  -35.707  -16.883    0.972   37.088 1.002\nNA res[4]     -19.010  27.431  -70.907  -37.537  -18.756   -0.634   35.109 1.002\nNA res[5]      22.692  27.391  -29.376    4.107   22.942   41.160   77.308 1.001\nNA res[6]      10.582  27.391  -41.038   -7.968   10.837   28.907   65.778 1.001\nNA res[7]     -70.449  27.431 -121.778  -88.991  -69.960  -52.214  -14.834 1.001\nNA res[8]      21.008  27.509  -30.615    2.408   21.241   39.197   76.881 1.001\nNA res[9]      12.673  27.627  -39.023   -6.003   12.972   31.145   68.777 1.001\nNA res[10]     22.988  27.783  -29.250    4.168   22.937   41.683   79.296 1.001\nNA res[11]    -40.986  26.942  -94.515  -59.717  -41.262  -22.608   11.356 1.001\nNA res[12]    -57.652  26.765 -110.392  -76.201  -57.806  -39.438   -5.452 1.001\nNA res[13]     14.195  26.627  -37.606   -4.203   14.145   32.218   66.504 1.001\nNA res[14]    -29.460  26.529  -81.001  -47.601  -29.683  -11.482   22.755 1.001\nNA res[15]    -14.892  26.472  -66.711  -32.886  -15.051    3.149   37.784 1.001\nNA res[16]     15.000  26.456  -36.965   -3.255   14.703   33.011   68.048 1.001\nNA res[17]    -11.656  26.481  -63.530  -29.872  -12.202    6.352   41.041 1.001\nNA res[18]     -2.098  26.546  -53.836  -20.736   -2.884   15.886   51.209 1.001\nNA res[19]     40.030  26.652  -11.158   21.366   39.102   58.171   93.729 1.001\nNA res[20]    112.896  26.799   61.236   94.306  112.162  131.227  167.261 1.001\nNA res[21]   -173.333  28.266 -230.343 -192.502 -172.496 -153.934 -119.008 1.002\nNA res[22]   -146.973  28.095 -203.369 -166.084 -146.394 -127.937  -93.044 1.002\nNA res[23]    -66.390  27.961 -122.215  -85.409  -65.713  -47.492  -12.298 1.002\nNA res[24]    -62.498  27.865 -117.833  -81.428  -61.625  -43.630   -7.786 1.001\nNA res[25]    -16.892  27.808  -71.818  -35.704  -16.206    2.104   37.863 1.001\nNA res[26]     37.026  27.790  -17.773   18.140   37.505   55.996   91.643 1.001\nNA res[27]     73.127  27.811   18.616   54.532   73.671   91.725  127.823 1.001\nNA res[28]    125.466  27.871   71.518  106.855  125.737  144.036  180.266 1.001\nNA res[29]    132.725  27.969   78.584  113.924  133.004  151.381  187.753 1.001\nNA res[30]    128.171  28.106   73.520  109.247  128.243  147.168  184.155 1.001\nNA res[31]   -215.417  28.519 -271.907 -234.515 -215.477 -196.031 -160.014 1.001\nNA res[32]   -172.464  28.331 -228.398 -191.559 -172.626 -153.223 -117.470 1.001\nNA res[33]   -107.754  28.180 -163.519 -126.542 -107.988  -88.710  -53.225 1.001\nNA res[34]    -46.543  28.067 -102.122  -65.407  -46.791  -27.844    8.278 1.001\nNA res[35]    -47.922  27.992 -103.460  -66.793  -48.157  -29.112    7.029 1.001\nNA res[36]     71.901  27.955   16.630   52.816   71.633   90.451  126.471 1.001\nNA res[37]     97.292  27.958   42.344   78.098   97.282  115.785  151.917 1.001\nNA res[38]    113.786  27.999   59.110   94.749  113.857  132.397  169.334 1.001\nNA res[39]    188.465  28.078  134.278  169.268  188.455  207.026  244.767 1.001\nNA res[40]    152.859  28.196   98.827  133.783  152.725  171.438  209.588 1.001\nNA res[41]   -138.298  27.589 -192.214 -157.018 -137.668 -119.884  -85.511 1.001\nNA res[42]     -0.435  27.414  -54.092  -19.066    0.070   17.947   51.755 1.001\nNA res[43]     -5.275  27.278  -58.296  -23.835   -4.661   12.955   46.693 1.001\nNA res[44]     76.163  27.182   23.390   57.931   76.757   94.384  128.160 1.001\nNA res[45]     -8.105  27.124  -60.470  -26.335   -7.526   10.317   43.968 1.001\nNA res[46]     -8.933  27.107  -61.592  -27.005   -8.421    9.419   43.130 1.001\nNA res[47]     23.039  27.130  -29.532    5.000   23.698   41.485   75.548 1.001\nNA res[48]     48.884  27.192   -3.759   30.945   49.511   67.086  101.297 1.001\nNA res[49]      8.462  27.295  -44.134   -9.630    8.812   26.700   61.215 1.001\nNA res[50]     31.557  27.436  -20.973   13.381   31.746   50.020   84.424 1.001\nNA res[51]   -128.097  27.415 -180.770 -146.327 -127.910 -110.576  -73.568 1.001\nNA res[52]    -60.141  27.254 -112.252  -78.239  -59.732  -42.653   -5.561 1.001\nNA res[53]     17.013  27.132  -35.210   -1.266   17.578   34.265   71.525 1.001\nNA res[54]    -82.564  27.049 -134.784 -100.658  -82.293  -65.282  -27.984 1.002\nNA res[55]     14.994  27.006  -36.578   -3.173   15.282   32.302   69.851 1.002\nNA res[56]    -26.609  27.004  -78.326  -44.704  -26.580   -9.314   28.128 1.002\nNA res[57]     29.085  27.041  -22.987   10.998   29.221   46.243   83.856 1.002\nNA res[58]     88.084  27.118   35.657   69.858   88.230  105.263  142.463 1.004\nNA res[59]     96.481  27.235   43.718   78.058   96.354  113.557  150.554 1.004\nNA res[60]     85.123  27.391   32.058   66.554   85.013  102.661  139.651 1.005\nNA res[61]   -114.949  27.627 -168.618 -134.296 -114.394  -96.450  -60.949 1.001\nNA res[62]    -68.590  27.512 -121.492  -87.723  -68.056  -50.063  -14.649 1.001\nNA res[63]     -5.091  27.435  -57.658  -24.187   -4.767   13.171   48.423 1.001\nNA res[64]    -96.888  27.399 -149.185 -115.932  -96.721  -78.893  -43.557 1.001\nNA res[65]   -145.340  27.401 -197.553 -164.388 -145.424 -127.432  -91.411 1.001\nNA res[66]    -26.577  27.444  -78.399  -45.694  -26.594   -8.455   27.555 1.001\nNA res[67]     53.432  27.525    1.678   34.253   53.095   71.854  107.546 1.001\nNA res[68]     91.059  27.646   39.251   71.830   90.780  109.558  145.538 1.003\nNA res[69]    139.341  27.805   87.022  120.095  138.966  157.961  193.921 1.001\nNA res[70]    192.254  28.001  139.613  173.011  191.889  211.174  247.685 1.001\nNA res[71]   -140.697  26.945 -192.131 -158.462 -140.921 -122.682  -86.498 1.001\nNA res[72]    -53.861  26.803 -104.692  -71.382  -53.867  -36.045   -0.026 1.001\nNA res[73]    -59.969  26.701 -110.888  -77.531  -60.116  -42.419   -5.779 1.001\nNA res[74]    -24.888  26.639  -75.800  -42.339  -25.220   -7.759   28.792 1.001\nNA res[75]    -18.925  26.617  -70.569  -36.419  -19.228   -1.820   34.501 1.001\nNA res[76]     48.388  26.637   -3.281   30.722   48.168   65.796  102.013 1.001\nNA res[77]     45.403  26.697   -6.795   27.656   45.293   62.830   99.324 1.001\nNA res[78]     90.550  26.797   38.062   72.851   90.464  108.076  144.636 1.001\nNA res[79]     85.396  26.937   32.309   67.374   84.992  103.181  139.994 1.001\nNA res[80]     50.353  27.116   -2.874   32.360   50.071   68.275  105.635 1.001\nNA res[81]   -163.315  27.183 -218.512 -180.651 -163.268 -145.149 -111.213 1.001\nNA res[82]   -109.549  27.050 -164.522 -126.836 -109.407  -91.446  -57.185 1.001\nNA res[83]    -13.182  26.957  -68.058  -30.309  -13.185    4.672   38.759 1.001\nNA res[84]    -14.857  26.904  -69.375  -32.207  -15.130    2.965   37.149 1.001\nNA res[85]     40.982  26.891  -12.987   23.736   40.580   58.715   93.178 1.001\nNA res[86]    132.360  26.918   78.198  115.020  131.926  150.358  184.466 1.001\nNA res[87]     96.805  26.986   42.362   79.456   96.215  114.807  149.347 1.001\nNA res[88]     20.474  27.093  -33.942    3.067   19.924   38.470   72.341 1.001\nNA res[89]     17.776  27.240  -36.170    0.154   17.205   35.757   69.869 1.001\nNA res[90]     21.069  27.426  -33.512    3.375   20.566   38.921   73.507 1.001\nNA res[91]   -139.087  27.401 -192.251 -157.857 -139.429 -120.731  -85.751 1.001\nNA res[92]    -36.783  27.249  -89.549  -55.363  -37.012  -18.606   16.371 1.001\nNA res[93]    -52.676  27.136 -105.357  -71.142  -52.862  -34.505    0.149 1.001\nNA res[94]    -26.246  27.062  -78.812  -44.487  -26.694   -8.200   27.216 1.001\nNA res[95]    -30.949  27.028  -83.569  -49.064  -31.432  -12.883   22.822 1.001\nNA res[96]    -18.119  27.035  -70.366  -36.181  -18.630   -0.071   35.799 1.001\nNA res[97]     11.013  27.081  -42.062   -7.308   10.371   29.090   64.960 1.001\nNA res[98]     89.212  27.167   36.723   70.825   88.487  107.474  143.357 1.001\nNA res[99]     95.614  27.293   43.369   77.242   94.962  114.096  150.199 1.001\nNA res[100]   134.498  27.457   80.966  116.052  134.051  153.002  189.417 1.001\nNA res[101]   -76.108  27.204 -130.082  -94.332  -75.847  -57.878  -24.353 1.001\nNA res[102]   -34.250  27.032  -87.681  -52.048  -34.152  -16.151   17.341 1.001\nNA res[103]   -71.035  26.900 -124.029  -88.589  -71.012  -53.026  -19.557 1.001\nNA res[104]    13.787  26.807  -38.887   -3.638   13.685   31.994   65.003 1.001\nNA res[105]   -14.560  26.754  -67.146  -32.253  -14.734    3.512   36.383 1.001\nNA res[106]   -19.293  26.742  -72.025  -36.855  -19.308   -1.263   32.398 1.001\nNA res[107]    56.600  26.770    3.888   38.981   56.413   74.590  108.690 1.001\nNA res[108]   116.002  26.839   62.639   98.370  115.911  133.987  168.211 1.001\nNA res[109]    48.507  26.948   -4.467   30.803   48.406   66.425  100.281 1.001\nNA res[110]     9.543  27.096  -43.482   -8.442    9.395   27.710   61.257 1.001\nNA res[111]  -132.106  27.068 -185.510 -150.360 -132.361 -113.290  -81.768 1.002\nNA res[112]   -91.911  26.941 -144.896 -110.112  -92.368  -73.497  -41.675 1.002\nNA res[113]   -56.418  26.853 -110.121  -74.546  -56.821  -38.144   -6.066 1.002\nNA res[114]   -13.873  26.805  -66.754  -32.098  -14.064    4.250   36.754 1.002\nNA res[115]    33.751  26.798  -19.527   15.438   33.439   51.710   84.504 1.002\nNA res[116]    18.335  26.831  -34.945   -0.042   17.997   36.508   69.231 1.003\nNA res[117]    -9.402  26.904  -62.947  -27.827   -9.801    8.739   41.996 1.003\nNA res[118]    73.620  27.018   20.102   55.043   73.349   91.735  125.589 1.003\nNA res[119]   130.437  27.170   77.165  111.769  130.243  148.448  183.288 1.003\nNA res[120]    67.067  27.362   14.395   48.375   66.985   85.315  120.963 1.004\nNA res[121]   -62.784  27.602 -117.124  -80.547  -63.554  -44.541   -5.745 1.002\nNA res[122]   -79.284  27.432 -132.901  -96.959  -80.152  -61.213  -23.216 1.002\nNA res[123]  -147.038  27.300 -200.594 -164.755 -147.987 -129.337  -91.122 1.002\nNA res[124]   -86.011  27.208 -139.363 -103.583  -87.079  -68.166  -30.561 1.001\nNA res[125]   -44.469  27.155  -96.901  -61.934  -45.519  -26.882   10.835 1.001\nNA res[126]    -7.957  27.143  -60.233  -25.501   -9.007    9.627   47.109 1.001\nNA res[127]    41.116  27.170  -10.769   23.449   40.140   58.470   96.951 1.001\nNA res[128]   117.876  27.237   66.278   99.824  117.103  135.377  173.458 1.003\nNA res[129]   149.628  27.343   98.060  131.402  148.903  167.283  205.474 1.002\nNA res[130]   138.114  27.489   85.817  119.560  137.512  155.791  194.856 1.002\nNA res[131]  -111.510  27.838 -167.229 -130.194 -111.277  -92.014  -56.583 1.002\nNA res[132]   -85.928  27.671 -141.664 -104.693  -85.568  -66.741  -30.867 1.002\nNA res[133]   -39.184  27.543  -95.068  -57.826  -39.100  -20.158   15.452 1.002\nNA res[134]   -65.087  27.454 -120.528  -83.757  -65.031  -46.398  -10.806 1.002\nNA res[135]   -38.854  27.404  -94.235  -57.313  -38.905  -20.233   15.932 1.002\nNA res[136]    43.897  27.394  -11.085   25.502   43.662   62.725   99.335 1.002\nNA res[137]    60.508  27.423    5.661   42.259   60.002   79.149  116.033 1.002\nNA res[138]    48.476  27.491   -6.511   30.204   47.961   67.171  103.838 1.002\nNA res[139]    64.161  27.599    9.521   45.658   63.389   82.927  119.918 1.002\nNA res[140]   150.425  27.745   95.799  131.748  149.642  169.252  206.380 1.003\nNA res[141]   -91.850  27.881 -148.268 -110.602  -91.492  -72.233  -39.029 1.001\nNA res[142]   -17.357  27.708  -73.303  -36.036  -17.162    1.868   35.260 1.001\nNA res[143]    -8.456  27.572  -63.972  -27.298   -8.090   10.649   43.948 1.001\nNA res[144]    16.411  27.476  -38.675   -2.242   16.566   35.463   68.828 1.001\nNA res[145]    -7.945  27.418  -62.620  -26.453   -7.775   11.028   44.009 1.001\nNA res[146]    34.948  27.400  -19.537   16.194   35.119   53.767   87.148 1.001\nNA res[147]    17.864  27.422  -36.519   -0.769   17.959   36.562   70.397 1.001\nNA res[148]   -37.328  27.483  -92.124  -55.967  -37.389  -18.410   15.393 1.001\nNA res[149]     0.133  27.583  -54.930  -18.289    0.029   18.855   53.369 1.001\nNA res[150]   125.181  27.722   69.744  106.772  124.931  144.185  179.294 1.001\nNA res[151]   -45.913  27.112 -101.600  -63.384  -46.504  -27.406    6.233 1.002\nNA res[152]    14.074  26.949  -41.422   -3.190   13.364   32.310   66.645 1.001\nNA res[153]    25.930  26.825  -28.692    8.617   25.258   43.842   78.335 1.001\nNA res[154]    -8.769  26.741  -62.803  -25.915   -9.134    8.810   43.512 1.001\nNA res[155]   -12.026  26.698  -66.536  -29.146  -12.358    5.538   40.695 1.001\nNA res[156]   -13.996  26.695  -68.442  -31.190  -14.374    3.511   39.057 1.001\nNA res[157]   -21.157  26.732  -75.444  -38.415  -21.645   -3.442   32.176 1.001\nNA res[158]    35.944  26.810  -17.938   18.462   35.409   53.573   89.820 1.001\nNA res[159]    -9.265  26.928  -63.044  -26.958   -9.879    8.560   44.932 1.001\nNA res[160]    61.941  27.086    8.272   44.148   61.146   79.841  116.112 1.001\nNA res[161]  -111.720  27.299 -164.219 -130.281 -111.941  -93.352  -57.961 1.002\nNA res[162]  -161.840  27.142 -214.803 -180.270 -162.077 -143.321 -108.335 1.002\nNA res[163]  -105.843  27.025 -158.096 -124.374 -106.136  -87.584  -52.496 1.002\nNA res[164]    32.678  26.947  -18.932   14.279   32.115   50.832   85.368 1.002\nNA res[165]    -1.691  26.909  -53.352  -20.188   -2.310   16.517   51.266 1.002\nNA res[166]    45.107  26.912   -6.211   26.808   44.518   63.279   97.999 1.002\nNA res[167]     5.376  26.954  -46.126  -12.677    4.882   23.392   58.513 1.002\nNA res[168]    72.424  27.037   20.741   54.308   72.204   90.456  125.956 1.002\nNA res[169]    87.205  27.159   35.982   68.965   87.205  105.288  141.275 1.002\nNA res[170]   173.321  27.321  121.880  155.039  173.059  191.695  228.030 1.003\nNA res[171]   -26.750  27.741  -79.583  -45.635  -26.886   -7.432   26.470 1.001\nNA res[172]   -18.152  27.612  -70.442  -37.010  -18.270    0.993   35.150 1.001\nNA res[173]   -30.868  27.522  -82.411  -49.842  -30.897  -11.752   22.359 1.001\nNA res[174]   -20.371  27.471  -72.112  -39.216  -20.399   -1.090   32.975 1.001\nNA res[175]   -29.402  27.460  -81.284  -48.575  -29.553   -9.855   23.536 1.001\nNA res[176]   -42.654  27.488  -94.488  -61.958  -42.932  -23.151   10.118 1.001\nNA res[177]    21.559  27.555  -30.491    2.158   21.213   40.881   74.590 1.001\nNA res[178]    -6.198  27.662  -58.988  -25.815   -6.244   13.036   47.593 1.001\nNA res[179]   104.363  27.807   51.627   84.332  104.497  123.625  158.553 1.001\nNA res[180]    63.487  27.989   10.897   43.415   63.657   82.927  118.256 1.002\nNA res[181]  -116.426  27.477 -170.058 -135.254 -115.973  -98.040  -63.859 1.001\nNA res[182]   -61.964  27.315 -115.028  -80.596  -61.367  -43.741   -8.748 1.001\nNA res[183]   -11.520  27.191  -65.196  -29.915  -10.904    6.736   41.954 1.001\nNA res[184]   -11.403  27.107  -64.157  -29.928  -10.938    7.033   42.438 1.001\nNA res[185]    11.940  27.063  -40.738   -6.618   12.599   30.143   65.828 1.001\nNA res[186]    78.890  27.059   26.180   60.481   79.578   97.004  132.471 1.001\nNA res[187]    15.749  27.094  -36.961   -2.786   16.169   33.923   68.912 1.001\nNA res[188]     3.160  27.170  -49.311  -15.573    3.325   21.357   56.992 1.001\nNA res[189]    52.382  27.285   -0.331   33.687   52.699   70.616  106.503 1.001\nNA res[190]    57.688  27.439    4.937   38.848   57.982   75.882  112.335 1.001\nNA res[191]   -39.277  27.823  -93.946  -58.037  -39.479  -19.984   14.965 1.001\nNA res[192]     9.409  27.662  -44.962   -9.063    9.312   28.464   63.569 1.001\nNA res[193]    22.489  27.540  -32.351    3.890   22.479   41.387   76.748 1.001\nNA res[194]   -41.856  27.457  -96.305  -60.387  -41.784  -23.180   12.866 1.001\nNA res[195]   -12.452  27.413  -66.510  -31.155  -12.405    6.113   42.359 1.001\nNA res[196]    -1.869  27.408  -55.985  -20.561   -1.913   16.685   53.271 1.001\nNA res[197]    21.489  27.443  -32.127    2.711   21.389   39.871   76.814 1.001\nNA res[198]    -9.600  27.518  -62.931  -28.632   -9.617    8.338   45.590 1.002\nNA res[199]    53.727  27.631   -0.059   34.427   53.676   71.736  108.974 1.002\nNA res[200]    31.695  27.783  -22.231   12.143   31.743   49.979   87.523 1.002\nNA res[201]   -86.890  27.181 -140.177 -104.770  -87.118  -68.603  -35.106 1.001\nNA res[202]   -30.563  27.031  -83.642  -48.350  -31.011  -12.345   20.910 1.002\nNA res[203]   -39.562  26.920  -92.523  -57.243  -39.987  -21.390   11.887 1.002\nNA res[204]    25.751  26.850  -27.313    8.074   25.467   43.881   77.877 1.002\nNA res[205]   -12.787  26.819  -65.884  -30.405  -13.246    5.237   39.562 1.002\nNA res[206]  -114.094  26.829 -166.864 -131.854 -114.195  -96.423  -61.166 1.002\nNA res[207]   -27.215  26.880  -80.040  -44.971  -27.431   -9.544   26.079 1.002\nNA res[208]    65.253  26.970   12.034   47.409   65.333   82.874  119.333 1.002\nNA res[209]    87.094  27.100   33.577   69.194   87.081  104.945  141.467 1.003\nNA res[210]   148.253  27.269   94.421  130.305  148.113  166.284  203.466 1.002\nNA res[211]  -161.236  27.503 -215.889 -178.736 -161.122 -142.886 -108.375 1.001\nNA res[212]  -129.522  27.348 -183.477 -146.819 -129.294 -111.282  -76.871 1.001\nNA res[213]  -190.682  27.231 -243.930 -208.062 -190.524 -172.562 -138.450 1.001\nNA res[214]   -79.289  27.154 -132.322  -96.647  -79.209  -61.143  -27.250 1.001\nNA res[215]    47.949  27.116   -4.985   30.474   48.166   66.107  100.695 1.001\nNA res[216]   114.631  27.119   61.912   96.825  114.839  132.864  167.496 1.001\nNA res[217]   117.095  27.161   63.624   99.215  117.216  135.276  169.783 1.001\nNA res[218]   122.954  27.243   68.976  105.183  123.040  140.999  175.833 1.001\nNA res[219]    57.691  27.364    2.985   40.012   57.620   75.929  110.855 1.001\nNA res[220]   124.990  27.525   70.453  107.235  124.767  143.105  178.606 1.001\nNA res[221]   -28.457  27.809  -85.147  -46.709  -28.877   -8.749   24.203 1.001\nNA res[222]   -40.195  27.642  -96.634  -58.154  -40.465  -20.895   12.714 1.001\nNA res[223]   -83.163  27.513 -139.662 -101.235  -83.213  -63.910  -30.005 1.001\nNA res[224]   -57.737  27.424 -113.878  -75.793  -57.651  -38.683   -5.179 1.001\nNA res[225]   -60.675  27.373 -116.422  -78.560  -60.376  -41.718   -8.273 1.001\nNA res[226]   -36.237  27.363  -91.706  -54.143  -35.975  -17.495   16.397 1.001\nNA res[227]   -53.528  27.391 -108.591  -71.787  -53.313  -34.530   -0.302 1.001\nNA res[228]    82.085  27.460   27.176   63.546   82.505  100.972  135.212 1.001\nNA res[229]   109.582  27.567   54.642   90.994  110.106  128.598  162.713 1.001\nNA res[230]   190.314  27.713  135.634  171.529  190.765  209.465  244.223 1.001\nNA res[231]  -120.252  27.694 -175.928 -138.875 -119.474 -101.313  -68.003 1.001\nNA res[232]  -100.688  27.530 -155.801 -119.226  -99.978  -81.979  -48.238 1.001\nNA res[233]   -91.964  27.405 -146.450 -110.666  -91.280  -73.148  -39.619 1.001\nNA res[234]  -108.171  27.319 -162.489 -126.710 -107.330  -89.436  -55.884 1.001\nNA res[235]    21.527  27.273  -32.755    3.074   22.292   40.459   73.543 1.001\nNA res[236]   -18.622  27.266  -73.068  -36.988  -18.139    0.348   33.406 1.001\nNA res[237]    40.129  27.299  -14.569   21.865   40.523   59.257   92.027 1.001\nNA res[238]   117.718  27.372   62.996   99.591  118.175  136.830  169.568 1.001\nNA res[239]   150.564  27.484   95.949  132.308  150.955  169.833  203.577 1.001\nNA res[240]   128.355  27.634   73.287  109.633  128.714  147.821  181.492 1.001\nNA res[241]   -43.717  26.592  -96.442  -61.945  -43.778  -25.366    7.706 1.002\nNA res[242]   -16.266  26.421  -68.915  -34.345  -16.394    1.657   34.738 1.002\nNA res[243]   -51.824  26.290 -104.250  -69.666  -51.904  -34.191   -0.773 1.002\nNA res[244]   -92.310  26.200 -144.512 -109.770  -92.356  -74.742  -41.007 1.002\nNA res[245]    53.225  26.151    1.165   35.773   53.155   70.545  104.484 1.002\nNA res[246]    35.579  26.143  -16.717   18.099   35.633   52.862   86.869 1.001\nNA res[247]   -33.204  26.177  -85.575  -50.833  -33.098  -15.939   18.146 1.001\nNA res[248]    35.397  26.252  -16.792   17.809   35.174   52.691   87.176 1.001\nNA res[249]    83.314  26.368   30.491   65.761   83.032  100.631  135.542 1.001\nNA res[250]    42.630  26.524   -9.672   24.976   42.309   59.916   95.055 1.001\nNA res[251]  -206.923  27.800 -260.529 -225.880 -206.782 -187.968 -153.986 1.001\nNA res[252]  -157.656  27.629 -211.249 -176.383 -157.567 -138.906 -104.887 1.001\nNA res[253]   -51.271  27.496 -104.867  -70.030  -51.271  -32.412    1.659 1.001\nNA res[254]    12.860  27.402  -40.341   -5.856   13.103   31.625   65.496 1.001\nNA res[255]    11.898  27.348  -41.371   -7.218   11.887   30.504   64.701 1.001\nNA res[256]    -1.747  27.333  -54.813  -20.847   -1.807   16.587   50.997 1.001\nNA res[257]    46.786  27.357   -6.081   27.648   46.704   65.069  100.383 1.001\nNA res[258]    84.114  27.422   31.253   65.110   83.938  102.405  137.813 1.001\nNA res[259]   141.780  27.525   88.996  122.717  141.448  159.933  195.838 1.001\nNA res[260]   140.825  27.667   88.214  121.349  140.766  158.831  195.301 1.001\nNA res[261]   -45.758  27.262  -98.677  -63.456  -45.803  -28.120    8.017 1.002\nNA res[262]   -73.536  27.103 -126.119  -91.086  -73.677  -55.880  -19.998 1.001\nNA res[263]   -66.558  26.984 -118.944  -84.155  -66.732  -49.089  -12.830 1.001\nNA res[264]    -3.221  26.904  -55.491  -20.926   -3.268   14.217   50.110 1.001\nNA res[265]    64.093  26.864   12.275   46.220   64.035   81.487  117.920 1.001\nNA res[266]   -24.292  26.865  -75.837  -42.281  -24.457   -6.832   29.548 1.001\nNA res[267]    11.470  26.906  -40.398   -6.459   11.267   29.011   65.223 1.001\nNA res[268]    69.347  26.987   17.413   51.396   68.896   87.025  123.112 1.001\nNA res[269]    16.854  27.108  -35.179   -1.244   16.533   34.478   70.967 1.001\nNA res[270]    75.619  27.268   23.067   57.629   75.329   93.227  129.655 1.001\nNA res[271]  -225.643  27.480 -278.886 -243.823 -225.694 -207.386 -171.764 1.003\nNA res[272]  -140.428  27.320 -193.206 -158.567 -140.609 -122.309  -86.874 1.003\nNA res[273]   -92.158  27.198 -145.009 -110.346  -92.252  -74.095  -38.573 1.003\nNA res[274]   -37.826  27.116  -90.730  -55.873  -38.139  -19.735   15.494 1.002\nNA res[275]   -22.591  27.074  -75.243  -40.456  -22.982   -4.368   30.621 1.002\nNA res[276]    30.930  27.072  -22.094   13.172   30.419   49.127   84.548 1.002\nNA res[277]   103.777  27.109   50.599   85.820  102.988  121.948  157.742 1.002\nNA res[278]   195.528  27.187  142.165  177.513  194.832  213.701  249.967 1.002\nNA res[279]   173.383  27.304  119.414  155.128  172.627  191.796  228.591 1.001\nNA res[280]    48.822  27.460   -5.443   30.590   47.952   67.271  104.943 1.001\nNA res[281]  -100.761  27.924 -155.664 -119.211 -100.098  -81.104  -47.734 1.001\nNA res[282]   -73.020  27.777 -127.369  -91.266  -72.520  -53.694  -20.563 1.001\nNA res[283]   -64.686  27.669 -119.132  -82.823  -64.075  -45.417  -12.494 1.001\nNA res[284]   -17.650  27.599  -72.206  -35.666  -17.169    1.377   35.020 1.001\nNA res[285]    46.285  27.569   -7.652   28.252   46.815   65.328   99.260 1.001\nNA res[286]    25.187  27.577  -28.572    7.101   25.553   44.083   78.569 1.001\nNA res[287]    23.689  27.625  -29.512    5.427   23.865   42.895   77.192 1.001\nNA res[288]    61.274  27.713    8.220   42.691   61.404   80.495  114.966 1.001\nNA res[289]    48.126  27.838   -5.127   29.210   48.155   67.369  102.003 1.001\nNA res[290]    73.868  28.002   20.444   54.993   73.670   93.109  128.079 1.001\nNA res[291]   -62.043  27.792 -115.822  -80.494  -62.779  -43.673   -4.435 1.001\nNA res[292]   -78.060  27.657 -131.681  -96.527  -78.957  -60.027  -21.083 1.001\nNA res[293]   -97.080  27.560 -150.130 -115.697  -97.882  -79.080  -40.450 1.001\nNA res[294]   -84.403  27.502 -137.305 -103.193  -85.127  -66.419  -27.873 1.001\nNA res[295]   -62.058  27.484 -115.409  -80.974  -62.634  -43.885   -5.395 1.001\nNA res[296]    13.785  27.505  -39.891   -5.375   13.338   32.209   69.885 1.001\nNA res[297]   108.636  27.565   55.243   89.343  108.404  127.130  164.689 1.001\nNA res[298]   103.229  27.665   49.562   83.789  102.942  122.033  159.137 1.001\nNA res[299]    89.617  27.803   35.620   70.171   89.485  108.312  146.507 1.001\nNA res[300]    80.672  27.979   26.414   60.959   80.808   99.562  137.744 1.001\nNA res[301]  -141.883  28.305 -196.509 -160.420 -141.920 -123.442  -84.721 1.001\nNA res[302]  -106.210  28.157 -160.120 -124.453 -106.309  -87.823  -49.394 1.001\nNA res[303]   -37.352  28.047  -91.475  -55.335  -37.285  -19.229   19.895 1.001\nNA res[304]   -18.327  27.975  -72.662  -36.286  -18.237   -0.397   38.713 1.001\nNA res[305]   -86.778  27.942 -140.890 -105.114  -86.791  -68.788  -29.209 1.001\nNA res[306]   -37.999  27.947  -91.849  -56.327  -37.867  -20.131   19.461 1.001\nNA res[307]    69.141  27.991   14.641   50.665   69.178   86.859  126.516 1.001\nNA res[308]   159.261  28.074  104.599  140.704  159.387  177.180  216.359 1.001\nNA res[309]   136.516  28.195   80.873  118.244  136.554  154.819  193.892 1.001\nNA res[310]    84.565  28.354   28.496   66.131   84.693  102.951  142.719 1.002\nNA res[311]     8.480  27.336  -47.030   -9.792    8.978   27.073   59.498 1.003\nNA res[312]    11.977  27.128  -43.062   -5.921   12.427   30.119   62.594 1.003\nNA res[313]    22.210  26.959  -32.858    4.243   22.751   40.219   72.580 1.003\nNA res[314]    60.484  26.829    5.199   42.716   60.942   78.598  110.330 1.002\nNA res[315]    -6.440  26.739  -61.507  -24.246   -6.091   11.508   43.556 1.002\nNA res[316]   -11.502  26.689  -66.443  -29.364  -11.033    6.418   38.397 1.002\nNA res[317]   -51.977  26.680 -107.165  -69.981  -51.390  -33.782   -1.318 1.002\nNA res[318]   -37.802  26.711  -93.392  -55.747  -37.309  -19.574   12.624 1.002\nNA res[319]    18.322  26.783  -37.480    0.309   19.012   36.680   68.620 1.001\nNA res[320]    -6.770  26.895  -62.754  -24.603   -5.931   11.437   44.809 1.001\nNA res[321]  -169.878  27.641 -225.412 -188.557 -170.087 -150.812 -115.846 1.003\nNA res[322]  -115.245  27.475 -170.155 -133.481 -115.610  -96.509  -61.338 1.002\nNA res[323]  -127.030  27.348 -181.991 -145.028 -127.405 -108.460  -73.235 1.002\nNA res[324]   -75.745  27.260 -130.493  -94.150  -76.056  -57.347  -21.877 1.002\nNA res[325]    -4.442  27.211  -59.066  -22.893   -4.731   14.091   49.373 1.002\nNA res[326]    72.438  27.202   17.923   54.115   72.083   90.962  125.624 1.002\nNA res[327]    74.431  27.233   20.320   56.096   74.054   93.130  128.264 1.001\nNA res[328]    86.173  27.303   32.254   68.056   86.103  104.893  140.473 1.001\nNA res[329]   100.100  27.413   46.110   82.027  100.161  118.754  154.311 1.001\nNA res[330]   191.836  27.562  137.087  173.612  191.900  210.527  246.650 1.001\nNA res[331]    -3.568  27.677  -58.111  -22.218   -3.240   14.872   51.330 1.001\nNA res[332]    35.490  27.505  -18.504   16.700   35.467   53.896   89.971 1.001\nNA res[333]    14.836  27.372  -38.705   -3.945   14.883   33.326   69.479 1.001\nNA res[334]    35.529  27.278  -17.795   16.742   35.624   53.845   89.880 1.001\nNA res[335]    57.772  27.224    4.745   39.406   57.712   76.032  112.258 1.001\nNA res[336]     7.505  27.209  -45.201  -11.277    7.241   25.550   61.787 1.001\nNA res[337]   -62.853  27.234 -115.231  -81.656  -63.102  -44.830   -8.470 1.001\nNA res[338]     3.554  27.299  -48.771  -15.196    3.261   21.590   57.887 1.001\nNA res[339]   -47.691  27.403 -100.181  -66.462  -48.175  -29.624    6.870 1.001\nNA res[340]    -7.951  27.546  -60.486  -26.990   -8.417   10.133   47.240 1.001\nNA res[341]  -167.838  27.534 -221.260 -186.767 -167.997 -149.518 -113.530 1.005\nNA res[342]  -109.245  27.409 -162.241 -128.030 -109.426  -91.138  -55.319 1.005\nNA res[343]   -76.905  27.323 -129.335  -95.553  -76.780  -58.727  -23.086 1.004\nNA res[344]   -92.961  27.276 -144.845 -111.585  -92.688  -74.742  -39.601 1.004\nNA res[345]   -15.658  27.269  -67.414  -34.298  -15.410    2.716   37.316 1.004\nNA res[346]    87.984  27.302   36.319   69.290   88.171  106.278  140.902 1.003\nNA res[347]   132.212  27.374   80.204  113.503  132.167  150.380  185.832 1.003\nNA res[348]    30.540  27.486  -21.770   11.869   30.461   48.883   84.590 1.003\nNA res[349]   120.550  27.636   67.565  101.960  120.450  138.866  174.815 1.002\nNA res[350]   123.963  27.825   70.604  105.055  123.691  142.633  178.437 1.002\nNA sigma       86.053   4.095   78.239   83.300   85.994   88.737   94.268 1.013\nNA sigma.B    317.817  38.958  252.469  291.832  313.873  339.888  405.125 1.002\nNA deviance  4111.820  21.476 4069.687 4097.266 4111.765 4126.251 4152.352 1.008\nNA           n.eff\nNA beta[1]    1400\nNA beta[2]     230\nNA gamma[1]   1000\nNA gamma[2]   3000\nNA gamma[3]   1300\nNA gamma[4]   2800\nNA gamma[5]   3000\nNA gamma[6]   3000\nNA gamma[7]   1800\nNA gamma[8]   3000\nNA gamma[9]   1800\nNA gamma[10]  2900\nNA gamma[11]  3000\nNA gamma[12]  1800\nNA gamma[13]  1900\nNA gamma[14]  3000\nNA gamma[15]  3000\nNA gamma[16]  1300\nNA gamma[17]  1100\nNA gamma[18]  3000\nNA gamma[19]  3000\nNA gamma[20]  3000\nNA gamma[21]  3000\nNA gamma[22]  3000\nNA gamma[23]  1800\nNA gamma[24]  3000\nNA gamma[25]   810\nNA gamma[26]  3000\nNA gamma[27]  1500\nNA gamma[28]   570\nNA gamma[29]  3000\nNA gamma[30]  3000\nNA gamma[31]  3000\nNA gamma[32]   660\nNA gamma[33]   730\nNA gamma[34]  3000\nNA gamma[35]   340\nNA res[1]     1100\nNA res[2]     1200\nNA res[3]     1400\nNA res[4]     1700\nNA res[5]     2100\nNA res[6]     2700\nNA res[7]     3000\nNA res[8]     3000\nNA res[9]     3000\nNA res[10]    3000\nNA res[11]    3000\nNA res[12]    3000\nNA res[13]    3000\nNA res[14]    3000\nNA res[15]    3000\nNA res[16]    3000\nNA res[17]    3000\nNA res[18]    3000\nNA res[19]    3000\nNA res[20]    3000\nNA res[21]    1200\nNA res[22]    1400\nNA res[23]    1700\nNA res[24]    2000\nNA res[25]    2500\nNA res[26]    3000\nNA res[27]    3000\nNA res[28]    3000\nNA res[29]    3000\nNA res[30]    3000\nNA res[31]    3000\nNA res[32]    3000\nNA res[33]    3000\nNA res[34]    3000\nNA res[35]    3000\nNA res[36]    3000\nNA res[37]    3000\nNA res[38]    3000\nNA res[39]    3000\nNA res[40]    3000\nNA res[41]    3000\nNA res[42]    3000\nNA res[43]    3000\nNA res[44]    3000\nNA res[45]    3000\nNA res[46]    3000\nNA res[47]    3000\nNA res[48]    3000\nNA res[49]    3000\nNA res[50]    3000\nNA res[51]    3000\nNA res[52]    2800\nNA res[53]    2200\nNA res[54]    1700\nNA res[55]    1400\nNA res[56]    1200\nNA res[57]    1000\nNA res[58]     710\nNA res[59]     630\nNA res[60]     550\nNA res[61]    2000\nNA res[62]    2400\nNA res[63]    3000\nNA res[64]    3000\nNA res[65]    3000\nNA res[66]    3000\nNA res[67]    3000\nNA res[68]    3000\nNA res[69]    3000\nNA res[70]    3000\nNA res[71]    3000\nNA res[72]    3000\nNA res[73]    3000\nNA res[74]    3000\nNA res[75]    3000\nNA res[76]    3000\nNA res[77]    3000\nNA res[78]    3000\nNA res[79]    3000\nNA res[80]    3000\nNA res[81]    2100\nNA res[82]    2600\nNA res[83]    3000\nNA res[84]    3000\nNA res[85]    3000\nNA res[86]    3000\nNA res[87]    3000\nNA res[88]    3000\nNA res[89]    3000\nNA res[90]    3000\nNA res[91]    3000\nNA res[92]    3000\nNA res[93]    3000\nNA res[94]    3000\nNA res[95]    3000\nNA res[96]    3000\nNA res[97]    3000\nNA res[98]    3000\nNA res[99]    3000\nNA res[100]   3000\nNA res[101]   3000\nNA res[102]   3000\nNA res[103]   3000\nNA res[104]   3000\nNA res[105]   3000\nNA res[106]   3000\nNA res[107]   3000\nNA res[108]   3000\nNA res[109]   3000\nNA res[110]   3000\nNA res[111]   1700\nNA res[112]   1400\nNA res[113]   1100\nNA res[114]    980\nNA res[115]    840\nNA res[116]    740\nNA res[117]    650\nNA res[118]    580\nNA res[119]    600\nNA res[120]    480\nNA res[121]   1600\nNA res[122]   2000\nNA res[123]   2400\nNA res[124]   3000\nNA res[125]   3000\nNA res[126]   3000\nNA res[127]   3000\nNA res[128]   3000\nNA res[129]   3000\nNA res[130]   3000\nNA res[131]   3000\nNA res[132]   3000\nNA res[133]   3000\nNA res[134]   3000\nNA res[135]   3000\nNA res[136]   3000\nNA res[137]   3000\nNA res[138]   3000\nNA res[139]   2700\nNA res[140]   3000\nNA res[141]   3000\nNA res[142]   3000\nNA res[143]   3000\nNA res[144]   3000\nNA res[145]   3000\nNA res[146]   3000\nNA res[147]   3000\nNA res[148]   3000\nNA res[149]   3000\nNA res[150]   3000\nNA res[151]   1600\nNA res[152]   1900\nNA res[153]   2300\nNA res[154]   3000\nNA res[155]   3000\nNA res[156]   3000\nNA res[157]   3000\nNA res[158]   3000\nNA res[159]   3000\nNA res[160]   3000\nNA res[161]   1000\nNA res[162]   1200\nNA res[163]   1400\nNA res[164]   1700\nNA res[165]   2100\nNA res[166]   2600\nNA res[167]   3000\nNA res[168]   3000\nNA res[169]   3000\nNA res[170]   3000\nNA res[171]   3000\nNA res[172]   3000\nNA res[173]   3000\nNA res[174]   3000\nNA res[175]   3000\nNA res[176]   3000\nNA res[177]   2700\nNA res[178]   2200\nNA res[179]   2000\nNA res[180]   1500\nNA res[181]   3000\nNA res[182]   3000\nNA res[183]   3000\nNA res[184]   3000\nNA res[185]   3000\nNA res[186]   3000\nNA res[187]   3000\nNA res[188]   3000\nNA res[189]   3000\nNA res[190]   3000\nNA res[191]   3000\nNA res[192]   3000\nNA res[193]   3000\nNA res[194]   3000\nNA res[195]   3000\nNA res[196]   2500\nNA res[197]   2000\nNA res[198]   1700\nNA res[199]   1400\nNA res[200]   1200\nNA res[201]   3000\nNA res[202]   3000\nNA res[203]   2700\nNA res[204]   2100\nNA res[205]   1700\nNA res[206]   1400\nNA res[207]   1200\nNA res[208]   1000\nNA res[209]    890\nNA res[210]   1100\nNA res[211]   3000\nNA res[212]   3000\nNA res[213]   3000\nNA res[214]   3000\nNA res[215]   3000\nNA res[216]   3000\nNA res[217]   3000\nNA res[218]   3000\nNA res[219]   3000\nNA res[220]   3000\nNA res[221]   1800\nNA res[222]   2200\nNA res[223]   2800\nNA res[224]   3000\nNA res[225]   3000\nNA res[226]   3000\nNA res[227]   3000\nNA res[228]   3000\nNA res[229]   3000\nNA res[230]   3000\nNA res[231]   3000\nNA res[232]   3000\nNA res[233]   3000\nNA res[234]   3000\nNA res[235]   3000\nNA res[236]   3000\nNA res[237]   3000\nNA res[238]   3000\nNA res[239]   3000\nNA res[240]   3000\nNA res[241]    900\nNA res[242]   1000\nNA res[243]   1200\nNA res[244]   1400\nNA res[245]   1800\nNA res[246]   2200\nNA res[247]   2800\nNA res[248]   3000\nNA res[249]   3000\nNA res[250]   3000\nNA res[251]   3000\nNA res[252]   3000\nNA res[253]   3000\nNA res[254]   3000\nNA res[255]   3000\nNA res[256]   3000\nNA res[257]   3000\nNA res[258]   3000\nNA res[259]   2600\nNA res[260]   2100\nNA res[261]   1600\nNA res[262]   1900\nNA res[263]   2400\nNA res[264]   3000\nNA res[265]   3000\nNA res[266]   3000\nNA res[267]   3000\nNA res[268]   3000\nNA res[269]   3000\nNA res[270]   3000\nNA res[271]    590\nNA res[272]    650\nNA res[273]    730\nNA res[274]    830\nNA res[275]    960\nNA res[276]   1100\nNA res[277]   1200\nNA res[278]   1500\nNA res[279]   1800\nNA res[280]   2500\nNA res[281]   3000\nNA res[282]   3000\nNA res[283]   3000\nNA res[284]   3000\nNA res[285]   3000\nNA res[286]   3000\nNA res[287]   3000\nNA res[288]   3000\nNA res[289]   3000\nNA res[290]   2800\nNA res[291]   3000\nNA res[292]   3000\nNA res[293]   3000\nNA res[294]   3000\nNA res[295]   3000\nNA res[296]   3000\nNA res[297]   3000\nNA res[298]   3000\nNA res[299]   3000\nNA res[300]   3000\nNA res[301]   3000\nNA res[302]   3000\nNA res[303]   3000\nNA res[304]   3000\nNA res[305]   3000\nNA res[306]   3000\nNA res[307]   3000\nNA res[308]   2700\nNA res[309]   2300\nNA res[310]   1700\nNA res[311]    610\nNA res[312]    670\nNA res[313]    760\nNA res[314]    860\nNA res[315]    990\nNA res[316]   1200\nNA res[317]   1400\nNA res[318]   1700\nNA res[319]   2100\nNA res[320]   2600\nNA res[321]    770\nNA res[322]    870\nNA res[323]   1000\nNA res[324]   1200\nNA res[325]   1400\nNA res[326]   1600\nNA res[327]   2000\nNA res[328]   2600\nNA res[329]   2700\nNA res[330]   3000\nNA res[331]   3000\nNA res[332]   3000\nNA res[333]   3000\nNA res[334]   3000\nNA res[335]   3000\nNA res[336]   3000\nNA res[337]   3000\nNA res[338]   3000\nNA res[339]   3000\nNA res[340]   3000\nNA res[341]    350\nNA res[342]    380\nNA res[343]    420\nNA res[344]    460\nNA res[345]    510\nNA res[346]    570\nNA res[347]    810\nNA res[348]    740\nNA res[349]   1200\nNA res[350]   1500\nNA sigma       130\nNA sigma.B    3000\nNA deviance    200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 229.5 and DIC = 4341.4\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\nmodelString3=\"\nmodel {\n   #Likelihood\n   y[1]~dnorm(mu[1],tau)\n   mu[1] &lt;- eta1[1]\n   eta1[1] ~ dnorm(eta[1], taueps)\n   eta[1] &lt;- inprod(beta[],X[1,]) + gamma[Block[1]]\n   res[1] &lt;- y[1]-mu[1]\n   for (i in 2:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- eta1[i]\n      eta1[i] ~ dnorm(temp[i], taueps)\n      temp[i] &lt;- eta[i] + -rho*(mu[i-1]-y[i-1])\n      eta[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   rho ~ dunif(-1,1)\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n   taueps &lt;- pow(sigma.eps,-2)\n   sigma.eps &lt;- z/sqrt(chSq.eps) \n   z.eps ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.eps ~ dgamma(0.5, 0.5)\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n   sd.y &lt;- sd(res)\n   sd.block &lt;- sd(gamma)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString3, con = \"matrixModel3.txt\")\n\nXmat &lt;- model.matrix(~Time,data.rm)\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=Xmat,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block)),\n         a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\n         )\n)\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\",'sigma.eps','rho','sd.y','sd.block')\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.mt &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel3.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 393\nNA    Total graph size: 3931\nNA \nNA Initializing model\n\ndata.rm.mt.mcmc &lt;- data.rm.r2jags.mt$BUGSoutput$sims.matrix\nsummary(as.mcmc(data.rm.mt.mcmc[,grep('beta|sigma|rho',colnames(data.rm.mt.mcmc))]))\n\nNA \nNA Iterations = 1:3000\nNA Thinning interval = 1 \nNA Number of chains = 1 \nNA Sample size per chain = 3000 \nNA \nNA 1. Empirical mean and standard deviation for each variable,\nNA    plus standard error of the mean:\nNA \nNA               Mean      SD Naive SE Time-series SE\nNA beta[1]     0.1093  1.0090 0.018422       0.018422\nNA beta[2]     9.1917  1.0421 0.019026       0.019026\nNA rho         0.6991  0.1635 0.002985       0.002985\nNA sigma      63.1901  7.6813 0.140241       0.140241\nNA sigma.B   314.5779 38.4731 0.702419       0.702419\nNA sigma.eps  32.8263  9.4551 0.172625       0.172625\nNA \nNA 2. Quantiles for each variable:\nNA \nNA               2.5%      25%       50%      75%    97.5%\nNA beta[1]    -1.8402  -0.5698   0.09792   0.7920   2.0576\nNA beta[2]     7.2315   8.4966   9.19938   9.8961  11.2232\nNA rho         0.4549   0.5508   0.68313   0.8453   0.9746\nNA sigma      49.8811  56.9814  62.70134  69.5262  77.4328\nNA sigma.B   251.2601 286.7395 310.40901 338.2187 398.3167\nNA sigma.eps  14.8598  25.5907  35.65555  40.2488  46.3558\n\n#head(data.rm.r2jags.mt$BUGSoutput$sims.list[[c('beta','rho','sigma')]]) \n#print(data.rm.r2jags.mt)\ndata.rm.mcmc.list.mt &lt;- as.mcmc(data.rm.r2jags.mt)\nData.Rm.mcmc.list.mt &lt;- data.rm.mcmc.list.mt\n\n# R2 calculations\nXmat &lt;- model.matrix(~Time, data.rm)\ncoefs &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['beta']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nX.var[1:10]\n\nNA        1        2        3        4        5        6        7        8 \nNA 814.3418 591.0181 634.1438 685.5362 900.1883 740.2397 864.5962 435.7952 \nNA        9       10 \nNA 672.4743 584.2064\n\nZ.var &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['sd.block']]^2\nR.var &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['sd.y']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.block &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.block &lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\n(r2 &lt;- rbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional))\n\nNA                      Mean     Median      lower     upper\nNA R2.block       0.52595295 0.52197768 0.40289527 0.6376026\nNA R2.marginal    0.07190426 0.06983026 0.03887103 0.1087763\nNA R2.res         0.40214279 0.40351594 0.28261806 0.5200679\nNA R2.conditional 0.59785721 0.59648406 0.47993214 0.7173819\n\n\nIt would appear that the incorporation of a first order autocorrelation structure is indeed appropriate. The degree of correlation between successive points is \\(0.733\\). Let’s have a look at a summary figure.\n\ncoefs &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['beta']]\nnewdata &lt;- with(data.rm, data.frame(Time=seq(min(Time, na.rm=TRUE), max(Time, na.rm=TRUE), len=100)))\nXmat &lt;- model.matrix(~Time, newdata)\npred &lt;- (coefs %*% t(Xmat))\npred &lt;- adply(pred, 2, function(x) {\n   data.frame(Mean=mean(x), Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n})\nnewdata &lt;- cbind(newdata, pred)\n#Also calculate the partial observations\nXmat &lt;- model.matrix(~Time, data.rm)\npred &lt;- colMeans(as.vector(coefs %*% t(Xmat))+data.rm.r2jags.mt$BUGSoutput$sims.list[['res']])\npart.obs &lt;- cbind(data.rm,Median=pred)\n\nggplot(newdata, aes(y=Median, x=Time)) +\n  geom_point(data=part.obs, aes(y=Median))+\n  geom_ribbon(aes(ymin=lower, ymax=upper), fill='blue',alpha=0.2) +\n  geom_line()+\n  scale_x_continuous('Time') +\n  scale_y_continuous('Y') +\n  theme_classic() +\n  theme(axis.title.y = element_text(vjust=2, size=rel(1.2)),\n        axis.title.x = element_text(vjust=-2, size=rel(1.2)),\n        plot.margin=unit(c(0.5,0.5,2,2), 'lines'))"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html",
    "href": "tutorials/2019-07-01-intro-stan/index.html",
    "title": "Super basic introduction to Stan",
    "section": "",
    "text": "The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using Stan via R.\nPrerequisites:"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#what-is-stan",
    "href": "tutorials/2019-07-01-intro-stan/index.html#what-is-stan",
    "title": "Super basic introduction to Stan",
    "section": "What is Stan?",
    "text": "What is Stan?\nStan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling\nStan is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Gelman, Lee, and Guo (2015)). Stan is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as Hamiltonian Monte Carlo methods (HMC). The latest version of Stan can be dowloaded from the web repository and is available for different OS. There are different R packages which function as frontends for Stan. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the rstan package (Stan Development Team (2018)) and show how to fit Stan models using this package."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#installing-stan-and-rstan",
    "href": "tutorials/2019-07-01-intro-stan/index.html#installing-stan-and-rstan",
    "title": "Super basic introduction to Stan",
    "section": "Installing Stan and rstan",
    "text": "Installing Stan and rstan\nUnlike other Bayesian software, such as JAGS or OpenBUGS, it is not required to separately install the program and the corresponding frontend R package. Indeed, installing the R package rstan will automatically install Stan on your machine. However, you will also need to make sure to having installed on your pc a C++ compiler which is used by rstan to fit the models. Under a Windows OS, for example, this can be done by installing Rtools, a collection of resources for building packages for R, which is freely available from the web repository.\nNext, install the package rstan from within R or Rstudio, via the package installer or by typing in the command line\n\ninstall.packages(\"rstan\", dependencies = TRUE)\n\nThe dependencies = TRUE option will automatically install all the packages on which the functions in the rstan package rely."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#simulate-data",
    "href": "tutorials/2019-07-01-intro-stan/index.html#simulate-data",
    "title": "Super basic introduction to Stan",
    "section": "Simulate data",
    "text": "Simulate data\nFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon  \n\\]\nThe R commands which I use to simulate the data are the following:\n\nn_sim=100; set.seed(123)\nx=rnorm(n_sim, mean = 5, sd = 2)\nepsilon=rnorm(n_sim, mean = 0, sd = 1)\nbeta0=1.5\nbeta1=1.2\ny=beta0 + beta1 * x + epsilon\nn_sim=as.integer(n_sim)\n\nThen, I define all the data for Stan in a list object\n\ndatalist=list(\"y\"=y,\"x\"=x,\"n_sim\"=n_sim)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#model-file",
    "href": "tutorials/2019-07-01-intro-stan/index.html#model-file",
    "title": "Super basic introduction to Stan",
    "section": "Model file",
    "text": "Model file\nNow, I write the model for Stan and save it as a stan file named \"basic.mod.stan\" in the current working directory\n\nbasic.mod= \"\ndata {\nint&lt;lower=0&gt; n_sim;\nvector[n_sim] y;\nvector[n_sim] x;\n}\nparameters {\nreal beta0;\nreal beta1;\nreal&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\nvector[n_sim] mu;\nmu=beta0 + beta1*x;\n} \nmodel {\nsigma~uniform(0,100);\nbeta0~normal(0,1000);\nbeta1~normal(0,1000);\ny~normal(mu,sigma);\n}\n\n\"\n\nStan models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).\nA Stan model is defined by six program blocks:\n\nData (required). The data block reads external information – e.g. data vectors, matrices, integers, etc.\nTransformed data (optional). The transformed data block allows for preprocessing of the data – e.g. transformation or rescaling of the data.\nParameters (required). The parameters block defines the sampling space – e.g. parameters to which prior distributions must be assigned.\nTransformed parameters (optional). The transformed parameters block allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.\nModel (required). In the model block we define our posterior distributions – e.g. choice of distributions for all variables.\nGenerated quantities (optional). The generated quantities block allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.\n\nFor this introduction I consider a very simple model which only requires the specification of four blocks in the Stan model. In the data block, I first define the size of the sample n_sim as a positive integer number using the expression int&lt;lower=0&gt; n_sim; then I declare the two variables y and x as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression beta0 and beta1 (as two real numbers) and the standard deviation parameter sigma (as a positive real number). In the transformed parameters block, I define the conditional mean mu (a real vector of length N) as a linear function of the intercept beta0, the slope beta1, and the covariate x. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data y using a normal distribution indexed by the conditional mean mu and the standard deviation sigma parameters. In many cases, Stan uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.\nTo write and save the model as the text file “basic.mod.stan” in the current working directory, I use the writeLines function\n\nwriteLines(basic.mod, \"basic.mod.stan\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#pre-processing",
    "href": "tutorials/2019-07-01-intro-stan/index.html#pre-processing",
    "title": "Super basic introduction to Stan",
    "section": "Pre-processing",
    "text": "Pre-processing\nDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in Stan\n\nparams=c(\"beta0\",\"beta1\")\ninits=function(){list(\"beta0\"=rnorm(1), \"beta1\"=rnorm(1))}\n\nThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object inits which is then passed to the stan function in rstan. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, Stan can automatically select the initial values for all parameters randomly. This can be achieved by setting inits=\"random\", which is then passed to the stan function in rstan.\nBefore using rstan for the first time, you need to load the package, and you may want to set a random seed number for making your estimates reproducible\n\nlibrary(rstan)\nset.seed(123)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#fit-the-model",
    "href": "tutorials/2019-07-01-intro-stan/index.html#fit-the-model",
    "title": "Super basic introduction to Stan",
    "section": "Fit the model",
    "text": "Fit the model\nNow, we can fit the model in Stan using the stan function in the rstan package and save it in the object basic.mod\n\nbasic.mod&lt;-stan(data = datalist, pars = params, iter = 9000, \n  warmup = 1000, init = inits, chains = 2, file = \"basic.mod.stan\")\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 2.2e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 9000 [  0%]  (Warmup)\nNA Chain 1: Iteration:  900 / 9000 [ 10%]  (Warmup)\nNA Chain 1: Iteration: 1001 / 9000 [ 11%]  (Sampling)\nNA Chain 1: Iteration: 1900 / 9000 [ 21%]  (Sampling)\nNA Chain 1: Iteration: 2800 / 9000 [ 31%]  (Sampling)\nNA Chain 1: Iteration: 3700 / 9000 [ 41%]  (Sampling)\nNA Chain 1: Iteration: 4600 / 9000 [ 51%]  (Sampling)\nNA Chain 1: Iteration: 5500 / 9000 [ 61%]  (Sampling)\nNA Chain 1: Iteration: 6400 / 9000 [ 71%]  (Sampling)\nNA Chain 1: Iteration: 7300 / 9000 [ 81%]  (Sampling)\nNA Chain 1: Iteration: 8200 / 9000 [ 91%]  (Sampling)\nNA Chain 1: Iteration: 9000 / 9000 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.04 seconds (Warm-up)\nNA Chain 1:                0.277 seconds (Sampling)\nNA Chain 1:                0.317 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 6e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 9000 [  0%]  (Warmup)\nNA Chain 2: Iteration:  900 / 9000 [ 10%]  (Warmup)\nNA Chain 2: Iteration: 1001 / 9000 [ 11%]  (Sampling)\nNA Chain 2: Iteration: 1900 / 9000 [ 21%]  (Sampling)\nNA Chain 2: Iteration: 2800 / 9000 [ 31%]  (Sampling)\nNA Chain 2: Iteration: 3700 / 9000 [ 41%]  (Sampling)\nNA Chain 2: Iteration: 4600 / 9000 [ 51%]  (Sampling)\nNA Chain 2: Iteration: 5500 / 9000 [ 61%]  (Sampling)\nNA Chain 2: Iteration: 6400 / 9000 [ 71%]  (Sampling)\nNA Chain 2: Iteration: 7300 / 9000 [ 81%]  (Sampling)\nNA Chain 2: Iteration: 8200 / 9000 [ 91%]  (Sampling)\nNA Chain 2: Iteration: 9000 / 9000 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.041 seconds (Warm-up)\nNA Chain 2:                0.327 seconds (Sampling)\nNA Chain 2:                0.368 seconds (Total)\nNA Chain 2:\n\n\nDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the bayesplot package (Gabry and Mahr (2017)) to obtain graphical diagnostics and results.\n\ninstall.packages(\"bayesplot\")\nlibrary(bayesplot)\n\nFor example, density and trace plots can be obtained by typing\n\nmcmc_combo(as.array(basic.mod),regex_pars=\"beta0|beta1\")\n\n\n\n\n\n\n\n\nBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains)."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nMissing Data\n\n\nHealth Economics\n\n\n\nmissingHE is a R package aimed at providing some useful tools to analysts in order to handle missing outcome data under a Full Bayesian framework in economic evaluations … \n\n\n\n\n\nMar 21, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/volley/volley.html",
    "href": "research/volley/volley.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or “modules” form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match (\\(y_h\\) and \\(y_a\\)); (2) the module of the binary indicator for the number of sets played (\\(d^s\\)); (3) the module of the binary indicator for the winner of the match (\\(d^m\\)). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\n\n\nIn the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters.\n\n\n\nIn the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]\n\n\n\nThe last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\).\n\n\n\nAlthough the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "href": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters."
  },
  {
    "objectID": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "href": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]"
  },
  {
    "objectID": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "href": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "The last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\)."
  },
  {
    "objectID": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "href": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Although the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/reviewNL/reviewNL.html",
    "href": "research/reviewNL/reviewNL.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Introduction\nIn the Netherlands, the Dutch National Health Care Institute (Zorginstituut Nederland or ZIN) is the body in charge of issuing recommendations and guidance on good practice in health economic evaluations, not just for pharmaceutical products, but also in relation to other fields of application such as medical devices, long-term care and forensics. In 2016, ZIN issued an update on the guidance for health economic evaluations, which aggregated into a single document and revised three separately published guidelines for pharmacoeconomics evaluation, outcomes research and costing manual. The novel aspects and future policy direction introduced by these guidelines have already been object of discussion, particularly with respect to the potential impact and concerns associated with their implementation in standard health economics practice in the Netherlands. Given the importance covered by these guidelines, an assessment of their impact on economic evaluation practice is desirable.\nThe objective of this paper was to review the evolution of health economic evaluation practice in the Netherlands before and after the introduction of the ZIN’s 2016 guidelines. Based on some key components within the health economics framework addressed by the new guidelines, we specifically focus on reviewing the statistical methods, missing data methods and software implemented by health economists. Given the intrinsic complexity of analysing health economics data, the choice of the analytical approaches to deal with these problems as well as transparent information on their implementation is crucial in determining the degree of confidence that decision-makers should have towards cost-effectiveness results obtained from these studies\n\n\nThe ZIN 2016 guidelines\nThe main objective of the guidelines is to ensure the comparability and quality of health economic evaluations in the Netherlands, therefore facilitating the task of the decision-maker regarding the reimbursement of new health care interventions. Following the example of guidelines issued by decision-making bodies in other countries, including the National Institute for Health and Care Excellence (NICE) in the UK, the recommended features for economic evaluations are summarised in a typical scenario referred to as ‘reference case’, although deviations from it are allowed when properly justified.\nBased on the structure of the reference case, four essential components of a health economic evaluation are identified: framework, analytic approach, input data and reporting. For the purpose of the review, we only focus on these components in the reference case as the main elements upon which evaluating health economics practice.\n\n\nMethods\nWe performed a bibliographic search in June 2021 using search engines of two online full-text journal repositories: (1) PubMed and (2) Zorginstituut. These sources were chosen to maximise the number of studies that could be accessed given the scoping nature of the review and the lack of a search strategy based on a pre-defined and rigid approach typical of systematic reviews. Articles were considered eligible for the review only if they were cost-effectiveness or cost-utility analyses targeting a Dutch population. To allow the inclusion of a reasonable amount of studies, the key words used in the search strategy were (cost-effectiveness OR cost-utility OR economic evaluation), and we targeted studies published between January 2016 and April 2021.\n\n\nAnalytic approaches\nAlmost all reviewed empirical analyses used bootstrapping (95%), although the number of replications varied largely across the studies, with the most popular choices being 5000 (55%) followed by 2000 (29%). Studies showed even more variability in the choice of the methods used in combination with bootstrapping. Seven general classes of statistical approaches were identified, among which unadjusted methods were the most popular choice across both time periods. A clear change in the type of statistical methods used between the two periods is denoted by a strong decrease (from 64 to 39) in the number of unadjusted analyses in 2016–2020 compared to the earlier period, which is compensated by a rise in the number of adjusted analyses using either SUR or linear mixed effects model (LMM) methods.\nFrom Figure 1 we can look at the different type and combination of software programs used as an indication of the implementation preferences of analysts for health economic evaluations. Although in principle the choice of software should have no impact on the quality of the statistical methods implemented, it has been highlighted how use of simpler software (e.g. spreadsheet calculators such as Excel) may become increasingly cumbersome for matching more realistic and therefore complex modelling requirements.\n\n\n\n\n\n\nFigure 1: Heatmap of the combination of software programs used\n\n\n\nThe most popular software was SPSS, chosen by 87 (52%) of the studies, either in the base-case (33%) or secondary (19%) analyses, often used in combination with Excel or by itself. When either STATA (26%) or R (13%) was used in the base-case analysis, SPSS was still the most popular choice in secondary analyses. Other combinations of software were less frequently chosen, even though 38 (23%) of the studies were unclear about the software implemented.\n\n\nMissing data methods\nAcross both periods limited changes are observed in terms of order of choice for missing data methods, with MI being the most popular base-case analysis, followed by complete case analysis (CCA), as the most popular SA choice. However, two noticeable variations in the frequency of these methods are observed between the two periods. First, the proportion of studies using MI in the base-case analysis has considerably increased over time (from 28 to 39%), which is compensated by a decrease in the proportion of less advanced methods such as CCA (from 14 to 5%) and single imputation (SI) (from 21 to 16%). Second, the number of studies not clearly reporting the methods has also considerably decreased (from 12 to 5%). The observed trend between the two periods may be the result of the specific recommendations from the 2016 guidelines in regards to the ‘optimal’ missing data strategy, resulting in a more frequent adoption of MI techniques and, at the same time, a less frequent use of CCA in the base-case analysis. However, in contrast to these guidelines, a large number of studies still does not perform any SA to missing data assumptions (about 65% in 2010–2015 and 63% in 2016–2020).\nMost of the studies lie in the middle and lower parts of the plot, and are associated with a limited or sufficient quality of information. However, only a few of these studies rely on very strong and unjustified missing data assumptions, while the majority provides either adequate justifications or uses methods associated with weak assumptions. Only 11 (14%) studies are associated with both high-quality scores and less restrictive missingness assumptions. No study was associated with either full information or adequate justifications for the assumptions explored in base-case and sensitivity analysis.\n\n\nDiscussion\nDescriptive information extracted from the reviewed studies provides some first insights about changes in practice in the years following the publication of the guidelines. First, a clear trend is observed towards an increase in the adoption of a societal and health care perspective and of CUA as the reference base-case analysis approach. Second, a similar increment is observed in the use of recommended instruments for the collection and valuation of health economic outcomes, such as EQ-5D-5L for QALYs and friction method for costs. Most of these changes are in accordance with the 2016 guidelines, which are likely to have played a role in influencing analysts and practitioners towards a clearer and more standardised way to report health economic results.\nWhen looking at the type of statistical methods used to perform the analysis, an important shift occurs between the two periods towards the use of methods that allow for regression adjustment, with a considerable uptake in the use of SURs and LMMs in the context of empirical analyses. These techniques are strongly supported by the 2016 guidelines in that they allow us to correct for potential bias due to confounding effects, deal with clustered data and formally take into account the correlation between costs and effects. Bootstrapping remains the most popular methods to quantify uncertainty around parameter estimates across both periods. However, the health economic analysis framework requires that the level of complexity of the analysis model is reflected in the way uncertainty surrounding the estimates is generated.\nThe transition between the two time periods reveals an increase in the use of MI techniques in the base-case analysis together with a decrease in the overall use of CCA. This change is in line with the 2016 guidelines which warns about the inherent limitations and potential bias of simple methods (e.g. CCA) when compared to MI as the potential reference method to handle missing values. Nevertheless, improvements are still needed given that many studies (more than 6%) performed the analysis under a single missing data assumption. This is not ideal since by definition missing data assumptions can never be checked, making the results obtained under a specific method (i.e. assumption) potentially biased.\n\n\nConclusions\nGiven the complexity of the health economics framework, the implementation of simple but likely inadequate analytic approaches may lead to imprecise cost-effectiveness results. This is a potentially serious issue for bodies such as ZIN in the Netherlands that use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new health care interventions. Our review shows, over time, a change in common practice with respect to different analysis components in accordance with the recent ZIN’s 2016 guidelines. This is an encouraging movement towards the standardised use of more suitable and robust analytic methods in terms of both statistical, uncertainty and missing data analysis. Improvements are however still needed, particularly in the choice of adequate statistical techniques to deal with the complexity of the data analysed and in the assessment of the impact of alternative missing data assumptions on the results in SA."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html",
    "href": "research/mnarHTA/mnarHTA.html",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "",
    "text": "Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "href": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "Modelling framework",
    "text": "Modelling framework\nThe distribution of the observed responses \\(\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})\\) is specified in terms of a model for the utility and cost variables at time \\(j=\\{0,1,2\\}\\), which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for \\(\\boldsymbol y_{ijt}\\) is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on \\([0,1]\\) through the transformation \\(u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}\\), and fit the model to these transformed variables. To account for the structural values \\(u_{ij} = 1\\) and \\(c_{ij} = 0\\) we use a hurdle approach by including in the model the indicator variables \\(d^u_{ij}:=\\mathbb{I}(u_{ij}=1)\\) and \\(d^c_{ij}:=\\mathbb{I}(c_{ij}=0)\\), which take value \\(1\\) if subject \\(i\\) is associated with a structural value at time \\(j\\) and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time \\(j=1,2\\), the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at \\(j=0\\)) and the utilities at the previous times (only at \\(j=\\{1,2\\}\\)). The model is summarised by Figure 1:\n\n\n\n\n\n\nFigure 1: Longitudinal model for missingness.\n\n\n\nWe use partial identifying restrictions to link the observed data distribution \\(p(\\boldsymbol y_{obs},\\boldsymbol r)\\) to the extrapolation distribution \\(p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)\\) and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern \\(\\boldsymbol y^{\\boldsymbol r}_{mis}\\) by averaging across the corresponding components that are observed and add the sensitivity parameters \\(\\boldsymbol \\Delta_j\\).\nWe define \\(\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})\\) to be time-specific location shifts at the marginal mean in each pattern and set \\(\\boldsymbol \\Delta_j = \\boldsymbol 0\\) as the benchmark scenario. We then explore departures from this benchmark using alternative priors on \\(\\boldsymbol \\Delta_j\\), which are calibrated using the observed standard deviations for costs and utilities at each time \\(j\\) to define the amplitude of the departures from \\(\\boldsymbol \\Delta_j=\\boldsymbol 0\\)."
  },
  {
    "objectID": "research/lmmHTA/lmmHTA.html",
    "href": "research/lmmHTA/lmmHTA.html",
    "title": "Linear mixed models to handle missing at random data in trial based economic evaluations",
    "section": "",
    "text": "Introduction\nCost‐effectiveness analyses (CEAs) conducted alongside randomised controlled trials are an important source of information for decision-makers in the process of technology appraisal (Ramsey et al., 2015). The analysis is based on healthcare outcome data and health service use, typically collected at multiple time points and then combined into overall measures of effectiveness and cost. A popular approach to handle missingness is to discard the participants with incomplete observations (complete case analysis or CCA), allowing for derivation of the overall measures based on the completers alone. We note that slightly different definitions of CCA are possible, depending on the form of the model of interest, the type of missingness and the inclusion of observed covariates. This approach, although appealing by its simplicity, has well-recognised limitations including loss of efficiency and an increased risk of bias. We propose the use of linear mixed effects models (LMMs) as an alternative approach under MAR. LMMs are commonly used for the modelling of dependent data (e.g. repeated-measures) and belong to the general class of likelihood-based methods. LMMs appear surprisingly uncommon for the analysis of repeated measures in trial-based CEA, perhaps because of a lack of awareness or familiarity with fitting LMMs.\n\n\nMethods\nLinear mixed model extends the usual linear model framework by the addition of “random effect” terms, which can take into account the dependence between observations.\n\\[\nY_{ij}=\\beta_1+\\beta_2 X_{i1}+\\ldots+\\beta_(P+1) X_{iP}+\\omega_i+\\epsilon_{ij},\n\\tag{1}\\]\nwhere \\(Y_{ij}\\) denotes the outcome repeatedly collected for each individual \\(i=1,\\ldots,N\\) at multiple times \\(j=1,\\ldots,J\\). The model parameters commonly referred to as fixed effects include an intercept \\(\\beta_1\\) and the coefficients \\((\\beta_2,\\ldots,\\beta_{(P+1)})\\) associated with the predictors \\(X_{i1},\\ldots,X_{iP}\\), while \\(\\omega_i\\) and \\(\\epsilon_{ij}\\) are two random terms: \\(\\epsilon_{ij}\\) is the usual error term and \\(\\omega_i\\) is a random intercept which captures variation in outcomes between individuals. The models can be extended to deal with more complex structures, for example by allowing the effect of the covariates to vary across individuals (random slope) or a different covariance structure of the errors. LMMs can be fitted even if some outcome data are missing and provide correct inferences under MAR.\nA particular type of LMMs commonly used in the analysis of repeated measures in clinical trials is referred to as Mixed Model for Repeated Measurement (MMRM). The model includes a categorical effect for time, an interaction between time and treatment arm, and allows errors to have different variance and correlation over time (i.e. unstructured covariance structure). Figure 1 shows some examples of possible covariance structures that may be explored for LMMs.\n\n\n\n\n\n\nFigure 1: Some examples of covariance structures in LMM\n\n\n\nIncremental (between-group) or marginal (within-group) estimates for aggregated outcomes over the trial period, such as quality-adjusted life years (QALYs) or total costs can be retrieved as linear combinations of the parameter estimates from Equation 1. For example, the mean difference in total cost is obtained by summing up the estimated differences at each follow-up point, while differences on a QALY scale can be obtained as weighted linear combinations of the coefficient estimates of the utility model.\n\n\nConclusions\nWe believe LMMs represent an alternative approach which can overcome some of these limitations.\n\nFirst, practitioners may be more comfortable with the standard regression framework.\nSecond, LMMs can be tailored to address other data features (e.g. cluster-randomised trials or non-normal distribution) while also easily combined with bootstrapping.\nThird, LMMs do not rely on imputation, and results are therefore deterministic and easily reproducible, whereas the Monte Carlo error associated with multiple imputation may cause results to vary from one imputation to another, unless the number of imputations is sufficiently large.\n\nAlthough the methodology illustrated is already known, particularly in the area of statistical analyses, to our knowledge LMMs have rarely been applied to health economic data collected alongside randomised trials. We believe the proposed methods is preferable to a complete-case analysis when CEA data are incomplete, and that it can offer an interesting alternative to imputation methods."
  },
  {
    "objectID": "research/hurdleHTA/hurdleHTA.html",
    "href": "research/hurdleHTA/hurdleHTA.html",
    "title": "Bayesian Modelling for Health Economic Evaluations",
    "section": "",
    "text": "Modelling Framework\nWe propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables \\((e_{it}, c_{it})\\) calculated for the \\(i-\\)th person in group \\(t\\) of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator \\(t\\). Equation 1 specifies the joint distribution \\(p(e_i,c_i)\\) as\n\\[\np(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i)\n\\tag{1}\\]\nwhere, for example, \\(p(e_i)\\) is the marginal distribution of the QALYs and \\(p(c_i\\mid e_i)\\) is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. \\(p(e_i)\\) or \\(p(e_i\\mid c_i)\\), which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either \\(e_i\\) determines \\(c_i\\) or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution \\(p(e_i,c_i)\\) in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution \\(p(e_i \\mid \\boldsymbol \\theta_e)\\) indexed by a set of parameters \\(\\boldsymbol \\theta_e\\) comprising a location \\(\\boldsymbol \\phi_{ie}\\) and a set of ancillary parameters \\(\\boldsymbol\\psi_e\\) typically including some measure of marginal variance \\(\\sigma^2_e\\). Equation 2 models the location parameter using a generalised linear structure\n\\[\ng_e(\\phi_{ie})= \\alpha_0 \\,\\,[+ \\ldots]\n\\tag{2}\\]\nwhere \\(\\alpha\\_0\\) is the intercept and the notation \\([+\\ldots]\\) indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version \\(x_i^{\\star} = (x_i - \\bar{x})\\) is used, the parameter \\(\\mu_e = g_e^{-1}(\\alpha_0)\\) represents the population average QALYs. For the costs, we consider a conditional model \\(p(c_i\\mid e_i,\\boldsymbol\\theta_c)\\), which explicitly depends on the QALYs, as well as on a set of quantities \\(\\boldsymbol\\theta_c\\), again comprising a location \\(\\phi_{ic}\\) and ancillary parameters \\(\\boldsymbol \\psi_{c}\\). For example, when normal distributions are assumed for both \\(p(e_i \\mid \\boldsymbol \\theta\\_e)\\) and \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\), i.e. bivariate normal on both outcomes, the ancillary parameters \\(\\boldsymbol\\psi_c\\) include a conditional variance \\(\\tau^2_c\\), which can be expressed as a function of the marginal variance \\(\\sigma^2_c\\). More specifically, the conditional variance of \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\) is a function of the marginal effectiveness and cost variances and has the closed form \\(\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2\\), where \\(\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}\\) and \\(\\rho\\) is the parameter capturing the correlation between the variables.\nEquation 3 models the location as a function of the QALYs as\n\\[\ng\\_c(\\phi\\_{ic}) = \\beta\\_{0} + \\beta\\_{1}(e\\_{i}-\\mu\\_{e})\\,\\,[+\\ldots]\n\\tag{3}\\]\nHere, \\((e_i-\\mu_e)\\) is the centered version of the QALYs, while \\(\\beta_{1}\\) quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, \\(\\mu_c = g_c^{-1}(\\beta_{0})\\) is the estimated population average cost. The Figure 1 shows a graphical representation of the general modelling framework.\n\n\n\n\n\n\nFigure 1: Modelling framework.\n\n\n\nThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\n\n\nExample\nThree model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. Figure 2 shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and \\(90\\%\\) HPD intervals.\n\n\n\n\n\n\nFigure 2: Imputed QALYs under alternative model specifications.\n\n\n\nThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\n\n\nConclusions\nWe have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n\nJointly model costs and QALYs;\nAccount for skewness and structural values;\nAssess the robustness of the results under a set of differing missingness assumptions.\n\nThe original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nEconomic evaluations have been increasingly conducted in different countries to aid national decision-making bodies in resource allocation problems … \n\n\n\n\n\nJun 3, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLinear mixed models to handle missing at random data in trial‐based economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nTrial-based cost-effectiveness analyses (CEAs) are an important source of evidence in the assessment of health interventions … \n\n\n\n\n\nApr 10, 2022\n\n\nAndrea Gabrio, Catrin Plumpton, Sube Banerjee, Baptiste Leurent\n\n\n\n\n\n\n\n\n\n\n\n\nA Scoping Review of Item-Level Missing Data in Within-Trial Cost-Effectiveness Analysis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nCost-effectiveness analysis (CEA) alongside randomized controlled trials often relies on self-reported multi-item questionnaires … \n\n\n\n\n\nMar 10, 2022\n\n\nXiaoxiao Ling, Andrea Gabrio, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nA Bayesian framework for patient-level partitioned survival cost-utility analysis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nPatient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal … \n\n\n\n\n\nNov 17, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nJoint longitudinal models for dealing with missing at random data in trial-based economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nHealth economic evaluations based on patient-level data collected alongside clinical trials (e.g. health related quality of life and resource use measures) are an important component … \n\n\n\n\n\nMay 11, 2020\n\n\nAndrea Gabrio, Rachael M Hunter, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Hierarchical Models for the Prediction of Volleyball Results\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nstatistics\n\n\n\nStatistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives … \n\n\n\n\n\nNov 22, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nTrial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study … \n\n\n\n\n\nSep 26, 2019\n\n\nAndrea Gabrio, Michael J Daniels, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistical Economic Evaluation Methods for Health Technology Assessment\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nThe evidence produced by healthcare economic evaluation studies is a key component of any health technology assessment (HTA) process … \n\n\n\n\n\nJun 1, 2019\n\n\nAndrea Gabrio, Gianluca Baio, Andrea Manca\n\n\n\n\n\n\n\n\n\n\n\n\nA Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nEconomic evaluations from individual level data are an important component of the process of technology appraisal … \n\n\n\n\n\nApr 1, 2019\n\n\nAndrea Gabrio, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nCost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed … \n\n\n\n\n\nJun 1, 2017\n\n\nAndrea Gabrio, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/2022-b-my-publication/index.html",
    "href": "publication/2022-b-my-publication/index.html",
    "title": "Linear mixed models to handle missing at random data in trial‐based economic evaluations",
    "section": "",
    "text": "Abstract\nTrial-based cost-effectiveness analyses (CEAs) are an important source of evidence in the assessment of health interventions. In these studies, cost and effectiveness outcomes are commonly measured at multiple time points, but some observations may be missing. Restricting the analysis to the participants with complete data can lead to biased and inefficient estimates. Methods, such as multiple imputation, have been recommended as they make better use of the data available and are valid under less restrictive Missing At Random (MAR) assumption. Linear mixed effects models (LMMs) offer a simple alternative to handle missing data under MAR without requiring imputations, and have not been very well explored in the CEA context. In this manuscript, we aim to familiarise readers with LMMs and demonstrate their implementation in CEA. We illustrate the approach on a randomised trial of antidepressant, and provide the implementation code in R and Stata. We hope that the more familiar statistical framework associated with LMMs, compared to other missing data approaches, will encourage their implementation and move practitioners away from inadequate methods.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2022,\n  author = {Gabrio, Andrea and Plumpton, Catrin and Banerjee, Sube and\n    Leurent, Baptiste},\n  title = {Linear Mixed Models to Handle Missing at Random Data in\n    Trial‐based Economic Evaluations},\n  volume = {31},\n  number = {6},\n  date = {2022-04-10},\n  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/hec.4510},\n  doi = {10.1002/hec.4510},\n  langid = {en},\n  abstract = {{[}Trial-based cost-effectiveness analyses (CEAs) are an\n    important source of evidence in the assessment of health\n    interventions ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Catrin Plumpton, Sube Banerjee, and Baptiste Leurent.\n2022. “Linear Mixed Models to Handle Missing at Random Data in\nTrial‐based Economic Evaluations.” Health Economics. April 10,\n2022. https://doi.org/10.1002/hec.4510."
  },
  {
    "objectID": "publication/2020-b-my-publication/index.html",
    "href": "publication/2020-b-my-publication/index.html",
    "title": "A Bayesian framework for patient-level partitioned survival cost-utility analysis",
    "section": "",
    "text": "Abstract\nPatient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. For end of life treatments, such as cancer treatments, modelling of cost-effectiveness/utility data may involve some form of partitioned survival analysis, where measures of health-related quality of life and survival time for both pre- and post-progression periods are combined to generate some aggregate measure of clinical benefits (e.g. quality-adjusted survival). In addition, resource use data are often collected from health records on different services from which different cost components are obtained (e.g. treatment, hospital or adverse events costs). A critical problem in these analyses is that both effectiveness and cost data present some complexities, including non-normality, spikes, and missingness, that should be addressed using appropriate methods. Bayesian modelling provides a powerful tool which has become more and more popular in the recent health economics and statistical literature to jointly handle these issues in a relatively easy way. This paper presents a general Bayesian framework that takes into account the complex relationships of trial-based partitioned survival cost-utility data, potentially providing a more adequate evidence for policymakers to inform the decision-making process. Our approach is motivated by, and applied to, a working example based on data from a trial assessing the cost-effectiveness of a new treatment for patients with advanced non-small-cell lung cancer.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2020,\n  author = {Gabrio, Andrea},\n  title = {A {Bayesian} Framework for Patient-Level Partitioned Survival\n    Cost-Utility Analysis},\n  volume = {41},\n  number = {8},\n  date = {2020-11-17},\n  url = {https://journals.sagepub.com/doi/full/10.1177/0272989X211012348},\n  doi = {10.1177/0272989X211012348},\n  langid = {en},\n  abstract = {{[}Patient-level health economic data collected alongside\n    clinical trials are an important component of the process of\n    technology appraisal ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2020. “A Bayesian Framework for Patient-Level\nPartitioned Survival Cost-Utility Analysis.” Medical Decision\nMaking. November 17, 2020. https://doi.org/10.1177/0272989X211012348."
  },
  {
    "objectID": "publication/2019-d-my-publication/index.html",
    "href": "publication/2019-d-my-publication/index.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Abstract\nStatistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women’s volleyball Italian Serie A1 2017-2018 season.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea},\n  title = {Bayesian {Hierarchical} {Models} for the {Prediction} of\n    {Volleyball} {Results}},\n  volume = {48},\n  number = {2},\n  date = {2019-11-22},\n  url = {https://www.tandfonline.com/doi/full/10.1080/02664763.2020.1723506?casa_token=biq9li_YO4gAAAAA%3AusWgpHzm1x-FNL0tOjGOCyYYwUMpeXXmoAqQBVlWKqpgzHYO1wIb8uVfkfl4JwSvHJIk5_P1kOY},\n  doi = {10.1080/02664763.2020.1723506},\n  langid = {en},\n  abstract = {{[}Statistical modelling of sports data has become more\n    and more popular in the recent years and different types of models\n    have been proposed to achieve a variety of objectives\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2019. “Bayesian Hierarchical Models for the\nPrediction of Volleyball Results.” Journal of Applied Statistics.\nNovember 22, 2019. https://doi.org/10.1080/02664763.2020.1723506."
  },
  {
    "objectID": "publication/2019-b-my-publication/index.html",
    "href": "publication/2019-b-my-publication/index.html",
    "title": "Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment",
    "section": "",
    "text": "Abstract\nThe evidence produced by healthcare economic evaluation studies is a key component of any health technology assessment (HTA) process designed to inform resource allocation decisions in a budget limited context. To improve the quality (and harmonize the generation process) of such evidence, many HTA agencies have established methodological guidelines describing the normative framework inspiring their decision-making process. The information requirements that economic evaluation analyses for HTA must satisfy typically involve the use of complex quantitative syntheses of multiple available datasets, handling mixtures of aggregate and patient-level information, and the use of sophisticated statistical models for the analysis of non-Normal data (e.g. time-to-event, quality of life and costs). Much of the recent methodological research in economic evaluation for healthcare has developed in response to these needs, in terms of sound statistical decision-theoretic foundations, and is increasingly being formulated within a Bayesian paradigm. The rationale for this preference lies in the fact that by taking a probabilistic approach, based on decision rules and available information, a Bayesian economic evaluation study can explicitly account for relevant sources of uncertainty in the decision process and produce information to identify an optimal course of actions. Moreover, the Bayesian approach naturally allows the incorporation of an element of judgement or evidence from different sources (e.g.~expert opinion or multiple studies) into the analysis. This is particularly important when, as often occurs in economic evaluation for HTA, the evidence base is sparse and requires some inevitable mathematical modelling to bridge the gaps in the available data. The availability of free and open source software in the last two decades has greatly reduced the computational costs and facilitated the application of Bayesian methods and has the potential to improve the work of modellers and regulators alike, thus advancing the fields of economic evaluation of health care interventions. This chapter provides an overview of the areas where Bayesian methods have contributed to the address the methodological needs that stem from the normative framework adopted by a number of HTA agencies.\n\n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and Baio, Gianluca and Manca, Andrea},\n  title = {Bayesian {Statistical} {Economic} {Evaluation} {Methods} for\n    {Health} {Technology} {Assessment}},\n  date = {2019-06-01},\n  url = {https://oxfordre.com/economics/view/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-451},\n  doi = {10.1093/acrefore/9780190625979.013.451},\n  langid = {en},\n  abstract = {{[}The evidence produced by healthcare economic evaluation\n    studies is a key component of any health technology assessment (HTA)\n    process ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Gianluca Baio, and Andrea Manca. 2019. “Bayesian\nStatistical Economic Evaluation Methods for Health Technology\nAssessment.” Oxford Research Encyclopedia of Economics and\nFinance, Oxford University Press. June 1, 2019. https://doi.org/10.1093/acrefore/9780190625979.013.451."
  },
  {
    "objectID": "publication/2017-a-my-publication/index.html",
    "href": "publication/2017-a-my-publication/index.html",
    "title": "Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations",
    "section": "",
    "text": "Abstract\nCost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed to collect resource use and preference-based health status data for the purpose of healthcare technology assessment. However, because of the way these measures are collected, they are prone to missing data, which can ultimately affect the decision of whether an intervention is good value for money. We examine how missing cost and effect outcome data are handled in RCT-based CEAs, complementing a previous review (covering 2003-2009, 88 articles) with a new systematic review (2009-2015, 81 articles) focussing on two different perspectives. First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a quality evaluation scheme, providing a structured approach that can be used to guide the collection of information, elicitation of the assumptions, choice of methods and considerations of possible limitations of the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods. Our review shows that missing data in within-RCT CEAs are still often inadequately handled and the overall level of information provided to support the chosen methods is rarely satisfactory\n      \n\n\n\n\nCitationBibTeX citation:@online{gabrio2017,\n  author = {Gabrio, Andrea and J Mason, Alexina and Baio, Gianluca},\n  title = {Handling {Missing} {Data} in {Within-Trial}\n    {Cost-Effectiveness} {Analysis:} {A} {Review} with {Future}\n    {Recommendations}},\n  volume = {1},\n  number = {2},\n  date = {2017-06-01},\n  url = {https://link.springer.com/article/10.1007/s41669-017-0015-6},\n  doi = {10.1007/s41669-017-0015-6},\n  langid = {en},\n  abstract = {{[}Cost-effectiveness analyses (CEAs) alongside randomised\n    controlled trials (RCTs) are increasingly designed\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Alexina J Mason, and Gianluca Baio. 2017.\n“Handling Missing Data in Within-Trial Cost-Effectiveness\nAnalysis: A Review with Future Recommendations.”\nPharmacoeconomics-Open. June 1, 2017. https://doi.org/10.1007/s41669-017-0015-6."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Markov Models in Economic Evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nNov 10, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Tree Models in Economic Evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nIn which Dutch province do people travel the most?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nSep 14, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Dutch province provides the most Olympic medals?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nAug 28, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSome time off …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew website and more !\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nJun 12, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Parameter Logistic Model\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nIRT\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Parameter Logistic Model\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nIRT\n\n\n\n\n\n\n\n\n\nApr 13, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nRasch Model\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nIRT\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nConfounders, Colliders, Mediator. What to adjust for\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy we cannot interpret relative risks in case-control studies\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nJust need a break\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial on using R to conduct trial-based CEA\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian statistics in health economic evaluations - part 2\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian statistics in health economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBack from lola\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nItem-Response Theory: intro\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nIRT\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStructural values in health economics data\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nIRT\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSome updates …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nconference\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe missing data dilemma\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStructural values in health economics data\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHow to jointly handle skewed data in economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHow to handle correlated data in economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMore health economics updates\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHow to handle longitudinal data in economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use bootstrapping in economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nSep 9, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBack from holidays\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nEuHEA 2022 Conference\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nJul 7, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nEngaging with the HTA community\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\npublication\n\n\n\n\n\n\n\n\n\nJun 15, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nR markdown for teaching\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nteaching\n\n\n\n\n\n\n\n\n\nMay 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper out\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAttending conferences and invited talks\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStudying Item Response Theory\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\nIRT\n\n\n\n\n\n\n\n\n\nFeb 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now what?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now what?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 20, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline adjustment in trial based CEA\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nOct 10, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBack to work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHolidays, finally\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\nAug 2, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA nice workshop\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nJul 2, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo hot\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nconference\n\n\n\n\n\n\n\n\n\nJun 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe good, the bad and the ugly\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nconference\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nGoing back to teaching, hurray!\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nteaching\n\n\nconference\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSunny days which I cannot fully enjoy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nMar 25, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDoing some teaching…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\npublication\n\n\nteaching\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Bayesian credible interval?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nIt is Xmas again Yeah\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nDec 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA couple of updates\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\npublication\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy health economists do not care about statistical significance?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStarting a new adventure!\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Bayesian inference?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy be Bayesian\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew tutorials for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSorry, an error occurred\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew updates for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSo much time but also not really\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLiving and working at home is nice, right?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nMar 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLockdown\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nFeb 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nFinally here …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\nconference\n\n\npublication\n\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNot a very good start…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nDec 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo many things, again….\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nCopenhagen, I am coming …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMore good news…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 1, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMissingHE 1.2.1\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussing my thesis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\ninterview\n\n\n\n\n\n\n\n\n\nSep 15, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe P value fallacy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHESG Summer Meeting 2019\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-10-7-my-blog-post/index.html",
    "href": "posts/2024-10-7-my-blog-post/index.html",
    "title": "Decision Tree Models in Economic Evaluations",
    "section": "",
    "text": "Hello folks and welcome back to a new update of my blog, which today is finally back focussing on my main field of interest, health economic evaluations! After a couple of months of detour from the topic just to talk about something different for once, I am happy to go back to health economics and, more specifically, how we can use statistics to perform different types of tasks in cost-effectiveness analyses. In particular, today’s topic is quite a broad one and also something that I am not super familiar with myself, namely a specific type of decision analytic models (DAMs) and reasons for using such approach to perform certain types of health economic analyses. I hope that I will be able to quickly summarise the main concepts related to these models and offer a general overview of what they are used for and why so. As a warning for the casual reader: in this post I may use some terms that are specific to the health economics literature and field without diving into too many explanations; when this is the case, please bear with me and perhaps make a quick online search (or check my previous posts) to check if anything is unclear to you. Also, as a more in depth introduction on this topic, I recommend reading the book Khan (2015), specifically Chapter 6. For a look at how to implement more complex types of decision trees in R and perform different types of ancillary analyses, I recommend checking this nice post"
  },
  {
    "objectID": "posts/2024-10-7-my-blog-post/index.html#decision-tree-models",
    "href": "posts/2024-10-7-my-blog-post/index.html#decision-tree-models",
    "title": "Decision Tree Models in Economic Evaluations",
    "section": "Decision Tree Models",
    "text": "Decision Tree Models\nWhen patient-level data are not available (due to limited follow-up time), an alternative approach to estimating the mean costs, effects and incremental cost-effectiveness ratio (ICER) is required. One way is to use published data or data summarised in the clinical study report to build an overarching economic model using evidence from difference sources. This post is particularly focussed on a type of decision models known as Decison Tree as one of the most popular and simple DAGs to implement to perform economic analyses. Decision trees are normally used to predict what would happen in terms of the outcomes (or payoffs) after a decision has been made about a treatment strategy. Each treatment strategy will involve a series of pathways that will ultimately lead to some kind of outcome. Some examples of outcomes common with decision tree analysis are survival, response, life years, and quality-adjusted life years (QALYs). Another element that is also evaluated is the cost of the treatment strategy. Just like the outcomes, each treatment strategy will have costs associated with each pathway. Summing these up for each pathway, an health economist can determine the total cost of that treatment strategy. Decision trees are used to evaluate the value of a treatment strategy by measuring the outcomes alongside the costs associated with each treatment pathway and comparing these to other treatment strategies (e.g. usual care or standard care). This corresponds to a systematic evaluation of the value of a treatment strategy compared to an alternative treatment strategy."
  },
  {
    "objectID": "posts/2024-10-7-my-blog-post/index.html#a-simple-example",
    "href": "posts/2024-10-7-my-blog-post/index.html#a-simple-example",
    "title": "Decision Tree Models in Economic Evaluations",
    "section": "A simple example",
    "text": "A simple example\nA typical decision tree model structure is shown in ?@fig-dt: the “tree” has branches that relate to either actions or decisions taken by patients or clinicians, or outcomes (such as measures of treatment effect). The starting point is always a Root node (R), followed by Decision nodes (A and B) denoting a decision between alternative treatments, followed in turn by Leaf nodes (A1, A2, B1 and B2) indicating events. Each branch has a probability, graphically represented via arrows (often expressed as a rate, percentage or proportion), and each decision or leaf node is associated with a specific effects/costs (i.e. some form of payoffs). For each branch, the expected costs and effects are calculated by multiplying the probability of decisions/outcomes by the costs/effects for the branch. The expected costs and effects are then summed up for each treatment and used to derive the ICER of the treatments under comparison. A critical part is the determination of the inputs (i.e. justification for the choice of probabilities, rates, costs, etc.), which are typically obtained from either published clinical trials, observational studies or surveys. An important aspect of designing the “branches” of the tree is understanding the treatment pathway, that is, what happens to patients in terms of their treatment plan and how they are likely to be followed during their treatment programme. For example, in a the very simplified tree structure in ?@fig-dt, the pathway might be:\n\nRoot node (R): Patients start taking one of two treatments to treat the disease, let’s say with a chance of 50%, therefore being assigned to one of two alternative decision nodes with a 50% probability (A or B).\nDecision node A (or B): Patients may experience some pain relief from taking one of two treatments, each being associated with specific implementation and delivery costs for each individual (say \\(C_a=1000\\) and \\(C_b=500\\) euros).\nLeaf nodes A1 and A2: About 60% of the patients who undergo treatment A experience pain relief and an improved quality of life (say \\(E_{a1}=0.9\\) QALYs), while the remaining 40% does not experience any pain relief (say \\(E_{a2}=0.6\\) QALYs).\nLeaf nodes B1 and B2: About 30% of the patients who undergo treatment B experience pain relief and an improved quality of life (say \\(E_{b1}=0.8\\) QALYs), while the remaining 70% does not experience any pain relief (say \\(E_{b2}=0.6\\) QALYs).\n\nNow, the objective of decision trees is to estimate the total costs and benefits (in this case measured by QALYs) for the target patient population and the treatments involved so to be able to compare the two alternative options in terms of their total expected costs and benefits generated using standardised health economic measures, e.g. ICER or Net Benefit. For our simple case, we can calculate this in R for an hypothetical patient population cohort of, say, \\(n=100000\\) subjects and using as model inputs the information detailed before for each branch of the decision tree:\n\n#set number of individuals in cohort\nn &lt;- 100000\n#set model inputs\nn_a &lt;- n/2; n_b &lt;- n-n_a\nC_a &lt;- 1000; C_b &lt;- 500 #treatment costs for each individual\nE_a1 &lt;- 0.9; E_a2 &lt;- 0.6 #possible QALYs for each individual in arm A\nE_b1 &lt;- 0.8; E_b2 &lt;- 0.6 #possible QALYs for each individual in arm B\np_a1 &lt;- 0.6; p_a2 &lt;- 1 -p_a1 #prob of resulting in outcome a1 or a2\np_b1 &lt;- 0.3; p_b2 &lt;- 1 -p_b1 #prob of resulting in outcome b1 or b2\np_a &lt;- 0.5 ; p_b &lt;- 1- p_a #prob of being assigned to treat A or B\n\n#create simple function to \"run this model\"\ndectree &lt;- function(n, p_a, p_b, C_a, C_b, E_a1, E_a2, E_b1, E_b2, \n                    p_a1, p_a2, p_b1, p_b2){\n  \n#number of individuals from cohort in each arm\nn_a &lt;- n*p_a; n_b &lt;- n*p_b \n#total costs for using treat A or B    \ncost_a &lt;- C_a*n_a; cost_b &lt;- C_b*n_b\n#number of individuals with each possible QALY outcome\nn_a1 &lt;- n_a*p_a1; n_a2 &lt;- n_a*p_a2\nn_b1 &lt;- n_b*p_b1; n_b2 &lt;- n_b*p_b2\n#total QALYs for each possible leaf node \nQALY_a1 &lt;- n_a1*E_a1; QALY_a2 &lt;- n_a2*E_a2\nQALY_b1 &lt;- n_b1*E_b1; QALY_b2 &lt;- n_b2*E_b2\n#sum up QALY results by arm\nQALYs_a &lt;- QALY_a1 + QALY_a2\nQALYs_b &lt;- QALY_b1 + QALY_b2\n#store all results in terms of QALYs and costs\nCosts &lt;- list(\"A\"=cost_a, \"B\"=cost_b)\nQALYs &lt;- list(\"A\"=QALYs_a, \"B\"=QALYs_b)\noutput &lt;- list(\"Cost\"=Costs, \"QALYs\"=QALYs)\nreturn(output)\n}\n\n#run function\ndt_model &lt;- dectree(n=100000, p_a=0.5, p_b=0.5, C_a=1000, C_b=500, E_a1=0.9, E_a2=0.6, E_b1=0.8, E_b2=0.6, p_a1=0.6, p_a2=0.4, p_b1=0.3, p_b2=0.7)\n\n#print\ndt_model\n\n$Cost\n$Cost$A\n[1] 50000000\n\n$Cost$B\n[1] 25000000\n\n\n$QALYs\n$QALYs$A\n[1] 39000\n\n$QALYs$B\n[1] 33000\n\n\nAs we can see, the model produced as results the total costs and QALYs associated with each treatment arm assuming an initial population cohort size of 100000 and the other model inputs that we provided. We can then derive the average outcomes per treatment arm by dividing the costs and QALYs by the number of individuals assigned to each arm within the cohort, that is:\n\nmu_cA &lt;- dt_model$Cost$A/n_a\nmu_cB &lt;- dt_model$Cost$B/n_b\nmu_eA &lt;- dt_model$QALYs$A/n_a\nmu_eB &lt;- dt_model$QALYs$B/n_b\n\n#avg costs and QALYs in arm A\nmu_cA; mu_eA\n\n[1] 1000\n\n\n[1] 0.78\n\n#avg costs and QALYs in arm B\nmu_cB; mu_eB\n\n[1] 500\n\n\n[1] 0.66\n\n\nUsing these summarises, we can compute standardised measures of cost-effectiveness when comparing treatment A and B, such as the ICER and the Net Benefit (NB), after we make an assumption about a plausible value for the decision-maker preferences expressed in terms of a decision threshold value, say \\(K=10000\\) in our case.\n\nK &lt;- 10000\nICER &lt;- (mu_cA - mu_cB)/(mu_eA-mu_eB)\nNB &lt;- K*(mu_eA-mu_eB) - (mu_cA - mu_cB)\n\nAssuming a value of the threshold parameter of 10000, we can see that treatment A seems to be cost-effective compared to B given the value of the ICER or 4166.6666667 (&lt;K) and a value of the Net Benefit of 700 (&gt;0). Note that for simplicity here I have not considered the need to account for time discount or price inflation which instead may be needed, especially when trying to predict the generation of outcomes in the long term."
  },
  {
    "objectID": "posts/2024-10-7-my-blog-post/index.html#what-about-uncertainty",
    "href": "posts/2024-10-7-my-blog-post/index.html#what-about-uncertainty",
    "title": "Decision Tree Models in Economic Evaluations",
    "section": "What about uncertainty?",
    "text": "What about uncertainty?\nEverything seems so easy right? well this is in reality only part of the story since when looking at the values of point statistics such as ICER and NB we are implicitly assuming that there is no uncertainty about these derived values, i.e. everything is obtained in a deterministic way in the model without any source of uncertainty. In reality, decision-makers will never make a decision about reimbursement on the basis of such strong assumptions as the impact of uncertainty on the results should be at least partially captured from the model. This is where uncertainty analysis plays a key role in any type of health economic analysis since it is realistic to assume that there are many possible sources of uncertainty that may affect our results. For example, we may not be so sure about the actual individual implementation/delivery costs per treatment, the actual QALYs associated with presence or absence of relief for each treatment, or even about the chance of experiencing a relief for someone undergoing a given treatment. More realistically, we can use the inputs provided before about all these scenarios as good indicators around which center our model input values but without assuming their values to be constant and identical for everyone in the cohort. To account for this sort of uncertainty about model input values and quantify their impact on the results, probability distributions are often used as they allow to parameterise in a convenient way the level of uncertainty associated with each input in the model. For example, we may want to use a Uniform distribution to express our level of uncertainty around the probability for a patient to experience pain relief or not in each treatment arm, centering these distributions around the assumed value in the determinsitic analysis as good starting point. We can do the same with other model parameters, such as expected QALYs and costs associated with each treatment, taking care that appropriate probability distributions are used to express our level of uncertainty around specific parameter values. Let’s now try to replicate the model run before using these distributions to account for this sort of uncertainty.\n\n#set number of model replications to account for parameter uncertainty\nS &lt;- 500\n#create simple function to \"run this model\"\ndectree_prob &lt;- function(n, p_a, p_b, C_a_mu, C_a_sigma, C_b_mu, C_b_sigma, \n                    E_a1_mu, E_a1_sigma, E_a2_mu, E_a2_sigma,\n                    E_b1_mu, E_b1_sigma, E_b2_mu, E_b2_sigma,\n                    p_a1_low, p_a1_up, p_a2_low, p_a2_up, \n                    p_b1_low, p_b1_up, p_b2_low, p_b2_up, \n                    S){\n#number of individuals from cohort in each arm\nn_a &lt;- n*p_a; n_b &lt;- n*p_b \n#create empty vectors to contain simulation results\ncost_a &lt;- cost_b &lt;- rep(NA, S)\nQALYs_a &lt;- QALYs_b &lt;- rep(NA, S)\nfor(s in 1:S){\n#assign Gamma distributions to generate costs\nC_a &lt;- rgamma(1,shape=C_a_mu^2/C_a_sigma^2,scale=C_a_sigma^2/C_a_mu)\nC_b &lt;- rgamma(1,shape=C_b_mu^2/C_b_sigma^2,scale=C_b_sigma^2/C_b_mu)\n#assign Uniform distributions to generate prob of each possible QALY outcome\np_a1 &lt;- runif(1,min=p_a1_low,max=p_a1_up)\np_a2 &lt;- 1-p_a1\np_b1 &lt;- runif(1,min=p_b1_low,max=p_b1_up)\np_b2 &lt;- 1-p_b1\nn_a1 &lt;- n_a*p_a1; n_a2 &lt;- n_a*p_a2\nn_b1 &lt;- n_b*p_b1; n_b2 &lt;- n_b*p_b2\n#assign Beta distributions to generate QALY for each possible leaf node \nE_a1 &lt;- rbeta(1, shape1 = (E_a1_mu*(1-E_a1_mu)/E_a1_sigma^2-1)*E_a1_mu, shape2 = (E_a1_mu*(1-E_a1_mu)/E_a1_sigma^2-1)*(1-E_a1_mu))\nE_a2 &lt;- rbeta(1, shape1 = (E_a2_mu*(1-E_a2_mu)/E_a2_sigma^2-1)*E_a2_mu, shape2 = (E_a2_mu*(1-E_a2_mu)/E_a2_sigma^2-1)*(1-E_a2_mu))\nE_b1 &lt;- rbeta(1, shape1 = (E_b1_mu*(1-E_b1_mu)/E_b1_sigma^2-1)*E_b1_mu, shape2 = (E_b1_mu*(1-E_b1_mu)/E_b1_sigma^2-1)*(1-E_b1_mu))\nE_b2 &lt;- rbeta(1, shape1 = (E_b2_mu*(1-E_b2_mu)/E_b2_sigma^2-1)*E_b2_mu, shape2 = (E_b2_mu*(1-E_b2_mu)/E_b2_sigma^2-1)*(1-E_b2_mu))\nQALY_a1 &lt;- n_a1*E_a1; QALY_a2 &lt;- n_a2*E_a2\nQALY_b1 &lt;- n_b1*E_b1; QALY_b2 &lt;- n_b2*E_b2\n#sum up QALY results by arm\n#store results for simualtion S\ncost_a[s] &lt;- C_a*n_a\ncost_b[s] &lt;- C_b*n_b\nQALYs_a[s] &lt;- QALY_a1 + QALY_a2\nQALYs_b[s] &lt;- QALY_b1 + QALY_b2\n}\n#store all simulation results in terms of QALYs and costs\nCosts &lt;- list(\"A\"=cost_a, \"B\"=cost_b)\nQALYs &lt;- list(\"A\"=QALYs_a, \"B\"=QALYs_b)\noutput &lt;- list(\"Cost\"=Costs, \"QALYs\"=QALYs)\nreturn(output)\n}\n\n#run function\nset.seed(2345) #set rng seed for replicability\ndt_model_prob &lt;- dectree_prob(n=100000, p_a=0.5, p_b=0.5, C_a_mu=1000, C_a_sigma=200, C_b_mu=500, C_b_sigma=150, E_a1_mu=0.9, E_a1_sigma=0.15, E_a2_mu=0.6, E_a2_sigma=0.1, E_b1_mu=0.8, E_b1_sigma=0.15, E_b2_mu=0.6, E_b2_sigma=0.1,p_a1_low=0.35, p_a1_up=0.45, p_b1_low=0.25, p_b1_up=0.35, S=S)\n\n#show first 10 simulations for total costs and QALY values per arm\nlapply(dt_model_prob$Cost, head, 10)\n\n$A\n [1] 37915244 51911020 68692051 40656708 39710123 45713125 56910465 38386590\n [9] 62994357 63095265\n\n$B\n [1] 12404090 27321412 14019824 25987217 35368975 21888540 19007477 21374435\n [9] 17313133 25258554\n\nlapply(dt_model_prob$QALYs, head, 10)\n\n$A\n [1] 35783.21 40150.06 37249.58 39033.29 37068.99 43786.33 29538.04 35267.35\n [9] 40420.99 36133.27\n\n$B\n [1] 36926.62 36105.92 29120.86 37453.09 38721.19 37408.60 35173.30 38094.58\n [9] 31830.00 36830.35\n\n\nWe have changed the type of inputs to be provided to the model since now we are using probability distributions to generate input values, which require the need to specify the assumed values associated with the parameters indexing such distributions. For example, we require now the specification of mean and standard deviation for cost inputs per treatment arm (modelled using Gamma distributions), mean and standard deviation for QALY inputs per treatment arm per type of outcome, i.e. pain relief or no relief (modelled using Beta distributions), and lower and upper bounds for the probability of experiencing pain relief in each arm (modelled using Uniform distributions). The core structure of the model remains the same but the need to account for paraemter uncertainty requires the generation of different outcomes depending on the specific values generated by the assumed distributions every time a new “random sample” is drawn from such distributions. The combination of these generated values put together will allow to represent and quantify the uncertainty surrounding the possible outcomes associated with each treatment. In this simple case, we choose a number of replications/simulations equal to 500 but, in general, larger values are typically required to ensure that results are robust to slight variations of the input parameters.\nWe can now replicate the approach used for the deterministic model by summarising the results in terms of common health economic measures, such as ICER and NB, assuming a acceptance threshold parameter of 10000 for decision-making purposes.\n\n#obtain mean outcome values per treatment per simulation\nmu_cA &lt;- dt_model_prob$Cost$A/n_a\nmu_cB &lt;- dt_model_prob$Cost$B/n_b\nmu_eA &lt;- dt_model_prob$QALYs$A/n_a\nmu_eB &lt;- dt_model_prob$QALYs$B/n_b\n\n#create data frame\nmu_res &lt;- data.frame(mu_cA,mu_cB,mu_eA,mu_eB)\n\n#summary statistics for costs and QALYs by arm across simulations\nsummary(mu_res) \n\n     mu_cA            mu_cB            mu_eA            mu_eB       \n Min.   : 490.4   Min.   : 202.9   Min.   :0.3749   Min.   :0.3903  \n 1st Qu.: 860.1   1st Qu.: 401.0   1st Qu.:0.6798   1st Qu.:0.6062  \n Median : 990.4   Median : 486.3   Median :0.7301   Median :0.6625  \n Mean   : 995.9   Mean   : 501.5   Mean   :0.7231   Mean   :0.6616  \n 3rd Qu.:1127.5   3rd Qu.: 584.1   3rd Qu.:0.7817   3rd Qu.:0.7236  \n Max.   :1565.7   Max.   :1051.5   Max.   :0.9139   Max.   :0.8721  \n\nICER &lt;- (mu_cA - mu_cB)/(mu_eA - mu_eB)\nNB &lt;- K*(mu_eA-mu_eB) - (mu_cA - mu_cB)\n\n#summary statistics for ICER/NB across simulations\nsummary(ICER) \n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-4077649    -2351     2888   -14177     6623   807046 \n\nsummary(NB)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-4096.7  -597.5   124.3   121.1   988.6  3701.5 \n\n\nThanks to the implementation of probability distributions to express the degree of uncertainty around the model inputs, we are now able to produce summary statistics around standard measures of cost-effectiveness, such as ICER and NB. In addition, given \\(K=10000\\), we can also quantify the probability \\(p_{AB}\\) that treatment A is cost-effective compared to B: using the NB distribution, this is represented by the proportion of NB values that are above \\(0\\).\n\n#prob of CE at K=10000\npCE_nb &lt;- sum(NB&gt;0)/S \n\npCE_nb\n\n[1] 0.574\n\n\nFinally, another typical source of uncertainty in these analyses is around the value of the acceptance threshold itself, as in many cases governments’ decisions are always characterised by some degree of uncertainty around what value for \\(K\\) to use. Often this is quantified by computing the above probability of cost-effectiveness for a range of values of \\(K\\).\n\n#range of values to consider for K\nk_value &lt;- seq(from = 1000, to = 20000, by = 50)\nn_kval &lt;- length(k_value)\nNB_k &lt;- matrix(NA,nrow = S, ncol = n_kval)\np_CE_nb_k &lt;- rep(NA, n_kval)  \n\n#compute prob CE based on NB at each value of K\nfor(k in 1:n_kval){\n  for(s in 1:S){\n  NB_k[s,k] &lt;- k_value[k]*(mu_eA[s] - mu_eB[s]) - (mu_cA[s] - mu_cB[s])\n  }\n  p_CE_nb_k[k] &lt;- sum(NB_k[,k]&gt;0)/S\n}\n\nand then plot the results in Figure 1 to show what is known as the cost-effectiveness acceptability curve (CEAC), showing how the probability of cost-effectiveness changes as the value of \\(K\\) is varied.\n\n\n\n\n\n\n\n\nFigure 1: Example of a typical structure of a decision tree model.\n\n\n\n\n\nI hope that you found today’s topic interesting as I wanted to explore something different from my usual comfort zone of trial-based analysis. To be honest I am not a huge fan of decision analytic models as they sometimes allow people to play around with the results and choose the inputs to ensure some “good” results are obtained. When the models become very complicated, it is also quite easy to hide some information in the model code which can be hard to detect if you are not expert and have time to do so. Regardless, in many cases it may not even be possible to run a trial to assess cost-effectiveness (e.g. over a lifetime horizon) and reliance on these methods becomes a necessity. This means that care must be used when either building or reviewing these types of models before committing to a final decision that is based on such evidence."
  },
  {
    "objectID": "posts/2024-08-28-my-blog-post/index.html",
    "href": "posts/2024-08-28-my-blog-post/index.html",
    "title": "Which Dutch province provides the most Olympic medals?",
    "section": "",
    "text": "Hello folks, I hope everything is ok with you and that you had a nice summer break. As for me, I am still recovering from a quite sad period but I think that the best thing for me right now is to try to get back to my work, especially research which, I must confess, I have a bit neglected due to many reasons. While looking for some new ideas to work on, I thought that it could be fun to just simply do some simple coding and analysis to re-ignite my passion on the matter. To be fair, I feel that I am still ready to do lots of things but, due to time constraints and official academic duties, I am not engaged in my research in person since some time now.\nSo, my idea for today is to simply do some statistics! And then, why not researching something I am interested in? I think this would make the work so much easier for me, especially since I am looking for something different from what I have done so far. Then, allow me just for this time to change the topic of my work and try something quite different. To join pleasure and duty, I also thought to do some simple analyses that perhaps I could re-use in one of my introductory courses on statistics in some way.\nHaving said so, what am I going to do? let’s say that I am interested the Olympic medals that were won by the Netherlands in the last edition of the games, Paris 2024. Well, looking this up on the internet I found that the country won a total of \\(34\\) medals, of which \\(15\\) golds, \\(7\\) silvers, and \\(12\\) bronzes. Quite impressive for such a small country! Now, what I am really curious about is how the winners of these medals are spread around the country, that is were these athletes born evenly across the \\(12\\) provinces of the Netherlands? or perhaps there are some clusters of provinces that seem to be more “productive” in terms of Olympic medal winners? Well, let’s see how we can do this.\nFirst, I will draw a map of the Netherlands on which I will show where the different provinces are located and how big they are:\n\nlibrary(tmap)\ntmap_mode(\"plot\")\ndata(NLD_prov)\ntm_shape(NLD_prov) +\n  tm_borders(lwd=2) + \n  tm_fill(\"name\") + \n  tm_format(\"NLD\", title=\"Dutch provinces\", bg.color=\"white\")\n\n\n\n\n\n\n\n\nNext, I will create a new dataframe which contains all the information used to draw the map related to each province, with in addition the information about how many medal winners were born in each of the provinces. Keep in mind that the number of “winners” will be effectively higher than the nominal number of “medals” won by the country since usually, when a team of athletes wins a medal in a team discipline, then the country is awarded a single medal but in reality multiple athletes won and therefore multiple medals were awarded. Since here I am interested in the spreading of the medal winners across the Dutch provinces, then I will consider each individual winner as a separate medal holder. Also, in case a medal winner wins more than one medal, I will count them as a different athlete for as many medals as they won.\nMore specifically, the Dutch team at the 2024 Paris Olympics won a total of 8 medals in the rowing discipline:\n\n1 gold medal in the Men’s quadruple sculls –&gt; Lennart van Lierop (Zuid-Holland); Finn Florijn (Zuid-Holland); Tone Wieten (Noord-Holland); Koen Metsemakers (Overijssel)\n1 gold medal in the Women’s coxless four –&gt; Benthe Boonstra (Gelderland); Hermijntje Drenth (Gelderland); Tinka Offereins (Noord-Holland); Marloes Oldenburg (Zuid-Holland)\n1 gold medal in the Women’s coxless pair –&gt; Ymkje Clevering (Friesland); Veronique Meester (Noord-Holland)\n1 gold medal in the Women’s single sculls –&gt; Karolien Florijn (Zuid-Holland)\n1 silver medal in the Women’s quadruple sculls –&gt; Laila Youssifou (Noord-Holland); Bente Paulis (Zuid-Holland); Roos de Jong (Noord-Holland); Tessa Dullemans (Zuid-Holland)\n1 silver medal in the Men’s double sculls –&gt; Stef Broenink (Zuid-Holland); Melvin Twellaar (Groningen)\n1 silver medal in the Men’s eight –&gt; Ralf Rienks (Noord-Holland); Olav Molenaar (Noord-Holland); Sander de Graaf (Noord-Brabant); Ruben Knab (Gelderland); Gert-Jan van Doorn (Zuid-Holland); Jacob van de Kerkhof (Gelderland); Jan van der Bij (Friesland); Mick Makker (Noord-Holland); Dieuwke Fetter (Noord-Holland)\n1 bronze medal in the Men’s single sculls –&gt; Simon van Dorp (Noord-Holland)\n\nA total of 7 medals in cycling discipline:\n\n1 gold medal in the Men’s team sprint –&gt; Roy van den Berg (Overijssel); Jeffrey Hoogland (Overijssel); Harrie Lavreysen (Noord-Brabant)\n1 gold medal in the Men’s sprint –&gt; Harrie Lavreysen (Noord-Brabant)\n1 gold medal in the Men’s keirin –&gt; Harrie Lavreysen (Noord-Brabant)\n1 silver medal in the Women’s BMX –&gt; Manon Veenstra (Overijssel)\n1 silver medal in the Women’s individual road race –&gt; Marianne Vos (Noord-Brabant)\n1 silver medal in the Women’s keirin –&gt; Hetty van de Wouw (Noord-Brabant)\n1 bronze medal in the Women’s madison –&gt; Lisa van Belle (Zuid-Holland); Maike van der Duin (Drenthe)\n\nA total of 6 medal in the athletics discipline:\n\n1 gold medal in the Mixed 4 × 400 metres relay –&gt; Isaya Klein Ikkink (Zuid-Holland); Lieke Klaver (Noord-Holland); Eugene Omalla (Zuid-Holland); Cathelijn Peeters (Noord-Brabant); Femke Bol (Utrecht)\n1 gold medal in the Women’s marathon –&gt; Sifan Hassan (Ethiopia\\(^\\star\\))\n1 silver medal in the Women’s 4 × 400 metres relay –&gt; Lisanne de Witte (Zuid-Holland); Lieke Klaver (Noord-Holland); Eveline Saalberg (Gelderland); Myrte van der Schoot (Noord-Holland); Femke Bol (Utrecht); Cathelijn Peeters (Noord-Brabant)\n1 bronze medal in the Women’s 5000 metres –&gt; Sifan Hassan (Ethiopia\\(^\\star\\))\n1 bronze medal in the Women’s 400 metres hurdles –&gt; Femke Bol (Utrecht)\n1 bronze medal in the Women’s 10,000 metres –&gt; Sifan Hassan (Ethiopia\\(^\\star\\))\n\nA total of 4 medals in the sailing discipline:\n\n1 gold medal in the Women’s 49er FX –&gt; Odile van Aanholt (Netherlands Antilles\\(^\\star\\)); Annette Duetz (Gelderland)\n1 gold medal in the ILCA 6 –&gt; Marit Bouwmeester (Friesland)\n1 bronze medal in the Women’s Formula Kite –&gt; Annelous Lammerts (Zuid-Holland)\n1 bronze medal in the Men’s IQFoil –&gt; Luuc van Opzeeland (Noord-Holland)\n\nA total of 3 medals in the swimming discipline:\n\n1 gold medal in the 10 km open water –&gt; Sharon van Rouwendaal (Utrecht)\n1 bronze medal in the Men’s 200 metre breaststroke –&gt; Caspar Corbeau (USA\\(^\\star\\))\n1 bronze medal in the Women’s 200 metre breaststroke –&gt; Tes Schouten (Zuid-Holland)\n\nA total of 2 medals in the Field hockey discipline:\n\n1 gold medal in the Men’s tournament –&gt; Seve van Ass (Zuid-Holland); Lars Balk (Utrecht); Koen Bijen (Zuid-Holland); Pirmin Blaak (Zuid-Holland); Justen Blok (Zuid-Holland); Thierry Brinkman (Utrecht); Jorrit Croon (Zuid-Holland); Thijs van Dam (Zuid-Holland); Jonas de Geus (Noord-Holland); Tjep Hoedemakers (Noord-Brabant); Jip Janssen (Noord-Holland); Floris Middendorp (Overijssel); Joep de Mol (Noord-Brabant); Tijmen Reyenga (Noord-Brabant); Duco Telgenkamp (Zuid-Holland); Derck de Vilder (Noord-Holland); Floris Wortelboer (Noord-Brabant)\n1 gold medal in the Women’s tournament –&gt; Felice Albers (Noord-Holland); Joosje Burg (Noord-Brabant); Pien Dicke (Zuid-Holland); Luna Fokke (Zuid-Holland); Yibbi Jansen (Noord-Brabant); Marleen Jochems (Noord-Brabant); Sanne Koolen (Gelderland); Renée van Laarhoven (Gelderland); Frédérique Matla (Noord-Holland); Freeke Moes (Noord-Brabant); Laura Nunnink (Noord-Brabant); Lisa Post (Noord-Brabant); Pien Sanders (Noord-Brabant); Marijn Veen (Utrecht); Anne Veenendaal (Noord-Holland); Maria Verschoor (Zuid-Holland); Xan de Waard (Gelderland)\n\nA total of 1 medal in the Artistic swimming, Basketball, Equestrian, Water polo discipline:\n\n1 gold medal in the Men’s 3x3 tournament –&gt; Worthy de Jong (Suriname\\(^\\star\\)); Arvin Slagter (Noord-Holland); Jan Driessen (Zuid-Holland); Dimeo van der Horst (Noord-Holland)\n1 bronze medal in the Individual jumping –&gt; Maikel van der Vleuten (Noord-Brabant)\n1 bronze medal in the Women’s tournament –&gt; Laura Aarts (Gelderland); Nina ten Broek (Utrecht); Sarah Buis (Utrecht); Kitty-Lynn Joustra (Noord-Holland); Maartje Keuning (Noord-Holland); Simone van de Kraats (Gelderland); Lola Moolhuijzen (Gelderland); Bente Rogge (Noord-Holland); Vivian Sevenich (Gelderland); Brigitte Sleeking (Zuid-Holland); Sabrina van der Sloot (Zuid-Holland); Iris Wolves (Gelderland)\n1 bronze medal in the Women’s duet –&gt; Bregje de Brouwer (Noord-Brabant); Noortje de Brouwer (Noord-Brabant)\n\nNote that those athletes who were not born in one of the \\(12\\) provinces of the Netherlands are denoted with an \\(^\\star\\) and are therefore excluded from the counting.\n\n#\"Groningen\", \"Friesland\", \"Drenthe\", \"Overijssel\", \"Flevoland\", \"Gelderland\", \"Utrecht\", \"Noord-Holland\", \"Zuid-Holland\", \"Zeeland\", \"Noord-Brabant\", \"Limburg\"\n\nNLD_prov_medals &lt;- NLD_prov\nNLD_prov_medals$Gold &lt;- c(0,2,0,4,0,6,5,12,17,0,15,0)\nNLD_prov_medals$Silver &lt;- c(1,1,0,1,0,3,1,8,5,0,5,0)\nNLD_prov_medals$Bronze &lt;- c(0,0,1,0,0,5,3,5,5,0,3,0)\nNLD_prov_medals$All &lt;- c(1,3,1,5,0,14,9,25,27,0,23,0)\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(\"All\", palette = \"Greys\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.44),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\n\nWe can see that the provinces Noord-Holland, Zuid-Holland and Noord-Brabant are those associated with the highest numbers, with respectively \\(25\\), \\(27\\) and \\(23\\) medals, out of the total \\(108\\) medals. Now, let’s try to split the analysis and obtain different figures depending on the type of medal won by the athletes, namely whether they were gold, silver or bronze medals.\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(c(\"Gold\"), palette = \"Greys\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.44),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(c(\"Silver\"), palette = \"Greys\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.44),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(c(\"Bronze\"), palette = \"Greys\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.44),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\n\nStratifying the analysis by type of medal, we can see that:\n\nGold medals are mostly won by athletes from Zuid-Holland (\\(17\\)) and Noord-Brabant (\\(15\\)), followed by Noord-Holland (\\(12\\))\nSilver medals are mostly won by athletes from Noord-Holland (\\(8\\)), followed by Zuid-Holland (\\(5\\)) and Noord-Brabant (\\(5\\))\nBronze medals are mostly won by athletes from Zuid-Holland (\\(5\\)), Noord-Holland (\\(5\\)) and Gelderland (\\(5\\))\n\nAt this point, we may also want to consider the fact that different provinces are associated with a different population density and birth rate which may be an important factor to consider when taking into account the number of medals won. To achieve this, I have obtained 2021 birth rate data from the Statistica website (\\(%\\) per \\(1000\\) people).\n\n#add birth rate date by province (2021)\nNLD_prov_medals$br2021 &lt;- c(9,9.7,9,9.9,12,9.9,11.6,10.7,10.8,9.3,9.9,8.4)\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(\"br2021\", palette = \"Blues\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.44),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\n\nWhile using the population data already embedded in the dataset obtained via tmap, we can draw\n\ntm_shape(NLD_prov_medals) +\n  tm_borders(lwd=2) + \n  tm_fill(\"population\", palette = \"Reds\") + \n  tm_layout(\n    legend.title.size = 1,\n    legend.text.size = 0.6,\n    legend.position = c(0,0.74),\n    legend.bg.alpha = 1)\n\n\n\n\n\n\n\n\nFrom these preliminary descriptive analyses we can see that, particularly in terms of population, both the Noord-Holland and Zuid-Holland provinces are associated with high numbers, therefore suggesting how the higher number of medals won by them may be potentially explained by the bigger number of people living within their borders compared to other provinces.\nI hope you enjoyed a bit this little diversion from my usual posts as I really wanted to try out something different for once, which I found quite interesting. Well, see you next time then!"
  },
  {
    "objectID": "posts/2024-06-12-my-blog-post/index.html",
    "href": "posts/2024-06-12-my-blog-post/index.html",
    "title": "New website and more !",
    "section": "",
    "text": "Hello dear readers and, for those of you who found this post in time, welcome back to my new brand website and blog !\nFinally, after a few months of stop due to some technical issues with my older website version which could not successfully render (damn you Hugo updates!), I have finally decided to re-construct my website from scratch. Indeed, after figuring out that making the website conform to the new Hugo updates would have required me basically the same amount of time, if not more, than building it anew, I embraced myself and build a new website that you should be abkle to see now.\nAll previous posts and information that were put on the older website can still be found here, although they will not be updated anymore. I tried my best to also copy all previous posts and key information, tutorials, etc … and adapt them to my new website so that you should be able to find any previous sections in this website too. I took quite a lot of effort and I had to find time during the weekend and in other relaxed moments of my workday in order to have the new website build and ready to go.\nI also took this unfortunate chance as an opportunity to learn something new, that is, how to use Quarto, an open-source scientific and technical publishing system, and combine it with R and Markdown to create presentations, blogs, and even websites! At first I was bit hesitant to learn from scratch a new way system but, after some initial effort, I am really glad to have learned this new way of creating different types of academic output. Some key advantages of learning Quarto compared to simple Markdown are:\n\nCreation of dynamic content with different software, such as R\nAbility to publish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in different formats, e.g. HTML, PDF, and even MS Word.\nAbility to write using Pandoc markdown, including equations, citations, crossrefs, figure panels, callouts, advanced layout, and more.\n\nThese are only some summary elements that make learning Quarto a quite cost-effective approach to produce different types of output and, especially in the academic context, I can really see the advantages of using such a system,\nEvery time is the same story, when something new comes out it can be quite scary to embrace due to familiarity with old system, fear of initial learning costs, etc… However, if the product is valid, then you can immediately see the advantages of using the new system and almost forget how much you were doubtful of it at the beginning. This happened to me countless times, with R, RStudio, LaTeX, R Markdown, and now Quarto. In all cases, I must say that in the end I turned out quite fine with using all of these tools. I like progress !!!\n\n\n\n\n\nI hope you will enjoy my new website in the future, as I plan to expand it even more in the upcoming months: starting from continuing to add some past content from the previous website (especially the missing data methods review of which I am quite proud), to create new sections for dedicated topics (e.g. perhaps on IRT?), to take advantage of Quarto’s functionalities to make the website’s content even more interactive and new. If you have suggestions on what to do next, feel free to let me know your feedback.\nFinally, I want to give an update on my upcoming conferences that I plan to attend in the next months:\n\nThe first one is the bi-annual EuHEA conference, which this year will be hosted in Vienna, and for which my abstract for presentation has already been accepted. In addition, I found out that I will also be (for the first time) chair of my parallel session, i.e. I will need to manage the organisation of the room and speakers’ turn, while also being the last speaker of the session. Although this may not seem a too complex task to accomplish, I am actually quite thrilled to make this new experience. As for my presentation, the topic will be my R package missingHE, which I have recently updated with some new functions that I will summarise in my talk.\nThe second one is the ICTMC conference which this year will take place in Edinburgh. This conference is a bit broader compared to EuHEA as it is related to methodology and statistics in clinical trials, rather than simply health economics. I already presented once my work here, although as a postdoc I think, but I plan to present a new project I have been working on the past year. The work is still undergoing but I should be able to present some preliminary results and I am looking forward to receive some feedback from the audience, especially from statisticians. However, there is still some uncertainty about whether I will be able to present as abstract notification will be available only from July 1st, well fingers crossed !!!."
  },
  {
    "objectID": "posts/2024-04-13-my-blog-post/index.html",
    "href": "posts/2024-04-13-my-blog-post/index.html",
    "title": "Two-Parameter Logistic Model",
    "section": "",
    "text": "Hello folks, I hope everything is running smoothly for you. As for me, it has been quite a hectic period due to a mix of personal circumstances, teaching and grant application stuff overlapping. It seems I survived the first wave of tasks to do, and now look forward to finish off my remaining duties, mostly related to teaching in a new programme I will be coordinating these upcoming months.\nAs a way to take a break from my daily duties, today I would like to continue the topic from the last post about IRT models (check my last post out in case you are not sure what I am talking about!). In particular, I first introduced the theory behind the definition of the Rasch Model and its features and interpretation of its parameters in the IRT context. I then simulated some data from such model and showed how to estimate and assess the model to the data using standard R packages and functions.\nToday, I would like to move on and show alternative IRT models which try to address some of the limitations intrinsic to the Rasch model. For example, a quite strong assumptions underneath the Rasch model is that all items \\(k=1,\\ldots,K\\) discriminate respondents in the same way (i.e. they are equally related to the individual ability parameter \\(\\theta_i\\)), but are only differentiated in terms of their difficulty parameter \\(b_k\\). This assumption can be eased through the two-parameter logistic model\n\\[\nP(Y_{ik}=1 | \\theta_i,a_k,b_k) = \\frac{\\text{exp}(a_k\\theta_i-b_k)}{1+\\text{exp}(a_k\\theta_i-b_k)},\n\\]\nwhere a item-level discrimination parameter \\(a_k\\) is added, thus allowing items to discriminate between respondents given the same value of the latent ability \\(\\theta_i\\). In terms of the item characteristic curve (ICC), this implies that the ICC has a slope parameter \\(a_k\\) which varies depending on the item considered, there implying a different type of relationship between \\(\\theta_i\\) and \\(P(Y_{ik}=1)\\) based on the item selected. We can see by simulating data from the two-parameter logistic model ICC with different values for the discrimination parameter \\(a_k=\\{2,1,0.5\\}\\) and same values for the difficulty parameter \\(b_k=0\\) over the same range of the latent ability parameter \\(\\theta_i\\sim \\text{Uniform}(-4,4)\\).\n\nset.seed(768)\n\nicc_tplm &lt;- function(theta,a,b){\n  p_resp &lt;- (exp(a*theta - b)) / (1 + exp(a*theta - b))\nreturn(p_resp)\n}\n\ntheta_ex &lt;- seq(from = -4, to = 4, by = 0.01)\nicc_ex1 &lt;- icc_tplm(theta = theta_ex, a = 2, b = 0)\nicc_ex2 &lt;- icc_tplm(theta = theta_ex, a = 1, b = 0)\nicc_ex3 &lt;- icc_tplm(theta = theta_ex, a = 0.5, b = 0)\n\nWe can then plot the generated ICCs under each scenario for the item-discrimination parameter \\(a_k\\).\n\n#plot ICC\nlibrary(ggplot2)\nn &lt;- length(icc_ex1)\ntplm_df &lt;- data.frame(\n  prob = c(icc_ex1, icc_ex2, icc_ex3),\n  theta = c(theta_ex, theta_ex, theta_ex),\n  a = c(rep(\"a=2\", n), rep(\"a=1\", n), rep(\"a=0.5\", n))\n)\ntplm_df$a &lt;- factor(tplm_df$a, levels = c(\"a=2\", \"a=1\", \"a=0.5\"))\n\nggplot(tplm_df, aes(x = theta, y = prob, color = a)) +\n  geom_point() +\n  xlim(-4,4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\"),\n        legend.title = element_blank(), legend.key = element_rect(fill = \"white\"),\n        axis.text = element_text(colour = 'black', face=\"bold\"), axis.ticks.x=element_blank(),\n        strip.background =element_rect(fill=\"white\"), strip.text = element_text(colour = 'black',face = \"bold\")) + labs(y = \"Probability of correct response\", x = \"latent ability\") \n\n\n\n\n\n\n\n\nThe three ICCs have different discrimination parameters \\(a_k\\):\n. The ICC with the highest value \\(a_k=2\\) is the one associated with the steepest slope. This implies that items with such a discrimination ability are able to easily distinguish respondents with latent ability values between \\(-1/1\\) and associated these to a very high/low probability of correct response \\(P(Y_{ik}=1)\\).\n. The ICC with the middle value \\(a_k=1\\) instead is associated with the average slope. It is less able to discriminate between respondents with \\(\\theta_i\\) values between \\(-2/2\\) compared to the ICC with \\(a_k=2\\).\n. The ICC with the lowest value \\(a_k=0.5\\) is associated with the flattest slope. It is more able to discriminate between respondents with \\(\\theta_i\\) values between \\(-2/2\\) compared to the other two ICCs, but is less able to distinguish between respondents as the ability level is increased further.\nKeep in mind that such a distinction between ICCs is made for the same value of the difficulty parameter \\(b_k\\), here assumed to be \\(0\\). As I showed for the Rasch model, changing the value of \\(b_k\\) will also affect the position of the ICC and thus how a given value of \\(\\theta_i\\) is related to \\(P(Y_{ik})\\). However, we saw that \\(b_k\\) only affects ICCs by shifting its position on the plot, i.e. either more to the right (larger \\(b_k\\)) or to the left (smaller \\(b_k\\)), but will not affect their slope. Conversely, given the same level of difficulty \\(b_k\\) for an item, its discrimination ability \\(a_k\\) modifies the slope of the ICC and thus also affecting \\(P(Y_{ik})\\), i.e. either making ICCs steeper (larger \\(a_k\\)) or flatter (smaller \\(a_k\\)).\nA probit version of the two-parameter model is also defined in the literature as the normal ogive model in which the ICC is based on a cumulative normal distribution\n\\[\nP(Y_{ik}=1 | \\theta_i,a_k,b_k) = \\Phi(a_k\\theta_i - b_k)= \\int^{a_k\\theta_i-b_k}_{-\\infty}\\phi(z)d(z),\n\\]\nwhere \\(\\Phi(\\cdot)\\) and \\(\\phi(\\cdot)\\) are the cumulative and density normal distribution function. The two model formulations resemble each other when the logistic item parameter values are multiplied with a constant scaling factor \\(d=1.7\\).\nUsing the example generated before, we can replicate the ICC graph using the same inputs for the normal ogive model as follows.\n\nset.seed(768)\n\nicc_ogive &lt;- function(theta,a,b){\n  p_resp &lt;- pnorm(a*theta - b)\nreturn(p_resp)\n}\n\ntheta_ex &lt;- seq(from = -4, to = 4, by = 0.01)\nicc_ex1 &lt;- icc_ogive(theta = theta_ex, a = 2, b = 0)\nicc_ex2 &lt;- icc_ogive(theta = theta_ex, a = 1, b = 0)\nicc_ex3 &lt;- icc_ogive(theta = theta_ex, a = 0.5, b = 0)\n\nogive_df &lt;- data.frame(\n  prob = c(icc_ex1, icc_ex2, icc_ex3),\n  theta = c(theta_ex, theta_ex, theta_ex),\n  a = c(rep(\"a=2\", n), rep(\"a=1\", n), rep(\"a=0.5\", n))\n)\nogive_df$a &lt;- factor(ogive_df$a, levels = c(\"a=2\", \"a=1\", \"a=0.5\"))\n\nggplot(ogive_df, aes(x = theta, y = prob, color = a)) +\n  geom_point() +\n  xlim(-4,4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\"),\n        legend.title = element_blank(), legend.key = element_rect(fill = \"white\"),\n        axis.text = element_text(colour = 'black', face=\"bold\"), axis.ticks.x=element_blank(),\n        strip.background =element_rect(fill=\"white\"), strip.text = element_text(colour = 'black',face = \"bold\")) + labs(y = \"Probability of correct response\", x = \"latent ability\")\n\n\n\n\n\n\n\n\nThe term \\(a_k\\theta_i - b_k\\) is often presented in the literature as \\(a_k(\\theta_i-b^\\star_k)\\) to denote the fact that \\(b_k\\) is defined on the same scale as \\(\\theta_i\\), i.e. \\(b^\\star_k=\\frac{b_k}{a_k}\\) represents the point on the ability scale where a respondent has the same chance of answering correctly or incorrectly on the item \\(k\\) (\\(P(Y_{ik}=1)=0.5\\)). The metric of the ability parameters is known from item response data only up to a linear transformation. The metric can be identified by fixing a discrimination and difficulty parameter or by adding constraints that the sum of item difficulties and the product of item parameter values equals, for instance, zero and one, respectively, or by fixing the mean and variance of the population distribution of ability parameters. Note that the choice of the identifying restrictions can lead to numerical problems in the estimation of the parameters.\nGood! I think I am getting th gist of these introductory models, which is precisely why I think writing about them here can help me into learning or at least give me the motivation to learn such topic. I just hope that I can speed up the process a bit as I already want to learn more!"
  },
  {
    "objectID": "posts/2024-02-10-my-blog-post/index.html",
    "href": "posts/2024-02-10-my-blog-post/index.html",
    "title": "Confounders, Colliders, Mediator. What to adjust for",
    "section": "",
    "text": "Hello everybody. here we are with my usual monthly update. Today, similarly to last time, I would like to talk about a bit about classical statistical analysis problems, such as those which I find myself routinely explaining to my students. To all health economists who were looking for some more HTA-related content I, once again, apologise and ask them to tune in next time in case what I will talk about today is not of their interest. The main reason why I wanted to go back to this topic is that I am currently teaching a classical stats course for 3rd year medical students who seem to be a bit confused about some basic stats terminology with reference to regression analysis. As usual, I take the opportunity to write here in order to also help myself in making these concepts as clear as possible so to be able to effectively communicate them to my students. I hope some of you may find this topic interesting as much as I do.\nSo, the topic today will be the distinction between different types of independent variables which are often use as covariates within a regression framework. More specifically, there is quite a big literature work on the distinction between three general categories of covariates: confounders, colliders, and moderators. Based on this classification, there has also been quite a discussion on whether an adjustment for the effects of such variables on the influence of the main determinant on the outcome is needed, depending on the type of analysis/research question formulated.\nLet’s see if I am able to make these concepts clear and provide some general suggestions on whether and if an adjustment for these variables is needed in a standard statistical analysis. As usual, let me start with an example, mostly inspired from another publicly available simulation example that can be found here."
  },
  {
    "objectID": "posts/2024-02-10-my-blog-post/index.html#partition-data-into-sub-groups",
    "href": "posts/2024-02-10-my-blog-post/index.html#partition-data-into-sub-groups",
    "title": "Confounders, Colliders, Mediator. What to adjust for",
    "section": "Partition data into sub-groups",
    "text": "Partition data into sub-groups\nThis is the most easy way to go and almost self-explanatory. See the graphs and code for the example of car usage among Green party voters above. If you want to make sure that a confounding variable isn’t biasing your results, you simply filter your data such that the confounding variable is constant in the sub-set. This option is viable if you have a small number of confounders (i.e. only one or two) and these variables have only a few distinct values."
  },
  {
    "objectID": "posts/2024-02-10-my-blog-post/index.html#multivariable-regression",
    "href": "posts/2024-02-10-my-blog-post/index.html#multivariable-regression",
    "title": "Confounders, Colliders, Mediator. What to adjust for",
    "section": "Multivariable regression",
    "text": "Multivariable regression\nThere are of course many variants and advancements over the standard linear model which can help you identify causal effects (or tentatively claim that you might have identified them) when you have special data structures. For instance, if you have longitudinal data of multiple individuals (or countries, etc.), also known as time-series cross-sectional or panel data, there are special regression models that can make use of both the temporal and the inter-individual variation. If you are less concerned with identifying the causal effect of \\(X\\), but more interested in predicting the outcome \\(Y\\) (e.g. which individuals will develop severe Covid illness based on a multitude of clinical characteristics), models other than regression are often better-suited since they can overcome a number of limitations to regression (e.g. multi-collinearity). Examples include machine-learning algorithms such as random forests."
  },
  {
    "objectID": "posts/2024-02-10-my-blog-post/index.html#propensity-score-matching",
    "href": "posts/2024-02-10-my-blog-post/index.html#propensity-score-matching",
    "title": "Confounders, Colliders, Mediator. What to adjust for",
    "section": "Propensity-score matching",
    "text": "Propensity-score matching\nMatching means that you don’t use all your data, but you try to find, for your selected group of “treatment” units, counterparts that are most similar in all other regards except for the treatment. Note that matching is no perfect cure for multi-collinearity, you still need some degree of “overlap” of the distributions (i.e. not all poor countries should have high fertility and vice versa), otherwise you don’t find any matching partners except for, again, the unusual outliers."
  },
  {
    "objectID": "posts/2023-12-10-my-blog-post/index.html",
    "href": "posts/2023-12-10-my-blog-post/index.html",
    "title": "Just need a break",
    "section": "",
    "text": "Hello folks, today I would like to simply give a brief update about what I am looking for for next year, particularly in terms of conferences I am actuallu planning to attend as I think there is a couple that look very interesting. Unfortunately, I did not find the time to prepare something cool like the last few months with some applied examples on how to fit Bayesian models or how to implement some health economic analyses in R, but I promise I will make you forgive with the next update (a big one!). I have been so busy with lots of new beginning projects and supervision tasks the last few weeks that I barely had the time to do any research by myself. I have plenty of ideas to try but lack the time to do so. Well I guess I am used to this by now.\nAnyway, back to the topic at hand. There are thre main conferences I am currently looking forward to attend next year at which present some on-going work I was involved with ovee the last few months.\n\nThe first one is the annual lolaHESG 2024 which this year will take place in Leusden, a small town close to Amersfoort in the middle of the Netherlands. I know little to nothing about this location and I am curious to know why it was chosen for this year’s edition of lola. However, I am afraid this year I will not have the material time to attend this very nice health economics conference since I will be coordinating a new course which starts in May and have clashes with the actual dates of the conference. Unless I can find someone to replace me and even then, since this is my first year as coordinator of this course, I probably need to be present during the course in case any issues arise. Nevertheless, I would strongly suggest that you consider attending this conference if you are interested in current developments in the field of HTA and health economics from a broad perspective - usually there are many different topics and authors with a varied background experience. Plus, the conference event the last two years were a blast and super well organised, so I am sure also these years the organisers will nail it.\nThe second one is the biannual EuHEA conference which in July 2024 will be held in Vienna. I attended the previous edition of the conference in Oslo and was a nice and well-organised conference which I really appreciated, particularly in terms of getting in touch with health economists spread all over the world who were working on such a diversity of projects. I believe the strength of these conferences is mainly the international environment of the audience and the generally constructive feedback presenting authors can receive from each other. They also plan each session in a way that ties up with the topics covered by the authors who present during the session, which is a nice way to ensure people with similar interests can interact with each other and potentially talk about future joint work. I am really considering submitting something to this conference, although I would be more inclined to submit materials for a workshop in R or BUGS/STAN as way to support my work (and also to motivate myself to start working on my R package!) and make it known to people who might benefit for using it in routine health economic analyses.\nFinally, there is also a quite special UK-based conference named ICTMC, which I already attended a few years back during my PhD, and which is more focussed on statistical methodology for the analysis of trial data. This upcoming year the conference will be held in October in Edinburgh and I would be really interested in presenting some new work I have been developing over the last few months in order to receive some feedback from statisticians and analysts. The conference has also specialised sessions to different “fields” within the general topic of trial data anlayses, with also a focus on health economic evaluations which I think would fit perfectly with my own work. It is still quite early to apply for this conference but I would be really interested in attending such conference and share my work with other experts to receive some nice feedback and suggestions for improvement.\n\nAs I said, today I wanted to be short and simply give an update of my current situation with a focus on the type of conferences I am looking forward to attend in 2024. Despite the focus of my work being strongly connected to HTA and missing data analyses, I feel that the applicability of the methods I am developing could become beneficial to other quantitative researchers who may be involved in a different types of data analysis which share similar characteristics to those of trial-based economic evaluations. In addition, we all know that missing data problems will never go away and therefore I hoop I can provide some suggestions or nice feedback to anyone who would be interested to learn how to deal with them without using standard and well-known problematic methods.\nYes I am talking about you, dear complete-case analysis statistician!\n\n\n\n\n\nWish you all a merry Christmas and a happy new year !"
  },
  {
    "objectID": "posts/2023-10-05-my-blog-post/index.html",
    "href": "posts/2023-10-05-my-blog-post/index.html",
    "title": "Bayesian statistics in health economic evaluations - part 2",
    "section": "",
    "text": "Hello dear audience and welcome back to my regular post updates. This time I would like to continue discussing the application of Bayesian methods in HTA that I picked-up in my previous post and show some customisation options of the modelling strategy that may come in handy when dealing with such analyses. More specifically, a notorious problem affecting trial-based economic evaluations is the presence of skewed data which, when coupled with small sample sizes, may question the validity of mean estimate based on normal distribution assumptions. In the previous post I provided a simple modelling framework based on normal distributions for both CEA aggregated outcomes, namely effectiveness (usually in terms of QALYs) and total costs computed over the study period. Today, I would like to show how, within a Bayesian setting, the use of alternative parametric distributions is relatively simple and allows to tackle issues such as skewness while simultaneously dealing with uncertainty quantification for each unknown quantity in the model. This is extremely helpful in that there is no need to reply on re-sampling methods (e.g. bootstrapping) to generate uncertainty around the desired estimates and to combine such methods with alternative modelling approaches, e.g. need to specify how bootstrapping should be done when using non-Normal distributions, multilevel models, or even missing data.\nLet’s take the same example I simulated in my previous post and re-perform the exact same analysis based on Normal distributions and then try to compare its results to an analysis based on alternative, more flexible, parametric distributions that can better capture the features of the data (e.g. skewness). We start by simulating some non-Normal bivariate cost and QALY data from an hypothetical study for a total of \\(300\\) patients assigned to two competing intervention groups (\\(t=0,1\\)). When generating the data, we can try to mimic the typical skewness features of the outcome data by using alternative distributions such as Gamma for costs and Beta for QALYs.\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nmean_e1 &lt;- c(0.5)\nmean_e2 &lt;- c(0.7)\nsigma_e &lt;- 0.15\ntau1_e &lt;- ((mean_e1*(1-mean_e1))/(sigma_e^2)-1)\ntau2_e &lt;- ((mean_e2*(1-mean_e2))/(sigma_e^2)-1)\nalpha1_beta &lt;- tau1_e*mean_e1\nbeta1_beta &lt;- tau1_e*(1-mean_e1)\nalpha2_beta &lt;- tau2_e*mean_e2\nbeta2_beta &lt;- tau2_e*(1-mean_e2)\ne1 &lt;- rbeta(n/2, alpha1_beta, beta1_beta)\ne2 &lt;- rbeta(n/2, alpha2_beta, beta2_beta)\n\nmean_c1 &lt;- 500\nmean_c2 &lt;- 1000\nsigma_c &lt;- 300\ntau1_c &lt;- mean_c1/(sigma_c^2)\ntau2_c &lt;- mean_c2/(sigma_c^2)\nln.mean_c1 &lt;- log(500) + 5*(e1-mean(e1)) \nc1 &lt;- rgamma(n/2, (exp(ln.mean_c1)/sigma_c)^2, exp(ln.mean_c1)/(sigma_c^2))\nln.mean_c2 &lt;- log(1000) + 5*(e2-mean(e2)) + rgamma(n/2,0,tau2_c)\nc2 &lt;- rgamma(n/2, (exp(ln.mean_c2)/sigma_c)^2, exp(ln.mean_c2)/(sigma_c^2))\n\nQALYs &lt;- c(e1,e2)\nCosts &lt;- c(c1,c2)\n\ndata_sim_ec &lt;- data.frame(id, trt, QALYs, Costs)\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\nWe can now inspect the empirical distributions of the two outcomes by treatment group to have an idea of the level of the associated skewness.\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\n\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nQALY_hist &lt;- ggplot(data_sim_ec, aes(x=QALYs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist &lt;- ggplot(data_sim_ec, aes(x=Costs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)"
  },
  {
    "objectID": "posts/2023-10-05-my-blog-post/index.html#step-1-fit-a-standard-normal-model",
    "href": "posts/2023-10-05-my-blog-post/index.html#step-1-fit-a-standard-normal-model",
    "title": "Bayesian statistics in health economic evaluations - part 2",
    "section": "Step 1: fit a standard normal model",
    "text": "Step 1: fit a standard normal model\nAs in the previous post, we rely on factoring the joint distribution \\(p(e,c\\mid  \\boldsymbol \\theta)\\) into the product of a marginal distribution \\(p(e\\mid \\boldsymbol \\theta_e)\\) of the effects and a conditional distribution \\(p(c \\mid e, \\boldsymbol \\theta_c)\\) of the cost given the effects, each indexed by corresponding set of parameters. We can rely on the freely-available Bayesian software named JAGS to fit the model. We write the model code into a txt file and save it into our current wd to be called from R. Then, we load the package R2jags which allows to call the software from R through the function jags and after providing the data and some technical parameters needed to run the model.\n\n#save data input\nn &lt;- dim(data_sim_ec)[1]\nQALYs &lt;- data_sim_ec$QALYs\nCosts &lt;- data_sim_ec$Costs\ntrt &lt;- data_sim_ec$trt\n\n#load package and provide algorithm parameters\nlibrary(R2jags)\nset.seed(2345) #set seed for reproducibility\ndatalist&lt;-list(\"n\",\"QALYs\",\"Costs\",\"trt\") #pass data into a list\n#set up initial values for algorithm\ninits1 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 1)\ninits2 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 2)\n#set parameter easimates to save\nparams&lt;-c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\",\"gamma2\",\"s_c\",\"s_e\",\"nu_c\",\"nu_e\")\nfilein&lt;-\"model_bn.txt\" #name of model file\nn.iter&lt;-20000 #n of iterations\n\n#fit model\njmodel_bn&lt;-jags(data=datalist,inits=list(inits1,inits2),\n                parameters.to.save=params,model.file=filein,\n                n.chains=2,n.iter=n.iter,n.thin=1)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 7\n   Total graph size: 1522\n\nInitializing model\n\n\nWe then proceed to extract the key quantities of interest of this analysis by post-processing the model estimates via simulation methods, with the aim to obtain the mean effects and costs outcome posterior distributions \\((\\mu_e,\\mu_c)\\) in each treatment group.\n\n#obtain estimates of means by arm\n\n#extract estimates for each mean parameter by trt group\nnu_e0 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==0]\nnu_e1 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==1]\nnu_c0 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==0]\nnu_c1 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==1]\n#extract estimates for std\ns_e &lt;- jmodel_bn$BUGSoutput$sims.list$s_e\ns_c &lt;- jmodel_bn$BUGSoutput$sims.list$s_c\n\n#create empty vectors to contain results for means by trt group\nmu_e0 &lt;- mu_c0 &lt;- c()\nmu_e1 &lt;- mu_c1 &lt;- c()\n\n#set number of replications\nL &lt;- 5000\n\nset.seed(2345) #set seed for reproducibility\n#generate replications and take mean at each iteration of the posterior\nfor(i in 1:n.iter){\n mu_e0[i] &lt;- mean(rnorm(L,nu_e0[i,],s_e[i])) \n mu_e1[i] &lt;- mean(rnorm(L,nu_e1[i,],s_e[i]))\n mu_c0[i] &lt;- mean(rnorm(L,nu_c0[i,],s_c[i])) \n mu_c1[i] &lt;- mean(rnorm(L,nu_c1[i,],s_c[i])) \n}\n\n#calculate mean differences\nDelta_e &lt;- mu_e1 - mu_e0\nDelta_c &lt;- mu_c1 - mu_c0"
  },
  {
    "objectID": "posts/2023-10-05-my-blog-post/index.html#step-2-fit-a-beta-gamma-model",
    "href": "posts/2023-10-05-my-blog-post/index.html#step-2-fit-a-beta-gamma-model",
    "title": "Bayesian statistics in health economic evaluations - part 2",
    "section": "Step 2: fit a Beta-Gamma model",
    "text": "Step 2: fit a Beta-Gamma model\nAs a comparative approach, we now try to directly address the skewness in the observed data by using a Beta distribution for modelling the QALYs and a Gamma distribution for the Total costs. The main idea is that, by using distributions that can naturally allow for non-symmetric data, estimates will likely be more robust and uncertainty concerning such estimates will be more properly quantified. We begin by writing the model file and save it as a txt file.\n\nmodel_bg &lt;- \"\nmodel {\n\n#model specification\nfor(i in 1:n){\n      QALYs[i]~dbeta(shape1[i],shape2[i])\n      shape1[i]&lt;-nu_e[i]*(nu_e[i]*(1-nu_e[i])/pow(s_e,2) - 1)\n      shape2[i]&lt;-(1-nu_e[i])*(nu_e[i]*(1-nu_e[i])/pow(s_e,2) - 1)\n      logit(nu_e[i])&lt;-beta0 + beta1*trt[i]\n\n      Costs[i]~dgamma(shape[i],rate[i])\n      shape[i]&lt;-pow(nu_c[i],2)/pow(s_c,2)\n      rate[i]&lt;-nu_c[i]/pow(s_c,2)\n      log(nu_c[i])&lt;-gamma0 + gamma1*trt[i] + gamma2*QALYs[i]\n}\n\n#prior specification\ns_c ~ dunif(0,1000)\ns_e_limit&lt;- sqrt(0.5885263*(1-0.5885263))\ns_e ~ dunif(0,s_e_limit)\nbeta0 ~ dnorm(0,0.001)\nbeta1 ~ dnorm(0,0.001)\ngamma0 ~ dnorm(0,0.001)\ngamma1 ~ dnorm(0,0.001)\ngamma2 ~ dnorm(0,0.001)\n\n}\n\"\nwriteLines(model_bg, con = \"model_bg.txt\")\n\nWe then proceed to pass the data and algorithm parameters to fit the model in JAGS. It is important to stress that, since Gamma and Beta are defined within bounds of \\((0,+\\infty)\\) and \\((0,1)\\), then we need to “modify” the data to ensure that such extremes are not present (in case they occur). This is often done by adding/subtracting a small constant to the data.\n\n#save data input\nn &lt;- dim(data_sim_ec)[1]\nQALYs &lt;- data_sim_ec$QALYs - 0.001\nCosts &lt;- data_sim_ec$Costs + 0.001\ntrt &lt;- data_sim_ec$trt\n\n#load package and provide algorithm parameters\nlibrary(R2jags)\nset.seed(2345) #set seed for reproducibility\ndatalist&lt;-list(\"n\",\"QALYs\",\"Costs\",\"trt\") #pass data into a list\n#set up initial values for algorithm\ninits1 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 1)\ninits2 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 2)\n#set parameter easimates to save\nparams&lt;-c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\",\"gamma2\",\"s_c\",\"s_e\",\"nu_c\",\"nu_e\",\n          \"shape\",\"rate\",\"shape1\",\"shape2\")\nfilein&lt;-\"model_bg.txt\" #name of model file\nn.iter&lt;-20000 #n of iterations\n\n#fit model\njmodel_bg&lt;-jags(data=datalist,inits=list(inits1,inits2),\n                parameters.to.save=params,model.file=filein,\n                n.chains=2,n.iter=n.iter,n.thin=1)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 7\n   Total graph size: 2739\n\nInitializing model\n\n\nWe then proceed to extract the key quantities of interest of this analysis by post-processing the model estimates via simulation methods, with the aim to obtain the mean effects and costs outcome posterior distributions \\((\\mu_e,\\mu_c)\\) in each treatment group.\n\n#obtain estimates of means by arm\n\n#extract estimates for each mean parameter by trt group\nshape1_e0 &lt;- jmodel_bg$BUGSoutput$sims.list$shape1[,trt==0]\nshape1_e1 &lt;- jmodel_bg$BUGSoutput$sims.list$shape1[,trt==1]\nshape2_e0 &lt;- jmodel_bg$BUGSoutput$sims.list$shape2[,trt==0]\nshape2_e1 &lt;- jmodel_bg$BUGSoutput$sims.list$shape2[,trt==1]\n\nshape_c0 &lt;- jmodel_bg$BUGSoutput$sims.list$shape[,trt==0]\nshape_c1 &lt;- jmodel_bg$BUGSoutput$sims.list$shape[,trt==1]\nrate_c0 &lt;- jmodel_bg$BUGSoutput$sims.list$rate[,trt==0]\nrate_c1 &lt;- jmodel_bg$BUGSoutput$sims.list$rate[,trt==1]\nscale_c0 &lt;- 1/rate_c0\nscale_c1 &lt;- 1/rate_c1\n\n\n#create empty vectors to contain results for means by trt group\nmu_e0_bg &lt;- mu_c0_bg &lt;- c()\nmu_e1_bg &lt;- mu_c1_bg &lt;- c()\n\n#set number of replications\nL &lt;- 5000\n\nset.seed(2345) #set seed for reproducibility\n#generate replications and take mean at each iteration of the posterior\nfor(i in 1:n.iter){\n mu_e0_bg[i] &lt;- mean(rbeta(L,shape1 =  shape1_e0[i,], shape2 =  shape2_e0[i])) \n mu_e1_bg[i] &lt;- mean(rbeta(L,shape1 =  shape1_e1[i,], shape2 =  shape2_e1[i]))\n mu_c0_bg[i] &lt;- mean(rgamma(L,shape =  shape_c0[i,],scale =  scale_c0[i])) \n mu_c1_bg[i] &lt;- mean(rgamma(L,shape =  shape_c1[i,],scale =  scale_c1[i])) \n}\n\n#calculate mean differences\nDelta_e_bg &lt;- mu_e1_bg - mu_e0_bg\nDelta_c_bg &lt;- mu_c1_bg - mu_c0_bg\n\nAt this point it would be wise to check the results of both models and compare them in terms of some measures of relative or absolute performance, such as information criteria or posterior predictive checks. In general, the idea is that IC should reveal a better performance of the distributions that better fit the observed data, while PPCs can be used to detect possible problems in the model replications that may fail to capture some aspects of the observed data. These can be used to decide which model should be trusted more in terms of relative fit to the data. For the sake of simplicity, here I omit these comparisons as it is something I would like to dedicate a more substantial discussion.\nFinally, we can then produce all standard CEA output, e.g. CEAC or CE Plane, by post-processing these posterior distributions and compare the cost-effectiveness results of the two models fitted. If you want to skip the fun, we can take advantage of the R package BCEA which is dedicated to post-processing the results from a Bayesian CEA model.\n\n#load package and provide means e and c by group as input \nlibrary(BCEA)\nmu_e_bn &lt;- cbind(mu_e0,mu_e1)\nmu_c_bn &lt;- cbind(mu_c0,mu_c1)\nmu_e_bg &lt;- cbind(mu_e0_bg,mu_e1_bg)\nmu_c_bg &lt;- cbind(mu_c0_bg,mu_c1_bg)\n\n#produce CEA output\ncea_res_bn &lt;- bcea(eff = mu_e_bn, cost = mu_c_bn, ref = 2)\ncea_res_bg &lt;- bcea(eff = mu_e_bg, cost = mu_c_bg, ref = 2)\n\n#CE Plane (set wtp value)\ncep_bn &lt;- ceplane.plot(cea_res_bn, graph = \"ggplot2\", wtp = 10000)\ncep_bg &lt;- ceplane.plot(cea_res_bg, graph = \"ggplot2\", wtp = 10000)\n\n#CEAC \nceac_bn &lt;- ceac.plot(cea_res_bn, graph = \"ggplot2\")\nceac_bg &lt;- ceac.plot(cea_res_bg, graph = \"ggplot2\")\n\ngridExtra::grid.arrange(cep_bn, cep_bg, ceac_bn, ceac_bg, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n#other output\nsummary(cea_res_bn)\n\n\nCost-effectiveness analysis summary \n\nReference intervention:  intervention 2\nComparator intervention: intervention 1\n\nOptimal decision: choose intervention 1 for k &lt; 4000 and intervention 2 for k &gt;= 4000\n\n\nAnalysis for willingness to pay parameter k = 25000\n\n               Expected net benefit\nintervention 1                11911\nintervention 2                15635\n\n                                    EIB CEAC   ICER\nintervention 2 vs intervention 1 3724.1    1 3959.2\n\nOptimal intervention (max expected net benefit) for k = 25000: intervention 2\n                \nEVPI -1.5852e-13\n\nsummary(cea_res_bg)\n\n\nCost-effectiveness analysis summary \n\nReference intervention:  intervention 2\nComparator intervention: intervention 1\n\nintervention 2 dominates for all k in [0 - 50000] \n\n\nAnalysis for willingness to pay parameter k = 25000\n\n               Expected net benefit\nintervention 1               8507.6\nintervention 2              14523.2\n\n                                    EIB CEAC    ICER\nintervention 2 vs intervention 1 6015.6    1 -7817.7\n\nOptimal intervention (max expected net benefit) for k = 25000: intervention 2\n                \nEVPI -6.3236e-13\n\n\nIn the end results may be quite different between the models used and it is crucial that model selection is done based on appropriate criteria in order to decide which results to trust more according to how well the model fit the data and potential issues in MCMC convergence (i.e. need to check algorithm diagnostics to make sure posterior estimates are reliable). I really think that, especially in a HTA context, Bayesian methods are naturally suited for the job since they allow to flexibly define your model to tackle different aspects of the data while also addressing uncertainty in a structural and consistent way, being it uncertainty about model estimates, missing values, predictions, etc …. Although the same can also be achieved within a frequentist framework, this comes at the cost of an increasingly complex implementation effort (e.g. need to combine bootstrapping and non-normal distributions) which, instead, in a Bayesian setting can be achieved in a relatively easy way. As the models we need to fit become increasingly complex, it makes sense to me that more flexible approaches that can be tailored to deal with different situations and peculiarities should become more and useful to analysts to accomplish their tasks."
  },
  {
    "objectID": "posts/2023-07-05-my-blog-post/index.html",
    "href": "posts/2023-07-05-my-blog-post/index.html",
    "title": "Item-Response Theory: intro",
    "section": "",
    "text": "Hello everyone, here we are again with my monthly blog update. Today I would like to continue my initial discussion about Item Response Models (IRMs) which I introduced in one of my previous posts a couple of months ago. In that post I mostly talked about the concept of IRM and how I have become more and more interested in the topic as I believe there is a huge potential for their implementation in the analysis of HE data. I also referred to a very nice book from Jean-Paul Fox called Bayesian Item Response Modelling which I really enjoyed reading this past few months. In this and future posts I would like to extend my initial thoughts about the book and topic and elaborate a bit more about my current understanding of IRMs and how they can used to answer research questions in HE. Today I would like to start this post with an introduction of what are IRMs and how they may be useful to analyse questionnaire data. Most of this information is taken from previous literature on IRMs which for the sake of display I will brutally summarise here to simply get the message through. Please do not blindly trust my posts but think of them as an incentive to check the topics in more depth through an appropriate reference!"
  },
  {
    "objectID": "posts/2023-07-05-my-blog-post/index.html#why-are-irm-needed",
    "href": "posts/2023-07-05-my-blog-post/index.html#why-are-irm-needed",
    "title": "Item-Response Theory: intro",
    "section": "Why are IRM needed ?",
    "text": "Why are IRM needed ?\nInitially, IRMs were developed with the objective to analyse item responses in questionnaires collected under standardised conditions for specific types of data. Nowadays, however, their use has become much broader across a wide variety of item-response data which are more routinely collected along surveys or experimental studies and are used for their measurement properties. In addition, the increasing richness and quality of the data collected has posed new and challenging problems at the analysis stage that require the adoption of more sophisticated models that are flexible enough to take them into account; these include: occurrence of item and/or unit non-response, multilevel and longitudinal data structures or respondent heterogeneity. To handle these characteristics of the data, the adoption of more advanced and flexible models allows to avoid relying on simplifying assumptions at the basis of standard statistical methods which, when violated, may distort the results. The Bayesian framework offers a natural approach to deal with these problems that can be summarised as problems related to the quantification of the impact of different levels of uncertainty (e.g. missingness, hierarchical, etc…) on the results of the analysis. Although alternative methods could be used to try to deal with these complex problems (e.g. nonparametric bootstrapping), their implementation becomes exponentially more difficult as multiple features of the data are added into the mix. On the contrary, although Bayesian methods are certainly not needed for the analysis of simple data structures for which the simplicity of standard methods makes them much more appealing to use, their flexibility make them perfect for addressing complex data structures in a relatively easy way through the use of a fully probabilistic approach.\nA key concept in the context of IRMs is represent by latent variables, often referred to as latent construct, in that item responses are often assumed to be indicators of an underlying construct or latent variable. IRMs allow the estimation of such construct through the specification of an underlying model structure linking the item responses and the value of the latent variable (supposedly generating such responses). Indeed, the definition of a latent variable that generates the responses allows to reduce the dimensionality of the data thus simplifying the analysis task through the specification of relationships between fewer sets of variables with respect to those measured."
  },
  {
    "objectID": "posts/2023-07-05-my-blog-post/index.html#traditional-irms-the-rasch-model",
    "href": "posts/2023-07-05-my-blog-post/index.html#traditional-irms-the-rasch-model",
    "title": "Item-Response Theory: intro",
    "section": "Traditional IRMs: the Rasch model",
    "text": "Traditional IRMs: the Rasch model\nThere is a huge literature on IRM for analysing item-response data but here I will simply focus on one of the simplest and most popular example for the analysis of binary item-responses known as the Rasch Model. In general, key features of all IRMs include the assumption that:\n\nThe probability of changing the response to an item depends on changes in the latent variable generating such responses, often mathematically expressed in the form of an Item Characteristic Curve (ICC).\nResponses to pair of items are independent given the value of a the underlying latent variable, i.e. assumption of conditional independence.\n\nFrom the assumptions above, we can then say that: the \\(i\\)-th respondent associated with a latent construct value \\(\\theta_i\\) has a conditional probability (given \\(\\theta_i\\)) of producing response data \\(\\boldsymbol y_i\\).\nIn the case of binary responses, one of the simplest and the most widely used IRM is the Rasch model or the one-parameter logistic response model, in which the probability of a correct response is given by:\n\\[\nP(Y_{ik}=1 | \\theta_i,b_k) = \\frac{\\text{exp}(\\theta_i - b_k)}{1+\\text{exp}(\\theta_i-b_k)},\n\\]\nfor individual \\(i\\) with ability level \\(\\theta_i\\) and item \\(k\\) with item-difficulty parameter \\(b_k\\). The code below shows how to generate the ICC for the Rasch model in three different scenarios by fixing the value for the item-parameter difficulty (\\(-1,0,1\\)).\n\nset.seed(768)\n\nicc_rasch &lt;- function(theta,b){\n  p_resp &lt;- (exp(theta - b)) / (1 + exp(theta - b))\nreturn(p_resp)\n}\n\ntheta_ex &lt;- seq(from = -4, to = 4, by = 0.01)\nicc_ex1 &lt;- icc_rasch(theta = theta_ex, b = -1)\nicc_ex2 &lt;- icc_rasch(theta = theta_ex, b = 0)\nicc_ex3 &lt;- icc_rasch(theta = theta_ex, b = 1)\n\nWe can then plot the generated ICCs for a range of \\(\\theta_i\\) values to show what the ICC looks like under the Rasch model under each scenario for the item-difficulty parameter \\(b_k\\).\n\n#plot ICC\nlibrary(ggplot2)\nn &lt;- length(icc_ex1)\nrasch_df &lt;- data.frame(\n  prob = c(icc_ex1, icc_ex2, icc_ex3),\n  theta = c(theta_ex, theta_ex, theta_ex),\n  b = c(rep(\"b=-1\", n), rep(\"b=0\", n), rep(\"b=1\", n))\n)\nrasch_df$b &lt;- factor(rasch_df$b, levels = c(\"b=-1\", \"b=0\", \"b=1\"))\n\nggplot(rasch_df, aes(x = theta, y = prob, color = b)) +\n  geom_point() +\n  xlim(-4,4) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_blank(), axis.line = element_line(colour = \"black\"),\n        legend.title = element_blank(), legend.key = element_rect(fill = \"white\"),\n        axis.text = element_text(colour = 'black', face=\"bold\"), axis.ticks.x=element_blank(),\n        strip.background =element_rect(fill=\"white\"), strip.text = element_text(colour = 'black',face = \"bold\")) + labs(y = \"Probability of correct response\", x = \"latent ability\") \n\n\n\n\n\n\n\n\nEach ICC describes the item-specific relationship between the ability level and the probability of a correct response, with an item defined as easier compared to another when the probability of a correct response is higher with respect to another item given the same level of \\(\\theta_i\\). In the figure it can be seen that the ICCs from the left to the right are associated with increasing item difficulty parameters, with ICCs being parallel to each other. This is a key property of the ICC for Rasch models where for an item an increase in ability leads to the same increase in the probability of success, i.e. the items discriminate in the same way between success probabilities for related ability levels.\nOther properties of the Rasch model include:\n\nThe probability distribution is a member of the exponential family of distributions which allows algebraic separation of the ability and item parameters.\nA response probability can be increased/decreased by adding/subtracting a constant to the ability/difficulty parameter, thus creating an identification problem. This is often solved by the use of some constraints to make the location identifiable, for example by setting the sum of the difficulty parameters equals zero or by restricting the mean of the scale to zero.\nAll items are assumed to discriminate between respondents in the same way through the item-difficulty parameter.\n\nI hope I was able to summarise the key concept at the basis of IRMs in a sufficiently clear way. The Rasch model provides the simplest and most intuitive modelling approach to represent such scheme and its ICC specification can be used to reduce the dimensionality of binary item responses into a framework where interest is in the estimation of a latent construct representing the individuals’ ability to answer correctly to the questions. The model suffers from some limitations, such as the use of a single item-parameter to discriminate between respondents or the possibility of using the model only for binary data. However, it perfectly embeds the idea behind the use of IRMs: define a theoretical and plausible mathematical structure relating a latent construct that needs to be estimated to the generated answers from the respondents without the need to specify a joint distribution on all responses.\nBut what about the Bayesian approach? well next time I will try to show why the use of Bayesian framework may be beneficial when estimating Rasch models or, in general, IRMs and how results can be summarised. If this caught your interest, stay tuned!"
  },
  {
    "objectID": "posts/2023-05-10-my-blog-post/index.html",
    "href": "posts/2023-05-10-my-blog-post/index.html",
    "title": "Structural values in health economics data",
    "section": "",
    "text": "Good day folks, today I would like to make a digression from the usual health economics’ topic addressed on this blog and talk about something different that I have recently taken an interest in. I must apologise for digressing a lot recently but I promise I will cover the topics left back in future posts. It is just that when I would like to talk about something new that interested me I really need to do it as otherwise I will forget it. So, please bear with me when I have these moments.\nOf course, we still talk about statistics (what else!) but a kind if statistics to which I have never been exposed to during my student’s years, namely the Item Response Theory (IRT) framework for the analysis of questionnaire data. From my studies I knew the classical test theory consisting of calculating scales’ scores from a given questionnaire or sub-questionnaire items (usually on defined a Likert scale) but I never found the statistics behind this framework particularly appealing. To me it mostly resolved around the use of standard parametric testing methods which do not leave much room for extension from a methodological (particularly modelling) perspective. I do not claim to be an expert in the field and there are certainly ground-breaking development in that statistics’ field too, it is simply something that I did not picture myself working on. In the past I also heard something about IRT as a more sophisticated statistical framework to handle multi-item questionnaires but I never delved into the topic becuase of eithe rlack of time or not the right timing.\nWell, perhaps that moment has finally arrived as in the last few weeks I have started reading quite intensively some background theory behind IRT due to some overlap with some consultancy work I had to do for some clients for the analysis of some questionnaire data. Initially I tried to start from the beginning of IRT looking at some old manuals or reference books, such as the Handbook of Modern Item Response Theory from Linden or Fundamentals of Item Response Theory from Hambleton. However, I found these books to be written in a quite old-fashion (which can be quite challenging to read if you know little about the topic or typical methods used in the field) and mostly concerned about detailed explanation of the working behind the algorithm or methods used for estimation in different scenarios. This is certainly useful for someone who already knows what type of model they would like to fit and needs some help on how to implement them, but given my limited knowledge in the field I was quite struggling in following the theoretical motivation and justification for the adoption of such models. In addition the models introduced were quite old and their implementation problems were a result of the limited computational power existing at the time which nowadays can be considerably eased through modern techniques. Finally, none of these references had substantial introduction to IRT from a Bayesian perspective, which is the type of approach I would like to use for such analyses given its flexibility and ease of implementation when dealing with models of high complexity. This is probably due to lack of interest of the authors with such framework and even possibly its limited development from an application perspective of the Bayesian inference due to again computational issues.\nLuckily, I have recently discover a very nice and quite recent book called Bayesian Item Response Modeling, from Fox, which nicely summarises the key concepts of IRT from a Bayesian perspective. This is exactly what I was looking for! I have seen that the author from previous IRT manuals was also involved in the writing/giving feedback of the book which is a further hint of reassurance about the quality of the work. I have started reading the first chapters and I really like it so far. The writing is simple and easy to follow and a clear distinction between the more theoretical parts delving into the logic behind the use of such models for handling different types of questionnaire data and the more computational parts where more in-depth explanation about the algorithm used to fit the models are given. I have noticed how the book also goes beyond standard IRT framework with a strong focus on later chapters on multilevel data handling and joint modelling of multivariate outcomes which, for the moment, is not of my interest (but you never know!).\nA nice thing about the book is certainly its gentle introduction to IRT which is really appealing to someone like me who only had a vague idea behind the scope of the models, while also providing a quick recap about Bayesian inference (well I did not need that but it is always good to read it again) and some real data examples of most standard modelling approaches with their pros and cons. There are even some snapshots of the old-dear WinBUGS software that was used to fit most of the models (the book is from 1010 so not super recent) with some example codes. Perhaps, if I have to find a negative element so far is precisely the limited amount of software code sessions present across the chapters. I think that a full provision of codes (even just as links to some online repository) would have improved even further the accessibility and appealing of the book to non-statisticians or analysts who would like to try the models themselves but do not know ehre to start with a Bayesian model script.\nRegardless, the first few chapters are really nice in terms of providing the background of standard IRT models based on different types of questionnaire data (e.g. binary, ordinal/nominal polytomous, etc…) with some nice explanations on why fitting such models within a Bayesian framework could be beneficial compared to standard classical methods. Here I take some of the author’s words directly from the introduction of the book to make the point:\n\nThe Bayesian modeling framework supports in a natural way extensions of common item response models. The response model parameters are described via prior models at separate levels to account for different sources of uncertainty, complex dependencies, and other sources of information. This flexibility in defining prior models for the item response model parameters is one of the strengths of Bayesian modeling that makes it possible to handle for example more complex sampling designs comprising complex dependency structures.\n\nI completely agree with the statement and I am curious to see in the next chapters how much further the Bayesian potential is covered. From the topics mentioned I can already imagine that, beside an advantage from a computational perspective via MCMC methods, possible reasons will have to deal with the need to account for multilevel structures (e.g. longitudinal or cross-country data) and uncertainty quantification at different hierarchical levels (e.g. person/item level). I thing I am eager to know is the main reason I started the investigation of this topic, i.e. handling of missing item values. Of course, it is well known that such questionnaire data often suffer from missingness problems and it is also quite known (even though often put under the carpet) that standard manuals recommend some sort of ad-hoc imputation in case of partially-observed answers. This is something done to make easier the analysis of total scale scores through standard statistical methods although the implication of such assumptions about the unobserved values is rarely discussed in most cases. I do not expect this to be the main topic of the book but I would be glad to see if this is mentioned anywhere as a possible advantage of Bayesian inference (which I think it is!) since it allows quantification of any unobserved quantity into a model, being it a parameter or missing value. In addition, the use of prior distributions to incorporate missingness assumptions is a powerful tool to assess the robustness of the results of an analysis to a range of plausible assumptions about missingness which standard statistical methodology often struggles with (i.e. some sort of combination of multiple imputation and iterative algorithm to fit the main model - good luck fitting that!).\nI think I rumbled enough for today. However, I would certainly talk about this topic more in the future as it is something I am currently have a ton of fun learning and I that I would also like to experiment in my own research at some point (yeah just find the time). Well, till next time!"
  },
  {
    "objectID": "posts/2023-03-10-my-blog-post/index.html",
    "href": "posts/2023-03-10-my-blog-post/index.html",
    "title": "The missing data dilemma",
    "section": "",
    "text": "Hello folks, as per usual I would like to try to update my blog regularly with some new posts and talk about the stuff I like. Today is the day of one of my most beloved topics, yeah we talk about missing data. In future posts I will try to provide some concrete examples in the field of health economics with also some R coding to show some neat cases in which missing data may be a substantial problem when conducting an analysis and why it is important that appropriate methods are used to deal with them. Today instead I would like to provide a gentle introduction to the general problem of addressing missing data i ntour analysis as I feel it is extremely important to introduce the topic in a more theoretical way to justify the way I will deal with them in the future within my code. I will also try to make the content of the post as much related as possible to the field of health economics to make my argument easier to follow for people involved in this field (which is also mine!).\nSo, let’s start from the basic question, what are missing data and why are they such a big problem?\nVery briefly, missing data are defined as data that were intended to be collected but that for some reasons were not. They are a common problem that complicate the statistical analysis of data collected in almost every discipline, including health economics in which almost inevitably some individuals in the sample have their cost and/or effectiveness observations missing at one or more occasions of the follow-up (e.g. missing items or questionnaires due to dropout). When the data collected from a study are incomplete, there are important implications for their analysis.\n\nFirst, the reduction of the available number of observations causes loss of information and a decrease in the precision of the estimates, which is directly related to the amount of missing data.\nSecond, missing data can introduce bias, resulting in misleading inferences about the parameters of interest. More specifically, the validity of any method is dependent on untestable assumptions about the reasons why missing values occur being true.\n\nAs a result, caution is required in drawing conclusions from the analysis of incomplete data and the reasons behind any missingness must always be carefully considered. Before conducting any analysis with incomplete data, it is important to understand a number of aspects of the missingness to guide the choice of appropriate methods to handle them. This can be achieved by answering three general questions: where (which variables have missing vales); how much (amount and patterns of missingness); why (reasons for the missing data).\nWhere. A first important distinction is whether missingness affects the outcomes (e.g. costs and effects) or the covariates. In the context of trial-based analyses, outcome data are typically collected at a set of time points, while covariates are often available only at baseline.\nHow much. A second important aspect to consider before conducting the analysis is to check the amount and pattern of missing data, which are likely to provide some preliminary information about the magnitude of the potential impact of missing data uncertainty on the inferences. Depending on which outcome variable and at which occasions missingness occurred, different missing data patterns may arise (e.g. when a missing observation at time \\(j\\) implies that all observations measured at subsequent occasions are also missing, then missingnes is refereed to as dropout).\nWhy. A third, and also the most important, aspect to take into consideration when analysing incomplete data, is to think about possible reasons for the missingness. Why subjects dropped out from the study? Are reasons likely to be different between missingness patterns, treatment groups and type of outcome variables? Do the subjects who dropped out have similar characteristics to individuals who remained in the study (also known as completers)? Any sort of information that helps answering these questions may be key in order to determine the plausibility of the missingness assumptions in the context considered and guide the choice of the methods.\nInvestigating possible reasons behind missing data becomes a crucial element in the analysis to quantify the uncertainty associated with the missed responses. This can be formally translated into questions about the process responsible of missingness, often referred to as the missing data mechanism, and its implications and links to the data generating process. The missing data mechanism describes the probabilistic process which determines the chance that an observation is missing. Different “classes” or types of mechanisms can be defined according to different assumptions about how the missing data indicator (M) is related to the outcome (Y) and covariates (Xs). Based on Rubin’s taxonomy, three general classes of missingness mechanisms can be identified. Each of these classes is associated with different reasons for missingness and have therefore implications in terms of the appropriateness of different methods of analysis. In the following, I will show some graphical examples to show how different classes of missingness mechanism look like in a general analysis context. To clarify notation: variables and parameters are represented through nodes of different shapes and colours. Parameters are indicated by grey circles with logical parameters defined by double circles, while predictor variables are assumed fixed and drawn as white squares. Fully observed variables are denoted by white circles, partially observed variables by darker grey circles. Nodes are related to each other through dashed and solid arrows which respectively represent logical functions and stochastic dependence. MoA = model of analysis (model of interest to fit), MoM = model of missingness (missing data mechanism).\n1 Missing Completely At Random (MCAR), where missingness is independent of both any observed or unobserved data. The below diagram displays an hypothetical example of a MCAR mechanim.\n\n\n\nMCAR mechanism example\n\n\nIn this case, the probability of missingness is fully independent of any other partially or fully observed variable. Consequently, MoA and MoM are not connected and the missingness probability (\\(\\pi_i\\)) does not depend on any quantity in the MoA. This amounts to assuming that there is no systematic difference between partially and fully observed individuals in terms of the outcome \\(y_i\\). For example, a MCAR mechanism arises when a sample of patient questionnaires is lost and therefore any missing value is exclusively due to random chance. Two important conclusions can be drawn under MCAR: first, the completers can be thought as a random sample of the target population; second, any method that yields valid inferences in the absence of missingness will also yield valid (but inefficient) inferences using the completers alone.\n2 Missing At Random (MAR), where missingness is independent of both any unobserved data after conditioning on the observed data. The below diagram displays an hypothetical example of a MAR mechanism.\n\n\n\nMAR mechanism example\n\n\nIn this case, the missingness probability may depend on a fully observed variable. As a result, MoA and MoM are connected by means of the predictor variable affecting both the mechanisms generating \\(y_i\\) and \\(m_i\\). Because of this relationship, the partially observed cases are systematically different from the fully observed cases; crucially, however, the difference is fully captured by \\(x_i\\). An example of a MAR mechanism is when an individual is removed from a study as soon as the value of a specific observed variable (e.g. age or BMI) falls outside a pre-specified range. Missingness is therefore under the control of the investigator and is related to some observed components. The MAR assumption has the following important implications: first, the completers are not a random sample from the target population and any analysis restricted to them will yield biased inferences; second, provided the parameters indexing the missingness process are “distinct” from those of the data process, the missing data mechanism is referred to as ignorable. This implies that the missing values can be validly “predicted” using the observed data and a correct model for the joint distribution of \\(Y\\).\n3 Missing Not At Random (MNAR), here missingness depends on some unobserved data even after conditioning on the observed data. The below diagram displays an hypothetical example of a MNAR mechanism.\n\n\n\nMNAR mechanism example\n\n\nIn this case, the probability of missingness depends on both the partially and fully observed variables. Thus, \\(\\pi_i\\) depends on both the fully observed predictor \\(x_i\\) and the partially observed outcome \\(y_i\\). This means that the difference between fully and partially observed cases still depends on the missing values, even after taking \\(x_i\\) into account. Therefore, it is necessary to make more structured assumptions about this relationship that go beyond the information contained in the data. A MNAR mechanism is often referred to as non-ignorable missingness because the missing data mechanism cannot be ignored but must be explicitly modelled in order to make inferences about the distribution of the complete data. We note that the term informative missingness is also sometimes used to describe MNAR mechanisms since the model assumed for \\(M\\) is critical and can drive the results of the analysis.\nIt is important to clarify that any assumptions made about the missingness process can never be verified from the data at hand, i.e. the observed data provide no information that can either support or reject a specific assumption about the mechanism over another. Because of this, identification is driven by unverifiable assumptions and conducting sensitivity analyses becomes a crucial task. Indeed, the sensitivity of inferences to a set of plausible assumptions concerning the missingness process should always be carefully assessed. For a more comprehensive and adequate review of this topic, please see the book Statistical Analysis with Missing Data by Rubin and Little, which I consider a mandatory reading if you aim to dive into the missing data literature and know something more about it.\nI think that is enough for today’s topic as the next one that will need to be addressed is about the available methods that can be used to address the different types of mechanisms’ assumptions. This is a whole new argument which in itself can take a lot of time to discuss depending on how in-depth its presentation is talked. To make sure the content of such an argument goes through I will simply talk about it next time. So, if you are eager to know about what methods are available to deal with what type of mechanisms’ assumptions, say tuned! (spoiler, with missing data most of time there is no unique correct method!). More on this soon."
  },
  {
    "objectID": "posts/2023-01-10-my-blog-post/index.html",
    "href": "posts/2023-01-10-my-blog-post/index.html",
    "title": "How to jointly handle skewed data in economic evaluations",
    "section": "",
    "text": "Hello folks and happy new year! It feels like a million years since last time I posted here but it is in fact only a month (I really try to keep it constant despite all things happening!) since my last update. Maybe the Xmas holidays in between give me the illusion of lot of time passing. Regardless, here I am back again with some new exciting content about doing economic evaluations with R.\nToday I wanted to look at the importance that the violation of the assumption of Normality for health economics data, e.g. typically costs but also QALYs, may have on the final results of a typical trial-based analyis. In addition, as perhaps those familiar with this filed knows, presence of substantial level of skewness (and so clear indication of non-Normality) often characterises HE data in combination with small sample sizes which, in turn, may lead to results that are not robust to departures from the Normality assumption. This is often true for costs (by definition on how this variable is computed - positive skewness) but also for the effects (e.g. QALYs - negative or positive skewness) in many other cases. As usual, in order to deal with these concepts in a simplified framework, I will focus in my examples on total costs and QALYs variables that are assumed to be computed on all individuals in a trial after aggregating the individual costs and utilities collected at each time point during the trial (see previous posts to know more about this). In addition to what we saw so far, we now also incorporate the element of correlation between the two outcomes that was introduced in the last post, therefore taking into account the association between the two non-Normally distributed variables.\nFor example, let’s start by simulating some non-Normal bivariate cost and QALY data for a total of \\(300\\) patients assigned to two competing intervention groups (\\(t=0,1\\)). Typically, costs are skewed to the right with a few individuals presenting very high cost values and the majority having small costs; the nature of the distribution of the QALYs can be quite different depending on the field considered: in a context of relatively healthy subjects and a non-life threatening intervention, QALYs can be expected to have a distribution skewed to the left. When generating the data, we can try to mimic these features by using alternative distributions to the Normal that possess such characteristics, e.g. Gamma for costs and Beta for QALYs.\n\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nmean_e1 &lt;- c(0.5)\nmean_e2 &lt;- c(0.7)\nsigma_e &lt;- 0.15\ntau1_e &lt;- ((mean_e1*(1-mean_e1))/(sigma_e^2)-1)\ntau2_e &lt;- ((mean_e2*(1-mean_e2))/(sigma_e^2)-1)\nalpha1_beta &lt;- tau1_e*mean_e1\nbeta1_beta &lt;- tau1_e*(1-mean_e1)\nalpha2_beta &lt;- tau2_e*mean_e2\nbeta2_beta &lt;- tau2_e*(1-mean_e2)\ne1 &lt;- rbeta(n/2, alpha1_beta, beta1_beta)\ne2 &lt;- rbeta(n/2, alpha2_beta, beta2_beta)\n\nmean_c1 &lt;- 500\nmean_c2 &lt;- 1000\nsigma_c &lt;- 300\ntau1_c &lt;- mean_c1/(sigma_c^2)\ntau2_c &lt;- mean_c2/(sigma_c^2)\nln.mean_c1 &lt;- log(500) + 5*(e1-mean(e1)) \nc1 &lt;- rgamma(n/2, (exp(ln.mean_c1)/sigma_c)^2, exp(ln.mean_c1)/(sigma_c^2))\nln.mean_c2 &lt;- log(1000) + 5*(e2-mean(e2)) + rgamma(n/2,0,tau2_c)\nc2 &lt;- rgamma(n/2, (exp(ln.mean_c2)/sigma_c)^2, exp(ln.mean_c2)/(sigma_c^2))\n\nQALYs &lt;- c(e1,e2)\nCosts &lt;- c(c1,c2)\n\ndata_sim_ec &lt;- data.frame(id, trt, QALYs, Costs)\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\n\nIn the code above I simulated first QALY data within each arm and then specified the conditional mean of the cost variables given the effect values generated in the respective arm. This way I can generate the two variables in each arm while also capturing the dependence between the variables expressed in terms of the regression coefficient linking the conditional mean costs to the QALYs variables. To have a better understanding of how suc coefficient \\(\\beta\\) may impact the correlation between the two variables we can simulate the data with different values of \\(\\beta\\) affects the final correlation value between the variables by treatment group.\n\n#empirical correlation between e and c (across groups)\ncor(data_sim_ec$QALYs,data_sim_ec$Costs)\n\n[1] 0.8399915\n\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nggplot(data_sim_ec, aes(x=QALYs, y=Costs)) +\n  geom_point(size=2, shape=16) + theme_classic() +\n  facet_wrap(~trtf)\n\n\n\n\n\n\n\n\nBoth the computed correlation and graphs clearly indicate the existence of a strong positive correlation between the outcomes within each arm. The use of Beta and Gamma distribution also ensures that such variables are non-Normally distributed, and we can check this by plotting an histogram of the data.\n\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nQALY_hist &lt;- ggplot(data_sim_ec, aes(x=QALYs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist &lt;- ggplot(data_sim_ec, aes(x=Costs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\nIn order to account for correlation while also avoiding the specification of normal assumptions for both variables (which here is violated), health economists can use two different approaches:\n\nActually model the dependence between the variables through a statistical model that does not rely on Normality assumptions but which captures the relationship between the variables.\n\nThis is the most difficult solution as there are very few methods that can be easily implemented in standard software which are able to accomplish the task as most of these will rely on Normality. Within a classical statistical framework, methods such as Generalised Linear Regression models can be used to account for skewness, e.g. Beta regression or Gamma regression. These, however, fail to account for correlation between variables and may lead to estimates that are too uncertain compared to those obtained if correlation is modelled. A possible solution to this problem is to specify the models within a Bayesian statistical framework which by its nature is well equipped to deal with non-Normal distribution and joint model of multiple variables. This is something we will see in future posts.\n\nModel the variables as if they were independent and Normally distributed BUT then apply bootstrapping as a non-parametric technique (see previous posts for an introduction to the method) to generate estimate that have been proven to be robust to deviation from Normality and which are also able to indirectly capture the correlation between variables when these are bootstrapped together.\n\nAs we saw in past posts, in R, we can for example use the boot package which allows to flexibly define our bootstrapping procedure and to include within the procedure our linear regression model using the following commands:\n\nlibrary(boot)\nboot_lm &lt;- function(data, i){\n  data2 &lt;- data[i,]\n  lm.boot.Q &lt;- lm(QALYs ~ trtf, data = data2)\n  lm.boot.TC &lt;- lm(Costs ~ trtf, data = data2)\n  em.lm.boot.Q &lt;- emmeans(lm.boot.Q,~trtf)\n  em.lm.boot.TC &lt;- emmeans(lm.boot.TC,~trtf)\n  trt_diffQ &lt;- coef(lm.boot.Q)[2]\n  trt_diffTC &lt;- coef(lm.boot.TC)[2]\n  mean_Q &lt;- summary(em.lm.boot.Q)[,2]\n  mean_TC &lt;- summary(em.lm.boot.TC)[,2]\n  return(c(trt_diffQ,trt_diffTC,mean_Q,mean_TC))\n}\n\nset.seed(4567) \nlibrary(emmeans)\nboot_est_lm &lt;- boot(data_sim_ec, boot_lm, R=1000)\ndelta_e_boot&lt;-boot_est_lm$t[,1]\nmu_e_boot&lt;-cbind(boot_est_lm$t[,3],boot_est_lm$t[,4])\ndelta_tc_boot&lt;-boot_est_lm$t[,2]\nmu_tc_boot&lt;-cbind(boot_est_lm$t[,5],boot_est_lm$t[,6])\n\nThe code above proceeds to create a function that repeat the same process a given number of times, equivalent to \\(B=1000\\), the number bootstrap samples. The higher \\(B\\) the more robust will be the final estimate of the mean outcomes. Within the function, R fits separate linear regression to the QALY and Cost data to retrieve the corresponding mean estimates and differentials between groups. The process is finally iterated via the boot function to generate the bootstrap samples using sampling with replacement and I set the number of replications that I want to use in the argument R. Finally, after the command has finished running, I create objects containing the set of bootstrapped values for each of the stored quantities by extracting these from the output of the boot function. For example, we can inspect the mean estimates for QALYs and Total costs by group by typing:\n\ndata_est_mean &lt;- data.frame(c(mu_e_boot[,1],mu_e_boot[,2], mu_tc_boot[,1], mu_tc_boot[,2]))\nnames(data_est_mean)[1] &lt;- \"value\"\ndata_est_mean$outcome &lt;- c(rep(\"QALY\",dim(mu_e_boot)[1]*2), rep(\"Cost\",dim(mu_e_boot)[1]*2))\ndata_est_mean$trt &lt;- c(rep(\"control\",dim(mu_e_boot)[1]), rep(\"intervention\",dim(mu_e_boot)[1]), rep(\"control\",dim(mu_e_boot)[1]), rep(\"intervention\",dim(mu_e_boot)[1]))\ndata_est_mean$outcome &lt;- factor(data_est_mean$outcome)\ndata_est_mean$trt &lt;- factor(data_est_mean$trt)\n\nggplot(data_est_mean, aes(x=value))+ xlab(\"mean\")+\n  geom_histogram(color=\"black\", fill=\"grey\")+ theme_classic() +\n  facet_wrap( ~ outcome + trt, scales = \"free\")\n\n\n\n\n\n\n\n\nIf the number of bootstrapped samples is large enough, then estimates of mean QALYs and Costs will be guaranteed to converge to the true values even under violation of Normality assumptions in the sample. In addition, when bootstrapping QALYs and Costs for the same individuals at the same time, correlation is indirectly taken into account by means of the re-sampling procedure. To note that this is not as efficient as a proper modelling of the dependence between variables but it can be used as a trick to at least not completely ignoring the correlation.\nIn future posts I will talk about to go beyond this simple approach and specify a Bayesian model which properly takes into account both correlation and skewness in the observed data. The method still relies on an iterative algorithm procedure but the theoretical underpinnings and justifications for its use are completely different and much stronger!. Especially in HE where interest is not on point estimates by themselves but rather on posterior probabilities of such estimates, the Bayesian approach represents a powerful tool to not only analyse HE data but also take into account different sources of uncertainty (e.g. missing data, sampling, model) and how these quantitatively affect the final estimation procedure (and from then the final CEA decision). Stay tuned !!!!"
  },
  {
    "objectID": "posts/2022-11-11-my-blog-post/index.html",
    "href": "posts/2022-11-11-my-blog-post/index.html",
    "title": "How to handle correlated data in economic evaluations",
    "section": "",
    "text": "Hi everybody, as per usual I am here posting (again!) to continue the thread about cost-effectiveness analysis. Last time we discussed the importance of modelling CEA data at the longitudinal level when some missingness occurs in the outcome variables, i.e. costs and/or utilities. Today we take a step back and focus once more at the aggregated level, i.e. we assume that we already calculated total costs and QALYs over the trial duration and that no missingness occurred (I know very unrealistic!). The main reason for this is that I would like to keep the analysis framework relatively simple in order to focus on today’s topic: correlation.\nAs perhaps you already know, when analysing bivariate outcome data (such as costs and QALYs), it is important that the potential correlation between the variables is taken into account at the modelling stage of the statistical analysis. Indeed, so far we have ignored correlation and modelled the two outcomes (either at the aggregated or longitudinal level) separately, therefore assuming independence between the two types of variables. This, however, can be a quite strict assumption as in practice it makes sence that the two types of outcome variables show at least some degree of association, i.e. patients receiving a new and more expensive intervention may be more likely to observe higher improvements in their benefit measures compared to those receiving a cheaper control. You can also find the same principle within the more classical statistical analysis framework of hypothesis testing, for example when considering the difference between an independent sample vs paired sample t-test. By recognising the dependence in the data in the calculation of the test statistic (specifically in the standard error calculation), uncertainty about the parameters of interest can be decreased and more accurate inferences can be drawn. The same idea applies also when analysing CEA data in that, when outcomes are strongly associated between each other, proper recognition of this dependence allows to derive more precise (i.e. less uncertain) estimates about the mean cost/QALYs incrementals.\nFor example, let’s start by simulating some correlated bivariate CEA data for a total of \\(300\\) patients assigned to two competing intervention groups (\\(t=0,1\\)).\n\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nlibrary(mvtnorm)\nmean_e1 &lt;- c(0.5)\nmean_c1 &lt;- c(500)\nmean_e2 &lt;- c(0.7)\nmean_c2 &lt;- c(1000)\nsigma_e &lt;- 0.15\nsigma_c &lt;- 300\nMean_ec &lt;- c(0,0)\nSigma_ec &lt;- diag(2)\nSigma_ec[1,2] &lt;- Sigma_ec[2,1] &lt;- 0.85\nbiv_ec1_raw &lt;- rmvnorm(n/2,mean = Mean_ec, sigma = Sigma_ec)\nbiv_ec2_raw &lt;- rmvnorm(n/2,mean = Mean_ec, sigma = Sigma_ec)\nbiv_ec1_raw[,1] &lt;- (biv_ec1_raw[,1])*sigma_e + mean_e1\nbiv_ec1_raw[,2] &lt;- (biv_ec1_raw[,2])*sigma_c + mean_c1\nbiv_ec2_raw[,1] &lt;- (biv_ec2_raw[,1])*sigma_e + mean_e2\nbiv_ec2_raw[,2] &lt;- (biv_ec2_raw[,2])*sigma_c + mean_c2\nQALYs &lt;- c(biv_ec1_raw[,1],biv_ec2_raw[,1])\nCosts &lt;- c(biv_ec1_raw[,2],biv_ec2_raw[,2])\ndata_sim_ec &lt;- data.frame(id, trt, QALYs, Costs)\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\n\nIn the code above I simulated first bivariate standard normal data within each arm to ensure the desired correlation level between the variables (easy to specify on standard normal scale) and then apply a transformation to give each arm and variable a pre-defined level of standard deviation and mean. This way, even on a different scale, the correlation between the outcome variables within each arm is (generally) preserved. We can also check this by computing the empirical correlation between the costs and QALYs variables as well as by plotting the bivariate data.\n\n#empirical correlation between e and c (across groups)\ncor(data_sim_ec$QALYs,data_sim_ec$Costs)\n\n[1] 0.8937574\n\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nggplot(data_sim_ec, aes(x=QALYs, y=Costs)) +\n  geom_point(size=2, shape=16) + theme_classic() +\n  facet_wrap(~trtf)\n\n\n\n\n\n\n\n\nBoth the computed correlation and graphs clearly indicate the existence of a strong positive (linear) correlation between the outcomes within each arm. To demonstrate the point I was making before about the importance to account for this correlation in our analysis, let’s see what happens if we simply ignore it and fit two separate models to the outcome data, e.g. to keep things simple let’s consider a standard linear regression framework with treatment as the only predictor (in theory other covariates could be included if desired):\n\\[\n\\text{QALYs}=\\alpha_0 + \\alpha_1 \\text{trt} + \\varepsilon_e,\n\\]\n\\[\n\\text{Costs}=\\beta_0 + \\beta_1 \\text{trt} + \\varepsilon_c,\n\\]\nwhere \\(\\boldsymbol \\alpha\\) and \\(\\boldsymbol \\beta\\) are the set of regression coefficients including the intercepts and slopes of the two models, while \\(\\boldsymbol \\varepsilon\\) is the set of two error terms, each assumed to be normally distributed with mean 0 and variance \\(\\sigma^2_{\\varepsilon}\\). If we fit the model in R we obtain:\n\nlm_e &lt;- lm(QALYs ~ trtf, data = data_sim_ec)\nsummary(lm_e)\n\n\nCall:\nlm(formula = QALYs ~ trtf, data = data_sim_ec)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43152 -0.10173  0.00383  0.09873  0.44933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.50807    0.01227   41.42   &lt;2e-16 ***\ntrtfnew      0.19135    0.01735   11.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1502 on 298 degrees of freedom\nMultiple R-squared:  0.2899,    Adjusted R-squared:  0.2875 \nF-statistic: 121.7 on 1 and 298 DF,  p-value: &lt; 2.2e-16\n\nci_lm_e &lt;- confint(lm_e)\nci_lm_e\n\n                2.5 %    97.5 %\n(Intercept) 0.4839332 0.5322124\ntrtfnew     0.1572113 0.2254883\n\nlm_c &lt;- lm(Costs ~ trtf, data = data_sim_ec)\nsummary(lm_c)\n\n\nCall:\nlm(formula = Costs ~ trtf, data = data_sim_ec)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-787.45 -232.10   13.89  207.59  743.68 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   513.01      23.89   21.48   &lt;2e-16 ***\ntrtfnew       490.08      33.78   14.51   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 292.6 on 298 degrees of freedom\nMultiple R-squared:  0.4139,    Adjusted R-squared:  0.4119 \nF-statistic: 210.5 on 1 and 298 DF,  p-value: &lt; 2.2e-16\n\nci_lm_c &lt;- confint(lm_c)\nci_lm_c\n\n               2.5 %   97.5 %\n(Intercept) 466.0050 560.0242\ntrtfnew     423.5962 556.5593\n\n\nWe can see that in both models treatment is a significant variable in accordance with what specified in our simulation design, i.e. a mean population difference between groups of \\(0.2\\) for QALYs and \\(500\\) for Costs (estimates of parameters very close to dgp values). We can also see that the confidence intervals for these parameters cover the ranges \\([0.1572113; 0.2254883]\\) for QALYs and \\([423.5961573; 556.5593191]\\) for Costs.\nWell, so far we have ignored the correlation between the two variables. Now, let’s try to take that into account at the modelling stage. Different approaches can be used to achieve this task and here we will only look at a couple of these. Perhaps, the most intuitive choice is simply to include Costs and QALYs as predictor in the model for the other outcome. Indeed, since each otucome is correlated with the other and also with treatment, they essentially represent “confounders” that could be included into the model to obtain more precise estimates around the regression coefficients. Thus, the models become:\n\\[\n\\text{QALYs}=\\alpha_0 + \\alpha_1 \\text{trt} + \\alpha_2\\text{Costs} + \\varepsilon_e,\n\\]\n\\[\n\\text{Costs}=\\beta_0 + \\beta_1 \\text{trt} + \\beta_2\\text{QALYs} + \\varepsilon_c,\n\\]\nand in R we can fit the models by typing:\n\nlm_e2 &lt;- lm(QALYs ~ trtf + Costs, data = data_sim_ec)\nsummary(lm_e2)\n\n\nCall:\nlm(formula = QALYs ~ trtf + Costs, data = data_sim_ec)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.227528 -0.056729  0.000873  0.045915  0.209448 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.846e-01  1.038e-02  27.414   &lt;2e-16 ***\ntrtfnew     -2.217e-02  1.201e-02  -1.845    0.066 .  \nCosts        4.357e-04  1.577e-05  27.626   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.07965 on 297 degrees of freedom\nMultiple R-squared:  0.8011,    Adjusted R-squared:  0.7997 \nF-statistic:   598 on 2 and 297 DF,  p-value: &lt; 2.2e-16\n\nlm_c2 &lt;- lm(Costs ~ trtf + QALYs, data = data_sim_ec)\nsummary(lm_c2)\n\n\nCall:\nlm(formula = Costs ~ trtf + QALYs, data = data_sim_ec)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-415.32 -105.24    6.42  101.25  440.31 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -326.46      32.92  -9.917  &lt; 2e-16 ***\ntrtfnew       173.92      21.25   8.183 8.23e-15 ***\nQALYs        1652.27      59.81  27.626  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 155.1 on 297 degrees of freedom\nMultiple R-squared:  0.8358,    Adjusted R-squared:  0.8347 \nF-statistic:   756 on 2 and 297 DF,  p-value: &lt; 2.2e-16\n\n\nYou see that the estimates for the treatment effect are now off and this is simply because now we also have an additional variable in each model. In order to retrieve the correct estimates in terms of mean difference between arms we can either include the new predictors as centred variables (i.e. mean of 0) or calculate the marginal means of each outcome as linear combinations of the parameters from the model. In R the second approach can be easily achieved using the emmeans package.\n\nlibrary(emmeans)\nmu_e &lt;- emmeans(lm_e2, ~ trtf + Costs)\ndelta_QALYs &lt;- contrast(mu_e, list(QALYs = c(0,1)))\ndelta_QALYs\n\n contrast estimate      SE  df t.ratio p.value\n QALYs       0.593 0.00756 297  78.345  &lt;.0001\n\nconfint(delta_QALYs)\n\n contrast estimate      SE  df lower.CL upper.CL\n QALYs       0.593 0.00756 297    0.578    0.608\n\nConfidence level used: 0.95 \n\nmu_c &lt;- emmeans(lm_c2, ~ trtf + QALYs)\ndelta_Totalcosts &lt;- contrast(mu_c, list(Tcosts = c(0,1)))\ndelta_Totalcosts\n\n contrast estimate   SE  df t.ratio p.value\n Tcosts        845 13.9 297  60.805  &lt;.0001\n\nconfint(delta_Totalcosts)\n\n contrast estimate   SE  df lower.CL upper.CL\n Tcosts        845 13.9 297      818      872\n\nConfidence level used: 0.95 \n\n\nYou can see how the confidence intervals are now much narrower compared to what obtained under the independent models and this is precisely because of the fact that, when deriving these estimates, the correlation between the outcomes has been taken into account, thus allowing to reduce the uncertainty around the quantities estimated. Notice that estimates for the trt effect changes compared to the independent models also in terms of point estimates simply because when simulating each type of outcome data the mean outcome difference between groups was not affected by the other type of outcome. In reality, it may be plausible that the actual trt effect is also affected by the other outcome, thus making the inclusion of QALYs and Costs into the model for the other variable actually relevant in order to retrieve adjusted estimates for the trt effect.\nDespite allowing the inclusion of one outcome into the model for the other outcome, thus accounting for correlation between the two, a possible limitation of the above approach is that it requires both outcome variables to be fully observed. In practice this may be difficult to obtain and inclusion of QALYs or Costs are predictors into the other model may substantially reduce the available sample size to estimate the trt effects, i.e. individuals with missing values are discarded from the analyses, potentially leading to a situation in which estimates for the mean difference between groups for QALYs and Costs are derived based on a different number of observations. A possible solution to this problem is the implementation of the so-called Seemingly Unrelated Regression (SUR) models which allow the simultaneous fitting of multiple linear regression models while also indirectly taking into account the correlation between the outcomes of each model. The models look something like a “system” of independent models equations:\n\\[\n\\text{QALYs}=\\alpha_0 + \\alpha_1 \\text{trt} + \\varepsilon_e,\n\\]\n\\[\n\\text{Costs}=\\beta_0 + \\beta_1 \\text{trt} + \\varepsilon_c,\n\\]\nwith the error terms \\(\\boldsymbol \\varepsilon\\) that are assumed to be independent between observations but that may have cross-equation correlations between observations \\(i=1,\\ldots,n\\), i.e. we assume that \\(\\text{E}[\\varepsilon_{ie} \\varepsilon_{ic} \\mid \\text{trt}]=0\\) BUT \\(\\text{E}[\\varepsilon_{ie} \\varepsilon_{je} \\mid \\text{trt}]=\\sigma_{ij}\\) and \\(\\text{E}[\\varepsilon_{ic} \\varepsilon_{jc} \\mid \\text{trt}]=\\sigma_{ij}\\). The main idea behind the approach is that it allows to capture the correlation between different individuals within each outcome regression model. In fact, it can be shown to be equivalent to standard OLS models when no cross-equation correlations between the error terms are present. Since the purpose of this post is to illustrate the approach, rather than a theoretical discussion about it, I will simply avoid a lengthy explanation of the methods and go straight to the point. We can fit these models in R using the systemfit package.\n\nlibrary(systemfit)\nsur_e_formula &lt;- QALYs ~ trtf\nsur_c_formula &lt;- Costs ~ trtf\nsystem &lt;- list(QALYs = sur_e_formula, Costs = sur_c_formula)\nsur_ec &lt;- systemfit(system, data = data_sim_ec)\nsummary(sur_ec)\n\n\nsystemfit results \nmethod: OLS \n\n         N  DF      SSR detRCov  OLS-R2 McElroy-R2\nsystem 600 596 25506409 541.154 0.41391   0.266266\n\n        N  DF         SSR        MSE       RMSE       R2   Adj R2\nQALYs 300 298 6.72567e+00 2.2569e-02   0.150231 0.289925 0.287542\nCosts 300 298 2.55064e+07 8.5592e+04 292.561029 0.413910 0.411943\n\nThe covariance matrix of the residuals\n           QALYs      Costs\nQALYs  0.0225694    37.2908\nCosts 37.2907794 85591.9554\n\nThe correlations of the residuals\n         QALYs    Costs\nQALYs 1.000000 0.848448\nCosts 0.848448 1.000000\n\n\nOLS estimates for 'QALYs' (equation 1)\nModel Formula: QALYs ~ trtf\n\n             Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept) 0.5080728  0.0122663 41.4202 &lt; 2.22e-16 ***\ntrtfnew     0.1913498  0.0173472 11.0306 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.150231 on 298 degrees of freedom\nNumber of observations: 300 Degrees of Freedom: 298 \nSSR: 6.72567 MSE: 0.022569 Root MSE: 0.150231 \nMultiple R-Squared: 0.289925 Adjusted R-Squared: 0.287542 \n\n\nOLS estimates for 'Costs' (equation 2)\nModel Formula: Costs ~ trtf\n\n            Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept) 513.0146    23.8875 21.4763 &lt; 2.22e-16 ***\ntrtfnew     490.0777    33.7820 14.5070 &lt; 2.22e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 292.561029 on 298 degrees of freedom\nNumber of observations: 300 Degrees of Freedom: 298 \nSSR: 25506402.718561 MSE: 85591.955431 Root MSE: 292.561029 \nMultiple R-Squared: 0.41391 Adjusted R-Squared: 0.411943 \n\nci_sur_ec &lt;- confint(sur_ec)\nci_sur_ec\n\n                    2.5 %  97.5 %\nQALYs_(Intercept)   0.484   0.532\nQALYs_trtfnew       0.157   0.225\nCosts_(Intercept) 466.005 560.024\nCosts_trtfnew     423.596 556.559\n\n\nWe can see how the estimates and confidence intervals are those obtained from the independent models. From this it seems that they actually do not provide any advantage over fitting the models for each outcome separately and in this specific case that is correct. However, when the equations in the system are associated with different predictors (e.g. account for some comorbidities in the QALYs model but not in the Costs model), then differences between standard OLS estimate and SUR estimates will arise. Indeed, since SUR allows to capture correlations between observations for different individuals for each outcome separately, then the inclusion of additional predictors in each model will provide extra-information to estimate these correlations. Since this type of extra-information is specific to the outcome model chosen, then difference in the number of predictors will allow to obtain more precise (i.e. less uncertain) estimate for the trt effect in each equation. By carefully choosing the predictors included in each equation, SURs allow to improve the efficiency of the estimates of the regression coefficients based on the information provided within each equation of the system.\nI hope this post was helpful for anybody who wants to perform SUR or has heard about it and was not sure when the method is applicable and in which case it can actually be beneficial compared to simple independent OLS analysis. It certainly provides a more appropriate way to model bivariate data compared to assuming compelte independence between the outcome models but it has also some limitations in that it does not correspond to a proper joint modelling of the outcomes. Next times I will also discuss a different approach which can allow to overcome the limitations of SUR in order to account for the correlation between outcomes in a proper way. This is joint modelling in which both outcomes are assumed to follow some multivariate distribution (e.g. bivariate normal) in which the dependence between observations and outcomes can be fully specified through parameters. Till next time!"
  },
  {
    "objectID": "posts/2022-09-09-my-blog-post/index.html",
    "href": "posts/2022-09-09-my-blog-post/index.html",
    "title": "How to use bootstrapping in economic evaluations",
    "section": "",
    "text": "Hi everyone, I hope you are ok as this period has been quite busy for me. I had a lot going on, from the resuming of intense teaching activities, lots of boring staff meetings to attend and supervision to do. Luckily I was also able to submit my grant proposal for which I will not receive a response before the next 3-4 months. However, the important thing is that is now done and I just need to wait and see if this year I am lucky enough (wish me good luck!).\nAs for my updates, there is nothing important to report as this months I have been mostly focussed on getting back to the usual working rhythm and I do not have really much spare time to experiment something new myself. Hopefully, that will change in the future but I don’t want to be too optimistic as we know how this usually works out. Regardless, I thought it could be a nice opportunity to go through some basic applications in economic evaluations that I have noticed people who are not statisticians can have hard times with. Perhaps I will make these posts about conducting economic evaluations a thread if I feel like it as there is a lot to talk about. Today I thought to start with the basic of bootstrapping, a nonparametric technique that is often used in economic evaluations to derive standardised output of decision making such as the cost-effectiveness plane and acceptability curve. To be honest I am not a fan of bootstrapping as I find it quite unclear in terms of statistical properties and there is no general rule that suggests which specific approach works best across different scenarios (Bayesian methods are much better for this, trust me!). However, since it is a very popular and relatively easy to implement method in practice, I think it may be useful that I also cover it here. Next to come for possible future posts could be more advanced stuff such as mixed models and Bayesian approaches for trial-based cost-effectiveness analysis.\nSo, let’s start by considering a fake example where we generate some cost-effectiveness data from an hypothetical trial comparing two treatments (new \\(t=0\\) versus old \\(t=1\\)) on a total of \\(n=300\\) individuals. For the sake of simplicity, here I will assume that aggregated variables, such as QALYs and Total costs, have already been computed across the entire duration of the trial based on self-reported instruments (e.g. EQ5D-5L questionnaire) collected for each participant at different times. First, we generate our effect \\(e_i\\) and cost \\(c_i\\) data using some probability distributions to replicate their usual features (e.g. skewness) while also trying to respect their typical boundary values, e.g. between 0 and 1 for utility scores and positive values for costs.\n\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- sample(c(rep(0, n/2),rep(1, n/2)))\nQALY &lt;- Tcost &lt;- rep(NA, n)\nQALY[trt==0] &lt;- rbeta(n/2, 0.5, 1)\nQALY[trt==1] &lt;- rbeta(n/2, 0.7, 0.9)\ntcost_mean &lt;- 200 + 300*trt + 700*QALY\ntcost_sd &lt;- 300\nshape_c &lt;- (tcost_mean)^2/(tcost_sd)^2\nrate_c &lt;- tcost_mean/(tcost_sd)^2\nTcost &lt;- rgamma(n, shape_c, rate_c)\ndata_sim_ec &lt;- data.frame(id, trt, QALY, Tcost)\n\nWith the above commands I have generated QALY and Total cost data for \\(n=300\\) individuals enrolled in the two treatments using Beta and Gamma distributions, respectively. I chose these distributions as they are quite popular for giving the data typical features encountered in practice, such as positive skewness for cost and QALYs. We can inspect the generated data using histograms:\n\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nQALY_hist &lt;- ggplot(data_sim_ec, aes(x=QALY))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist &lt;- ggplot(data_sim_ec, aes(x=Tcost))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\nFrom the graphs it is apparent that both outcomes do not nicely follow a normal distribution but present some degrees of skewness. This, typically, is not a problem when inference are about the means but given the limited amount of data and that the objective is more about representing uncertainty rather than statistical inference, it is a good idea to use methods that can take this feature of the data into account when reflecting the uncertainty about the cost-effectiveness of the new treatment compared to the old one. In addition, the code above also generates the bivariate data assuming some dependence relationship between the variables since cost data are generated as a function of effect data. This is done in order to reflect the typical feature that these types of data are usually associated. We can inspect summary statistics of the data by treatment group by typing:\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\ndata_sim_ec_stats &lt;- data_sim_ec[,c(\"QALY\",\"Tcost\",\"trtf\")]\nd.summary &lt;- data_sim_ec_stats %&gt;%                               \n  group_by(trtf) %&gt;% \n  summarize(sdQ = sd(QALY), sdTC = sd(Tcost),\n            medianQ = median(QALY), medianTC = median(Tcost),\n            meanQ = mean(QALY), meanTC = mean(Tcost),\n            rangeQ = range(QALY), rangeTC = range(Tcost),\n            corQTC = cor(QALY,Tcost))\nkable(d.summary, caption = \"Summary statistics\", format = \"html\", digits = 1)\n\nSummary statistics\n\n\ntrtf\nsdQ\nsdTC\nmedianQ\nmedianTC\nmeanQ\nmeanTC\nrangeQ\nrangeTC\ncorQTC\n\n\n\n\nold\n0.3\n385.6\n0.2\n301.1\n0.3\n427.5\n0\n0.5\n0.6\n\n\nold\n0.3\n385.6\n0.2\n301.1\n0.3\n427.5\n1\n1602.7\n0.6\n\n\nnew\n0.3\n354.3\n0.4\n855.8\n0.5\n864.3\n0\n161.9\n0.7\n\n\nnew\n0.3\n354.3\n0.4\n855.8\n0.5\n864.3\n1\n1780.8\n0.7\n\n\n\n\n\nWe can see that both groups have similar sd, ranges and positive correlations between QALY and Total cost data. However, the key location measures, such as the mean, show some noticeable differences with \\(t=1\\) being associated with both higher QALY and Total cost values. Let’s now try to run a quick linear regression model to check if there is any statistically significant effect in terms of mean differences between the two groups in the two outcomes. Keep in mind that this analysis is only for demonstrative purposes as in real economic evaluations you don’t really want to assess statistical significance since: both outcomes are usually secondary outcomes so there is usually not much power to detect anything;the objective of the analysis is to assess uncertainty about probability of cost-effectiveness between the groups not statistical inference. Having specified this, let’s run the model:\n\nlm_Q &lt;- lm(QALY ~ trtf, data = data_sim_ec)\nlm_TC &lt;- lm(Tcost ~ trtf, data = data_sim_ec)\nsummary(lm_Q)\n\n\nCall:\nlm(formula = QALY ~ trtf, data = data_sim_ec)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4811 -0.2759 -0.0612  0.2646  0.6712 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.32600    0.02510  12.989  &lt; 2e-16 ***\ntrtfnew      0.15605    0.03549   4.397 1.53e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3074 on 298 degrees of freedom\nMultiple R-squared:  0.06091,   Adjusted R-squared:  0.05776 \nF-statistic: 19.33 on 1 and 298 DF,  p-value: 1.532e-05\n\nconfint(lm_Q)\n\n                 2.5 %    97.5 %\n(Intercept) 0.27660392 0.3753880\ntrtfnew     0.08620041 0.2259021\n\nsummary(lm_TC)\n\n\nCall:\nlm(formula = Tcost ~ trtf, data = data_sim_ec)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-702.44 -300.62  -60.65  218.74 1175.16 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   427.54      30.23   14.14   &lt;2e-16 ***\ntrtfnew       436.80      42.76   10.22   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 370.3 on 298 degrees of freedom\nMultiple R-squared:  0.2594,    Adjusted R-squared:  0.2569 \nF-statistic: 104.4 on 1 and 298 DF,  p-value: &lt; 2.2e-16\n\nconfint(lm_TC)\n\n               2.5 %   97.5 %\n(Intercept) 368.0447 487.0387\ntrtfnew     352.6620 520.9449\n\n\nWe can also retrieve the mean outcomes and related statistics for each group using the emmeans package’s command:\n\nlibrary(emmeans)\nemmeans(lm_Q,~ trtf)\n\n trtf emmean     SE  df lower.CL upper.CL\n old   0.326 0.0251 298    0.277    0.375\n new   0.482 0.0251 298    0.433    0.531\n\nConfidence level used: 0.95 \n\nemmeans(lm_TC,~ trtf)\n\n trtf emmean   SE  df lower.CL upper.CL\n old     428 30.2 298      368      487\n new     864 30.2 298      805      924\n\nConfidence level used: 0.95 \n\n\nThe output of both regressions suggests that for both outcomes there is a statistically significant effect in favour of a more effective and also expensive new treatment as compared with the old one. However, this does not answer our main question about cost-effectiveness probability. In addition, the model makes some strong assumptions such as the normality of both outcomes (clearly violated here) as well as the independence between them (also unrealistic here). So, how can we take these features into account while also deriving an answer to our main question for decision makers? Well different approaches can be used. Here we focus on the use of nonparametric bootstrapping.\nThe main idea behind the method is based on the following steps:\n1 First, perform a sample with replacement of your QALY and Total cost data in each treatment group so to generate a new sample, called bootstrap sample, of the same size as the original but with each element that is randomly drawn using the pool of the original data.\n2 Compute summary statistics of interest (e.g. means and sds) or fit the desired model of analysis (e.g. linear regression) to the bootstrap sample instead of the original dataset and derive and store the desired estimates.\n3 Repeat step 1 and 2 for a large number of times \\(B\\) (e.g. typically in the order of thousands) so that at the end a total of \\(B\\) bootstrap estimates are derived and stored and use this set of estimates to assess uncertainty about the probability of cost-effectiveness of the treatments.\nLet’s see how this works in practice. In R, we can for example use the boot package which allows to flexibly define our bootstrapping procedure and to include within the procedure our linear regression model using the following commands:\n\nlibrary(boot)\nboot_lm &lt;- function(data, i){\n  data2 &lt;- data[i,]\n  lm.boot.Q &lt;- lm(QALY ~ trtf, data = data2)\n  lm.boot.TC &lt;- lm(Tcost ~ trtf, data = data2)\n  em.lm.boot.Q &lt;- emmeans(lm.boot.Q,~trtf)\n  em.lm.boot.TC &lt;- emmeans(lm.boot.TC,~trtf)\n  trt_diffQ &lt;- coef(lm.boot.Q)[2]\n  trt_diffTC &lt;- coef(lm.boot.TC)[2]\n  mean_Q &lt;- summary(em.lm.boot.Q)[,2]\n  mean_TC &lt;- summary(em.lm.boot.TC)[,2]\n  return(c(trt_diffQ,trt_diffTC,mean_Q,mean_TC))\n}\n\nset.seed(4567) \nboot_est_lm &lt;- boot(data_sim_ec, boot_lm, R=1000)\ndelta_e_boot&lt;-boot_est_lm$t[,1]\nmu_e_boot&lt;-cbind(boot_est_lm$t[,3],boot_est_lm$t[,4])\ndelta_tc_boot&lt;-boot_est_lm$t[,2]\nmu_tc_boot&lt;-cbind(boot_est_lm$t[,5],boot_est_lm$t[,6])\n\nIn the code above what I do is to first write a function which contains all the steps that I want to be repeated within each bootstrap step, i.e. the models that must be fitted to each of the \\(B=1000\\) bootstrap samples that I generate, as well as the specific quantities that I want to store from each of these models, i.e. the estimates for the mean difference in QALY and Total cost between the groups as well as the mean estimates for both outcomes for each group. In the second part of the code I use the boot function to generate the bootstrap samples using sampling with replacement and I set the number of replications that I want to use in the argument R. Finally, after the command has finished running, I create objects containing the set of bootstrapped values for each of the stored quantities by extracting these from the output of the boot function. For example, we can inspect the mean difference set of estimates for QALYs and Total costs by typing:\n\ndata_est_delta &lt;- data.frame(delta_e_boot,delta_tc_boot)\nQALY_delta_hist &lt;- ggplot(data_est_delta, aes(x=delta_e_boot))+ xlab(\"QALY difference\")+\n  geom_histogram(color=\"black\", fill=\"grey\")+ theme_classic()\nTcost_delta_hist &lt;- ggplot(data_est_delta, aes(x=delta_tc_boot))+ xlab(\"Tcost difference\")+\n  geom_histogram(color=\"black\", fill=\"grey\") + theme_classic()\ngridExtra::grid.arrange(QALY_delta_hist, Tcost_delta_hist, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\nTo note that, since bootstrapping is a nonparametric method, no explicit assumptions are made about the distributions of QALY and cost data, although the performance of the method highly depends on the number of subjects available as it still relies on asymptotic theory. In addition, since we bootstrapped both outcomes at the same time, correlation between the variables is also taken into account, although only indirectly (i.e. it was not explicitly captured in the fitted models but the joint sampling of individuals might be enough to preserve the correlation in the bootstrapped samples).\nUsing these set of values or distributions for the mean difference and mean outcome in each group we can now compute standard CEA output. To that hand, we use the function contained in the BCEA package which allows us to retrieve such graphical output in a relatively easy way. We can, for example, extract the value of the ICER based on the bootstrapped estimates:\n\nlibrary(BCEA)\ncea_res&lt;-bcea(e=mu_e_boot,c=mu_tc_boot, ref=2, Kmax = 40000)\ncea_res$ICER\n\nintervention 1 \n      2801.223 \n\n\nAnd, finally, we can plot the usual graphs that allow to assess the probability of cost-effectiveness, such as the cost-effectiveness plane and acceptability curve.\n\ng1_cea &lt;- ceplane.plot(cea_res, graph = \"ggplot2\")\ng2_cea &lt;- ceac.plot(cea_res, graph = \"ggplot2\")\ngridExtra::grid.arrange(g1_cea, g2_cea, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\nHopefully this post was helpful to someone. Although it was just a silly example I think it is important to know how these methods work starting from simple scenarios. In future posts I will also go over more advanced stuff, such as dealing with longitudinal data, taking into account possible confounders, clustering, proper modelling techniques such as seemingly unrelated regression and, of course, how to do everything you need with a single and beautiful Bayesian analysis!!!!"
  },
  {
    "objectID": "posts/2022-07-07-my-blog-post/index.html",
    "href": "posts/2022-07-07-my-blog-post/index.html",
    "title": "EuHEA 2022 Conference",
    "section": "",
    "text": "Here for my usual monthly update! This month I have some exciting news as I had the pleasure to attend the EuHEA 2022 Conference in Oslo between 5-8 July. It was my first time attending this bi-annaul conference and I must say I really enjoyed the event and the location. Oslo is a beautiful city and the conference was held at the perfect time of the year with very sunny and mild temperatures throughout the whole week. I even I had the chance to explore the city and about one day: I visit some museums (including the famous scream from Munch!), I enjoyed the mild wheather and nice coastline view, and walked through some of the most beautiful parks I have ever seen.\nBut wait, that’s not all! As one of the antendee of the conference, I was also invited and greeted by the major of the city to visit the town hall of the city with an amazing refreshment party before the start of the conference and, icing on the cake, I had a lovely dinner inside the national opera house! It was incredible!\n\n\n\nThe unique and specific design of the national opera hours in Oslo\n\n\nNow, leaving impotant things aside, let me talk a bit about the actual conference. The event was super well organised with multiple parallel sessions that were consecutively held in different buildings of the Blindern campus of the University of Oslo. The audience was quite large with scholars, people working in the industry and health economists coming from all over the world to discuss their reserach or applied works. Each session has a specific theme and allpresentations within the sessions were tied together by the theme topic. Unfortunately for me, my session was the last one of the last day and it was related to methodological work in health economics. The session was very interesting, with three other talks beside my own one, all related to quantitative aspects of collecting, analysing and interpreting the results based on health economics data. My talk was about recent VIH paper on missing data methods in economic evaluations and, I must say, took the interest of a few people in the audience with lots of questions and interesting suggestions for future work. I was also able to follow the other presentations in my session with great interest as they were related to topics I am kind of familiar with, including methods for the generation of value sets for EQ-5D questionnaires and value of information analysis methods. It was a pity that some of the attendees could not attend the last session (also because it was the day after the conference dinner) but, overall, I feel quite satisfied with the feedback I received on my work.\nAmong all, I must mention some special and familiar faces who I met at the conference: the lovely Chris Sampson from the Office of Health Economics, author and manager of the wonderful The academic health economists’ blog which I really recommend to follow if you want to keep updated about research in health economics; the dear and my previous PhD supervisor Rachael Hunter from the department of PCPH of UCL together with her husband and also health economist Matt Franklin; many other colleagues from different places of the Netherlands who I happened to recognise from our last encounter at the lolaHESG 2022 Conference. Finally, I also would like to sincerely thank the director of CAPHRI Silvia Evers, who provided some very nice comments and questions during my session and with whom I had the pleasure to exchange some words about possible future research collaboration and projects here at Maastricht University. She was really interested in my work and I would love to keep working with her and her team in the field of economic evaluations as I believe there is plenty of room for extending current methodological and applied work in the context of CEA.\nSo, overall, I must say I really enjoyed the conference and the international atmosphere surrounding the event. I also had the chance to network a bit with few colleagues from different countries who had interest in my own research or with whom I was able to find some common point of interest. Well, I guess that’s the end for my golden period this year as I am afraid I will not be able to attend other conferences this year. Now it is time to get back to work and figure out some new projects and grant proposals to write so that I can attend even more conferences next years! Wish me good luck!"
  },
  {
    "objectID": "posts/2022-05-05-my-blog-post/index.html",
    "href": "posts/2022-05-05-my-blog-post/index.html",
    "title": "R markdown for teaching",
    "section": "",
    "text": "Hi guys, today I wanted to post something different from usual research tediousness that is only liked by statisticians and try to focus on another component my academic career, education or teaching, which has recently become quite prominent (with pros and cons of course).\nSpecifically, I have been recently interested in incorporating some reproducible documentation within my own teaching material since I am not course coordinators in statistics for bachelor students here at UM. In the past, I saw that people tended to use these very nice but really time-consuming word documents that needed update every year for making questions and assignment tasks. Do not get me wrong the different examples and ideas behind these assessments are really nice but I have to admit that, given the inevitable need to use the same type of dataset every year, the chance that students received some tips about these tasks was quite nonignorable.\nThus, after receiving some inputs from one of my colleagues (thanks Sophie!), I have decided to try out full R markdown documentation to generate examples similar to the ones used in the past but that could be generated in a slightly different way every year taking advantage of the reproducibility of such documents with an incorporated R coding at the basis of the data generation procedure. I really think these approaches will represent the future for any type of teaching activity that involves some sort of data analysis and interpretation such as statistics: the possibility they offer to efficiently provide new examples without the need to look for new data or each time generate new datasets is so enticing for teachers and researchers who are involved in education. It makes our job so much easier in the long term which more than compensates the cost of learning how to implement these approaches.\nAs an example, I will focus here on the topic of simple linear regression which I give to students of the second year. For example, let’s say that the objective of the tutorial is to learn some basic concepts of linear regression modelling, e.g. interpretation of coefficients and correlation measures. You can start with an introductory part to describe the type of dataset students will need to work on.\nBackground\nThe dataset XXX comprises 40 countries (Country) in the world with populations of more than 20 million as of 1990 and records the life expectancy at birth (Lex), the number of people per television set (Ptel), and the number of people per physician (Pphy). The average life expectancies between males and females are provided as the country’s overall life expectancy.\nThis text will directly appear on the final document as you typed in the Rmd file. So for it works like a standard word file but it is now that the magic happens. We can create chunks of R code and embed them within the Rmd file and use different options to decide whether to show or not such code lines. For example, let’s say we want to generate the data of the above mentioned dataset. We can create a new chunk which incorporates the following R code that simulates the desired variables, their relationships, and combine them together into the new dataset.\n\nset.seed(768)\nn &lt;- 40\nln_pphy &lt;- rnorm(n, 7.2, 1.3)\nln_ptel &lt;- rnorm(n, 2.38, 1.56)\npphy &lt;- exp(ln_pphy)\nptel &lt;- exp(ln_ptel)\nerror &lt;- rnorm(n, 0, 6)\nlex &lt;- 70 - 0.023*ptel - 0.001*pphy + error\ncountry &lt;- c(\"Argentina\", \"Bangladesh\", \"Brazil\", \"Canada\", \"China\", \"Colombia\", \"Egypt\", \"Ethiopia\", \"France\", \"Germany\", \"India\", \"Indonesia\", \"Iran\", \"Italy\", \"Japan\", \"Kenya\", \"Korea North\", \"Korea South\", \"Mexico\", \"Morocco\", \"Myanmar Bur\", \"Pakistan\", \"Peru\", \"Philippines\", \"Poland\", \"Romania\", \"Russia\", \"South Afric\", \"Spain\", \"Sudan\", \"Taiwan\", \"Tanzania\", \"Thailand\", \"Turkey\", \"Ukraine\", \"UK\", \"USA\", \"Venezuela\", \"Vietnam\", \"Zaire\")\ndata_le_tv_sim &lt;- data.frame(country, lex, ptel, pphy)\n\nNow all the generated variables, namely lex, ptel, pphy and country, are included into the data frame called data_le_tv_sim which has been created in the R workspace. If the focus on the tutorial is not on coding, it is desirable to hide the R code from the final document so that it is not displayed (but is still present and saved within R). R markdown allows you to do this in a really straightforward way by means of chunk options which can be customised for each created chunk. For example,\n\nThe option echo=FALSE allows to hide the R lines (while setting it to TRUE shows the lines)\nThe option eval=TRUE tells the software to actually run the lines inside the chunk (setting it to FALSE prevents from doing so)\n\nThese are only two of many different options that can be customised in regard to the display of tables, figures, code lines color, size, font, etc…. For a full illustration of the high degree of customisation provided by R markdown I refer to the dedicated webpage.\nAfter the introduction is done and perhaps the context and objective of the analysis presented, we can start asking questions. Let’s start with somehting simple, such as\nExercise\na. Examine and comment on the distributions of each variable separately (descriptives, histograms etc).\nWell we can answer our own question (i.e. we give ourselves the solutions) by creating a new chunk in which we generate the desired output in R. Since we generated the dataset in the chunk before, it is still available within the R environment and we do not need to re-create it!\n\nlibrary(ggplot2)\nhist_lex &lt;- r2spss::histogram(data_le_tv_sim, variable = \"lex\") + xlab(\"life expectancy\")\nhist_ptel &lt;- r2spss::histogram(data_le_tv_sim, variable = \"ptel\") + xlab(\"people per television\")\nhist_pphy &lt;- r2spss::histogram(data_le_tv_sim, variable = \"pphy\") + xlab(\"people per physician\")\nbox_lex &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"lex\") + xlab(\"life expectancy\")\nbox_ptel &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"ptel\") + xlab(\"people per television\")\nbox_pphy &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"pphy\") + xlab(\"people per physician\")\ngridExtra::grid.arrange(hist_lex, hist_ptel, hist_pphy,\n             box_lex, box_ptel, box_pphy, nrow = 2)\n\n\n\n\n\n\n\n\nHere I used some ggplot2 coding to generate some boxplots and histograms of the data but of course the choice is entirely yours to decide which graphs or summaries should be provided by the students. For generating tables, we can also take advantage of the package knitr and its function kable which allows to display standard R tables in a much prettier format.\nlibrary(knitr)\nlibrary(dplyr)\nd.summary.extended &lt;- data_le_tv_sim %&gt;%\n    dplyr::select(lex, ptel, pphy) %&gt;%\n    psych::describe(quant=c(.25,.75)) %&gt;%\n    as_tibble(rownames=\"rowname\")\nd.summary &lt;- d.summary.extended %&gt;%\n    dplyr::select(var=rowname, min, q25=Q0.25, median, q75=Q0.75, max, mean, sd)\nkable(d.summary, caption = \"Summary statistics\", format = \"html\", digits = 1)\n\nSummary statistics\n\n\nvar\nmin\nq25\nmedian\nq75\nmax\nmean\nsd\n\n\n\n\nlex\n28.7\n62.3\n68.0\n70.0\n81.3\n65.9\n8.2\n\n\nptel\n0.2\n3.6\n19.5\n41.2\n1349.5\n62.9\n211.4\n\n\npphy\n106.1\n710.2\n1375.0\n2767.5\n11031.2\n2206.0\n2321.1\n\n\n\n\n\nAlso kable provides a lot of customisation options that allow to have many different types of formats and styles for your table, which can also be further extended using the package kableExtra. But let’s continue with our test.\nb. Would you expect an association between a country’s life expectancy and its density of people per television set? If yes, would that be positive or negative (use the scatter plot function)?\nTo answer this question we could produce some scatter plots between the variables in R. An alternative would be to calculate the Pearson’s correlation coefficient as an indicative number. We could obtain such number by creating a corresponding chunk code but given that we only need a number it is actually more convenient to generate it within our line of text. How can we do that? simple, in the Rmd file you can include some inline R code by using the quotes signs. This means that instead of having:\n\n#corr coeff\nround(cor(lex, ptel, method = c(\"pearson\")), digits = 2)\n\n[1] -0.76\n\n#corr coeff test\nas.numeric(cor.test(lex, ptel, method = c(\"pearson\"))[\"p.value\"])\n\n[1] 1.70736e-08\n\n\nyou can directly write a sentence an inlcude the output of the two above functions within the text. The results would be something like: the estimated Pearson’s correlation coefficient value between life expectancy and people per television is -0.76. The p-value given by the correlation test is 1.7073596^{-8}. What if now we want to include some theoretical stuff for the students?\nd. Interpret your results. Would the sending of television shiploads to countries with short life expectancies improve the latter? Is there an explanation for your findings?\nWe can do it in an easy way since R markdown also supports latex math environments in combination with inline R code. For example, we can answer with something like the following. Since we are looking at the correlation coefficient \\(\\rho\\) (Pearson’s), then the null and alternative hypotheses about the test for the linear association between life expectancy and people per television can be formulated as:\nH0. \\(\\rho = 0\\) (no linear association between variables)\nH1. \\(\\rho \\neq 0\\) (linear association between variables)\nWe can use the correspnding p-value of this test equal to 1.7073596^{-8} to make a decision to whether reject or not the null hypothesis based on the evidence from the observed data. Finally, let’s do what we are here for, linear regression!\ne. Perform a procedure that predicts life expectancy from people per television\nIn the following code I run the model and save some key output that I will need to display later on. A convenient feature of R markdown writing is that, after running a code chunk, everything that was successfully run is saved in the current R workspace and can be called back later on in the document very easily.\n\nfitm1_r &lt;- lm(lex ~ ptel, data = data_le_tv_sim)\nfitm1rsq &lt;- round(summary(fitm1_r)$r.square, digits = 2)\nfitm1RSS &lt;- round(anova(fitm1_r)[\"ptel\", \"Sum Sq\"], digits = 2)\nfitm1ESS &lt;- round(anova(fitm1_r)[\"Residuals\", \"Sum Sq\"], digits = 2)\nfitm1TSS &lt;- fitm1RSS + fitm1ESS\nfitm1pval &lt;- round(summary(fitm1_r)$coefficients[2, 4], digits = 4)  \n\nWe can then decide whether we want to show the summary results directly from the R output\n\nsummary(fitm1_r)\n\n\nCall:\nlm(formula = lex ~ ptel, data = data_le_tv_sim)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.792 -3.218  1.341  3.231 13.850 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 67.784092   0.895708  75.677  &lt; 2e-16 ***\nptel        -0.029235   0.004108  -7.117 1.71e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.424 on 38 degrees of freedom\nMultiple R-squared:  0.5714,    Adjusted R-squared:  0.5601 \nF-statistic: 50.66 on 1 and 38 DF,  p-value: 1.707e-08\n\n\nor perhaps hide the above output (setting the chunk option echo=FALSE) and provide a textual explanation while also embedding the R code generating the numeric results of interest (e.g. estimates or CI bounds) within the written text. If doing so, then we could have something like the following.\nThe model intercept and coefficient are 67.78 and -0.03, respectively. This means that 67.78 change in life expectancy is associated with a one unit increase in people per television. The p-value of this estimate is 0 with corresponding \\(95\\%\\) confidence interval being (-0.04, -0.02).\nThe analysis of variance reveals that the residual sum of squares (ESS) is 1118.01, the regression sum of squares (RSS) is 1490.38, for a total sum of squares (TSS) of 2608.39. The corresponding \\(R^2=\\frac{\\text{RSS}}{TSS}\\) value of the model is given by 0.57, which suggests how about 57 % of the variation in life expectancy is explained by people per television.\nConclusions\nR markdown provides a really nice opportunity to replace old and static documents with files that can be easily updated by simply changing a couple of lines of code. The results of this exercise can be replicated by using the same seed number but can also be varied by simply changing such number or by setting it to be randomly generated every time. Although not very popular yet, I truly believe R markdown will innovate the way teaching is done, at least within the scientific and quantitative data analysis field. For statistics, this is perfect since it allows an automated procedure to obtain new datasets that are consistent with the coding source created by the educator while also providing students with different numeric examples that makes it harder to cheat by simply “copying the answers from the previous years”.\nI have started using R markdown for my own research a few years ago but its extreme flexibility and advantage over standard education tools for writing up documents has lead me to make it my best option also within my educational activities. Of course, I understand that the barrier to entry can be quite steep, especially for people who are not familiar with coding. However, I believe that getting familiar with these methods has become more and more relevant in the last years and will represent an essential skill for every teacher involved in data analysis topics.\nNot convinced yet? are you not a fan of latex or html coding? no problem, R markdown allows also to produce these documents in word and even power point formats!\nHurray for R markdown!!!"
  },
  {
    "objectID": "posts/2022-03-15-my-blog-post/index.html",
    "href": "posts/2022-03-15-my-blog-post/index.html",
    "title": "Attending conferences and invited talks",
    "section": "",
    "text": "Hello folks, here I am with my monthly update about my work and basically what I am up to. A few (mostly happy) things have happened since last time so thought it would be nice to share them with you.\n\nFirst, the work of a UCL PhD student I am currently supervising has been published in Value in Health, congratulations Xiaoxiao you did an excellent job! The paper is an updated review of current approaches used in the heath economics literature for dealing with missing data (you don’t say), this time taking a more diaggregated perspective compared to previous reviews and looking at which “level” missingness was reported/addressed in these studies, namely questionnarie item, questionnaire score or aggregate levels. It took a lot of work to get this work done since it requires a huge amount of time to review the literature as well as to find an intuitive way to communicate the results from this extraction work, which it turned out to be quite complex (weirdly enough).\nSecond, my abstract submission for lolaHESG 2022 has been accepted and I will be present my new work at the conference this upcoming May here in Maastricht. This is the first time I will be attending this health economics conference which is very specific to the Netherlands but which shares the same structure and format of the standard HESG confernce in the UK (which I already attended a couple of times). I think there will be room for some nice discussions about both theoretical and applied works and I am eager to meet up new colleagues and to share our research ideas and, why not, get to know each other and possibly find new collaborations!\nThird, my other abstract submission for EUHEA 2022 has also been accepted as an oral presentation which I will disucss this June in Oslo. I am very excited for this conference given that I have never been to Oslo before (which looks amazing, I mean look at the picture!) and which was initially planned in the summer of 2020 but that, for obvious reasons, was then cancelled. I will talk about my work in a more international context still related to health economics and I hope I will be able to meet up with some old colleagues and to have an update of what they have been doing these last couple of years. I stil need to sort out a few things about the dates due to some issues in booking my registration but I hope I will be able to fix these problems as soon as possible!\nFourth, I have been kindly invited to give some talks about my reserahc work at a few different online health economics seminar series in the next months, which I would be glad to do. These include: a seminar held by the Health Economics and Health Technology Assessment group at VU Amsterdam with whom I had a very nice chat and fruitful research discussion back in February; the Statistics, Health Economics and Methodology Seminar at PRIMENT Clinical Trials Unit which I attended for a few years while I was doing my post-doc research in the UK and for whom I would be happy to share my current work; the King’s Health Economics seminar held by the Health Service and Population Research Department which has showed some interest in my work about missing data methods which makes me already happy to give my availability. Lots of interesting stuff. I just need to find the time to prepare all these talks …\n\nAmong other things I have been doing and which are worth mentioning there is this on-going teaching qualification course I am attending which is very funny but also quite useful in order to take the official role of course coordinator for future courses, some talks with private consultancy companies for which I am about to start collaborating to earn some money on behalf of my department, and getting my booster vaccination dose which had a quite strong impact on my last two days (basically a walking zombie for most of the days). Perhaps, the last thing to mention is that I will need to start working on my research grant VENI pre-proposal, whose deadline is September 2022. Although this is just a pre-proposal phase (done to screen out the many applications), I really need to focus and put some effort in laying down my application to the best of my abilities to at least pass this phase. Last year I did not make it but this was also due to the fact that I only had about 2 months time to write down my proposal with basically no time for feedback from any colleagues which, instead, I fully intend to ask in order to improve the chances of success. Fingers crossed for this year!\nAt least I am excited to enjoy myself at one of these upcoming conferences and meet up with some new/old colleagues in my research field given that it has been a while since last time I was able to travel for research!"
  },
  {
    "objectID": "posts/2022-01-05-my-blog-post/index.html",
    "href": "posts/2022-01-05-my-blog-post/index.html",
    "title": "And now what?",
    "section": "",
    "text": "Hello folks and happy new year!\nDespite all the problems that occurred in this period in relation to the pandemic and travel restrictions, I hope you had a nice break and recharged your batteries for the upcoming months. As for me, my holidays were decent, let’s say that. To start the new year in the best possible way, I thought today I could share my opinion on a recently published work in my research area, specifically missing data models in trial-based economic analyses. The first author of this paper entitled Flexible Bayesian longitudinal models for cost-effectiveness analyses with informative missing data is one of my ex PhD supervisor, Dr. Alexina Mason, which by itself is already a guarantee of some nice and careful research work. I take this chance to go through her most recent work (of course Bayesian) and try to summarise the key aspects I am interested in.\nThe paper is a well-written and structured illustration of how standard modelling in trial-based CEAs can be extended, taking advantage of the Bayesian flexibility in model specification, in order to take into account some of the typical statistical idiosyncrasies that affect health economic outcome data, including:\n\nThe correlation between cost and effectiveness measures\nThe skewness in the distributions of costs and utilities\nThe presence of “structural values”, e.g. many people having a perfect health state corresponding to an utility of one, that cause a huge spike in the observed distributions\nThe need to take into account missing data uncertainty under MAR while also exploring some MNAR departures through a proper modelling approach and external information\n\nThese are all very valid and important aspects that, despite having been mentioned and discussed by many authors in the literature, are usually only partially tackled in routine analyses due to complexity of model specification based on standard software commands. Nonetheless, I believe this is still an important point that must be highlighted until people will start to move away from standard methods towards the implementation of more advanced approaches, either by using more powerful software (e.g. R instead of Excel) or by acquiring some basic knowledge to implement more advanced methods (e.g. multiple imputation or Bayesian approach). But I am getting away from the topic at hand. So, let’s go back to it.\nThe paper first summarises the literature that has been done in relation to discussing the issues and potential pitfalls of standard methods that do not properly recognise all the features of the data and provides a real case study characterised by the majority of these statistical issues. Next, the modelling framework is provided following the typical and very intuitive (in my opinion!) Bayesian structure based on submodules linked and built one on top of the other to create a coherent and fully probabilistic approach.\n1 The first module consists in the main analysis model required in order to derive the key estimates of interest, formed by the marginal model of the effectiveness variables (HRQoL data collected a five different time points) and the conditional model of the cost variables (aggregated over the whole trial duration). This corresponds to a longitudinal model for the utilities which is linked to a model for the total costs, therefore taking into account the longitudinal structure of the available data as well as the correlation between the two types of outcome. The authors accounts for the skewness in the distribution of the costs using Gamma distributions, while also dealing with the presence of structural ones in the HRQoL data via hurdle models in combination with Gamma distributions which are used to model the complement of the utilities, i.e. \\(u^\\star=1-u\\) (this transformation is done to allow Gamma distributions to be fitted to the data).\n2 The second module is optional and consists in an imputation model for any partially-observed covariate that enters the model in step 1. This is important as any unobserved covariate value will cause the removal of the corresponding case from the analysis unless the missing value is first imputed. Within a Bayesian framework this requires the specification of a distribution for these covariates or, when considered acceptable, some simple imputation methods may be used prior to the analysis.\n3 The third module is related to the missingness model for the utilities, which is specified in terms of a multinomial model in which different types of missingness (e.g. intermittent vs dropout) are associated with different probabilities, estimated based on the available missingness indicator patterns and prior probabilities. In addition, covariates may also be included in this module to make missingness assumptions more plausible.\nIn my opinion this third step is perhaps the most innovative compared to other literature works. Indeed, although this modelling framework is nothing new as it refers to a selection model specification based on a model for the outcome \\(\\boldsymbol u\\) and a conditional model for missingness \\(\\boldsymbol m\\):\n\\[\np(\\boldsymbol u,\\boldsymbol m) = p(\\boldsymbol u)p(\\boldsymbol m \\mid \\boldsymbol u),\n\\]\nthe use of a multinomial distribution within this context was not really considered before. This is because usually a Bernoulli distribution would be considered enough to distinguish missing from observed data. However, through this framework, analysts are free to choose different types of missingness patterns and assign to each of this a different prior probability, perhaps related to different reasons associated with the missing data. Thus, even if the selection model framework notoriously suffers from problems related to the not transparent identification of the unobserved distribution of the model (e.g. unclear weight of the choice of the modelling distributions on missingness assumptions or unclear definition of sensitivity parameters), the possibility to allow for different types of missingness within the same model specification gives more flexibility with respect to the formulation of the missingness assumptions. For example, analysts may wish to explore some MNAR assumption for a specific pattern of missing data (e.g. dropout) while keeping a MAR assumption for other patterns (e.g. intermittent). Of course, an inconvenience of this approach, compared to standard selection models, is that under MNAR the number of parameters to be identified via external information is increased since there are multiple types of missingness patterns. It is therefore required extra-care in the choice of these parameters (or informative priors on these parameters) in that different estimates should be considered for each pattern that is modelled. In addition, since sensitivity analysis is en essential component for any type of missing data model, the presence of additional informative parameters requires the exploration of more alternative scenarios in order to have a general idea of the impact that different assumptions for each of these parameters may have on the final conclusions.\nFinally, the authors conclude with applying the proposed methods to the case study data and elicit alternative prior specifications for the unidentified parameters of the missingness model to assess the robustness of their conclusions to alternative missingness assumptions. In total they consider up to \\(8\\) different missingness scenarios. These are distinguished in terms of different assumptions about the strength and direction for each missing data pattern (dropout vs intermitten) and treatment arm (control and intervention) in the study. The also compared the results obtained under the different MNAR scenarios with those from the complete cases and under MAR to give an idea of what is the impact that a change in the assumptions related to the different types of missingness may have on the final cost-effectiveness conclusions. They chose to summarise this using the the net benefit measure as the key indicator to reflect changes in cost-effectiveness under the different missingness scenarios.\nOverall, I must say that I really enjoed myself reading this paper since it provides a ver well-written piece of work which illustrates how relatively complex models can be fitted to health economic data using software that are available to everybody although they may require some time to be learned (authors provided the entire model code in JAGS). I really liked the model specification which sorts of corresponds to an extension of one of my previous works to take into account the longitudinal nature of the data and, especially, the specification of alternative MNAR scenarios via sensitivity analysis based on multinomial distributions. I think that providing these methods in some sort of package in R would be really helpful for analysts who are not familiar with Bayesian software. Otherwise, I am afraid that people will continue using more standardised methods (e.g. MI) simply on the basis of the fact that they can be implemented in more straightforward ways (although these are likely to be less flexible).\nReally some nice work Alexina, well done! This reminds me that I need to get back to my current projects (oh no, so much to do!)"
  },
  {
    "objectID": "posts/2021-10-10-my-blog-post/index.html",
    "href": "posts/2021-10-10-my-blog-post/index.html",
    "title": "Baseline adjustment in trial based CEA",
    "section": "",
    "text": "Recently I have come across something I found a little odd when performing a statistical analysis of trial-based CEA data and I would like to share here my experience in the hope that anybody may be able to read it (and correct me if I am wrong). It is something related to the implementation of baseline adjustment for utility score data via regression approach.\nTo give an idea of the context of the analysis I quickly use some simulated data as an example of a dataset that could be object for this type of analysis. To make things simple, I simulated individual-level utility score data which are measured at baseline (\\(u_0\\)), 6 (\\(u_1\\)) and 12 (\\(u_12\\)) months follow up for two competing intervention groups, say a control (t=0) and an intervention (t=1). Again, to make things super easy I simulated these assuming a multivariate normal distribution with constant variance and no time correlation. Although this is not realistic it only serves the purpose to illustrate the issue I am facing. So let’s simulate the data.\n\n#load and pre-process the simulated dataset\nlibrary(readstata13)\n\n#wide format\ndata_wide&lt;-read.dta13(\"ex_data.dta\")\ndata_wide$trt &lt;- as.numeric(data_wide$arm)\ndata_wide$subjects&lt;-rep(1:length(data_wide$arm))\ndata_wide &lt;- data_wide[data_wide$arm %in% c(\"Placebo\",\"Mirtazapine\"),]\ndata_wide$trt &lt;- ifelse(data_wide$trt == 1, 0, 1)\ndata_wide$id &lt;- rep(1:nrow(data_wide))\ndata_wide$eq5d_1 &lt;- data_wide$eq5d0\ndata_wide$eq5d_2 &lt;- data_wide$eq5d13\ndata_wide$eq5d_3 &lt;- data_wide$eq5d39\n\n#long format\nlibrary(reshape)\ndata_long.eq5d&lt;-reshape(data_wide, varying = c(\"eq5d_1\",\"eq5d_2\",\"eq5d_3\"),\n                        direction = \"long\",idvar = \"id\",sep = \"_\")\ndata_long.eq5d$time_u &lt;- data_long.eq5d$time\ndata_long.eq5d&lt;-data_long.eq5d[,c(\"id\",\"trt\",\"time_u\",\"eq5d\",\"eq5d0\",\"qaly\")]\n\nNext, I previously computed individual-level QALYs (\\(e_i\\)) in each group by aggregating the utilities over the duration of the analysis, i.e. 1 year, using the AUC formula (see post thumbnail):\n\\[\ne_i = \\sum_{j=1}^{J}\\frac{u_{ij-1} + u_{j}}{2} \\times \\delta,\n\\]\nwhere the subscript \\(i\\) and \\(j\\) denote the individual and time indices, while \\(\\delta\\) is the portion of time covered between each successive pair of measurements. Since these measures are assumed to be collected at 6 months intervals, then in our case \\(\\delta=0.5\\). At this point I have all the data I need to perform a regression analysis and try to estimate the mean QALYs in each group, adjusting for baseline values. The simplest way to do this is to fit a linear regression model at the level of the QALY variable and then include treatment to obtain estimates of unadjusted mean QALYs. If I also add \\(u_{i0}\\) as a covariate into the model, then I obtain adjusted mean estimates. The model is:\n\\[\ne_i = \\boldsymbol \\beta \\boldsymbol X_i + \\varepsilon_i,\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of regression parameters, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model (including an intercept, trt and \\(u_{i0}\\)).\n\n#perform the analysis\nlibrary(emmeans)\nlibrary(nlme)\nlibrary(lme4)\n\n#linear regression for QALYs (focus on complete cases for simplicity)\ndata_wide$trtf &lt;- factor(data_wide$trt)\ndata_wide.cc&lt;-data_wide[!is.na(data_wide$qaly),]\n\nols.cc.wide.qalys&lt;-lm(qaly ~ trtf + eq5d0,data = data_wide.cc)\nci_ols.cc_qalys.means&lt;-emmeans(ols.cc.wide.qalys,~trtf+ eq5d0)\nprint(ci_ols.cc_qalys.means)\n\n trtf eq5d0 emmean     SE  df lower.CL upper.CL\n 0    0.718  0.553 0.0152 103    0.523    0.583\n 1    0.718  0.582 0.0164 103    0.549    0.614\n\nConfidence level used: 0.95 \n\n\nSo far so good right? well now the problem pops up. It is generally known that, when some missing utility data occur, then it is more efficient (in the sense of using more information) to fit the model at the longitudinal level, i.e. at the level of the utility scores rather than at the QALYs level. In this was information from partially-observed cases will be used in the model when deriving the estimates for the mean utilities at each time, which can then be combined via the AUC formula to obtain the final QALY mean estimates. Here for simplicity we fit this longitudinal model even without any missingness. Although there is not much literature about this type of approach, let’s say that we want to fit a linear mixed-effects model to our data and then combine the model parameter estimates to derive the final estimates of interest. The model can be specified by including treatment, time, their first order interaction, and baseline values to derive the adjusted mean estimates.\n\\[\nu_{ij} = \\boldsymbol \\beta \\boldsymbol X_i + \\omega_i + \\varepsilon_{ij},\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of fixed effects, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model. This includes: an intercept; trt; \\(u_{i0}\\) and time, which is often expressed as a dummy-coded variable (with reference time \\(j=0\\)), and the interaction between trt and time. Finally, \\(\\omega_i\\) is the random effects term.\n\n#perform the analysis \ndata_long_cc &lt;- data_long.eq5d[!is.na(data_long.eq5d$qaly),]\ndata_long_cc$trtf &lt;- factor(data_long_cc$trt)\ndata_long_cc$timef_u &lt;- factor(data_long_cc$time_u)\n\n#mixed model for utilities (focus on complete cases for simplicity)\ncgm3_u_ml.cc&lt;-lme(eq5d ~ timef_u * trtf + eq5d0, random = ~ 1 | id, data=data_long_cc, method = \"ML\",na.action = na.omit)\n#derive mean utilities\nem3_u_ml.cc.eq5d&lt;-emmeans(cgm3_u_ml.cc,~timef_u * trtf)\n#derive mean QALYs as linear combination of mean utilities\ncgm3_u_ml.cc.qalys &lt;- contrast(em3_u_ml.cc.eq5d,list(mue1 = c(13/104,13/104 + 26/104,26/104,0,0,0), mue2=c(0,0,0,13/104,13/104 + 26/104,26/104)))\nci.cgm3_u_ml.cc.qalys &lt;- confint(cgm3_u_ml.cc.qalys)\nprint(ci.cgm3_u_ml.cc.qalys)\n\n contrast estimate     SE  df lower.CL upper.CL\n mue1        0.555 0.0126 103    0.530    0.580\n mue2        0.580 0.0136 103    0.553    0.607\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nDo you see the issue? the derived mean QALYs for both groups do not exactly match those obtained from the linear regression fitted to the QALYs despite the fact that the data used are the same, i.e. complete cases. This is a bit odd. However, what happens when I run the adjusted analysis including the interaction between time and baseline utilities alongside the main effects of \\(u_{i0}\\) ?\n\\[\nu_{ij} = \\boldsymbol \\beta \\boldsymbol X_i + \\omega_i + \\varepsilon_{ij},\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of fixed effects, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model. This includes: an intercept; trt; \\(u_{i0}\\) and time, which is often expressed as a dummy-coded variable (with reference time \\(j=0\\)), the interaction between trt and time, and also the interaction between time and \\(u_{i0}\\). Finally, \\(\\omega_i\\) is the random effects term.\n\ncgm3_u_ml.cc2 &lt;- lme(eq5d ~ timef_u * trtf + timef_u*eq5d0, random = ~ 1 | id, data=data_long_cc, method = \"ML\",na.action = na.omit)\n#derive mean utilities\nem3_u_ml.cc.eq5d2 &lt;- emmeans(cgm3_u_ml.cc2,~timef_u * trtf + timef_u*eq5d0)\n#derive mean QALYs as linear combination of mean utilities\ncgm3_u_ml.cc.qalys2 &lt;- contrast(em3_u_ml.cc.eq5d2,list(mue1 = c(13/104,13/104 + 26/104,26/104,0,0,0), mue2=c(0,0,0,13/104,13/104 + 26/104,26/104)))\nci.cgm3_u_ml.cc.qalys2&lt;-confint(cgm3_u_ml.cc.qalys2)\nprint(ci.cgm3_u_ml.cc.qalys2)\n\n contrast estimate     SE  df lower.CL upper.CL\n mue1        0.553 0.0125 103    0.528    0.578\n mue2        0.582 0.0135 103    0.555    0.608\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nTa da! the estimates now perfectly coincide. It turns out that when fitting this longitudinal model for the utility, it is important that an interaction between time and baseline utilities is included in the model to match the adjusted estimates that would be obtained via standard linear regressions fitted at the QALY level.\nI am not totally convinced of why this is the case but perhaps it has something to do with the fact that baseline utilities are used as outcome and covariate at the same time in both types of models ? I need to study this in more detail."
  },
  {
    "objectID": "posts/2021-08-02-my-blog-post/index.html",
    "href": "posts/2021-08-02-my-blog-post/index.html",
    "title": "Holidays, finally",
    "section": "",
    "text": "Holidays! Yes, really holidays!\nThis year has been particularly difficult and hard for many people and I am no exception. With the moving to Maastricht, the need to familiarise with a new working and living environment, the lockdown measures, the new teaching and administrative tasks, and mnay other little things, I arrived at the end of the academic year almost annihilated. Luckily, I am now on annual leave and back in Italy with my family who I really missed throughout the past year. Begin together in flesh and blood is a really nice feeling, something that online calls cannot replace. I really needed a break from the usual routine and this vacation is just perfect in order to recover my energies and be ready for the upcoming academic year. I was also lucky enough to received both my vaccinations before leaving the Netherlands as otherwise I could have troubles when traveling between countries.\nSo, not mush to say apart that I will be taking these holidays as a chance to put together some research ideas for next year. From thinking about how to plan a new grant application, to prepare myself for attending future courses in relation to learning Dutch, new teaching modules, the teaching qualification certificate, and of course some nice research papers. Speaking of the devil, I flash out my most recently published paper in MDM in the area of partitioned surivival CUA modelling, which has now been officially released. If you are interested in the topic, check it out!\nFinally, I would also like to give a brief review of a textbook which I really enjoyed reading. I consider this the masterpiece of one of my top philosophical authors, David Hume. It is entitled Dialogues Concerning Natural Religion and is one of the latest works of the Scottish philosopher written, as the title suggests, in the form of dialogues among three people who talk about many interesting and general concepts. From the nature of God’s existence, good and evil, and the role of religion in society. It is truly one of the most incredible and fascinating philosophical debate I have ever read in that it touches subjects in which I am really interested. This was the first “real” philosophy textbook I read after my high school readings and it was stuck in my mind for a long time due to its well-done structure and reasoning. It can be hard to follow at times, especially if you are not used to how these types of texts are written, but I can surely say that it is worth it. Perhaps, I will refer to some of my other top philosophy readings in my future posts, just in case someone might be intrigued by them and would like to give them a chance.\nAnyway, that is all from me for now. Now time for some vacation and relax in my warm, humid and very polluted “Pianura Padana” in Italy which, however, I love very much and I am super happy to see again. Tot September folks!"
  },
  {
    "objectID": "posts/2021-06-15-my-blog-post/index.html",
    "href": "posts/2021-06-15-my-blog-post/index.html",
    "title": "Too hot",
    "section": "",
    "text": "This month is extremely busy and I seriously need a week of days made by 34 hours each in order to avoid any delay. Apart from the usual teaching which is finally coming close to the end of this period (hopefully by the end of the month) but which kept me busy at least 3 days per week, there have been lots of meetings and work to do. Add the fact that in the last few weeks people here were going crazy for a very hot weather (I think they are not used to such “high” and constant temperatures of 30 degrees Celsius), then you can understand how much willingness I must possess in order to keep up with everything. Not that I am complaining about having lots of work to do, rather it is something which I have always found to be exciting, but having everything squeezed into a couple of weeks is not exactly very healthy.\nA very quick recap of the most important things I have done include the followings. First, lots of consultation for medical students who are now close to the deadline for the submissions of their theses and internship reports. I think I got at least 2/3 requests per week which is quite a lot and given that students are afraid of learning anything that has to do with a statistical analysis, patience will be an essential virtue that I must show off in order to survive the upcoming days. Wish me all good luck ! Second, alongside with teaching statistics in a course I get lots of emails from the students on different parts of the course. This is not something that really bothers me expect when their questions are all about what type of questions there will be in the exam rather than “I would like to understand better this concept” which can be a bit frustrating from time to time. Again part of the job, so I can complain as much as I want but it is something I really need to do. Third, I have given my first invited talk at my department of methodology and statistics and it has been a very interesting and nice experience. I think I overdid it since I wanted to give a nice overview of what my research is about but I ended up putting too many slides for a 1 hour presentation. This hurt a bit the pace of my talk since I had to speed up in certain points and I felt that many people were lost, especially towards the end. It was very nice to introduce my work to my colleagues and receive their feedback on some parts of the work as well as suggestions to explore in the future. I hope next time I will be able to adjust the presentation based on the actual time given to me so to make sure people will not feel disoriented/bored (at least not too much). Finally, next week I will also give another invited talk about my missingHE package at the R-HTA summer workshop. This annual conference is specifically directed towards people who would like to use R in health economic analyses and I hope I can capture the attention of someone who might decide to try out my package on their own analyses. I still have to finish the presentation but since this time I only have 15 minutes I need to make sure to not talk for more than 1 hour!\n\n\n\n\n\nI am also trying to work on my research but the available time is very little so I have made basically no progress. Next month I will have some break from my teaching duties so I hope I will be able to focus on my work. However, another big enemy is approaching who may threaten my ability to complete this task: sunny waether. Now more than ever I would be happy to have a fresh July which would make my life much easier. Let’s see what it will be.\nOh by the way, did I perhaps mention that my new article has been published already? check it out!"
  },
  {
    "objectID": "posts/2021-04-15-my-blog-post/index.html",
    "href": "posts/2021-04-15-my-blog-post/index.html",
    "title": "Going back to teaching, hurray!",
    "section": "",
    "text": "Hello everbody and it is good to be back after a nice and cozy Easter break. As probably most of you, I was too forced to spend my Easter holidays away from my family this year but at least here in the Netherlands the weather was pretty nice during the Easter weekend and I was able to enjoy a nice walk through the city center of Maastricht which was an amazing experience. Even though the week after it started snowing for some absurd reason, I really needed this short break from my teaching duties at the university to recharge my batteries.\nI also took this chance to going back to some old projects that needed my attention and was able to move them forwards a bit with some extra work but that is fine since I love my research. Now, I am ready again for some new teaching and consultancy in the upcoming two months and, hopefully, these will be the last official commitments for this academic year. In addition, going towards summer time brings some new fresh air and hope for a period of the year with fewer restrictions and the chance to engage with some other human beings. It is not that I am a huge fan of interacting with people but it is definitely necessary to the human nature to avoid becoming crazy. I also hope I will soon be able to travel back to Italy and see my familiy and friends which I have not had the chance to see in person from more than one year ago. Anyway, sorry about all the wining and let’s get back to the actual important news.\nFirst, I am happy to announce that my paper on Bayesian methods for modelling data in survival-partitioned cost-utility analyses has been accepted for publication by the journal Medical Decision Making and in the upcoming months it should be officially published as an open access paper so that everyone who has an interest on the matter can have access to the paper. In the meantime, you can also see a past version of the paper on my ArXiv which in any case is pretty close to the final version, I think. I am exremely happy to publish to solo work on which I spent lots of time and effort and which represents a nice application of Bayesian methods as a comprehensive modelling framework for handling different types of statistical issues that usually affect health economic data. Nice!\n\n\n\n\n\nSecond, I am considering submitting an abstract to the R-HTA 2021 workshop which this year will take place in Dublin, at the Trinity College Dublin - see picture below, between July 1-2. This is a very interesting opportunity to spread my work as this annual conference invites people who have different levels of skills with R to present their projects, especially in the form of R packages, to support the statistical and health economic analyses within the international HTA environment, from applicative to methodological works. It could be a very nice spot where to promote my own package for handling missing HTA data, missingHE, as well as to receive some nice feedback for experts on how to further improve its structure/functions. I would also love to visit Dublin as I never had the chance. Unfortunately this may not be possible as it depends on the current and future restrictions that are imposed and tht may hinder my ability to travel even in the summer. Having this as an online thing is also another option, although not really exicting.\n\n\n\nTrinity College Dublin\n\n\nAnyway, now stop talking and let’s go back to work as I need to do some teaching before anything else!"
  },
  {
    "objectID": "posts/2021-02-15-my-blog-post/index.html",
    "href": "posts/2021-02-15-my-blog-post/index.html",
    "title": "Doing some teaching…",
    "section": "",
    "text": "Hello everybody, it is time for some quick updates about myself and what I have been doing the past month. Well, essentially, I have been crazy busy doing lots of teaching on statistics-related subjects, which is the primary reason I was hired here in Maastricht. The students are mostly from undergraduate medical or health science programmes and therefore the level of statistics that I need to teach them is rather basic, although the crucial aspects of data analysis is very well-treated in the courses. So far I had a wide variety of students with different backgrounds and from my first impressions it seems that those who are younger, say in their first undergraduate year, have shown more interest and effort to learn compared with their older colleagues. However, I have been teaching only two courses here at UM and many more are coming up in the next few weeks/months. The statistical concepts covered are the usual frequentist ones, such as p-values, confidence intervals, regression analysis, etc… So, nothing incredible but since I was a student too some years ago I can related with some of the questions they have about these concepts, especially given that statistics is not exactly their main programme focus (although it is essential to learn it!).\nOf course, since we are living in this moment, all teaching is done online and this makes things a little more difficult sometimes but in general I think I managed ok for answering the students’ questions and providing them with feedback on their exercises and group works. It is quite annoying that this being my first time teaching these courses (already prepared by other colleagues) I have to spend a decent amount of time to prepare all the lectures and be ready to the many possible questions the students may have on each topic. This, however, was expected from the beginning and it is part of the job so I cannot complain too much!\n\n\n\n\n\nApart from my rumblings about teaching duties killing me, I have some other good news on my research activity, finally! I am happy to announce that my paper on longitudinal models for dealing with missing data under MAR in trial-based cost-effectiveness analysis is in press by the journal Value in Health and can be accessed at the following link. I am really happy about this publication as this took lots of time and effort and seeing this article being published is something that encourages me a bit as a reminder that all the efforts done has now repaid. Although joint longitudinal models are not exactly a novelty in statistics or even health economics, the way I present them here is kind of unique in that I do it from a Bayesian perspective (who would have guessed?) and I compare the performance of the method with respect to other more standard missing data appraoches such as complete case analysis, single and multiple imputation. The key message is that any method which discards some observed data is inevitably less efficient and more prone to bias compared with methods that do not ignore these data. Thus, given the characteristics of trial-based analysis, it is recommended that longitudinal models, which properly account for the time dependence between outcomes and use information from all collected observations, are used to minimise the impact of missingness assumptions on the conclusions of the study. These models can also be fitted using multiple imputation although, in my perspective, the main issue of that is that results must then be combined with bootstrapping in order to quantify the impact of uncertainty on the conclusions (while this is not necessary in a Baysian analysis!).\nHopefully, analysts will find the methods I propose in this article interesting and useful and they will agree that using simple but likely biased methods should not be the optimal choice when approaching to a trial-based analysis. Ok, I think that is all for the moment and it is time to go back to my teaching and (when I have time) researching!"
  },
  {
    "objectID": "posts/2020-12-20-my-blog-post/index.html",
    "href": "posts/2020-12-20-my-blog-post/index.html",
    "title": "It is Xmas again Yeah",
    "section": "",
    "text": "It is Xmas again! Wow, how quickly time flies. The situation here in the Netherlands is not ideal as the number of infected is on the rise again and the government has declared a full lockdown until January 19th. Starting from next month I will start teaching in the department but everything will still be online due to the uncertainty of the pandemic. It is unfortunate but there is nothing we can do about it and let us just hope the situation will improve in the next few weeks/months.\nAside from sad stuff, I have some good news about my research. My paper about longitudinal models in trial-based CEA, whose pre-print is available on my ArXiv account, has been officially accepted for publication in Value in Health, after a very long time (more than 2 years of peer-review process I believe!). I am really happy as this is a paper for which I spent a lot of time and effort, so it is nice to see some ouput out of it. I am also glad to announce that a new paper I am co-authoring with Baptiste (LSHTM) and Catrin (Bangor University) which is a tutorial on the use of mixed effects models for trial-based CEAs. The main idea is to make available and spread awareness in the HE community about the possibility to use these models to derive CEA results, rather than relying on the standard linear regression models. The methods are nothing special but we believe health economists are not well aware of the fact that sometimes they can be implemented in a much easier way compared to standard approaches. We tried to summarise the main advantages and disadvantages of the methods while also providing software code (in STATA and R) to show how they can be fitted. The paper has been accepted as a contributed presentation at the next HESG meeting. Unfortunately, I will not be able to attend the meeting as I will be quite busy with the teaching that week but Catrin has kindly agreed to present in behalf of all authors. I look forward to receive the feedback from the people attending the conference!\nThe past month I have been quite busy with some teaching and consultancy work here at UM, mostly for medical students about basic statistical concepts and techniques but nonetheless very interesting for me in order to get acquainted with the new job and duties. A big thank is due to my new colleagues who have been incredibly nice to me and have helped me a lot to get into the system. I was actually planning to post my news earlier this month but with incredible disappointment I found out that the most recent updated of Hugo (the system used by blogdown for building this site) completely messed up my previous version of the website which was broken. I had to rebuild the website again copying and pasting all my previous material. Nothing crazy, but certainly very annoying to do. I just hope the next update will not force me to do it again! So, if you see something different compared to before, you know why (especially in the sections for the tutorials on using BUGS/JAGS/STAN). Related to this, I must acknowledge Mr. Kim who recently contacted me with regard to an incorrect specification for one of the STAN models in the tutorial on generalised linear mixed models. I would like to thank him again very much for noticing this mistake which I fixed in the new version of the website now online (apologies for the long time it took for me to make the changes!). I am happy to see that my code can be of help to anyone who might be interested in doing some nice modelling.\nFinally, a quick note about somehting I found curious. A medical statistician who I deeply respect and admire, Dr.Tim Morris from MRCCTU, posted the following provocative tweet\n\nwhere he asks about the interpretation of a Bayesian credible interval while also saying that quick and simple answers will not be considered reliable. Well, I have much to say on this but I feel like twitter is not the best location to try and argument a proper discussion. I hope I will be able to find some time to post on my website a more constructive answer. The topic is not very quick to grasp, especially if someone is used to think in frequentist terms and theory which, by definition, are not very useful to think at statistics from a Bayesian point of view. For the moment, as a quick answer, I would just say that in my opinion statistics is not a uniquely defined discipline but there are different ways it can be approached, and the rules and theory of one approach do not necessarily apply to others!\nNext time, perhaps, I will follow this with a proper argument, stay tuned!"
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html",
    "href": "posts/2020-10-10-my-blog-post/index.html",
    "title": "Why health economists do not care about statistical significance?",
    "section": "",
    "text": "Hello dear readers!\nI have finally come back from my lethargy with a new exciting posts about why the job of health economists, although inevitably involving some statistics, can be very different from what standard statisticians typically do. To make this point, I will abuse and give my own opinion of the notorious (and now quite old) paper published by Dr. Claxton (Claxton (1999)).\nBefore starting my discussion I would like to point out a very nice post made by Sam Watson on the Academic Health Economists’ Blog, who nicely provided his own opinion on the matter and discussed the pros and cons of the paper. I really recommend people interested in knowing more about health economics to check out this blog which is one of the most popular in the field and which has contributions from many many health economists on different topics.\nIn this post, in contrast to what Sam did in his own, I will not focus on the technical details about the health economics decision-making problem of minimising the expected loss / maximising the utility function in terms of societal preferences (i.e. social welfare in terms of effectiveness and costs). Instead, here I would like to address the much simpler and basic question why health economists do not use statistical significance in their analyses?. In my own experience, especially in trial-based analyses, I had to work closely with both health economists and statisticians who often are unaware of what the other “people” are doing. Since cost-effectiveness analyses have become more and more important over the last decade, I believe it is imperative that even statisticians should learn at least the basics of what their colleagues are doing and most importantly why. Of course, the same applies for health economists who should know a bit of statistics in order to do their job. However, in general, I have the feeling statisticians are reluctant to care about health economics, perhaps due to different statistical methodology required for these analyses compared with standard methods used for the analysis of clinical outcomes.\nApologies for the very long premise. So, let’s start with the most basic question of all. what is the objective of health economics ? To inform decision-makers about the cost-effectiveness of a given treatment with respect to alternatives. Unlike the standard clinical analysis which focuses merely on detecting whether there is a “clinically relevant” difference between treatments in terms of some pre-defined outcome measure, the health economic analysis does not qualify as a mere yes/no problem due to the existence of opportunity costs. These are the losses, either in terms of forgone benefits or extra costs, that the society/health care provider would incur by funding a treatment which is not cost-effective compared to others. Precisely, the decision-making nature of the problem comes from the fact that only a limited amount of resources are available to decision-makers, who need to define some sort of rule to prioritise the distribution of the funds across a pool of possible treatments. Thus, if the objective is to maximise the health benefits for a given budget, then treatments should be selected based on a target quantity, which summarises effectiveness, costs and the preferences of decision-makers, while also taking into account the uncertainty about our conclusions. This quantity takes the name of incremental net benefit and is given by\n\\[\n\\text{INB} = K \\Delta_e - \\Delta_c,\n\\]\nwhere \\(\\Delta_e\\) and \\(\\Delta_c\\) are the mean differences between two competing interventions in terms of some effectiveness and cost measures, while \\(K\\) is the acceptance threshold representing the budget the decision-maker is willing to spend to obtain an increment of one unit of effectiveness (cost per unit of effectiveness gained). If we were to evaluate an hypothesis test about cost-effectiveness of treatment 2 vs treatment 1, we would then test: \\(\\text{INB} \\leq 0\\) (\\(H_0\\)) vs \\(\\text{INB} &gt; 0\\) (\\(H_1\\)).\nA natural test statistic for this hypothesis is:\n\\[\nt = \\frac{\\sqrt{n}Kd_e-d_c}{\\sqrt{K^2 s^2_e -2Ks_{ce}+s^2_c}},\n\\]\nwhere \\(d_e\\) and \\(d_c\\) are the in-sample estimates of the corresponding population values, while \\(s\\) are the sample standard deviations. Under the null, this test statistic has a t-student distribution with \\(n-1\\) degrees of freedom. However, we can see how not rejecting the null hypothesis when a new treatment has a positive but statistically insignificant mean incremental net benefit imposes unnecessary costs which can be valued in either monetary or effectiveness terms. Decisions should be based only on the mean irrespective of whether any differences in this quantity are regarded as statistically significant. This is because one of the mutually exclusive alternatives must be chosen and this decision cannot be deferred. The opportunity costs of failing to make the correct decision based on a single test statistic are symmetrical while the definition of which of the alternatives is regarded as current practice is completely irrelevant."
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "href": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "title": "Why health economists do not care about statistical significance?",
    "section": "So how do you make the decision if there no hypothesis test ?",
    "text": "So how do you make the decision if there no hypothesis test ?\nAlthough the ordinary rules of statistical analyses do not apply, we still need to summarise the uncertainty associated with our results based on the mean INB given that decision-making problems must take into account and quantify the impact that sampling uncertainty has on our conclusions. This is typically achieved through the use of re-sampling methods, such as bootstrapping, (frequentist approach) or Bayesian methods. The two approaches are very different from a theoretical perspective but I will not address this point to save me of few weeks of time. Here, I am most interested in what type of results we will obtain once applied these methods. Well, typically we will look at something like this:\n\n\n\nCost-Effectiveness Plane"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html",
    "href": "posts/2020-08-07-my-blog-post/index.html",
    "title": "What is Bayesian inference?",
    "section": "",
    "text": "What is probability? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:\nOne of the ways in which Bayesian statistics differs from classical statistics is in the interpretation of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.\nIn classical statistics probability is often understood as a property of the phenomenon being studied: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if \\(A\\) is an event of interest (e.g. the coin lands heads up) then\n\\[\n\\text{Pr}(A) = \\lim_{n\\rightarrow\\infty}\\frac{m}{n}\n\\]\nis the probability of \\(A\\), where \\(m\\) is the number of times we observe the event \\(A\\) and \\(n\\) is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as frequentist and objectivist. However, historians of science stress that at least two notions of probability were under development from the late \\(1600\\)s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighbourhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if \\(p_1, p_2, \\ldots\\) are a set of betting quotients on hypotheses \\(h_1, h_2,\\ldots\\) , then if the \\(p_j\\) do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "href": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "title": "What is Bayesian inference?",
    "section": "Subjective probability",
    "text": "Subjective probability\nBayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes probability does not exist: “The abandonment of superstitious beliefs about \\(\\ldots\\) Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.\nThe use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.\n\nConditional probability. Let \\(A\\) and \\(B\\) be events with \\(P(B)&gt;0\\). Then the conditional probability of \\(A\\) given \\(B\\) is\n\n\\[\nP(A\\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThe following two useful results are also implied by the probability axioms, plus the definition of conditional probability\n\nMultiplication rule\n\n\\[\nP(A \\cap B) = P(A\\mid B)P(B) = P(B\\mid A)P(A)\n\\]\n\nLaw of total probability\n\n\\[  \nP(B) = P(A\\cap B)+ P\\overline{(A\\cap B)} = P(B\\mid A)P(A) + P(B \\mid \\overline{A})P(\\overline{A})\n\\]"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "title": "What is Bayesian inference?",
    "section": "Bayes theorem",
    "text": "Bayes theorem\nBayes Theorem can now be stated, following immediately from the definition of conditional probability. If \\(A\\) and \\(B\\) are events with \\(P(B)&gt;0\\), then\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n\\]\nIf we consider the event \\(A=H\\) to be an hypothesis and the event \\(B=E\\) to be observing some evidence, then \\(Pr(H\\mid E)\\) is the probability of \\(H\\) after obtaining \\(E\\), and \\(\\text{Pr}(H)\\) is the prior probability of \\(H\\) before considering \\(E\\). The conditional probability on the left-hand side of the theorem, \\(\\text{Pr}(H\\mid E)\\), is usually referred to as the posterior probability of \\(H\\). Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis \\(H\\) from data \\(E\\).\nIn most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be \\(\\theta\\) and denote the data available for analysis as \\(\\boldsymbol y = (y_1, \\ldots , y_n)\\). In the case of continuous parameters, beliefs about the parameter are represented as probability density functions or pdfs; we denote the prior pdf as \\(p(\\theta)\\) and the posterior pdf as \\(p(\\theta \\mid \\boldsymbol y)\\). Then, Bayes Theorem for a continuous parameter is as follows:\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int p(y \\mid \\theta) p(\\theta) d\\theta},\n\\]\nwhich is often approximated by\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta),\n\\]\nwhere the proportionality constant is \\(\\left[ \\int p(y \\mid \\theta) p(\\theta) d\\theta \\right]^{-1}\\) which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the likelihood function, the probability density of the data \\(y\\), considered as a function of \\(\\theta\\). This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function \\(p(\\boldsymbol y|\\theta)\\) can be “inverted” to generate a probability statement about \\(\\theta\\), given data \\(y\\). Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density \\(p(\\theta)\\) belongs to a class of parametric of densities, \\(F\\). More specifically, the prior density is said to be conjugate with respect to a likelihood \\(p(y \\mid \\theta)\\) if the posterior density \\(p(\\theta \\mid y )\\) is also in \\(F\\).\nBayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precision. That is, Bayesian analysis with conjugate priors over a parameter \\(\\theta\\) is equivalent to taking a precision-weighted average of prior information about \\(\\theta\\) and the information in the data about \\(\\theta\\). Thus:\n\nThus, when prior beliefs about \\(\\theta\\) are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);\nWhen prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior\n\nNote also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating has been dubbed Cromwell’s Rule by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of \\(\\theta\\) are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "title": "What is Bayesian inference?",
    "section": "Bayesian updating of information",
    "text": "Bayesian updating of information\nBayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to pooling information. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "href": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "title": "What is Bayesian inference?",
    "section": "Parameters as random variables",
    "text": "Parameters as random variables\nOne of the critical ways in which Bayesian statistical inference differs from frequentist inference is that the result of a Bayesian analysis, the posterior density \\(p(\\theta \\mid y)\\) is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over \\(\\theta\\), conditional on having observed data. Contrast the frequentist approach, in which \\(\\theta\\) is not random, but a fixed (but unknown) property of a population from which we randomly sample data \\(\\boldsymbol y\\). Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted \\(\\hat{\\theta} = \\hat{\\theta}(y)\\), this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the \\(\\hat{\\theta}(y)\\) vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter \\(\\theta\\) is a constant feature of the population from which data sets are drawn. The distribution of values of \\(\\hat{\\theta}(y)\\) that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of \\(\\hat{\\theta}\\) is the standard error of \\(\\hat{\\theta}\\), which plays a key role in frequentist inference. The Bayesian approach does not rely on how \\(\\hat{\\theta}\\) might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about \\(\\theta\\) in light of the data available for analysis, \\(y\\) ?”\nThe critical point to grasp is that in the Bayesian approach, the roles of \\(\\theta\\) and \\(\\hat{\\theta}\\) are reversed relative to their roles in classical, frequentist inference: \\(\\theta\\) is random, in the sense that the researcher is uncertain about its value, while \\(\\hat{\\theta}\\) is fixed, a feature of the data at hand."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "title": "What is Bayesian inference?",
    "section": "Conclusions",
    "text": "Conclusions\nSo, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeatable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artificial way” or relying on asymptotic results.\nI hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics."
  },
  {
    "objectID": "posts/2020-06-05-my-blog-post/index.html",
    "href": "posts/2020-06-05-my-blog-post/index.html",
    "title": "New tutorials for missingHE",
    "section": "",
    "text": "Nothing major to report for the past month, mostly spent at home still in lockdown. A few offices and shops have already opened in London but all the public stuff, including my office at UCL will remain close until who knows when. So, in the meantime I put some work into prpearing some tutorials about how to use the different functions from my R package missingHE. These are built directly into the package in the form of vignettes which can be easily accessed from the R terminal once the package is installed locally.\nI have worked on three main tutorials dedicated to explain the basic functions of the package and to show how to customise the different models using different combinations of input choices. The three vignettes have each a specific target of users, starting from the beginners in using R to those who would like to have a more flexible specification of the models based on different modelling assumptions. I use the built-in dataset in the package, the MenSS study, to give practical examples of how the different changes to the models may affect the results in a standard analysis.\nThe three tutorials are:\n\nIntroduction to missingHE, which is intended for those who have little familiarity with R and just want an overview of the different functions of the package, what they do and how to extract the relevant information from the fitted models.\nFitting MNAR models in missingHE, which is intended for those who already know about the main functions of the package and would like to explore more deeply how to perform sensitivity analysis to missing not at random assumtpions using the arguments of each function.\nModel Customisation in missingHE, which is intended for those who have already grasped the basic idea behind the different functions and would like to customise their models and not just stick with the default settings. Examples include how to specify random effects, different prior distributions and so on.\n\nI believe these tutorials provide a reasonable summary about the key elements for anyone who would like to use the package but is a bit uncertain about what he or she can actually do with the functions and to which extent customisation is possible. For the moment the vignettes are only available from my GitHub version of the package (1.4.1) and can be accessed by installing the package using the command\n\ndevtools::install_github(\"AnGabrio/missingHE\", build_vignettes = TRUE)\n\nand then typing\n\nutils::browseVignettes(package = \"missingHE\")\n\nNote that you need to locally install the packages devtools and utils to access their functions. As soon as I have a bit of time I will update the version on CRAN to make them available from there as well. I spent a bit of time creating these tutorials and I hope people will find them useful to understand the package. In case anything is still unclear, feel free to contact me to ask questions.\nThat is pretty much it for the moment from me. I should add new tutorials for using JAGS and STAN on the website but time is always never enough. Naaah, I am just very lazy these days."
  },
  {
    "objectID": "posts/2020-04-20-my-blog-post/index.html",
    "href": "posts/2020-04-20-my-blog-post/index.html",
    "title": "New updates for missingHE",
    "section": "",
    "text": "In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package missingHE with exciting updates!\nFor those who do not know, missingHE is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes. The cool new things in this version are the following:\n\nFirst, random effects can now be specified for each model implemented in missingHE (I know, Bayesians should not talk about “random” or “fixed” effects as we know that there are no real “fixed” effects but the terms have become quite popular and many people would prefer this way). These include selection, hurdle and pattern mixture models. The package allows a flexible implementation of either random intercept only, random slope only and both random intercept and slope models based on the input given by the user. The random effects term is specified via the formula \\(y \\sim x + (x \\mid z)\\) where x is a covariate included also as a fixed effects in the model and z is the clustering variable over which the random effects for x are specified. It is possible to remove the random intercept if desired by adding 0 + inside the brackets (by default this is included).\nSecond, new types of posterior predictive checks can now be chosen using the function ppc for each type of model fitted using the function of the package. These include plotting the Bayesian posterior p-values (which should not be confused with the usual p-values as they are completely different) based on the posterior replications of the models and a given statistics computed from the observed data. The statistic can be provided by the user under the form of a univariate function (e.g. mean or sd) or a specific type of bivariate function (e.g. cor).\nThird, a new generic function called coef has been added which allows to extract the regression coefficients from each type of model, either in terms of fixed effects or random effects (if specified).\n\nI am quite proud of this new update as it is something I considered for a long time which is now available. If even one person find this useful, I think it will be worth all my effort. Very nice.\n\n\n\n\n\nOh, and yes you can also find the new version of missingHE on my GitHub page. I plan to upload a more serious tutorial on how to use all the functions of the package at some point (hopefully not too far from now).\nSo, now that all the fun part is done, I need to go back to doing meetings, reviews, writing papers, etc … It will be a quite busy period again but now I feel motivated. Let’s see for how long this will last."
  },
  {
    "objectID": "posts/2020-03-20-my-blog-post/index.html",
    "href": "posts/2020-03-20-my-blog-post/index.html",
    "title": "Living and working at home is nice, right?",
    "section": "",
    "text": "It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outside for working and, if not risking thier life, at least put at risk the life of those who they care most. Last week was particularly bad in terms of supermarket products which were sold out for the most part. This week is a bit better as people may have realised that for the moment, if we just buy products as usual, we still have food and toilet paper for everyone.\nTo be honest, not much to update on my work which has slowed down due to this whole situation and also to me not feeling at my best. I hope I will have some time to look at the different projects I am involved with in the next few days. In the meantime, I worked a bit on my website with new JAGS and STAN tutorials and I have also uploaded on my GitHub page some materials (e.g. software code) related to some of the projects I did. For example, here the link to the JAGS and STAN code for the model I used to predict volleyball results\nNot sure what gif to use this time to conclude the post. So I guess I will just go for a random cat picture, which does not make any sense but which is always nice to look at."
  },
  {
    "objectID": "posts/2020-02-10-my-blog-post/index.html",
    "href": "posts/2020-02-10-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "The moment for the second edition of the HEART’s one-day introductory course to health economics arrived at last! The course, led by Rachael Hunter and called “Understanding health economics in clinical trials”, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants. Also, this time a new HEART member (Marie) joined the group and very nicely delivered the session about patient reported outcome measures (PROMs), engaging in nice discussions with the audience.\nFinally, a couple of personal notes.\n\nI recently attended a very interesting meeting about missing data methodology which was held by an international group of very talented senior and junior statisticians from different universities, including people like Ian White and James Carpenter from UCL and the LSHTM. It was really an amazing experience to meet so many people working in different stats area but with a common passion about missing data methods (also mine!). From what I understood this series of meetings (called “MiDIA”) have been held since years but do not have a very regular schedule due to people being busy I guess, which makes totally sense. Not sure when the next one will be held but now I am definitely looking forward to the next meetings!\n\n\n\n\n\n\n\nI would also like to highlight a recent tweet from UCL PRIMENT CTU, which advertises a new position as health economist in our HEART group for performing health economics using data from clinical trials. I would encourage anyone interested in some good applied health economic work to apply for this position. Deadline 15 March 2020.\n\n\n\n\n\n\n\nTo conclude, I would also like to say that I have done some updates to this website. From the inclusion of new tutorials on the use of JAGS and STAN on different statistical topics, to a restyle of the website. In particular I had fun by playing around with some Markdown code to add new features, e.g. customised alert notes and emoji, for example. Something like this:\n\n\n\n\n\n\n\n King’s note ! \n\n\n\nI  \\(\\LaTeX\\) very much.\n\n\nThis took me so much time but I am quite satisfied with the result if I may say so. You really never stop learning new things!"
  },
  {
    "objectID": "posts/2020-01-09-my-blog-post/index.html",
    "href": "posts/2020-01-09-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the HEART group are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our “pilot” we gave last year (which went really well by the way) and involves many different topics that will cover the entire day of February 11th. The attending list is already full and thw waiting list is also quite big; happy to see so much interest in economic evaluations.\nSecond, I will give a talk at the PRIMENT statistics and health economics and methodology seminar about an on-going project on missing data in trial-based analysis on Tuesday 28th, at UCL PRIMENT CTU. I am really happy to be back at these seminars which I feel I really nice and where you have the opportunity to interact with people from different backgrounds and job positions who may give some useful feedback on my work. Hopefully, people will find my research interesting!. I would also like to mention the fact that one of my HEART colleague, Marie, will give another talk at the same seminar just before me. Her topic is the economic analysis plan for a trial she has been involved with and I think she is really good, so may worth check her presentaiton out.\nThird, I have finalised a long-waited submission for a paper which has been discussed, written and re-written many times. I really hope we can get some useful feedback on it as I personally worked very hard to keep this work alive. Let see if my efforts have not been in vain and fingers crossed!\n\n\n\n\n\nFourth, as a side note, I have recently bought a new book on missing data called Semiparametric Thoery and Missing Data by Tsiatis, which looks very interesting. To be honest, the book is quite technical with many theoretical concpets and proofs which sometimes I find hard to follow. However, so far it gives a nice introduction to semiparametric models and I look forward to see how it approaches the missing data topic from a non likelihood-based approach. If you are into non/semiparametric statistics and want to find out more about this, I recommend the reading.\nFinally, more work is also coming up in the next weeks and some of this is not going to be very enjoyable, I think. Anyway, let us go through this busy period at our best and see how things will go."
  },
  {
    "objectID": "posts/2019-11-09-my-blog-post/index.html",
    "href": "posts/2019-11-09-my-blog-post/index.html",
    "title": "Too many things, again….",
    "section": "",
    "text": "I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.\nA couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).\nSecond, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).\nThird, I would like to quickly summarise my first experience at ISPOR Europe in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor Andrea Manca and the always very kind Chris Sampson for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under ICON plc. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend Ryan Pulleyblank, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.\nFinally, as a side note, I have found the time to upload on my arXiv page a nice application of Bayesian hierarchical models for the prediction of volleyball matches which I have been working on the past summer, taking inspiration from the work of Gianluca about predicting football macthes. I hope my work can turn out in something cool as well.\n\n\n\n\n\nThis is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself."
  },
  {
    "objectID": "posts/2019-10-01-my-blog-post/index.html",
    "href": "posts/2019-10-01-my-blog-post/index.html",
    "title": "More good news…",
    "section": "",
    "text": "I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methods for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of Baptiste and Alexina which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.\n\n\n\n\n\nI am also excited to visit Brighton, since many people keep telling me that I should go and visit this sort of british version of “Rimini”. To be honest, I do not expect to find a nice weather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.\nFinally, I have started a rubric called missing data on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months."
  },
  {
    "objectID": "posts/2019-09-15-my-blog-post/index.html",
    "href": "posts/2019-09-15-my-blog-post/index.html",
    "title": "Discussing my thesis",
    "section": "",
    "text": "I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled “Thesis Thursday” on the The Academic Health Economists blog.\nI happily accepted Chris’s invitation as I believe this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.\nHere you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.\n\n\n\n\n\nI shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so. In fact, there are not many blogs around health economics matters (here a non-comprehensive list), among which The Academic Health Economists and Gianluca’s blog are my favourites.\nI hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment."
  },
  {
    "objectID": "posts/2019-07-03-my-blog-post/index.html",
    "href": "posts/2019-07-03-my-blog-post/index.html",
    "title": "HESG Summer Meeting 2019",
    "section": "",
    "text": "I have just come back form my first Health Economists’ Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.\nI particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.\n\n\n\nThe beautiful Norwich’s cathedral\n\n\nOther nice people and colleagues from HEART and other UCL department came to HESG with me, including Caroline and Ekaterina (aka Katia), you can see them in thumbnail of this post. I was also pleased to meet Baptiste from LSHTM, who shares with me the interest in missing data methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. I also had the opportunity to talk about my work with the discussant of my session, Catrin Plumpton from the Centre for Health Economics and Medicines Evaluation, who gave me some nice feedback which I really appreciated, especially given her mathematical background.\nAn important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for Emma Mcmanus who was amazing.\n\n\n\n\n\nIn summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as WinBUGS, but this was mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am."
  },
  {
    "objectID": "missing_data/wa/wa.html",
    "href": "missing_data/wa/wa.html",
    "title": "Weighting Adjustments",
    "section": "",
    "text": "The notion of reducing bias due to missingness through reweighting methods has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (Little and Rubin (2019)). In particular, in probability sampling, a unit selected from a target population with probability \\(\\pi_i\\) can be thought as “representing” \\(\\pi^{-1}_i\\) units in the population and hence should be given weight \\(\\pi^{-1}_i\\) when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum \\(j\\) represents \\(\\frac{N_j}{n_j}\\) population units, where \\(n_j\\) indicates the units sampled from the \\(N_j\\) population units in stratum \\(j=1,\\ldots,J\\). The population total \\(T\\) can then be estimated by the weighted sum\n\\[\nT = \\sum_{i=1}^{n}y_i\\pi^{-1}_i,\n\\]\nknown as the Horvitz-Thompson estimate (Horvitz and Thompson (1952)), while the stratified mean can be written as\n\\[\n\\bar{y}_{w} = \\frac{1}{n}\\sum_{i=1}^{n}w_iy_i,\n\\]\nwhere \\(w_i=\\frac{n\\pi^{-1}_i}{\\sum_{k=1}^n\\pi^{-1}_k}\\) is the sampling weight attached to the \\(i\\)-th unit scaled tosum up to the sample size \\(n\\). Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit \\(\\phi_i\\) were known, then the probability of selection and response is \\(\\pi_i\\phi_i\\) and we have\n\\[\n\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\n\\]\nwhere the sum is now over responding units and \\(w_i=\\frac{n_r(\\pi_i\\phi_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\phi_k)^{-1}}\\). In practice, the response probability \\(\\phi_i\\) is not known and is typically estimated based on the information available for respondents and nonrespondents (Schafer and Graham (2002))."
  },
  {
    "objectID": "missing_data/wa/wa.html#weighting-class-estimator-of-the-mean",
    "href": "missing_data/wa/wa.html#weighting-class-estimator-of-the-mean",
    "title": "Weighting Adjustments",
    "section": "Weighting Class Estimator of the Mean",
    "text": "Weighting Class Estimator of the Mean\nA simple reweighting approach is to partition the sample into \\(J\\) “weighting classes” according to the variables observed for respondents and nonrespondents. If \\(n_j\\) is the sample size, \\(n_{rj}\\) the number of respondents in class \\(j\\), with \\(n_r=\\sum_{j=1}^Jr_j\\), then a simple estimator of the response probability for units in class \\(j\\) is given by \\(\\frac{n_{rj}}{n_j}\\). Thus, responding units in class \\(j\\) receive weight \\(w_i=\\frac{n_r(\\pi_i\\hat{\\phi}_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\hat{\\phi}_k)^{-1}}\\), where \\(\\hat{\\phi}_i=\\frac{n_{rj}}{n_j}\\) for unit \\(i\\) in class \\(j\\). The weighting class estimate of the mean is then\n\\[\n\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\n\\]\nwhich is unbiased under the quasirandomisation assumption (Oh and Scheuren (1983)), which requires respondents in weighting class \\(j\\) to be a random sample of the sampled units, i.e. data are Missing Completely At Random (MCAR) within adjustment class \\(j\\). Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance \\(\\sigma^2\\) for an outcome \\(y\\), and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is\n\\[\n\\text{Var}\\left(\\frac{1}{n_{r}}\\sum_{i=1}^{n_{r}}w_iy_i \\right) = \\frac{\\sigma^2}{n_{r}^2}\\left(\\sum_{i=1}^{n_{r}}w_{i}^{2} \\right) = \\frac{\\sigma^2}{n_{r}}(1+\\text{cv}^2(w_i)),\n\\]\nwhere \\(\\text{cv}(w_i)\\) is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (Kish (1992)). When the weighting class variable is predictive of \\(y\\), weighting methods can lead to a reduction in sampling variance. Little and Rubin (2019) summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome \\(y\\) and missing indicator \\(m\\) are high or low.\n\nEffect of weighting adjustments on bias and sampling variance of a mean.\n\n\n\nLow (y)\nHigh (y)\n\n\n\n\nLow (m)\nbias: /, var: /\nbias: /, var: -\n\n\nHigh (m)\nbias: /, var: +\nbias: -, var: -\n\n\n\n\n\nThus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction."
  },
  {
    "objectID": "missing_data/wa/wa.html#propensity-weighting",
    "href": "missing_data/wa/wa.html#propensity-weighting",
    "title": "Weighting Adjustments",
    "section": "Propensity Weighting",
    "text": "Propensity Weighting\nIn some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (Rosenbaum and Rubin (1983)) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable \\(c\\). Suppose the data are Missing At Random (MAR) such that\n\\[\np(m\\mid X,y,\\phi)=p(m\\mid X,\\phi),\n\\]\nwhere \\(\\phi\\) are unknown parameters and define the nonresponse propensity for unit \\(i\\) as\n\\[\n\\rho(x_i,\\phi)=p(m_i=1 \\mid \\phi),\n\\]\nassuming that this is strictly positive for all values of \\(x_i\\). Then, it can be shown that\n\\[\np(m\\mid \\rho(X,\\phi),y,\\phi)=p(m\\mid \\rho(X,\\phi),\\phi),\n\\]\nso that respondents are a random subsample within strata defined by the propensity score \\(\\rho(X,\\phi)\\). In practice the parameter \\(\\phi\\) is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator \\(m\\) on \\(X\\) based on respondent and nonrespondent data (Liu (2004)). A variant of this procedure is to weight respondents \\(i\\) directly by the inverse of the estimated propensity score \\(\\rho(X,\\hat{\\phi})^{-1}\\) (Cassel, Sarndal, and Wretman (1983)), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification."
  },
  {
    "objectID": "missing_data/sm/sm.html",
    "href": "missing_data/sm/sm.html",
    "title": "Selection Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Selection Models(SM)."
  },
  {
    "objectID": "missing_data/sm/sm.html#selection-models",
    "href": "missing_data/sm/sm.html#selection-models",
    "title": "Selection Models",
    "section": "Selection Models",
    "text": "Selection Models\nThe selection model approach factors the full data distribution as\n\\[\nf(y,r \\mid \\omega) = f(y \\mid \\theta) f(r \\mid y,\\psi),\n\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\theta,\\psi)\\). Thus, under the SM approach, the response model \\(f(y \\mid \\theta)\\) and the missing data mechanism \\(f(r \\mid y, \\psi)\\) must be specified by the analyst. SMs can be attractive for several reasons, including\n\nThe possibility to directly specify the model of interest \\(f(y \\mid \\theta)\\)\nThe SM factorisation appeals to Rubin’s missing data taxonomy, enabling easy characterisation of the missing data mechanism\nWhen the missingness pattern is monotone, the missigness mechanism can be formulated as a hazard function, where the hazard of dropout at some time point \\(j\\) can depend on parts of the full data vector \\(Y\\)\n\n\nExample of SM for bivariate normal data\nConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A SM factors the full data distribution as\n\\[\nf(y_1,y_2,r \\mid \\omega) = f(y_1 \\mid \\theta)f(r \\mid y_1,y_2,\\psi),\n\\]\nwhere we assume \\(\\omega=(\\theta,\\psi)\\). Suppose we specify \\(f(y_1,y_2 \\mid \\theta)\\) as a bivariate normal density with mean \\(\\mu\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). The distribution of \\(r\\) is assumed to be distributed as a Bernoulli variable with probability \\(\\pi_i\\), such that\n\\[\ng(\\pi_i) = \\psi_0 + \\psi_1y_{i1} + \\psi_2y_{i2},\n\\]\nwhere \\(g()\\) denotes a given link function which relates the expected value of the response to the linear predictors in the model. When this is taken as the inverse normal cumulative distribution function \\(\\Phi^{-1}()\\) the model corresponds to the Heckman probit selection model (Heckman (1976)). In general, setting \\(\\psi_2=0\\) leads to a Missing At Random(MAR) assumption; if, in addition, we have distinctness of the parameters \\(f(\\mu,\\Sigma,\\psi)=f(\\mu,\\Sigma)f(\\psi)\\), we have ignorability. We note that, even though the parameter \\(\\psi_2\\) characterises the association between \\(R\\) and \\(Y_2\\), the parametric assumptions made in this example will identify \\(\\psi_2\\) even in the absence of informative priors, that is the observed data likelihood is a function of \\(\\psi_2\\). Moreover, the parameter indexes the joint distribution of observables \\(Y_{obs}\\) and \\(R\\) and in general can be identified from the observed data. This property of parametric SMs make them ill-suited to assessing sensitivity to assumptions about the missingness mechanism.\nThe model can also be generalised to longitudinal data assuming a multivariate normal distribution for \\(Y=(Y_1,\\ldots,Y_J)\\) and replacing \\(\\pi_i\\) with a discrete time hazard function for dropout\n\\[\nh\\left(t_j \\mid \\bar{Y}_{j}\\right) = \\text{Prob}\\left(R_j = 0 \\mid R_{j-1} = 1, Y_{1},\\ldots,Y_{j} \\right).\n\\]\nUsing the logit function to model the discrete time hazard in terms of observed response history \\(\\bar{Y}_{j-1}\\) and the current but possibly unobserved \\(Y_j\\) corresponds to the model of Diggle and Kenward (1994)."
  },
  {
    "objectID": "missing_data/sm/sm.html#conlcusions",
    "href": "missing_data/sm/sm.html#conlcusions",
    "title": "Selection Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, SMs allows to generalise ignorable models to handle nonignorable missingness by letting \\(f(r \\mid y_{obs},y_{mis})\\) to depend on \\(y_{mis}\\) and their structure directly appeals to Rubin’s taxonomy. However, identification of the missing data distribution is accomplished through parametric assumptions about the full data response model \\(f(y \\mid \\theta)\\) and the explicit form of the missingness mechanism. This makes it difficult to disentagle the type of information that is used to identify the model, i.e. parametric modelling assumptions or information from the observed data, therefore complicating the task of assessing the robustness of the results to a range of transparent and plausible assumptions."
  },
  {
    "objectID": "missing_data/mice/mice.html",
    "href": "missing_data/mice/mice.html",
    "title": "Multiple Imputation by Chained Equations",
    "section": "",
    "text": "Multiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example."
  },
  {
    "objectID": "missing_data/mice/mice.html#rubins-rules",
    "href": "missing_data/mice/mice.html#rubins-rules",
    "title": "Multiple Imputation by Chained Equations",
    "section": "Rubin’s rules",
    "text": "Rubin’s rules\nLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\n\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\n\\]\nBecause the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\n\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\n\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\nT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\n\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\n(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\n\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\n\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\n\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution."
  },
  {
    "objectID": "missing_data/mice/mice.html#multiple-imputation-by-chained-equations",
    "href": "missing_data/mice/mice.html#multiple-imputation-by-chained-equations",
    "title": "Multiple Imputation by Chained Equations",
    "section": "Multiple Imputation by Chained Equations",
    "text": "Multiple Imputation by Chained Equations\nMI by Chained Equations, also known as Fully Conditional Specification(FCS), imputes multivariate missing data on a variable-by-variable basis, and therefore requires the specification of an imputation model for each incomplete variable to create imputations per variable in an iterative fashion (Van Buuren (2007)). In contrast to Joint MI, MICE specifies the multivariate distribution for the outcome and missingness pattern \\(p(y,r\\mid \\theta, \\phi)\\), indexed by the parameter vectors of the outcome (\\(\\theta\\)) and missingness models (\\(\\phi\\)), through a set of conditional densities \\(p(y_j \\mid y_{-j},r,\\theta_j, \\phi_j)\\), which is used to impute \\(y_j\\) given the other variables. Starting from a random draw from the marginal distribution of \\(y_1\\), imputation is then carried out by iterating over the conditionally specified imputation models for each \\(y_j=(y_2,\\ldots,y_J)\\) separately given the set of all the other variables \\(y_{-j}\\).\nTha main idea of MICE is to directly draw the missing data from the predictive distribution of conditional densities, therefore avoiding the need to specify a joint multivariate model for all the data. Different approaches can be used to implement MICE. For example, a possible strategy is the following:\n\nStart at iteration \\(t=0\\) by drawing randomly from the the distribution of the missing data given the observed data and all other variables, according to some probability model for each variable \\(y_j\\), that is\n\n\\[\n\\hat{y}^{mis}_{j,0} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, y_{-j}, r)\n\\]\n\nAt each iteration \\(t=1,\\ldots,T\\) and for each variable \\(j=\\ldots,J\\), set\n\n\\[\n\\hat{y}^{mis}_{-j,t}=\\left(\\hat{y}_{1,t},\\ldots, \\hat{y}_{j-1,t}, \\hat{y}_{j+1,t}, \\ldots, \\hat{y}_{J,t} \\right)\n\\]\nas the currently completed data except \\(y_j\\)\n\nDraw \\(h=1,\\ldots,H\\) imputations for each variable \\(y_j\\) from the predictive distribution of the missing data given the observed data and the currently imputed data at \\(t\\), that is\n\n\\[\n\\hat{y}^{mis}_{j,t} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, \\hat{y}_{-j,t}, r)\n\\]\nand repeat the steps 2 and 3 until convergence. It is important to stress out that MICE is essentially a Markov Chain Monte Carlo(MCMC) algorithm (Brooks et al. (2011)), where the state space is the collection of all imputed values. More specifically, when the conditional distributions of all variables are compatible with a joint multivariate distribution, the algorithm corresponds to a Gibbs sampler, a Bayesian simulation method that samples from the conditional distributions in order to obtain samples from the joint multivariate distribution of all variables via some conditional factorisation of the latter (Casella and George (1992), Gilks, Richardson, and Spiegelhalter (1996)). A potential issue of MICE is that, since the conditional distributions are specified freely by the user, these may not be compatible with a joint distribution and therefore it is not clear from which distribution the algorithm is sampling from. However, a general advatage of MICE is that it gives freedom to the user for the specification of the univariate models for the variables, which can be tailored to handle different types of variabes (e.g. continuous and categorical) and different statistical issues for each variable (e.g. skewness and non-liner associations).\nRegardless of the theoretical implications of MICE, as a MCMC method, the algorithm converges to a stationary distribution when three conditions are satisfied (Roberts (1996),Brooks et al. (2011)):\n\nThe chain is irreducible, i.e. must be able to reach any state from any state in the state space\nThe chain is aperiodic, i.e. must be able to return to each state after some unknown number of steps or transitions\nThe chain is recurrent, i.e. there is probability of one of eventually returning to each state after some number of steps\n\nTypically periodicity and non-recurrence can be a problem in MICE when the imputation models are not compatible, possibly leading to different inferences based on the stopping point of the chain or to non-stationary behaviours of the chain."
  },
  {
    "objectID": "missing_data/likinf/likinf.html",
    "href": "missing_data/likinf/likinf.html",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "",
    "text": "As for the inference under complete data, inference under incomplete data consists in deriving the likelihood for the parameters based on the available data, either using a Maximum Likelihood (ML) approach (solving the likelihood equation) or using the Bayes’ rule incorporating a prior distribution (performing necessary integrations to obtain the posterior distribution). However, asymptotic standard errors obtained from the information matrix, are more questionable when dealing with missing data since the sample will not be typically iid and results that imply the large sample normality of the likelihood function do not immediately apply. More complications arise when dealing with the process that lead to some of the data to be missing. This can be explained with a simple example.\nLet \\(Y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) and \\(j=1,\\ldots,J\\), denote the complete dataset if there were no missing values, with a total of \\(n\\) units and \\(J\\) variables. Let \\(M=(m_{ij})\\) denote the fully observed matrix of binary missing data indicators with \\(m_{ij}=1\\) if \\(y_{ij}\\) is missing and \\(0\\) otherwise. As an example, we can model the density of the joint distribution of \\(Y\\) and \\(M\\) using the selection model factorisation (Little and Rubin (2019))\n\\[\np(Y=y,M=m \\mid \\theta, \\psi) = f(y \\mid \\theta)f(m \\mid y, \\psi),\n\\]\nwhere \\(\\theta\\) is the parameter vector indexing the response model and \\(\\psi\\) is the parameter vector indexing the missingness mechanism. The observed values \\(m\\) effect a partition \\(y=(y_1,y_0)\\), where \\(y_0=[y_{ij} : m_{ij}=0]\\) is the observed component and \\(y_1=[y_{ij} : m_{ij}=1]\\) is the missing component of \\(y\\). The full likelihood based on the observed data and the assumed model is\n\\[\nL_{full}(\\theta, \\psi \\mid y_{0},m) = \\int f\\left(y_{0},y_{1} \\mid \\theta \\right) f\\left(m \\mid y_{0},y_{1}, \\psi \\right)dy_{1}\n\\]\nand is a function of the parameters \\((\\theta,\\psi)\\). Next, we define the likelihood of ignoring the missingness mechanism or ignorable likelihood as\n\\[\nL_{ign}\\left(\\theta \\mid y_{0} \\right) = \\int f(y_{0},y_{1}\\mid \\theta)dy_{1},\n\\]\nwhich does not involve the model for \\(M\\). In practice, modelling the joint distribution of \\(Y\\) and \\(M\\) is often challenging and, in fact, many approaches to missing data do not model \\(M\\) and (explicitly or implicitly) base inference about \\(\\theta\\) on the ignorable likelihood. It is therefore important to assess under which conditions inferences about \\(\\theta\\) based on \\(L_{ign}\\) can be considered appropriate. More specifically, the missingness mechanism is said to be ignorable if inferences about \\(\\theta\\) based on the ignorable likelihood equation evauluated at some realisations of \\(y_0\\) and \\(m\\) are the same as inferences about \\(\\theta\\) based on the full likelihood equation, evaluated at the same realisations of \\(y_0\\) and \\(m\\). The conditions for ignoring the missingness mechanism depend on whether the inferences are direct likelihood, Bayesian or frequentist."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#direct-likelihood-inference",
    "href": "missing_data/likinf/likinf.html#direct-likelihood-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Direct Likelihood Inference",
    "text": "Direct Likelihood Inference\nDirect Likelihood Inference refers to inference based solely on likelihood ratios for pair of values of the parameters, with the data fixed at their observed values. The missingness mechanism can be ignored for direct likelihood if the likelihood ratio based on the ignorable likelihood is the same as the ratio based on the full likelihood. More precisely, the missingness mechanism is said to be ignorable for direct likelihood inference at some realisations of \\((y_0,m)\\) if the likelihood ratio for two values \\(\\theta\\) and \\(\\theta^\\star\\) is the same whether based on the full or ignorable likelihood. That is\n\\[\n\\frac{L_{full}\\left( \\theta, \\psi \\mid y_{0}, m \\right)}{L_{full}\\left( \\theta^{\\star}, \\psi \\mid y_{0}, m \\right)}=\\frac{L_{ign}\\left( \\theta \\mid y_{0} \\right)}{L_{ign}\\left( \\theta^{\\star} \\mid y_{0}\\right)},\n\\]\nfor all \\(\\theta\\), \\(\\theta^\\star\\) and \\(\\psi\\). In general, the missingnes mechanism is ignorable for direct likelihood inference if the following two conditions hold:\n\nParameter distinctness. The parameters \\(\\theta\\) and \\(\\psi\\) are distinct, in the sense that the joint parameter space \\(\\Omega_{\\theta,\\psi}\\) is the product of the two parameter spaces \\(\\Omega_{\\theta}\\) and \\(\\Omega_{\\psi}\\).\nFactorisation of the full likelihood. The full likelihood factors as\n\n\\[\nL_{full}\\left(\\theta, \\psi \\mid y_{0},m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right)  L_{rest}\\left(\\psi \\mid y_{0},m \\right)\n\\]\nfor all values of \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). The distinctness condition ensures that each value of \\(\\psi \\in \\Omega_{\\psi}\\) is compatible with different values of \\(\\theta \\in \\Omega_{\\theta}\\). A sufficient condition for the factorisation of the full likelihood is that the missing data are Missing At Random(MAR) at the specific realisations of \\(y_{0},m\\). This means that the distribution function of \\(M\\), evaluated at the given realisations \\((y_{0},m)\\), does not depend on the missing values \\(y_1\\), that is\n\\[\nf\\left(m \\mid y_{0}, y_{1}, \\psi \\right)=f\\left(m \\mid y_{0}, y^{\\star}_{1} \\psi \\right),\n\\]\nfor all \\(y_{1},y^\\star_{1},\\psi\\). Thus, we have\n\\[\nf\\left(y_{0}, m \\mid \\theta, \\psi \\right) = f\\left(m \\mid y_{0}, \\psi \\right) \\int f\\left(y_{0},y_{1} \\mid \\theta \\right)dy_{1} = f\\left(m \\mid y_{0}, \\psi \\right) f\\left( y_{0} \\mid \\theta \\right).\n\\]\nFrom this it follows that, if the missing data are MAR at the given realisations of \\((y_{0},m)\\) and \\(\\theta\\) and \\(\\psi\\) are distinct, the missingnes mechanism is ignorable for likelihood inference."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#bayesian-inference",
    "href": "missing_data/likinf/likinf.html#bayesian-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nBayesian Inference under the full model for \\(Y\\) and \\(M\\) requires that the full likelihood is combined with a prior distribution \\(p(\\theta,\\psi)\\) for the parameters \\(\\theta\\) and \\(\\psi\\), that is\n\\[\np\\left(\\theta, \\psi \\mid y_{0}, m \\right) \\propto p(\\theta, \\psi) L_{full}\\left(\\theta, \\psi \\mid y_{0}, m \\right).\n\\]\nBayesian inference ignoring the missingness mechanism combines the ignorable likelihood with a prior distribution for \\(\\theta\\) alone, that is\n\\[\np(\\theta \\mid y_{0}) \\propto p(\\theta) L_{ign}\\left(\\theta \\mid y_{0} \\right).\n\\]\nMore formally, the missingness mechanism is said to be ignorable for Bayesian inference at the given realisations of \\((y_{0},m)\\) if the posterior distribution for \\(\\theta\\) based on the posterior distribution for the full likelihood and prior distribution for \\((\\theta,\\psi)\\) is the same as the posterior distribution for the ignorable likelihood and the prior distribution for \\(\\theta\\) alone. This holds when the following conditions are satisfied:\n\nThe parameters \\(\\theta\\) and \\(\\psi\\) are a priori independent, that is the prior distribution has the form\n\n\\[\np(\\theta , \\psi) = p(\\theta) p(\\psi)\n\\]\n\nThe full likelihood evaluated at the realisations of \\((y_{0},m)\\) factors as for direct likelihood inference\n\nUnder these conditions:\n\\[\np(\\theta, \\psi \\mid y_{0}, m) \\propto \\left(p(\\theta)L_{ign}\\left( \\theta \\mid y\\_{0} \\right) \\right) \\left(p(\\psi)L_{rest}\\left(\\psi \\mid y_{0},m \\right)  \\right).\n\\]\nAs for direct likelihood inference, MAR is a sufficient condition for the factorisation of the full likelihood. This means that, if the data are MAR at the given realisations of \\((y_{0},m)\\) and the parameters \\(\\theta\\) and \\(\\psi\\) are a prior independent, then the missingness mechanism is ignorable for Bayesian inference. We note that the a priori condition is more stringent than the distinctness condition because paramerers with distinct parameter spaces might have dependent prior distributions."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#frequentist-asymptotic-inference",
    "href": "missing_data/likinf/likinf.html#frequentist-asymptotic-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Frequentist Asymptotic Inference",
    "text": "Frequentist Asymptotic Inference\nFrequentist Asymptotic Inference requires that, in order to ignore the missingness mechanism, the factorisation of the full likelihood needs to be valid for values of the observed data under repeated sampling. This means that we require\n\\[\nL_{full}\\left(\\theta,\\psi \\mid y_{0}, m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right) L_{rest}\\left(\\psi \\mid y_{0}, m \\right)\n\\]\nfor all \\(y_{0},m\\) and \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). For this form of inference, a sufficient condition for ignoring the missingness mechanism is given by the following conditions:\n\nParameter distinctness as defined for direct likelihood inference.\nMissing data are Missing Always At Random (MAAR), that is\n\n\\[\nf\\left(m \\mid y_{0},y_{1},\\psi \\right) = f\\left(m \\mid y_{0}, y^{\\star}_{1},\\psi \\right)\n\\]\nfor all \\(m,y_{0},y_{1},y^\\star_{1},\\psi\\). In the following example we discuss conditions for ignoring the missingness mechanism for direct likelihood and Bayesian inference, which can be extended to the case of frequentist asymptotic inference by requiring that they hold for for values of \\(y_{0},m\\) other than those observed that could arise in repeated sampling."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#bivariate-normal-sample-with-one-variable-subject-to-missingness",
    "href": "missing_data/likinf/likinf.html#bivariate-normal-sample-with-one-variable-subject-to-missingness",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Bivariate Normal Sample with One Variable Subject to Missingness",
    "text": "Bivariate Normal Sample with One Variable Subject to Missingness\nConsider a bivariate normal sample \\(y=(y_{i1},y_{i2})\\), for \\(i=1,\\ldots,n\\) units, but with the values of \\(y_{i2}\\) being missing for \\(i=(n_{cc}+1),\\ldots,n\\). This leads to a monotone missing data pattern with two variables. The loglikelihood of ignoring the missingness mechanism is\n\\[\nl_{ign}\\left(\\mu, \\Sigma \\mid y_{0} \\right) = \\log\\left(L_{ign}\\left(\\mu,\\Sigma \\mid y_{0} \\right) \\right) = - \\frac{1}{2}n_{cc}ln \\mid \\Sigma \\mid - \\frac{1}{2}\\sum_{i=1}^{n_{cc}}(y_i - \\mu ) \\Sigma^{-1}(y_i - \\mu)^{T} - \\frac{1}{2}(n-n_{cc})ln\\sigma_{1} - \\frac{1}{2}\\sum_{i=n_{cc}+1}^{n}\\frac{(y_{i1}-\\mu_1)^2}{\\sigma_{1}}.\n\\]\nThis loglikelihood is appropriate for inference provided the conditional distribution of \\(M\\) does not depend on the values of \\(y_{i2}\\), and \\(\\theta=(\\mu,\\Sigma)\\) is distinct from \\(\\psi\\). Under these conditions, ML estimates of \\(\\theta\\) can be found by maximising this loglikelihood. For Bayesian inference, if these conditions hold and the prior distribution for \\((\\theta,\\psi)\\) has the form \\(p(\\theta)p(\\psi)\\), then the joint posterior distribution of \\(\\theta\\) is proportional to the product of \\(p(\\theta)\\) and \\(L_{ign}(\\theta \\mid y_{0})\\)."
  },
  {
    "objectID": "missing_data/isi/isi.html",
    "href": "missing_data/isi/isi.html",
    "title": "Implicit Single Imputation",
    "section": "",
    "text": "All case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nIn this part, we focus on some of the most popular Implicit Single Imputation methods. These include: Hot Deck Imputation(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; Substitution(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; Cold Deck Imputation(SI-CD), where missing values are replaced with a constant value from an external source; Composite Methods, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these."
  },
  {
    "objectID": "missing_data/isi/isi.html#hot-deck-imputation",
    "href": "missing_data/isi/isi.html#hot-deck-imputation",
    "title": "Implicit Single Imputation",
    "section": "Hot Deck Imputation",
    "text": "Hot Deck Imputation\nSI-HD procedures refer to the deck of match Hollerith cards for the donors available for a nonrespondent. Suppose that a sample of \\(n\\) out of \\(N\\) units is selected and that \\(n_{cc}\\) out of \\(n\\) are recorded. Given an equal probability sampling scheme, the mean of \\(y\\) can be estimated from the filled-in data as the mean of the responding and the imputed units\n\\[\n\\bar{y}_{HD}=\\frac{(n_{cc}\\bar{y}_{cc}+(n-n_{cc})\\bar{y}^{\\star})}{n},\n\\]\nwhere \\(\\bar{y}_{cc}\\) is the mean of the responding units, and \\(\\bar{y}^\\star=\\sum_{i=1}^{n_{cc}}\\frac{H_iy_i}{n-n_{cc}}\\). \\(H_i\\) is the number of times \\(y_i\\) is used as substitute for a missing value of \\(y\\), with \\(\\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\\) being the number of missing units. The proprties of \\(bar{y}_{HD}\\) depend on the procedure used to generate the numbers \\(H_i\\) and in general the mean and sampling variance of this estimator can be written as\n\\[\nE[\\bar{y}_{HD}]=E[E[\\bar{y}_{HD}\\mid y_{obs}]] \\;\\;\\; \\text{and} \\;\\;\\; Var(\\bar{y}_{HD})=Var(E[\\bar{y}_{HD} \\mid y_{obs}]) + E[Var(\\bar{y}_{HD} \\mid y_{obs})],\n\\]\nwhere the inner expectations and variances are taken over the distribution of \\(H_i\\) given the observed data \\(y_{obs}\\), and the outer expectations and variances are taken over the model distribution of \\(y\\). The term \\(E[Var(\\bar{y}_{HD} \\mid y_{obs})]\\) represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include predictive mean matching or PMM(Little and Rubin (2019)) and last value carried forward or LVCF(Little and Rubin (2019)).\n\nPredictive Mean Matching\nA general approach to hot-deck imputation is to define a metric \\(d(i,j)\\) measuring the distance between units based on observed variables \\(x_{i1},\\ldots,x_{iJ}\\) and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for \\(y_i\\) from a donor pool of units \\(j\\) that are such that \\(y_j,x_1,\\ldots,x_J\\) are observed and \\(d(i,j)\\) is less than some value \\(d_0\\). Varying the value for \\(d_0\\) can control the number of available donors \\(j\\). When the choice of the metric has the form\n\\[\nd(i,j)=(\\hat{y}(x_i)-\\hat{y}(x_j))^2,\n\\]\nwhere \\(\\hat{y}(x_i)\\) is the predicted value of \\(y\\) from the regression of \\(y\\) on \\(x\\) from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of \\(y\\) on \\(x\\), even though better approaches are available when good matches to donor units cannot be found or the sample size is small.\n\n\nLast Value Carried Forward\nLongitudinal data are often subject to attrition when units leave the study prematurely. Let \\(y_i=(y_{i1},\\ldots,y_{iJ})\\) be a \\((J\\times1)\\) vector of partially-observed outcomes for subject \\(i\\), and denote with \\(y_{i,obs}\\) and \\(y_{i,mis}\\) the observed and missing components of \\(y_i\\), i.e. \\(y=(y_{i,obs},y_{i,mis})\\). Define the indicator variable \\(m_i\\) taking value 0 for complete units and \\(j\\) if subject \\(i\\) drops out between \\(j-1\\) and \\(j\\) time points. LVCF, also called last observation carried forward(Pocock (2013)), imputes all missing values for individual \\(i\\) (for whom \\(m_i=j\\)) using the last recorded value for that unit, that is\n\\[\n\\hat{y}_{it}=y_{i,j-1},\n\\]\nwhere \\(t=j,\\ldots,J\\). Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout."
  },
  {
    "objectID": "missing_data/isi/isi.html#conclusions",
    "href": "missing_data/isi/isi.html#conclusions",
    "title": "Implicit Single Imputation",
    "section": "Conclusions",
    "text": "Conclusions\nAccording to Little and Rubin (2019), imputation should generally be\n\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\nMultivariate, to preserve association between missing variables.\nDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\nNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html",
    "href": "missing_data/intro_mle/intro_mle.html",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "",
    "text": "A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#maximum-likelihood-methods-for-complete-data",
    "href": "missing_data/intro_mle/intro_mle.html#maximum-likelihood-methods-for-complete-data",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Maximum Likelihood Methods for Complete Data",
    "text": "Maximum Likelihood Methods for Complete Data\nLet \\(Y\\) denote the set of data, which are assumed to be generated according to a certain probability density function \\(f(Y= y,\\mid \\theta)=f(y \\mid \\theta)\\) indexed by the set of parameters \\(\\theta\\), which lies on the parameter space \\(\\Theta\\) (i.e. set of values of \\(\\theta\\) for which \\(f(y\\mid \\theta)\\) is a proper density function). The Likelihood function, indicated with \\(L(\\theta \\mid y)\\), is defined as any function of \\(\\theta \\in \\Theta\\) proportional that is to \\(f(y \\mid \\theta)\\). Note that, in contrast to the density function which is defined as a function of the data \\(Y\\) given the values of the parameters \\(\\theta\\), instead the likelihood is defined as a function of the parameters \\(\\theta\\) for fixed data \\(y\\). In addition, the loglikelihood function, indicated with \\(l(\\theta\\mid y)\\) is defined as the natural logarithm of \\(L(\\theta \\mid y)\\).\n\nUnivariate Normal Example\nThe joint density function of \\(n\\) independent and identially distributed units \\(y=(y_1,\\ldots,y_n)\\) from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), is\n\\[\nf(y \\mid \\mu, \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nand therefore the loglikelihood is\n\\[\nl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2},\n\\]\nwhich is considered as a function of \\(\\theta=(\\mu,\\sigma^2)\\) for fixed data \\(y\\).\n\n\nMultivariate Normal Example\nIf the sample considered has dimension \\(J&gt;1\\), e.g. we have a set of idependent and identically distributed variables \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables, which comes from a Multivariate Normal distribution with mean vector \\(\\mu=(\\mu_1,\\ldots\\mu_J)\\) and covariance matrix \\(\\Sigma=(\\sigma_{jk})\\) for $ j=1,,J, k=1,,K$ and \\(J=K\\), then its density function is\n\\[\nf(y \\mid \\mu, \\Sigma)=\\frac{1}{\\sqrt{\\left(2\\pi \\right)^{nK}\\left(\\mid \\Sigma \\mid \\right)^n}} \\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^{T}  \\right),\n\\]\nwhere \\(|\\Sigma|\\) denotes the determinant of the matrix \\(\\Sigma\\) and the superscript \\(T\\) denotes the transpose of a matrix or vector, while \\(y_i\\) denotes the row vector of observed values for unit \\(i\\). The loglikelihood of \\(\\theta=(\\mu,\\Sigma)\\) is\n\\[\nl(\\mu,\\Sigma \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(|\\Sigma|)-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^T.\n\\]"
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#mle-estimation",
    "href": "missing_data/intro_mle/intro_mle.html#mle-estimation",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "MLE estimation",
    "text": "MLE estimation\nFinding the maximum value of \\(\\theta\\) that is most likely to have generated the data \\(y\\), corresponding to maximising the likelihood or Maximum Likelihood Estimation(MLE), is a standard approach to make inference about \\(\\theta\\). Suppose a specific value for the parameter \\(\\hat{\\theta}\\) such that \\(L(\\hat{\\theta}\\mid y)\\geq L(\\theta \\mid y)\\) for any other value of \\(\\theta\\). This implies that the observed data \\(y\\) is at least as likely under \\(\\hat{\\theta}\\) as under any other value of \\(\\theta\\), i.e. \\(\\hat{\\theta}\\) is the value best supported by the data. More specifically, a maximum likelihood estimate of \\(\\theta\\) is a value of \\(\\theta \\in \\Theta\\) that maximises the likelihood \\(L(\\theta \\mid y)\\) or, equivalently, that maximises the loglikelihood \\(l(\\theta \\mid y)\\). In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating \\(L(\\theta \\mid y)\\) or \\(l(\\theta \\mid y)\\) with respect to \\(\\theta\\), setting the result equal to zero, and solving for \\(\\theta\\). The resulting equation, \\(D_l(\\theta)=\\frac{\\partial l(\\theta \\mid y)}{\\partial \\theta}=0\\), is known as the likelihood equation and the derivative of the loglikelihood as the score function. When \\(\\theta\\) consists in a set of \\(j=1,\\ldots,J\\) components, then the likelihood equation corresponds to a set of \\(J\\) simultaneous equations, obtained by differentiating \\(l(\\theta \\mid y)\\) with respect to each component of \\(\\theta\\).\n\nUnivariate Normal Example\nRecall that, for a Normal sample with \\(n\\) units, the loglikelihood is indexed by the set of parameters \\(\\theta=(\\mu,\\sigma^2)\\) and has the form\n\\[\nl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2}.\n\\]\nNext, the MLE can be derived by first differentiating \\(l(\\theta \\mid y)\\) with respect to \\(\\mu\\) and set the result equal to zero, that is\n\\[\n\\frac{\\partial l(\\theta \\mid y)}{\\partial \\mu}= -\\frac{2}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)(-1)=\\frac{\\sum_{i=1}^n y_i - n\\mu}{\\sigma^2}=0,\n\\]\nNext, after simplifying a bit, we can retrieve the solution\n\\[\n\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^n y_i=\\bar{y},\n\\]\nwhich corresponds to the sample mean of the observations. Next, we differentiate \\(l(\\theta \\mid y)\\) with respect to \\(\\sigma^2\\), that is we set\n\\[\n\\frac{\\partial l(\\theta \\mid y)}{\\partial \\sigma^2}= -\\frac{n}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i-\\mu)^2=0.\n\\]\nWe then simplify and move things around to get\n\\[\n\\frac{1}{\\sigma^3}\\sum_{i=1}^n(y_i-\\mu)^2=\\frac{n}{\\sigma} \\;\\;\\; \\rightarrow \\;\\;\\;  \\sigma^2=\\frac{1}{n}\\sum\\_{i=1}^n(y_i-\\mu)^2.\n\\]\nFinally, we replace \\(\\mu\\) in the expression above with the value \\(\\hat{\\mu}=\\bar{y}\\) found before and obtain the solution\n\\[\n\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\bar{y})^2=s^2,\n\\]\nwhich, however, is a biased estimator of \\(\\sigma^2\\) and therefore is often replaced with the unbiased estimator \\(\\frac{s^2}{(n-1)}\\). In particular, given a population parameter \\(\\theta\\), the estimator \\(\\hat{\\theta}\\) for \\(\\theta\\) is said to be unbiased when \\(E[\\hat{\\theta}]=\\theta\\). This is the case, for example, of the sample mean \\(\\hat{\\mu}=\\bar{y}\\) which is an unbiased estimator of the population mean \\(\\mu\\):\n\\[\nE\\left[\\hat{\\mu} \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i \\right]=\\frac{1}{n} (n\\mu)=\\mu.\n\\]\nHowever, this is not true for the sample variance \\(s^2\\). This can be seen by first rewriting the expression of the estimator as\n\\[\n\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i^2 -2y_i\\bar{y}+\\bar{y}^2)=\\frac{1}{n}\\sum_{i=1}^n y_i^2 -2\\bar{y}\\sum_{i=1}^n y_i + \\frac{1}{n}n\\bar{y}^2=\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2,\n\\]\nand then by computing the expectation of this quantity:\n\\[\nE\\left[\\hat{\\sigma}^2 \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i^2 \\right] - E\\left[\\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n (\\sigma^2 + \\mu^2) - (\\frac{\\sigma^2}{n}+\\mu^2)=\\frac{1}{n}\\left(n\\sigma^2+n\\mu^2\\right) - \\frac{\\sigma^2}{n}-\\mu^2=\\frac{(n-1)\\sigma^2}{n}.\n\\]\nThe above result is obtained by pluggin in the expression for the variance of a general variable \\(y\\) and retrieving the expression for \\(E[y^2]\\) as a function of the variance and \\(E[y]^2\\). More specifically, given that\n\\[\nVar(y)=\\sigma^2=E\\left[y^2 \\right]-E\\left[y \\right]^2,\n\\]\nthen we know that for \\(y\\), \\(E\\left[y^2 \\right]=\\sigma^2+E[y]^2\\), and similarly we can derive the same expression for \\(\\bar{y}\\). However, we can see that \\(\\hat{\\sigma}^2\\) is biased by a factor of \\((n-1)/n\\). Thus, an unbiased estimator for \\(\\sigma^2\\) is given by multiplying \\(\\hat{\\sigma}^2\\) by \\(\\frac{n}{(n-1)}\\), which gives the unbiased estimator \\(\\hat{\\sigma}^{2\\star}=\\frac{s^2}{n-1}\\), where \\(E\\left[\\hat{\\sigma}^{2\\star}\\right]=\\sigma^2\\).\n\n\nMultivariate Normal Example\nThe same procedure can be applied to an independent and identically distributed multivariate sample \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables (Anderson (1962),Rao et al. (1973),Gelman et al. (2013)). It can be shown that, maximising the loglikelihood with respect to \\(\\mu\\) and \\(\\Sigma\\) yields the MLEs\n\\[\n\\hat{\\mu}=\\bar{y} \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma=\\frac{(n-1)\\hat{\\sigma}^{2\\star}}{n},\n\\]\nwhere \\(\\bar{y}=(\\bar{y}_1,\\ldots,\\bar{y}_{J})\\) is the row vectors of sample means and \\(\\hat{\\sigma}^{2\\star}=(s^{\\star_{jk}})\\) is the sample covariance matrix with \\(jk\\)-th element \\(s^\\star_{jk}=\\frac{\\Sigma_{i=1}^n(y_{ij} - \\bar{y}_j)}{(n-1)}\\). In addition, in general, given a function \\(g(\\theta)\\) of the parameter \\(\\theta\\), if \\(\\hat{\\theta}\\) is a MLE of \\(\\theta\\), then \\(g(\\hat{\\theta})\\) is a MLE of \\(g(\\theta)\\)."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#conditional-distribution-of-a-bivariate-normal",
    "href": "missing_data/intro_mle/intro_mle.html#conditional-distribution-of-a-bivariate-normal",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Conditional Distribution of a Bivariate Normal",
    "text": "Conditional Distribution of a Bivariate Normal\nConsider an indpendent and identically distributed sample formed by two variables \\(y=(y_1,y_2)\\), each measured on \\(i=1\\ldots,n\\) units, which come from a Bivariate Normal distribution with mean vector and covariance matrix\n\\[\n\\mu=(\\mu_1,\\mu_2) \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma = \\begin{pmatrix} \\sigma^2_1 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\ \\end{pmatrix},\n\\]\nwhere \\(\\rho\\) is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are\n\\[\n\\hat{\\mu}_j=\\bar{y}_j \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}_{jk}=\\frac{(n-1)s_{jk}}{n},\n\\]\nwhere \\(\\sigma^2_j=\\sigma_{jj}\\), \\(\\rho\\sigma_{j}\\sigma_{k}=\\sigma_{jk}\\), for \\(j,k=1,2\\). By properties of the Bivariate Normal distribution (Ord and Stuart (1994)), the marginal distribution of \\(y_1\\) and the conditional distribution of \\(y_2 \\mid y_1\\) are\n\\[\ny_1 \\sim \\text{Normal}\\left(\\mu\\_1,\\sigma^2_1 \\right) \\;\\;\\; \\text{and} \\;\\;\\; y_2 \\mid y_1 \\sim \\text{Normal}\\left(\\mu_2 + \\beta(y_1-\\mu_1 \\right), \\sigma^2_2 - \\sigma^2_1\\beta^2),\n\\]\nwhere \\(\\beta=\\rho\\frac{\\sigma_2}{\\sigma_1}\\) is the parameter that quantifies the linear dependence between the two variables. The MLEs of \\(\\beta\\) and \\(\\sigma^2_2\\) can also be derived from the likelihood based on the conditional distribution of \\(y_2 \\mid y_1\\), which have strong connections with the least squares estimates derived in a multiple linear regression framework."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#multiple-linear-regression",
    "href": "missing_data/intro_mle/intro_mle.html#multiple-linear-regression",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(x=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\). The loglikelihood of \\(\\theta=(\\beta,\\sigma^2)\\) given the observed data \\((y,x)\\) is given by\n\\[\nl(\\theta \\mid y) = -\\frac{n}{2}\\text{ln}(2\\pi) -\\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n \\left(y_i - \\mu_i \\right)^2}{2\\sigma^2}.\n\\]\nMaximising this expression with respect to \\(\\theta\\), the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the \\(n\\)-th vector of the outcome values \\(Y\\) and the \\(n\\times (J+1)\\) matrix of the covariate values (including the constant term), then the MLEs are:\n\\[\n\\hat{\\beta}=(X^{T}X)^{-1}X^{T}Y \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}^{2}=\\frac{(Y-X\\hat{\\beta})(Y-X\\hat{\\beta})}{n},\n\\]\nwhere the numerator of the fraction is known as the Residual Sum of Squares(RSS). Because the denominator of is equal to \\(n\\), the MLE of \\(\\sigma^2\\) does not correct for the loss of degrees of freedom when estimating the \\(J+1\\) location parameters. Thus, the MLE should instead divide the RSS by \\(n-(J+1)\\) to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called weighted multiple linear regression, in which the regression variance is assumed to be equal to\\(\\frac{\\sigma^2}{w_i}\\), for \\((w_i) &gt; 0\\). Thus, the variable \\((y_i-\\mu)\\sqrt{w_i}\\) is Normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\), and the loglikelihood is\n\\[\nl(\\theta \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n w_i(y_i - \\mu_i)^2}{2\\sigma^2}.\n\\]\nMaximising this function yields MLEs given by the weighted least squares estimates\n\\[\n\\hat{\\beta}=\\left(X^{T}WX\\right)\\^{-1}\\left(X^{T}WY \\right) \\;\\;\\; \\text{and} \\;\\;\\; \\sigma^{2}=\\frac{\\left(Y-X\\hat{\\beta}\\right)^{T}W\\left(Y-X\\hat{\\beta}\\right)}{n},\n\\]\nwhere \\(W=\\text{Diag}(w_1,\\ldots,w_n)\\)."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#generalised-linear-models",
    "href": "missing_data/intro_mle/intro_mle.html#generalised-linear-models",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Generalised Linear Models",
    "text": "Generalised Linear Models\nConsider the previous example where we had an outcome variable \\(y\\) and a set of \\(J\\) covariates, each measured on \\(n\\) units. A more general class of models, compare with the Normal model, assumes that, given \\(x\\), the values of \\(y\\) are an independent sample from a regular exponential family distribution\n\\[\nf(y \\mid x,\\beta,\\phi)=\\text{exp}\\left(\\frac{\\left(y\\delta\\left(x,\\beta \\right) - b\\left(\\delta\\left(x,\\beta\\right)\\right)\\right)}{\\phi} + c\\left(y,\\phi\\right)\\right),\n\\]\nwhere \\(\\delta()\\) and \\(b()\\) are known functions that determine the distribution of \\(y\\), and \\(c()\\) is a known function indexed by a scale parameter \\(\\phi\\). The mean of \\(y\\) is assumed to linearly relate to the covariates via\n\\[\nE\\left[y \\mid x,\\beta,\\phi \\right]=g^{-1}\\left(\\beta_0 + \\sum_{j=1}^J\\beta_jx_{j} \\right),\n\\]\nwhere \\(E\\left[y \\mid x,\\beta,\\phi \\right]=\\mu_i\\) and \\(g()\\) is a known one to one function which is called link function because it “links” the expectation of \\(y\\) to a linear combination of the covariates. The canonical link function\n\\[\ng_c(\\mu_i)=\\delta(x_{i},\\beta)=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij},\n\\]\nwhich is obtained by setting \\(g()\\) equal to the inverse of the derivative of \\(b()\\) with respect to its argument. Examples of canonical links include\n\nNormal linear regression: \\(g_c=\\text{identity}\\), \\(b(\\delta)=\\frac{\\delta^2}{2},\\phi=\\sigma^2\\)\nPoisson regression: \\(g_c=\\log\\), \\(b(\\delta)=\\text{exp}(\\delta),\\phi=1\\)\nLogistic regression: \\(g_c=\\text{logit}\\), \\(b(\\delta)=\\log(1+\\text{exp}(\\delta)),\\phi=1\\)\n\nThe loglikelihood of \\(\\theta=(\\beta,\\phi)\\) given the observed data \\((y,x)\\), is\n\\[\nl(\\theta \\mid y,x)=\\sum_{i=1}^n \\left[\\frac{\\left(y_i\\delta\\left(x_i,\\beta\\right)-b\\left(\\delta\\left(x_i,\\beta\\right)\\right) \\right)}{\\phi}+c\\left(y_i,\\phi\\right)\\right],\n\\]\nwhich for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms."
  },
  {
    "objectID": "missing_data/esi/esi.html",
    "href": "missing_data/esi/esi.html",
    "title": "Explicit Single Imputation",
    "section": "",
    "text": "All case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nIn this part, we focus on some of the most popular Explicit Single Imputation methods. These include: Mean Imputation(SI-M), where means from the observed data are used as imputed values; Regression Imputation(SI-R), where missing values are replaced with values predicited from a regression of the missing variable on some other observed variables; and Stochastic Regression Imputation(SI-SR), where unobserved data are substituted with the predicted values from a regression imputation plus a randomly selected residual drawn to reflect uncertainty in the predicted values."
  },
  {
    "objectID": "missing_data/esi/esi.html#mean-imputation",
    "href": "missing_data/esi/esi.html#mean-imputation",
    "title": "Explicit Single Imputation",
    "section": "Mean Imputation",
    "text": "Mean Imputation\nThe simplest type of SI-M consists in replacing the missing values in a variable with the mean of the observed units from the same variable, a method known as Unconditional Mean Imputation (Little and Rubin (2019),Schafer and Graham (2002)). Let \\(y_{ij}\\) be the value of variable \\(j\\) for unit \\(i\\), such that the unconditional mean based on the observed values of \\(y_j\\) is given by \\(\\bar{y}_j\\). The sample mean of the observed and imputed values is then \\(\\bar{y}^{m}_j=\\bar{y}^{ac}_j\\), i.e. the estimate from ACA, while the sample variance is given by\n\\[\ns^{m}_{j}=s^{ac}_{j}\\frac{(n^{ac}-1)}{(n-1)},\n\\]\nwhere \\(s^{ac}_j\\) is the sample variance estimated from the \\(n^{ac}\\) available units. Under a Missing Completely At Random(MCAR) assumption, \\(s^{ac}_j\\) is a consistent estimator of the tru variance so that the sample variance from the imputed data \\(s^m_j\\) systematically underestimates the true variance by a factor of \\(\\frac{(n^{ac}-1)}{(n-1)}\\), which clearly comes from the fact that missing data are imputed using values at the centre of the distribution. The imputation distorts the empirical distribution of the observed values as well as any quantities that are not linear in the data (e.g. variances, percentiles, measures of shape). The sampel covariance of \\(y_j\\) and \\(y_k\\) from the imputed data is\n\\[\ns^{m}_{jk}=s^{ac}_{jk}\\frac{(n^{as}_{jk}-1)}{(n-1)},\n\\]\nwhere \\(n^{ac}_{jk}\\) is the number of units with both variables observed and \\(s^{ac}_{jk}\\) is the corresponding covariance estimate from ACA. Under MCAR \\(s^{ac}_{jk}\\) is a consistent estimator of the true covariance, so that \\(s^{m}_{jk}\\) underestimates the magnitude of the covariance by a factor of \\(\\frac{(n^{ac}_{jk}-1)}{(n-1)}\\). Obvious adjustments for the variance (\\(\\frac{(n-1)}{(n^{ac}_j-1)}\\)) and the covariance (\\(\\frac{(n-1)}{(n^{ac}_{jk}-1)}\\)) yield ACA estimates, which could lead to covariance matrices that are not positive definite."
  },
  {
    "objectID": "missing_data/esi/esi.html#regression-imputation",
    "href": "missing_data/esi/esi.html#regression-imputation",
    "title": "Explicit Single Imputation",
    "section": "Regression Imputation",
    "text": "Regression Imputation\nAn improvement over SI-M is to impute each missing data using the conditional means given the observed values, a method known SI-R or Conditional Mean Imputation. To be precise, it would also be possible to impute conditional means without using a regression approach, for example by grouping individuals into adjustment classes (analogous to weighting methods) based on the observed data and then impute the missing values using the observed means in each adjustment class (Little and Rubin (2019)). However, for the sake of simplicity, here we will assume that SI-R and conditional mean imputation are the same.\nTo generate imputations under SI-R, consider a set of \\(J-1\\) fully observed response variables \\(y_1,\\ldots,y_{J-1}\\) and a partially observed response variable \\(y_J\\) which has the first \\(n_{cc}\\) units observed and the remaiing \\(n-n_{cc}\\) units missing. SI-R computes the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) complete units and then fills in the missing values as predictions from the regression. For example, for unit \\(i\\), the missing value \\(y_{iJ}\\) is imputed using\n\\[\n\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij},\n\\]\nwhere \\(\\hat{\\beta}_{J0}\\) is the intercept and \\(\\hat{\\beta}_{Jj}\\) is the \\(j\\) coefficient of of the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) units.\nAn extension of regression imputation to a general pattern of missing data is known as Buck’s method (Buck (1960)). This approach first estimates the population mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\) from the sample mean and covariance matrix of the complete units and then uses these estimates to calculate the OLS regressions of the missing variables on the observed variables for each missing data pattern. Predictions of the missing data for each observation are obtained by replacing the values of the present variables in the regressions. The average of the observed and imputed values from this method are consistent estimates of the means and MCAR and mild assumptions about the moments of the distribution (Buck (1960)). They are also consistent when the missingness mechanism depends on observed variables, i.e. under a Missing At Random(MAR) assumption, although addtional assumptions are required in this case (e.g. using linear regressions it assumes that the “true” regression of the missing varables on the observed variables is linear).\nThe filled in data from Buck’s method typically yield reasonable estimates of means, while the sample variances and covariances are biased, although the bias is less than the one associated with unconditional mean imputation. Specifically, the sample variance \\(\\sigma^{2,SI-R}_j\\) from the imputed data underestimates the true variance \\(\\sigma^2_j\\) by a factor of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma^{2}_{ji}\\), where \\(\\sigma^{2}_{ji}\\) is the residual variance from regressing \\(y_j\\) on the variables observed in unit \\(i\\) if \\(y_{ij}\\) is missing and zero if \\(y_{ij}\\) is observed. The sample covariance of \\(y_j\\) and \\(y_k\\) has a bias of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma_{jki}\\), where \\(\\sigma_{jki}\\) is the residual covariance from the multivariate regression of \\((y_{ij},y_{ik})\\) on the variables observed in unit \\(i\\) if both variables are missing and zero otherwise. A consistent estimator of \\(\\Sigma\\) can be constructed under MCAR by replacing consistent estimates of \\(\\sigma^{2}_{ji}\\) and \\(\\sigma_{jki}\\) in the expressions for bias and then adding the resulting quantities to the sample covariance matrix of the filled-in data."
  },
  {
    "objectID": "missing_data/esi/esi.html#stochastic-regression-imputation",
    "href": "missing_data/esi/esi.html#stochastic-regression-imputation",
    "title": "Explicit Single Imputation",
    "section": "Stochastic Regression Imputation",
    "text": "Stochastic Regression Imputation\nAny type of mean or regression imputation will lead to bias when the interest is in the tails of the distributions because “best prediction” imputation systematically underestimates variability and standard errors calculated from the imputed data are typically too small. These considerations suggest an alternative imputation strategy, where imputed values are drawn from a predictive distribution of a plausible set of values rather than from the centre of the distribution. This is the idea behind SI-SR, which imputes a conditional draw\n\\[\n\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij}+z_{iJ},\n\\]\nwhere \\(z_{iJ}\\) is a random normal deviate with mean 0 and variance \\(\\hat{\\sigma}^2_J\\), the residual variance from the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the complete units. The addition of the random deviate makes the imputation a random draw from the predictive distribution of the missing values, rather than the mean, which is likely to ameliorate the distortion of the predictive distributions (Little and Rubin (2019)).\n\nExample\nConsider a bivariate normal monotone missing data with \\(y_1\\) fully observed and \\(y_2\\) missing for a fraction \\(\\lambda=\\frac{(n-n_{cc})}{n}\\) and a MCAR mechanism. The following table shows the large sample bias of standard OLS estimates obtained from the filled-in data about the mean, the variance of \\(y_2\\), the regression coefficient of \\(y_2\\) on \\(y_1\\), and the regression coefficient of \\(y_1\\) on \\(y_2\\), using four different single imputation methods: uncondtional mean (UM), unconditional draw (UD), conditional mean (CM), and conditional draw (CD).\n\nBivariate normal monotone MCAR data; large sample bias of four imputation methods.\n\n\n\nmu_2\nsigma_2\nbeta_21\nbeta_12\n\n\n\n\nUM\n0\n-lambda * sigma_2\n-lambda * beta_21\n0\n\n\nUD\n0\n0\n-lambda * beta_21\n-lambda * beta_21\n\n\nCM\n0\n-lambda * (1-rho^2) * sigma_2\n0\n((lambda * (1-rho^2)) / (1-lambda * (1-rho^2)) ) * beta_12\n\n\nCD\n0\n0\n0\n0\n\n\n\n\n\nUnder MCAR, all four methods yield consistent estimates of \\(\\mu_2\\) but both UM and CM underestimate the variance \\(\\sigma_2\\), UD leads to attenuation of the regression coefficients, while CD yields consistent estimates of all four parameters. However, CD has some important drawbacks. First, adding random draws to the conditional mean imputations is inefficient as the large sample variance of the CD estimates of \\(\\mu_2\\) can be shown (Little and Rubin (2019)) to be\n\\[\n\\frac{[1-\\lambda\\rho^2+(1-\\rho^2)\\lambda(1-\\lambda)]\\sigma_2}{n_{cc}},\n\\]\nwhich is larger than the large sample sampling variance of the CM estimate of \\(\\mu_2\\), namely \\(\\frac{[1-\\lambda\\rho^2]\\sigma_2}{n_{cc}}\\). Second, the standard errors of the CD estimates from the imputed data are too small because they do not incorporate imputation uncertainty.\nWhen the analysis involves units with some covariates missing and other observed, it is common practice to condition on the observed covariates when generating the imputations for the missing covariates. It is also possible to condition on the outcome \\(y\\) to impute missing covariates, even if the final objective is to regress \\(y\\) on the full set of covariates and conditioning on \\(y\\) will lead to bias when conditional means are imputed. However, if predictive draws are imputed, this approach will yield consistent estimates of the regression coefficients. Imputing missing covariates using the means by conditioning only the observed covariates (and not also on \\(y\\)) also yields consistent estimates of the regression coefficients under certain conditions, although these are typically less efficient then those from CCA, but yields inconsistent estimates of other parameters such as variances and correlations (Little (1992))."
  },
  {
    "objectID": "missing_data/esi/esi.html#conclusions",
    "href": "missing_data/esi/esi.html#conclusions",
    "title": "Explicit Single Imputation",
    "section": "Conclusions",
    "text": "Conclusions\nAccording to Little and Rubin (2019), imputation should generally be\n\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\nMultivariate, to preserve association between missing variables.\nDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\nNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow."
  },
  {
    "objectID": "missing_data/cca/cca.html",
    "href": "missing_data/cca/cca.html",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Complete case analysis (CCA), also known as case or listwise deletion (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by \\(i=1,\\ldots,n\\) individuals and that we want to fit a linear regression on some outcome variable \\(y_i\\) using some other variables \\(x_{i1},\\ldots,x_{ik}\\) as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (completers).\nCCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:\n\nBias, when the missing data mechanism is not missing completely at random (MCAR) and the completers are not a random samples of all the cases\nLoss of efficiency, due to the potential loss of information in discarding the incomplete cases.\n\nCCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.\nLet \\(\\hat{\\theta}_{cc}\\) be an estimate of a parameter of interest from the completers. One might measure the increase in variance of \\(\\hat{\\theta}_{cc}\\) with respect to the estimate \\(\\hat{\\theta}\\) that would be obtained in the absence of missing values. Using the notation of Little and Rubin (2019):\n\\[\n\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta})(1 + \\Delta^{\\star}_{cc}),\n\\]\nwhere \\(\\Delta^{\\star}_{cc}\\) is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is \\(\\Delta_{cc}\\), where\n\\[\n\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta}_{eff})(1 + \\Delta_{cc}),\n\\]\nand \\(\\hat{\\theta}_{eff}\\) is an efficient estimate based on all the available data.\n\n\nConsider bivariate normal monotone data \\(\\bf y = (y_1,y_2)\\), where \\(n_{cc}\\) out of \\(n\\) cases are complete and \\(n - n_{cc}\\) cases have observed values only on \\(y_1\\). Assume for simplicity that the missingness mechanism is MCAR and that the mean of \\(y_j\\) is estimated by the empirical mean from the complete cases \\(\\bar{y}^{cc}_j\\). Then, the loss in sample size for estimating the mean of \\(y_1\\) is:\n\\[\n\\Delta_{cc}(\\bar{y}_1) = \\frac{n - n_{cc}}{n_{cc}},\n\\]\nso that if half the cases are missing, the variance is doubled. For the mean of \\(y_2\\), the loss of information alos depends on the squared correlation \\(\\rho^{2}\\) between the variables: (Little and Rubin (2019))\n\\[\n\\Delta_{cc}(\\bar{y}_2) \\approx \\frac{(n - n_{cc})\\rho^{2}}{n_{cc}(1 - \\rho^{2}) + n_{cc}\\rho^{2}}.\n\\]\n\\(\\Delta_{cc}(\\bar{y}_2)\\) varies from zero (when \\(\\rho=0\\)) to \\(\\Delta_{cc}(\\bar{y}_1)\\) as \\(\\rho^{2} \\rightarrow 1\\). However, for the regression coefficients of \\(y_2\\) on \\(y_1\\) we have that \\(\\Delta_{cc}=0\\) since the incomplete observations of \\(y_1\\) provide no information for estimating the parameters of the regression of \\(y_2\\) on \\(y_1\\).\n\n\n\nFor inference about the population mean \\(\\mu\\), the bias of CCA depends on the proportion of the completers \\(\\pi_{cc}\\) and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable \\(y\\) is partially-observed and that we partition the data into the subset of the completers \\(y_{cc}\\) and incompleters \\(y_{ic}\\), with associated population means \\(\\mu_{cc}\\) and \\(\\mu_{ic}\\), respectively. The overall mean can be written as a weighted average of the means of the two subsets\n\\[\n\\mu = \\pi_{cc}\\mu_{cc} + (1 - \\pi_{cc})\\mu_{ic}.\n\\]\nThe bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases\n\\[\n\\mu_{cc} - \\mu = (1 - \\pi_{cc})(\\mu_{cc} - \\mu_{ic}).  \n\\]\nUnder MCAR, we have that \\(\\mu_{cc} = \\mu_{ic}\\) and therefore the bias is zero.\n\n\n\nConsider the estimation of the regression of \\(y\\) on \\(x_1,\\ldots,x_K\\) from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients \\(\\beta_1,\\ldots,\\beta_K\\) associated with the covariates is null if the probbaility of being a completer depends on the \\(x\\)s but not \\(y\\), since the analysis conditions on the values of the covariates (Glynn and Laird (1986), White and Carlin (2010)). This class of missing data mechanisms includes missing not at random (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on \\(y\\) after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor Bartlett using some nice slides\n\n\n\nThe main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (Schafer and Graham (2002)). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable \\(y\\) based on complete cases with the distribution of \\(y\\) based on incomplete cases for which \\(y\\) is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the missing at random (MAR) assumption."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-1",
    "href": "missing_data/cca/cca.html#example-1",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Consider bivariate normal monotone data \\(\\bf y = (y_1,y_2)\\), where \\(n_{cc}\\) out of \\(n\\) cases are complete and \\(n - n_{cc}\\) cases have observed values only on \\(y_1\\). Assume for simplicity that the missingness mechanism is MCAR and that the mean of \\(y_j\\) is estimated by the empirical mean from the complete cases \\(\\bar{y}^{cc}_j\\). Then, the loss in sample size for estimating the mean of \\(y_1\\) is:\n\\[\n\\Delta_{cc}(\\bar{y}_1) = \\frac{n - n_{cc}}{n_{cc}},\n\\]\nso that if half the cases are missing, the variance is doubled. For the mean of \\(y_2\\), the loss of information alos depends on the squared correlation \\(\\rho^{2}\\) between the variables: (Little and Rubin (2019))\n\\[\n\\Delta_{cc}(\\bar{y}_2) \\approx \\frac{(n - n_{cc})\\rho^{2}}{n_{cc}(1 - \\rho^{2}) + n_{cc}\\rho^{2}}.\n\\]\n\\(\\Delta_{cc}(\\bar{y}_2)\\) varies from zero (when \\(\\rho=0\\)) to \\(\\Delta_{cc}(\\bar{y}_1)\\) as \\(\\rho^{2} \\rightarrow 1\\). However, for the regression coefficients of \\(y_2\\) on \\(y_1\\) we have that \\(\\Delta_{cc}=0\\) since the incomplete observations of \\(y_1\\) provide no information for estimating the parameters of the regression of \\(y_2\\) on \\(y_1\\)."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-2",
    "href": "missing_data/cca/cca.html#example-2",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "For inference about the population mean \\(\\mu\\), the bias of CCA depends on the proportion of the completers \\(\\pi_{cc}\\) and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable \\(y\\) is partially-observed and that we partition the data into the subset of the completers \\(y_{cc}\\) and incompleters \\(y_{ic}\\), with associated population means \\(\\mu_{cc}\\) and \\(\\mu_{ic}\\), respectively. The overall mean can be written as a weighted average of the means of the two subsets\n\\[\n\\mu = \\pi_{cc}\\mu_{cc} + (1 - \\pi_{cc})\\mu_{ic}.\n\\]\nThe bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases\n\\[\n\\mu_{cc} - \\mu = (1 - \\pi_{cc})(\\mu_{cc} - \\mu_{ic}).  \n\\]\nUnder MCAR, we have that \\(\\mu_{cc} = \\mu_{ic}\\) and therefore the bias is zero."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-3",
    "href": "missing_data/cca/cca.html#example-3",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Consider the estimation of the regression of \\(y\\) on \\(x_1,\\ldots,x_K\\) from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients \\(\\beta_1,\\ldots,\\beta_K\\) associated with the covariates is null if the probbaility of being a completer depends on the \\(x\\)s but not \\(y\\), since the analysis conditions on the values of the covariates (Glynn and Laird (1986), White and Carlin (2010)). This class of missing data mechanisms includes missing not at random (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on \\(y\\) after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor Bartlett using some nice slides"
  },
  {
    "objectID": "missing_data/cca/cca.html#conclusions",
    "href": "missing_data/cca/cca.html#conclusions",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "The main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (Schafer and Graham (2002)). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable \\(y\\) based on complete cases with the distribution of \\(y\\) based on incomplete cases for which \\(y\\) is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the missing at random (MAR) assumption."
  },
  {
    "objectID": "missing_data/aipw/aipw.html",
    "href": "missing_data/aipw/aipw.html",
    "title": "Augmented Inverse Probability Weighting",
    "section": "",
    "text": "A general problem associated with the implementatio of Inverse Probability Weighting (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (Schafer and Graham (2002)). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called Augmented Inverse Probability Weighting (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recover the information in the incomplete units and applying IPW to the residuals from the model (Little and Rubin (2019)).\nConsidering the IPW Generalised Estimating Equation (GEE)\n\\[\n\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator \\(m_i\\) on the vectors of the covariate and auxiliary variables \\(x_i\\) and \\(z_i\\), respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of \\(\\beta\\) which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of \\(\\beta\\) based on two terms:\nThe basis for the first term is a complete data unbiased estimating function for \\(\\beta\\), whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (Molenberghs et al. (2014))."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#doubly-robust-estimators",
    "href": "missing_data/aipw/aipw.html#doubly-robust-estimators",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Doubly Robust Estimators",
    "text": "Doubly Robust Estimators\nAn important class of AIPW methods is known as doubly robust estimators, which have desirable robustness properties (Robins, Rotnitzky, and Laan (2000),Robins and Rotnitzky (2001)). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for \\(y_i \\mid x_i\\). For example, doubly robust estimators for a population mean parameter \\(\\mu\\) could be obtained as follows:\n\nFit a logistic regression model for the probability of observing \\(y_i\\) as a function of \\(x_i\\) and \\(z_i\\) to derive the individual weights \\(w_i(\\hat{\\alpha})\\).\nFit a generalized linear model for the outcome of responders in function of \\(x_i\\) using weights \\(w_i(\\hat{\\alpha})\\) and let \\(g^\\star(x_i,\\beta)\\) denote the fitted values for subject \\(i\\).\nTake the sample average of the fitted values \\(g^\\star(x_i,\\beta)\\) of both respondents and nonrespondents as an estimate of the population mean \\(\\hat{\\mu}\\)\n\nDoubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term \\(g^\\star(x_i,\\beta)\\) is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of \\(\\beta\\) is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of \\(\\beta\\) (Scharfstein, Daniels, and Robins (2003),Bang and Robins (2005)). Doubly robust estimators therefore allow to obtain an unbiased estimating function for \\(\\beta\\) if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#example",
    "href": "missing_data/aipw/aipw.html#example",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Example",
    "text": "Example\nSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\np(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\n\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, consider the consistent IPW complete case estimating equation\n\\[\n\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu)=0,\n\\]\nwhich can be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i \\mid \\hat{\\alpha})\\), typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for \\(\\mu\\) within this class is the solution to the estimating equation\n\\[\n\\sum_{i=1}^n \\left(\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu) - \\frac{c_i-\\pi(z_i \\mid \\hat{\\alpha})}{\\pi(z_i \\\\mid \\hat{\\alpha})}\\text{E}[(y_i-\\mu)\\mid z_i] \\right),\n\\]\nwhich leads to the estimator\n\\[\n\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} -  \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} \\text{E}[y_i \\mid z_i] \\right).\n\\]\nThe conditional expectation \\(\\text{E}[y_i \\mid z_i]\\) is not known and must be estimated from the data. Under a Missing At Random (MAR) assumption we have that \\(\\text{E}[y \\mid z]=\\text{E}[y \\mid z, c=1]\\), that is the conditional expecation of \\(y\\) given \\(z\\) is the same as that among the completers. Thus, we can specify a model \\(m(z,\\xi)\\) for \\(\\text{E}[y \\mid z]\\), indexed by the parameter \\(\\xi\\), that can be estimated from the completers. If \\(y\\) is continuous, a simple choice is to estimate \\(\\hat{\\xi}\\) by OLS from the completers. The AIPW estimator for \\(\\mu\\) then becomes\n\\[\n\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} -  \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} m(z_i\\mid \\hat{\\xi}) \\right).\n\\]\nIt can be shown that this estimator is more efficient that the simple IPW complete case estimator for \\(\\mu\\) and that it has a double robustness property. This ensures that \\(\\mu_{aipw}\\) is a consitent estimator of \\(\\mu\\) if either\n\nthe model \\(\\pi(z\\mid\\alpha)\\) is correctly specified, or\nthe model \\(m(z\\mid \\xi)\\) is correctly specified.\n\nTo see a derivation of the double robustness property I put here a link to some nice paper."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#conlcusions",
    "href": "missing_data/aipw/aipw.html#conlcusions",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Conlcusions",
    "text": "Conlcusions\nAs all weighting methods, such as IPW, AIPW methods are semiparametric methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than Maximum Likelihood or Bayesian estimators under a well specified parametric model. With missing data, Rubin (1976) results show that likelihood-based methods perform uniformly well over any Missing At Random (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (Meng (2000))."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses.\n\n\n\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses."
  },
  {
    "objectID": "index.html#research-and-work",
    "href": "index.html#research-and-work",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "missing_data/aca/aca.html",
    "href": "missing_data/aca/aca.html",
    "title": "Available Case Analysis",
    "section": "",
    "text": "Complete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as Available Case Analysis (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.\nThe main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not missing completely at random (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as pairwise deletion or pairwise inclusion"
  },
  {
    "objectID": "missing_data/aca/aca.html#pairwise-measures-of-covariation",
    "href": "missing_data/aca/aca.html#pairwise-measures-of-covariation",
    "title": "Available Case Analysis",
    "section": "Pairwise measures of covariation",
    "text": "Pairwise measures of covariation\nOne possible approach to estimate pairwise measures of covariation for \\(y_j\\) and \\(y_k\\) is to use only those units \\(i=1,\\ldots,n_{ac}\\) for which both variables are observed (Little and Rubin (2019)). For example, one can compute pairwise sample covariances as:\n\\[\ns^{ac}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j}^{ac})(y_{ik}-\\bar{y}_{k}^{ac})}{(n_{ac}-1)},\n\\]\nwhere \\(I_{ac}\\) is the set of \\(n_{ac}\\) with both \\(y_j\\) and \\(y_k\\) observed, while the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\) are calculated over this set of units. We can also estimate the sample correlation\n\\[\nr^{\\star}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^2_{j}s^{2}_{k}}},\n\\]\nwhere \\(s^2_{j}\\) and \\(s^2_{k}\\) are the sample variances computed over the sets of observed units \\(I_{j}\\) and \\(I_{k}\\), respectively. A problem of this type of correlation estimate is that it can lie outside the range \\((-1,1)\\), which is typically addressed by computing pairwise correlations (Wilks (1932)), where variances are estimated from the set of units with both variables observed \\(I_{jk}\\), i.e. \n\\[\nr^{ac}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.\n\\]\nIn addition, we could also replace the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\), evaluated on the common set of units \\(I_{jk}\\), with \\(\\bar{y}_{j}\\) and \\(\\bar{y}_{k}\\), which are evaluated on the sets of units \\(I_{j}\\) and \\(I_{k}\\), respectively. This leads to the following estimates for the sample covariances (Matthai (1951)):\n\\[\ns^{\\star}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j})(y_{ik}-\\bar{y}_{k})}{(n_{ac}-1)},\n\\]\nPairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (Schafer and Graham (2002))."
  },
  {
    "objectID": "missing_data/aca/aca.html#conclusions",
    "href": "missing_data/aca/aca.html#conclusions",
    "title": "Available Case Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nAC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (Kim and Curry (1977)). However, when correlations are more substantial, ACA may become even less efficient than CCA (Haitovsky (1968), Azen and Van Guilder (1981))."
  },
  {
    "objectID": "missing_data/bis/bis.html",
    "href": "missing_data/bis/bis.html",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "",
    "text": "A useful alternative approach to Maximum Likelihood(ML) methods, particularly when the sample size is small, is to include a reasonable prior distribution for the parameters and compute the posterior distribution of the parameters of interest. The posterior distribution for a model with ignorable missingness is\n\\[\np(\\theta \\mid Y_0, M) \\equiv p(\\theta \\mid Y_0) \\propto p(\\theta)f(Y_0 \\mid \\theta),\n\\]\nwhere \\(p(\\theta)\\) is the prior and \\(f(Y_0 \\mid \\theta)\\) is the density of the observed data \\(Y_0\\). Simulation from the posterior without iteration can be accomplished if the likelihood can be factored into complete data components, while for general patterns of missing data, Bayesian simulation requires iteration."
  },
  {
    "objectID": "missing_data/bis/bis.html#data-augmentation",
    "href": "missing_data/bis/bis.html#data-augmentation",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData Augmentation(Tanner and Wong (1987)), or DA, is an iterative method of simulating the posteiror distribution of \\(\\theta\\) that combines features of the Expecation Maximisation(EM) algorithm and Multiple Imputation(MI). Starting with an initial draw \\(\\theta_0\\) from an approximation to the posterior, then given the value \\(\\theta_t\\) at iteration \\(t\\):\n\nDraw \\(Y_{1,t+1}\\) with density \\(p(Y_1 \\mid Y_0, \\theta_t)\\) (I step).\nDraw \\(\\theta_{t+1}\\) with density \\(p(\\theta \\mid Y_0, Y_{1,t+1})\\) (P step).\n\nThe procedure is motivated by the fact that the distributions in these two steps are often much easier to draw from than either of the posteriors \\(p(Y_1 \\mid Y_0)\\) and \\(p(\\theta \\mid Y_0)\\), or the joint posterior \\(p(\\theta, Y_1 \\mid Y_0)\\). The procedure can be shown to eventually yield a draw from the joint posterior of \\(Y_1\\) and \\(\\theta\\) given \\(Y_0\\), in the sense that as \\(t\\) tends to infinity this sequence converges to a draw from the joint distribution.\n\nBivariate Normal Data Example\nSuppose having a sample \\(y_i=(y_{1i},y_{2i})\\) from a Bivariate Normal distribution for \\(i=1,\\ldots,n\\) units, with mean vector \\(\\mu=(\\mu_1,\\mu_2)\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). Assume that one group of units has \\(Y_1\\) observed and \\(Y_2\\) missing, while a second group of units has both variables observed and a third group of units has \\(Y_1\\) missing and \\(Y_2\\) observed. Under DA methods, each iteration \\(t\\) consists of an I step and a P step. In the first, missing data are replaced with draws from its conditional distribution given the observed data and current values of the parameters (rather then its conditional mean as in the EM algorithm). Because units are conditionally independent given the parameters, each missing \\(y_{2i}\\) is drawn independently as\n\\[\ny_{2i,t+1} \\sim N\\left(\\beta_{20t} + \\beta_{21t}y_{1i}, \\sigma^2_{2t}  \\right),\n\\]\nwhere \\(\\beta_{20t},\\beta_{21t}\\) and \\(\\sigma^2_{2t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_2\\) on \\(Y_1\\). Analogously, each missing \\(y_{1i}\\) is drawn independently as\n\\[\ny_{1i,t+1} \\sim N\\left(\\beta_{10t} + \\beta_{11t}y_{2i}, \\sigma^2_{1t}  \\right),\n\\]\nwhere \\(\\beta_{10t},\\beta_{11t}\\) and \\(\\sigma^2_{1t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_1\\) on \\(Y_2\\). In the second step, these drawn values are treated as if they were the observed values and one draw of the bivariate Normal parameters is made from the complete data posterior. In the limit, the draws are from the joint posterior of the missing values and the parameters. Thus, a run of DA generates both a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\), and the procedure can be run \\(D\\) times to obtain \\(D\\) iid draws from the joint posterior of \\(\\theta\\) and \\(Y_1\\). Unlike the EM, estimates of the sampling covariance matrix from the filled-in data can be computed without any corrections to the estimated variances because draws from the posterior predictive distribution of the missing values are imputed in the I step of DA, rather than the conditional means as in the E step of EM. The loss of efficiency from imputing draws is limited when the posterior mean from DA is computed over many draws from the posterior."
  },
  {
    "objectID": "missing_data/bis/bis.html#the-gibbs-sampler",
    "href": "missing_data/bis/bis.html#the-gibbs-sampler",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "The Gibbs’ Sampler",
    "text": "The Gibbs’ Sampler\nThe Gibbs’s sampler is an iterative simulation method that is designed to yield draws from the joint posterior distribution in the case of a general pattern of missingness and provides a Bayesian analogous to the Expectation Conditonal Maximisation (ECM) algorithm for ML estimation. The Gibbs’ sampler eventually generates a draw from the distribution \\(p(x_1,\\ldots,x_J)\\) of a set of \\(J\\) random variables \\(X_1,\\ldots,X_J\\) in settings where draws from the joint distribution are hard to compute but draws from the conditional distributions \\(p(x_j \\mid x_1,\\ldots,x_{j-1},x_{j+1},\\ldots, x_J)\\) are relatively easy to compute. Initial values \\(x_{10},\\ldots,x_{J0}\\) are chosen in some way and then, given current values of \\(x_{1t},\\ldots,x_{Jt}\\) at iteration \\(t\\), new values are found by drawing from the following sequence of conditional distributions:\n\\[\nx_{1t+1} \\sim p\\left(x_1 \\mid x_{2t},\\ldots,x_{Jt} \\right),\n\\]\n\\[\nx_{2t+1} \\sim p\\left(x_2 \\mid x_{1t+1},\\ldots,x_{Jt} \\right),\n\\]\nup to\n\\[\nx_{Jt+1} \\sim p\\left(x_J \\mid x_{2t+1},\\ldots,x_{J-1t+1} \\right).\n\\]\nIt can be shown that, under general conditions, the sequence of \\(J\\) iterates converges to a draw from the joint posterior of the variables. When \\(J=2\\), the Gibbs’ sampler is the same as DA if \\(x_1=Y_1\\) and \\(x_2=\\theta\\) and the distributions condition on \\(Y_0\\). We can then obtain a draw from the joint posterior of \\(Y_1,\\theta \\mid Y_0\\) by applying the Gibbs’ sampler, where at iteration \\(t\\) for the \\(d\\)-th imputed data set:\n\\[\nY^d_{1t+1} \\sim p\\left(Y_1 \\mid Y_0, \\theta^d_{t}\\right) \\;\\;\\; \\text{and} \\;\\;\\; \\theta^d_{t+1} \\sim p\\left(\\theta \\mid Y^d_{1t+1}, Y_0\\right),\n\\]\nsuch that one run of the sampler converges to a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\). The sampler can be run independently \\(D\\) times to generate \\(D\\) iid draws from the approximate joint posterior of \\(\\theta\\) and \\(Y_1\\). The values of \\(Y_1\\) are multiple imputations of the missing values, drawn from their posterior predictive distribution."
  },
  {
    "objectID": "missing_data/bis/bis.html#assessing-convergence",
    "href": "missing_data/bis/bis.html#assessing-convergence",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\nAssessing convergence of the sequence of draws to the target distribution is more difficult than assessing convergence of an EM-type algorithm because there is no single target quantity to monitor like the maximum value of the likelihood. Methods have been proposed to assess convergence of a single sequence (Geyer (1992)), but a more reliable approach is to simulate \\(D&gt;1\\) sequences with starting values dispersed throughout the parameter space, and the convergence of all quantities of interest can then be monitored by comparing variation between and within simulated sequences, until the “within” variation roughly equals the “between” variation. The idea is that when the distribution of each simulated sequence is close enough to the distribution of all the sequences mixed together, they can all be approximating the target distribution. Gelman and Rubin (1992) developed an explicit monitoring statistic based on the following idea. For each scalar estimand \\(\\psi\\), label the draws from \\(D\\) parallel sequences as \\(\\psi^d_{t}\\), for \\(t=1,\\ldots,T\\) iterations and \\(d=1,\\ldots,D\\) sequences, and compute the between \\(B\\) and within \\(\\bar{V}\\) sequence variances as:\n\\[\nB=\\frac{T}{D-1}\\sum_{d=1}^D(\\bar{\\psi}_{d.} - \\bar{\\psi}_{..})^2, \\;\\;\\; \\text{and} \\;\\;\\; \\bar{V}=\\frac{1}{D}\\sum_{d=1}^D s^2_{d},\n\\]\nwhere \\(\\bar{\\psi}_{d.}=\\frac{1}{T}\\sum_{t=1}^T \\psi_{dt}\\), \\(\\bar{\\psi}_{..}=\\frac{1}{D}\\sum_{d=1}^D \\bar{\\psi}_{d}\\), and \\(s^2_{d}=\\frac{1}{T-1}\\sum_{t=1}^T(\\psi_{dt} - \\bar{\\psi}_{d.})^2\\). We can then estimate the marginal posterior variance of the estimand as\n\\[\n\\widehat{Var}(\\psi \\mid Y_0) = \\frac{T-1}{T}\\hat{V} + \\frac{1}{T} B,\n\\]\nwhich will overestimate the marginal posterior variance assuming the starting distribution is appropriately over-dispersed but is unbiased under stationarity (starting distribution equals the target distribution). For any finte \\(T\\), the within variance \\(\\hat{V}\\) will underestimate the marginal variance because individual sequences have not had time to range over all the target distribution and should have smaller variance then B. In the limit as \\(T \\rightarrow \\infty\\) the expecation of \\(\\hat{V}\\) approaches the marginal variance. These facts suggest monitoring convergence by estimating the factor by which the scale of the current distribution for \\(\\psi\\) might be reduced if the simulations were continued. This is the potential scale reduction factor and is estimated by\n\\[\n\\sqrt{\\hat{R}} = \\sqrt{\\frac{\\widehat{Var}(\\psi \\mid Y_0)}{\\hat{V}}},\n\\]\nwhich declines to 1 as \\(T \\rightarrow \\infty\\). When this quantity is high, there is evidence to proceed the simulations further to improve our inference about the target distribution."
  },
  {
    "objectID": "missing_data/bis/bis.html#other-simulation-methods",
    "href": "missing_data/bis/bis.html#other-simulation-methods",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Other Simulation Methods",
    "text": "Other Simulation Methods\nWhen draws from the sequence of conditional distributions forming the Gibbs’ sampler are not easy to obtain, other simulation approaches are needed. Among these there are the Sequential Imputation (Kong, Liu, and Wong (1994)), Sampling Imprtance Resampling (Gelfand and Smith (1990)), Rejection Sampling (Von Neumann et al. (1951)). One of these alternatives are the Metropolis-Hastings (Metropolis et al. (1953)) algorithms, of which the Gibbs’ sampler is a particular case, which constitute the so-called Markov Chain Monte Carlo (MCMC) algorithms as the sequence of iterates forms a Markov Chain (Gelman et al. (2013))."
  },
  {
    "objectID": "missing_data/em/em.html",
    "href": "missing_data/em/em.html",
    "title": "Expectation Maximisation Algorithm",
    "section": "",
    "text": "Patterns of incomplete data in practice often do not have the forms that allow explicit Maximum Likelihood(ML) estimates to be calculated. Suppose we have a model for the complete data \\(Y\\), with density \\(f(Y\\mid \\theta)\\), indexed by the set of unknown parameters \\(\\theta\\). Writing \\(Y=(Y_0,Y_1)\\) in terms of the observed \\(Y_0\\) and missing \\(Y_1\\) components, and assuming that the missingness mechanism is Missing At Random(MAR), we want to maximise the likelihood\n\\[\nL\\left(\\theta \\mid Y_0 \\right) = \\int f\\left(Y_0, Y_1 \\mid \\theta  \\right)dY_1\n\\]\nwith respect to \\(\\theta\\). When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation\n\\[\nD_l\\left(\\theta \\mid Y_0 \\right) \\equiv \\frac{\\partial ln L\\left(\\theta \\mid Y_0 \\right)}{\\partial \\theta} = 0,\n\\]\nwhile, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular Expectation Maximisation(EM) algorithm (Dempster, Laird, and Rubin (1977)).\nThe EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:\nThe procedure is then iterated until apparent convergence. Each iteration of EM consists of an expectation step (E step) and a maximisation step (M step) which ensure that, under general conditions, each iteration increases the loglikelihood \\(l(\\theta \\mid Y_0)\\). In addition, if the loglikelihood is bounded, the sequence \\(\\{l(\\theta_t \\mid Y_0), t=(0,1,\\ldots)\\}\\) converges to a stationary value of \\(l(\\theta \\mid Y_0)\\)."
  },
  {
    "objectID": "missing_data/em/em.html#the-e-step-and-the-m-step",
    "href": "missing_data/em/em.html#the-e-step-and-the-m-step",
    "title": "Expectation Maximisation Algorithm",
    "section": "The E step and the M step",
    "text": "The E step and the M step\nThe M step simply consists of performing ML estimation of \\(\\theta\\) as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not \\(Y_0\\) but the functions of \\(Y_0\\) appearing in the complete data loglikelihood \\(l(\\theta \\mid Y)\\). Specifically, let \\(\\theta_t\\) be the current estimate of \\(\\theta\\), then the E step finds the expected complete data loglikelihood if \\(\\theta\\) were \\(\\theta_t\\):\n\\[\nQ\\left(\\theta \\mid \\theta_t \\right) = \\int l\\left(\\theta \\mid Y \\right)f\\left(Y_0  \\mid Y_1 , \\theta = \\theta_t \\right)dY_0.\n\\]\nThe M step determines \\(\\theta_{t+1}\\) by maximising this expected complete data loglikelihood:\n\\[\nQ\\left(\\theta_{t+1} \\mid \\theta_t \\right) \\geq Q\\left(\\theta \\mid \\theta_t \\right),\n\\]\nfor all \\(\\theta\\).\n\nUnivariate Normal Data Example\nSuppose \\(y_i\\) form a an iid sample from a Normal distribution with population mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,\\ldots,n_{cc}\\) observed units and \\(i=n_{cc}+1,\\ldots,n\\) missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing \\(y_i\\) given \\(Y_{obs}\\) and \\(\\theta=(\\mu,\\sigma^2)\\) is \\(\\mu\\). Since the loglikelihood based on all \\(y_i\\) is linear in the sufficient statistics \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1}^n y^2_i\\), the E step of the algorithm calculates\n\\[\nE\\left(\\sum_{i=1}^{n}y_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\\mu_t\n\\]\nand\n\\[\nE\\left(\\sum_{i=1}^{n}y^2_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\\left(\\mu^2_t + \\sigma^2_t \\right)\n\\]\nfor current estimates \\(\\theta_t=(\\mu_t,\\sigma_t)\\) of the parameters. Note that simply substituting \\(\\mu_t\\) for the missing values \\(y_{n_{cc}+1},\\ldots,y_n\\) is not correct since the term \\((n-n_{cc})(\\sigma_t^2)\\) is omitted. Without missing data, the ML estimate of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\frac{\\sum_{i=1}^ny_i}{n}\\) and \\(\\frac{\\sum_{i=1}^ny^2_i}{n}-\\left(\\frac{\\sum_{i=1}^ny_i}{n}\\right)^2\\), respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates\n\\[\n\\mu_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y_i \\mid \\theta_t, Y_0 \\right)}{n}\n\\]\nand\n\\[\n\\sigma^2_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y^2_i \\mid \\theta_t, Y_0 \\right)}{n} - \\mu^2_{t+1}.\n\\]\nSetting \\(\\mu_t=\\mu_{t+1}=\\hat{\\mu}\\) and \\(\\sigma_t=\\sigma_{t+1}=\\hat{\\sigma}\\) in these equations shows that a fixed point of these iterations is \\(\\hat{\\mu}=\\frac{\\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\\) and \\(\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \\hat{\\mu}^2\\), which are the ML estimates of the parameters from \\(Y_0\\) assuming MAR and distinctness of the parameters."
  },
  {
    "objectID": "missing_data/em/em.html#extensions-of-em",
    "href": "missing_data/em/em.html#extensions-of-em",
    "title": "Expectation Maximisation Algorithm",
    "section": "Extensions of EM",
    "text": "Extensions of EM\nThere are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a Generalised Expectation Maximisation(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the Expectation Conditional Maximisation(ECM) algorithm (Meng and Rubin (1993)), which replaces the M step with a sequence of \\(S\\) conditional maximisation (CM) steps, each of which maximises the Q function over \\(\\theta\\) but with some vector function of \\(\\theta\\), say \\(g_s(\\theta)\\), fixed at its previous values for \\(s=1,\\ldots,S\\). Very briefly, assume that we have a parameter \\(\\theta\\) that can be partitioned into subvectors \\(\\theta=(\\theta_1,\\ldots,\\theta_S)\\), then we can take the \\(s\\)-th of the CM steps to be maximisation with respect to \\(\\theta_s\\) with all other parameters held fixed. Alternatively, it may be useful to take the \\(s\\)-th of the CM steps to be simultaneous maximisation over all of the subvectors expect \\(\\theta_s\\), which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of \\(\\theta\\). When the set of functions \\(g\\) is “space filling” in the sense that it allows unconstrained maximisation over \\(\\theta\\) in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.\nThe Expectation Conditional Maximisation Either(ECME) algorithm (Liu and Rubin (1994)) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The Alternative Expectation Conditional Maximisation(AECM) algorithm (Meng and Van Dyk (1997)) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html",
    "href": "missing_data/intro_bayes/intro_bayes.html",
    "title": "Introduction to Bayesian Inference",
    "section": "",
    "text": "Bayesian inference offers a convenient framework to analyse missing data as it draws no distinction between missing values and parameters, both interprted as unobserved quantities who are associated with a joint posterior distribution conditional on the observed data. In this section, I review basic concepts of Bayesian inference based on fully observed data, with notation and structure mostly taken from Gelman et al. (2013)."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#bayesian-inference-for-complete-data",
    "href": "missing_data/intro_bayes/intro_bayes.html#bayesian-inference-for-complete-data",
    "title": "Introduction to Bayesian Inference",
    "section": "Bayesian Inference for Complete Data",
    "text": "Bayesian Inference for Complete Data\nBayesian inference is the process of fitting a probability model to a set of data \\(Y\\) and summarising the results by a probability distribution on the parameters \\(\\theta\\) of the model and on unobserved quantities \\(\\tilde{Y}\\) (e.g. predictions). Indeed, Bayesian statistical conclusions about \\(\\theta\\) (or \\(\\tilde{Y}\\)) are made in terms of probability statements, conditional on the observed data \\(Y\\), typically indicated with the notation \\(p(\\theta \\mid y)\\) or \\(p(\\tilde{y} \\mid y)\\). Conditioning on the observed data is what makes Bayesian inference different from standard statistical approaches which are instead based on the retrospective evaluation of the procedures used to estimate \\(\\theta\\) (or \\(\\tilde{y}\\)) over the distribution of possible \\(y\\) values conditional on the “true” unknown value of \\(\\theta\\).\n\nBayes’ Rule\nIn order to make probability statements about \\(\\theta\\) given \\(y\\), we start with a model providing a joint probability distribution \\(p(\\theta,y)\\). Thus, the joint probability mass or density function can be written as a product of two densities that are often referred to as the prior distribution \\(p(\\theta)\\) and the sampling distribution \\(p(y \\mid \\theta)\\), respectively:\n\\[\np(\\theta,y) = p(\\theta)p(y \\mid \\theta),\n\\]\nand conditioning on the observed values of \\(y\\), using the basic property of conditional probability known as Bayes’ rule, yields the posterior distribution\n\\[\np(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(\\theta)p(y \\mid \\theta)}{p(y)},\n\\]\nwhere \\(p(y)=\\sum_{\\theta \\in \\Theta}p(\\theta)p(y\\mid \\theta)\\) is the sum (or integral in the case of continous \\(\\theta\\)) over all possible values of \\(\\theta\\) in the sample space \\(\\Theta\\). We can approximate the above equation by omitting the factor \\(p(y)\\) which does not depend on \\(\\theta\\) and, given \\(y\\), can be considered as fixed, yielding the unnormalised posterior density\n\\[\np(\\theta \\mid y) \\propto p(\\theta) p(y \\mid \\theta),\n\\]\nwith the purpose of the analysis being to develop the model \\(p(\\theta,y)\\) and adequately summarise \\(p(\\theta \\mid y)\\).\n\n\nUnivariate Normal Example (known variance)\nLet \\(y=(y_1,\\ldots,y_n)\\) denote an independent and identially distributed sample of \\(n\\) units, which are assumed to come from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), whose sampling density function is\n\\[\np(y \\mid \\mu)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nwhere for the moment we assume the variance \\(\\sigma^2\\) to be known (i.e. constant). Consider now a prior probability distribution for the mean parameter \\(p(\\mu)\\), which belongs to the family of conjugate prior densities, for example a Normal distribution, and parameterised in terms of a prior mean \\(\\mu_0\\) and variance \\(\\sigma^2_0\\). Thus, its prior density function is\n\\[\np(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\text{exp}\\left(-\\frac{1}{2}\\frac{(\\mu -\\mu_0)^2}{\\sigma^2} \\right),\n\\]\nunder the assumption tha the hyperparameters \\(\\mu_0\\) and \\(\\sigma^2_0\\) are known. The conjugate prior density implies that the posterior distribution for \\(\\mu\\) (with \\(\\sigma^2\\) assumed constant) belongs to the same family of distributions of the sampling function, that is Normal, but some algebra is required to reveal its specific form. In particular, the posterior density is\n\\[\np(\\mu \\mid y) = \\frac{p(\\mu)p(y\\mid \\mu)}{p(y)} \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2} \\left[\\frac{(\\mu - \\mu_0)^2}{\\sigma^2_0} + \\sum_{i=1}^n\\frac{(y_i-\\mu)^2}{\\sigma^2} \\right] \\right).\n\\]\nExapanding the components, collecting terms and completing the square in \\(\\mu\\) gives\n\\[\np(\\mu \\mid y) \\propto \\text{exp}\\left(-\\frac{(\\mu - \\mu_1)}{2\\tau^2_1} \\right),\n\\]\nthat is the posterior distribution of \\(\\mu\\) given \\(y\\) is Normal with posterior mean \\(\\mu_1\\) and variance \\(\\tau^2_1\\), where\n\\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2_0}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}} \\;\\;\\; \\text{and} \\;\\;\\; \\frac{1}{\\tau^2_1}=\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}.\n\\]\nWe can see that the posterior distribution depends on \\(y\\) only through the sample mean \\(\\bar{y}=\\sum_{i=1}^ny_i\\), which is a sufficient statistic in this model. When working with Normal distributions, the inverse of the variance plays a prominent role and is called the precision and, from the above expressions, it can be seen that for normal data and prior, the posterior precision \\(\\frac{1}{\\tau^2_1}\\) equals the sum of the prior precision \\(\\frac{1}{\\tau^2_0}\\) and the sampling precision \\(\\frac{n}{\\sigma^2}\\). Thus, when \\(n\\) is large, the posterior precision is largely dominated by \\(\\sigma^2\\) and the sample mean \\(\\bar{y}\\) compared to the corresponding prior parameters. In the specific case where \\(\\tau^2_0=\\sigma^2\\), the prior has the same weight as one extra observation with the value of \\(\\mu_0\\) and, as \\(n\\rightarrow\\infty\\), we have that \\(p(\\mu\\mid y)\\approx N\\left(\\mu \\mid \\bar{y},\\frac{\\sigma^2}{n}\\right)\\).\n\n\nUnivariate Normal Example (unknown variance)\nFor \\(p(y \\mid \\mu,\\sigma^2)=N(y \\mid \\mu, \\sigma^2)\\) with \\(\\mu\\) known and \\(\\sigma^2\\) unknown, the sampling distribution for a vector \\(y\\) of \\(n\\) units is\n\\[\np(y \\mid \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nwith the corresponding conjugate prior for \\(\\sigma^2\\) being the Inverse-Gamma distribution \\(\\Gamma^{-1}(\\alpha,\\beta)\\) with density function\n\\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha+1)}\\text{exp}\\left(-\\frac{\\beta}{\\sigma^2}\\right),\n\\]\nindexed by the hyperparameters \\(\\alpha\\) and \\(\\beta\\). A convenient parameterisation is as a Scaled Inverse-Chi Squared distribution \\(\\text{Inv-}\\chi^2(\\sigma^2_0,\\nu_0)\\) with scale and degrees of freedom parameters \\(\\sigma^2_0\\) and \\(\\nu_0\\), respectively. This means that the prior on \\(\\sigma^2\\) corresponds to the distribution of \\(\\frac{\\sigma^2_0 \\nu_0}{X}\\), where \\(X\\sim \\chi^2_{\\nu_0}\\) random variable. After some calculations, the resulting posterior for \\(\\sigma^2\\) is\n\\[\np(\\sigma^2 \\mid y) \\propto (\\sigma^2)^\\left(\\frac{n+\\nu_0}{2}+1\\right)\\text{exp}\\left(-\\frac{\\nu_0 \\sigma^2_0 + n \\nu}{2\\sigma^2} \\right)\n\\]\nwhere \\(\\nu=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\mu)^2\\). This corresponds to say that\n\\[\n\\sigma^2 \\mid y \\sim \\text{Inv-}\\chi^2\\left(\\nu_0 +n, \\frac{\\nu_0\\sigma^2_0+n\\nu}{\\nu_0 + n} \\right),\n\\]\nwith scale equal to the degrees of freedom-weighted average of the prior and data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.\n\n\nUnivariate Normal Example (unknown mean and variance)\nSuppose now that both the mean and variance parameters are unknown such that\n\\[\np(y \\mid \\mu, \\sigma^2) \\sim N(\\mu, \\sigma^2),\n\\]\nand that the interest is centred on making inference about \\(\\mu\\), that is we seek the conditional posterior distribution of the parameters of interest given the observed data \\(p(\\mu \\mid y)\\). This can be derived from the joint posterior distribution density \\(p(\\mu, \\sigma^2 \\mid y)\\) by averaging over all possible values of \\(\\sigma^2\\), that is\n\\[\np(\\mu \\mid y)=\\int p(\\mu, \\sigma^2 \\mid y)d\\sigma^2,\n\\]\nor, alternatively, the joint posterior can be factored as the product of the marginal distribution of one parameter and the conditional distribution of the other given the former and then taking the average over the values of the “nuisance” parameter\n\\[\np(\\mu \\mid y)=\\int p(\\mu \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)d\\sigma^2.\n\\]\nThe integral forms are rarely computed in practice but this expression helps us to understand that posterior distributions can be expressed in terms of the product of marginal and conditional densities, first drawing \\(\\sigma^2\\) from its marginal and then \\(\\mu\\) from its conditional given the drawn value of \\(\\sigma^2\\), so that the integration is indirectly performed. For example, consider the Normal model with both unknown mean and variance and assume a vague prior density \\(p(\\mu,\\sigma^2)\\propto (\\sigma^2)^{-1}\\) (corresponding to uniform prior on \\((\\mu, \\log\\sigma)\\)), then the joint posterior distribution is proportional to the sampling distribution multiplied by the factor \\((\\sigma^2)^{-1}\\), that is\n\\[\np(\\mu,\\sigma^2 \\mid y)\\propto \\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right),\n\\]\nwhere \\(s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2\\) is the sample variance. Next, the conditional posterior density \\(p(\\mu \\mid \\sigma^2)\\) can be shown to be equal to\n\\[\np(\\mu \\mid \\sigma^2,y) \\sim N(\\bar{y},\\frac{\\sigma^2}{n}),\n\\]\nwhile the marginal posterior \\(p(\\sigma^2 \\mid y)\\) can be obtained by averaging the joint \\(p(\\mu,\\sigma^2\\mid y)\\) over \\(\\mu\\), that is\n\\[\np(\\sigma^2 \\mid y)\\propto \\int \\left(\\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right)\\right)d\\mu,\n\\]\nwhich leads to\n\\[\np(\\sigma^2 \\mid ,y) \\sim \\text{Inv-}\\chi^2(n-1,s^2).\n\\]\nTypically, \\(\\mu\\) represents the estimand of interest and the obejective of the analysis is therefore to make inference about the marginal distribution \\(p(\\mu \\mid y)\\), which can be obtained by integrating \\(\\sigma^2\\) out of the joint posterior\n\\[\np(\\mu \\mid y)=\\int_{0}^{\\infty}p(\\mu,\\sigma^2\\mid y)d\\sigma^2 \\propto \\left[1+\\frac{n(\\mu-\\bar{y})}{(n-1)s^2} \\right]\n\\]\nwhich corresponds to a Student-\\(t\\) density with \\(n-1\\) degrees of freedom\n\\[\np(\\mu \\mid y)\\sim t_{n-1}\\left(\\bar{y},\\frac{s^2}{n}\\right)\n\\]\n\n\nMultivariate Normal Example\nSimilar considerations to those applied to the univariate case can be extended to the multivariate case when \\(y\\) is formed by \\(J\\) components coming from the Multivariate Normal distribution\n\\[\np(y\\mid \\mu, \\Sigma) \\sim N(\\mu, \\Sigma),\n\\]\nwhere \\(\\mu\\) is a vector of length \\(J\\) and \\(\\Sigma\\) is a \\(J\\times J\\) covariance matrix, which is symmetric and positive definite. The sampling distribution for a sample of \\(n\\) units is\n\\[\np(y\\mid \\mu, \\Sigma) \\propto \\mid \\Sigma \\mid^{-n/2}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-\\mu)^{T}\\Sigma^{-1}(y_i-\\mu) \\right),\n\\]\nAs with the univariate normal model, we can derive the posterior distribution for \\(\\mu\\) and \\(\\Sigma\\) according to the factorisation used of the joint posterior and the prior distributions specified. For example, using the conjugate normal prior for the mean \\(p(\\mu)\\sim N(\\mu_0,\\Sigma_0)\\), given \\(\\Sigma\\) known, the posterior can be shown to be\n\\[\np(\\mu \\mid y) \\sim N(\\mu_1,\\Sigma_1),\n\\]\nwhere the posterior mean is a weighted average of the data and prior mean with weights given by the data and prior precision matrices \\(\\mu_1=(\\Sigma^{-1}_0+n\\Sigma^{-1})^{-1} (\\Sigma_0^{-1}\\mu_0 + n\\Sigma^{-1}\\bar{y})\\), and the posterior precision is the sum of the data and prior precisions \\(\\Sigma^{-1}_1=\\Sigma^{-1}_0+n\\Sigma^{-1}\\).\nIn the situation in which both \\(\\mu\\) and \\(\\Sigma\\) are unknown, convenient conjugate prior distributions which generalise those used in the univariate case are the Inverse-Wishart for the covariance matrix \\(\\Sigma\\sim \\text{Inv-Wishart}(\\Lambda_0,\\nu_0)\\) and the Multivariate Normal for the mean \\(\\mu\\sim N(\\mu_0, \\Sigma_0)\\), where \\(\\nu_0\\) and \\(\\Lambda_0\\) represent the degrees of freedom and the scale matrix for the Inverse-Wishart distribution, while \\(\\mu_0\\) and \\(\\Sigma_0=\\frac{\\Sigma}{\\kappa_0}\\) are the prior mean and covariance matrix for the Multivariate Normal. Woking out the form of the posterior, it can be shown that the joint posterior distribution has the same form of the sampling distribution with parameters\n\\[\np(\\mu \\mid \\Sigma, y) \\sim N(\\mu_1,\\Sigma_1) \\;\\;\\; \\text{and} \\;\\;\\; p(\\Sigma \\mid y) \\sim \\text{Inv-Wishart}(\\Lambda_1,\\nu_1),\n\\]\nwhere \\(\\Sigma_1=\\frac{\\Sigma}{\\kappa_1}\\), \\(\\mu_1=\\frac{1}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_1=\\kappa_0+n\\), \\(\\nu_1=\\nu_0+n\\), and \\(\\Lambda_1=\\Lambda_0+\\sum_{i=1}^n(y_i-\\bar{y})(y_i-\\bar{y})^T+\\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)(\\bar{y}-\\mu_0)^2\\)."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#regression-models",
    "href": "missing_data/intro_bayes/intro_bayes.html#regression-models",
    "title": "Introduction to Bayesian Inference",
    "section": "Regression Models",
    "text": "Regression Models\nSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(X=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\)\n\\[\np(y \\mid \\beta,\\sigma^2,X) \\sim N(X\\beta,\\sigma^2I),\n\\]\nwhere \\(\\beta=(\\beta_0,\\ldots,\\beta_J)\\) is the set of regression coefficients and \\(I\\) is the \\(n\\times n\\) identity matrix. Within the normal regression model, a convenient vague prior distribution is uniform on \\((\\beta,\\log\\sigma)\\)\n\\[\np(\\beta,\\sigma^2)\\propto\\sigma^{-2}.\n\\]\nAs with normal distributions with unknown mean and variance we can first determine the marginal posterior of \\(\\sigma^2\\) and factor the joint posterior as \\(p(\\beta,\\sigma^2)=p(\\beta \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)\\) (omit X for simplicity). Then, the conditional distribtuion \\(p(\\beta \\mid \\sigma^2,y)\\) is Normal\n\\[\np(\\beta \\mid \\sigma^2, y) \\sim N(\\hat{\\beta},V_{\\beta}\\sigma^2),\n\\]\nwhere \\(\\hat{\\beta}=(X^{T}X)^{-1}(X^{T}y)\\) and \\(V_{\\beta}=(X^{T}X)^{-1}\\). The marginal posterior \\(p(\\sigma^2 \\mid y)\\) has a scaled Inverse-\\(\\chi^2\\) form\n\\[\np(\\sigma^2\\mid y) \\sim \\text{Inv-}\\chi^2(n-J,s^2),\n\\]\nwhere \\(s^2=\\frac{1}{n-J}(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta})\\). Finally, the marginal posterior \\(p(\\beta \\mid y)\\), averaging over \\(\\sigma^2\\), is multivariate \\(t\\) with \\(n-J\\) degrees of freedom, even though in practice since we can characterise the joint posterior by drawing from \\(p(\\sigma^2)\\) and then from \\(p(\\beta \\mid \\sigma^2)\\). When the analysis is based on improper priors (do not have finite integral), it is important to check that the posterior is proper. In the case of the regression model, the posterior for \\(\\beta \\mid \\sigma^2\\) is proper only if the number of observations is larger than the number of parameters \\(n&gt;J\\), and that the rank of \\(X\\) equals \\(J\\) (i.e. the columns of \\(X\\) are linearly independent) in order for all \\(J\\) coefficients to be uniquely identified from the data."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#generalised-linear-models",
    "href": "missing_data/intro_bayes/intro_bayes.html#generalised-linear-models",
    "title": "Introduction to Bayesian Inference",
    "section": "Generalised Linear Models",
    "text": "Generalised Linear Models\nThe purpose of Generalised Linear Models(GLM) is to extend the idea of linear modelling to cases for which the linear relationship between \\(X\\) and \\(E[y\\mid X]\\) or the Normal distribution is not appropriate. GLMs are specified in three stages\n\nChoose the linear predictor \\(\\eta=X\\beta\\)\nChoose the link fuction \\(g()\\) that relates the linear predictor to the mean of the outcome variable \\(\\mu=g^{-1}(\\eta)\\)\nChoose the random component specifying the distribution of \\(y\\) with mean \\(E[y\\mid X]\\)\n\nThus, the mean of the distribution of \\(y\\) given \\(X\\) is determined as \\(E[y\\mid X]=g^{-1}(X\\beta)\\). The Normal linear model can be thought as a special case of GLMs where the link function is the identity \\(g(\\mu)=\\mu\\) and the random component is normally distributed. Perhaps, the most commonly used GLMs are those based on Poisson and Binomial distributions to analyse count and binary data, respectively.\n\nPoisson\nCounted data are often modelled using Poisson regression models which assume that \\(y\\) is distributed according to a Poisson distribution with mean \\(\\mu\\). The link function is typically chosen to be the logarithm so that \\(\\log \\mu = X\\beta\\) and the distribution of the data has density\n\\[\np(y\\mid \\beta)=\\prod_{i=1}^n \\frac{1}{y_i}\\text{exp}\\left(-\\text{e}^{(\\eta_i)}(\\text{exp}(\\eta_i))^{y_i}\\right),\n\\]\nwhere \\(\\eta_i=(X\\beta)_i\\) is the linear predictor for the \\(i-\\)th unit.\n\n\nBinomial\nSuppose there are some binomial data \\(y_i \\sim \\text{Bin}(n_i,\\mu_i)\\), with \\(n_i\\) known. It is common to specify the model in terms of the mean of the proportions \\(\\frac{y_i}{n_i}\\) rather than the mean of \\(y_i\\). Choosing the logit tranformation of the probability of success \\(g(\\mu_i)=\\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\\) as the link function leads to the logistic regression where data have distribution\n\\[\np(y \\mid \\beta)=\\prod_{i=1}^n {n_i \\choose y_i} {e^{\\eta_i} \\choose 1+e^{\\eta_i}}^{y_i} {1 \\choose 1+e^{\\eta_i}}^{n_i-y_i}.\n\\]\nThe link functions used in the previous models are known as the canonical link functions for each family of distributions, which is the function of the mean parameter that appears in the exponent of the exponential family form of the probability density. However, it is also possible to use link functions which are not canonical."
  },
  {
    "objectID": "missing_data/ipw/ipw.html",
    "href": "missing_data/ipw/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. Weighting methods can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (Little and Rubin (2019)). Robins, Rotnitzky, and Zhao (1994) introduced Inverse Probability Weighting (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of Generalized Estimating Equations (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (Zeger, Liang, and Albert (1988)).\nLet \\(y_i=(y_{i1},\\ldots,y_{iK})\\) denote a vector of variables for unit \\(i\\) subject to missing values with \\(y_i\\) being fully observed for \\(i=1\\ldots,n_r\\) units and partially-observed for \\(i=n_r+1,\\ldots,n\\) units. Define \\(m_i=1\\) if \\(y_i\\) is incomplete and \\(m_i=0\\) if complete. Let \\(x_i=(x_{i1},\\ldots,x_{ip})\\) denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of \\(y_i\\) given \\(x_i\\), having the form \\(g(x_i,\\beta)\\), where \\(g()\\) is a possibly non-linear regression function indexed by a parameter \\(\\beta\\) of dimension \\(d\\). Let also \\(z_i=(z_{i1},\\ldots,z_{iq})\\) be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for \\(y_i \\mid x_i\\). When there are no missing data, a consistent estimate of \\(\\beta\\) is given by the solution to the following GEE, under mild regularity conditions (Liang and Zeger (1986)),\n\\[\n\\sum_{i=1}^n = D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(D_i(x_i,\\beta)\\) is a suitably chosen \\((d\\times k)\\) matrix of known functions of \\(x_i\\). With missing data, the equation is applied only to the complete cases (\\(n_{r}\\)), which yields consistent estimates provided that\n\\[\np(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,\\phi),\n\\]\nthat is, missingness does not depend on \\(y_i\\) or \\(z_i\\) after conditioning on \\(x_i\\). IPW GEE methods (Robins and Rotnitzky (1995)) replace the equation with\n\\[\n\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on \\(m_i\\) on \\(x_i\\) and \\(z_i\\). If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of \\(\\beta\\) provided that\n\\[\np(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,z_i\\phi).\n\\]"
  },
  {
    "objectID": "missing_data/ipw/ipw.html#example",
    "href": "missing_data/ipw/ipw.html#example",
    "title": "Inverse Probability Weighting",
    "section": "Example",
    "text": "Example\nSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). If data were fully observed for \\(i=1,\\ldots,n\\) individuals, an obvious estimator of \\(\\mu\\) would be the sample outcome mean\n\\[\n\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i,\n\\]\nwhich is equivalent to the solution to the estimating equation \\(\\sum_{i=1}^n(y_i-\\mu)=0\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\np(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\n\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, the sample mean of the complete cases \\(\\bar{y}_{cc}=\\frac{\\sum_{i=1}^nc_iy_i}{c_i}\\), i.e. the solution to the equation \\(\\sum_{i=1}^nc_i(y_i-\\mu)=0\\), is not a consistent estimator of \\(\\mu\\). To correct for this, the IPW complete case estimating equation\n\\[\n\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i)}(y_i-\\mu)=0,\n\\]\ncan be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i)\\). The solution of the equation corresponds to the IPW estimator\n\\[\n\\mu_{ipw}=\\left(\\sum_{i=1}^n \\frac{c_i}{\\pi(z_i)} \\right)^{-1} \\sum_{i=1}^n \\frac{c_iy_i}{\\pi(z_i)},\n\\]\nwhich is unbiased under MAR and for \\(\\pi(z)&gt;0\\). In case you want to have a look at the proof of this I put here the link. In most situations \\(\\pi(z_i)\\) is not known and must be estimated from the data, typically posing some model for \\(p(c=1 \\mid z, \\hat{\\alpha})\\), indexed by some parameter \\(\\hat{\\alpha}\\), for example a logistic regression\n\\[\n\\text{logit}(\\pi)=\\alpha_0 + \\alpha_1z.\n\\]\nOf course, if the model for \\(\\pi(z)\\) is misspecified, \\(\\mu_{ipw}\\) can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient."
  },
  {
    "objectID": "missing_data/ipw/ipw.html#conclusions",
    "href": "missing_data/ipw/ipw.html#conclusions",
    "title": "Inverse Probability Weighting",
    "section": "Conclusions",
    "text": "Conclusions\nThus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on \\(z_i\\) (Schafer and Graham (2002)). The basic intuition of IPW methods is that each subject’s contribution to the weighted Complete Case Analysis (CCA) is replicated \\(w_i\\) times in order to account once for herself and \\((1-w_i)\\) times for those subjects with the same responses and covariates who are missing. These models are called semiparametric because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (Molenberghs et al. (2014)). Older GEE methods can accommodate missing values only if they are Missing Completely At Random (MCAR), while more recent methods allow them to be MAR or even Missing Not At Random (MNAR), provided that a model for the missingness is correctly specified (Robins, Rotnitzky, and Zhao (1995),Rotnitzky, Robins, and Scharfstein (1998))."
  },
  {
    "objectID": "missing_data/jmi/jmi.html",
    "href": "missing_data/jmi/jmi.html",
    "title": "Joint Multiple Imputation",
    "section": "",
    "text": "Multiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example."
  },
  {
    "objectID": "missing_data/jmi/jmi.html#rubins-rules",
    "href": "missing_data/jmi/jmi.html#rubins-rules",
    "title": "Joint Multiple Imputation",
    "section": "Rubin’s rules",
    "text": "Rubin’s rules\nLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\n\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\n\\]\nBecause the imputations under MI are conditional draws, under a good imputation model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\n\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\n\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\nT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\n\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\n(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\n\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\n\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\n\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution."
  },
  {
    "objectID": "missing_data/jmi/jmi.html#joint-multiple-imputation",
    "href": "missing_data/jmi/jmi.html#joint-multiple-imputation",
    "title": "Joint Multiple Imputation",
    "section": "Joint Multiple Imputation",
    "text": "Joint Multiple Imputation\nJoint MI starts from the assumption that the data can be described by a multivariate distribution which in many cases, mostly for practical reasons, corresponds to assuming a multivariate Normal distribution. The general idea is that, for a general missing data pattern $ r$, missingness may occur anywhere in the multivariate outcome vector $ y=(y_1,,y_J)$, so that the distribution from which imputations should be drawn varies based on the observed variables in each pattern. For example, given $ r=(0,0,1,1)$, then imputations should be drawn from the bivariate distribution of the missing variables given the observed variables in that pattern, that is from \\(f(y^{mis}_1,y^{mis}_2 \\mid y^{obs}_3, y^{obs}_4, \\phi_{12})\\), where \\(\\phi_{12}\\) is the probability of being in pattern $ r$ where the first two variables are missing.\nConsider the multivariate Normal distribution \\(y \\sim N(\\mu,\\Sigma)\\), where \\(\\theta=(\\mu,\\Sigma)\\) represent the vector of the parameters of interest which need to be identified. Indeed, for non-monotone missing data, $ $ cannot be generally identified based on the observed data directly $ y^{obs}$, and the typical solution is to iterate imputation and parameter estimation using a general algorithm known as data augmentation(Tanner and Wong (1987)). Following Van Buuren (2018), the general procedure of the algorithm can be summarised as follows:\n\nDefine some plausible starting values for all parameters \\(\\theta_0=(\\mu_0,\\Sigma_0)\\)\nAt each iteration \\(t=1,\\ldots,T\\), draw \\(h=1,\\ldots,H\\) imputations for each missing value from the predictive distribution of the missing data given the observed data and the current value of the parameters at \\(t-1\\), that is\n\n\\[\n\\hat{y}^{mis}_{t} \\sim p(y^{mis} \\mid y^{obs},\\theta_{t-1})\n\\]\n\nRe-estimate the parameters \\(\\theta\\) using the observed and imputed data at \\(t\\) based on the multivariate Normal model, that is\n\n\\[\n\\hat{\\theta}_{t} \\sim p(\\theta \\mid y^{obs}, \\hat{y}^{mis}_{t})\n\\]\nAnd reiterate the steps 2 and 3 until convergence, where the stopping rule typically consists in imposing that the change in the parameters between iterations \\(t-1\\) and \\(t\\) should be smaller than a predefined “small” threshold \\(\\epsilon\\). Schafer (1997) showed that imputations generated under the multivariate Normal model can be robust to non-normal data, even though it is generally more efficient to transform the data towards normality, especially when the parameters of interest are difficult to estimate, such as quantiles and variances.\nThe multivariate Normal model is also often applied to categorical data, with different types of specifications that have been proposed in the literature (Schafer (1997),Horton, Lipsitz, and Parzen (2003),Allison (2005),Bernaards, Belin, and Schafer (2007),Yucel, He, and Zaslavsky (2008),Demirtas (2009)). For examples, missing data in contingency tables can be imputed using log-linear models (Schafer (1997)); mixed continuous-categorical data can be imputed under the general location model which combines a log-linear and multivariate Normal model (Olkin, Tate, et al. (1961)); two-way imputation can be applied to missing test item responses by imputing missing categorical data by conditioning on the row and column sum scores of the multivariate data (Van Ginkel et al. (2007))."
  },
  {
    "objectID": "missing_data/likinf_nig/likinf_nig.html",
    "href": "missing_data/likinf_nig/likinf_nig.html",
    "title": "Likelihood Based Inference with Incomplete Data (Nonignorable)",
    "section": "",
    "text": "In many cases, analysis methods for missing data are based on the ignorable likelihood\n\\[\nL_{ign}\\left(\\theta \\mid Y_0, X \\right) \\propto f\\left(Y_0 \\mid X, \\theta \\right),\n\\]\nregarded as a function of the parameters \\(\\theta\\) for fixed observed data \\(Y_0\\) and some fully observed covariates \\(X\\). The density \\(f(Y_0 \\mid X, \\theta)\\) is obtained by integrating out the missing data \\(Y_1\\) from the joint density \\(f(Y \\mid X, \\theta)=f(Y_0,Y_1\\mid X, \\theta)\\). Sufficient conditions for basing inference about \\(\\theta\\) on the ignorbale likelihood are that the missingness mechanism is Missing At Random(MAR) and the parameters of the model of analysis \\(\\theta\\) and those of the missingness mechanism \\(\\psi\\) are distinct. Here we focus our attention on the situations where the missingness mechanism is Missing Not At Random(MNAR) and valid Maximum Likelihood(ML), Bayesian and Multiple Imputation(MI) inferences generally need to be based on the full likelihood\n\\[\nL_{full}\\left(\\theta, \\psi \\mid Y_0, X, M \\right) \\propto f\\left(Y_0, M \\mid X, \\theta, \\psi \\right),\n\\]\nregarded as a function of \\((\\theta,\\psi)\\) for fixed \\((Y_0,M)\\). Here, \\(f(Y_0,M\\mid \\theta, \\psi)\\) is obtained by integrating out \\(Y_1\\) from the joint density \\(f(Y,M \\mid X, \\theta, \\psi)\\). Two main approaches for formulating MNAR models can be distinguished, namely selection models(SM) and pattern mixture models(PMM)."
  },
  {
    "objectID": "missing_data/likinf_nig/likinf_nig.html#selection-and-pattern-mixture-models",
    "href": "missing_data/likinf_nig/likinf_nig.html#selection-and-pattern-mixture-models",
    "title": "Likelihood Based Inference with Incomplete Data (Nonignorable)",
    "section": "Selection and Pattern Mixture Models",
    "text": "Selection and Pattern Mixture Models\nSMs factor the joint distribution of \\(m_i\\) and \\(y_i\\) as\n\\[\nf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, \\theta)f(m_i \\mid x_i,y_i,\\psi),\n\\]\nwhere the first factor is the distribution of \\(y_i\\) in the population while the second factor is the missingness mechanism, with \\(\\theta\\) and \\(\\psi\\) which are assumed to be distinct. Alternatively, PMMs factor the joint distribution as\n\\[\nf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, m_i,\\xi)f(m_i \\mid x_i),\n\\]\nwhere the first factor is the distribution of \\(y_i\\) in the strata defined by different patterns of missingness \\(m_i\\) while the second factor models the probabilities of the different patterns, with \\(\\xi\\) which are assumed to be distinct (Little (1993),Little and Rubin (2019)). The distinction between the two factorisations becomes clearer when considering a specific example.\nSuppose thta missing values are confined to a single variable and let \\(y_i=(y_{i,1},y_{i2})\\) be a bivariate response outcome where \\(y_{i1}\\) is fully observed and \\(y_{i2}\\) is observed for \\(i=1,\\ldots,n_{cc}\\) but missing for \\(i=n_{cc}+1,\\ldots,n\\). Let \\(m_{i2}\\) be the missingness indicator for \\(y_{i2}\\), then a PMM factors the denisty of \\(Y_0\\) and \\(M\\) given \\(X\\) as\n\\[\nf(y_0, M \\mid X, \\xi)=\\prod_{i=1}^{n_{cc}}f(y_{i1},y_{i2}\\mid x_i, m_{i2}=0,\\xi)Pr(m_{i2}=0 \\mid x_i, \\omega) \\times \\prod_{i=n_{cc}+1}^{n}f(y_{i1} \\mid x_i, m_{i2}=1,\\xi)Pr(m_{i2}=1 \\mid x_i, \\omega).\n\\]\nThis expression shows that there are no data with which to estimate directly the distribution \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)\\), because all units with \\(m_{i2}=1\\) have \\(y_{i2}\\) missing. Under MAR, this is identified using the distribution of the observed data \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)=f(y_{i2} \\mid x_i, m_{i2}=0,\\xi)\\), while under MNAR it must be identified using other assumptions. The SM formulation is\n\\[\nf(y_i, m_{i2} \\mid \\theta, \\psi) = f(y_{i1} \\mid x_i, \\theta)f(y_{i2} \\mid x_i, y_{i1},\\theta)f(m_{i2}\\mid x_i,y_{i1},y_{i2},\\psi).\n\\]\nTypically, the missingness mechanism \\(f(m_{i2} \\mid x_i,y_{i1},y_{i2},\\psi)\\) is modelled using some additive probit or logit regression of \\(m_{i2}\\) on \\(x_i\\),\\(y_{i1}\\) and \\(y_{i2}\\). However, the coefficient of \\(y_{i2}\\) in this regression is not directly estimable from the data and hence the model cannot be fully estimated without extra assumptions.\n\nNormal Models for MNAR data\nAssume we have a complete sample \\((y_i,x_i)\\) on a continuous variable \\(Y\\) and a set of fully observed covariates \\(X\\), for \\(i=1,\\ldots,n\\). Suppose that \\(i=1,\\ldots,n_{cc}\\) units are observed while the remaining \\(i=n_{cc}+1,\\ldots,n\\) units are missing, with \\(m_i\\) being the corresponding missingness indicator. Heckman (Heckman (1976)) proposed the following selection model to handle missingness:\n\\[\ny_i \\mid x_i, \\theta, \\psi \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2) \\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,y_i,\\theta,\\psi \\sim Bern\\left(\\Phi(\\psi_0 + \\psi_1x_i + \\psi_2y_i) \\right),\n\\]\nwhere \\(\\theta=(\\beta_0,\\beta_1,\\sigma^2)\\) and \\(\\Phi\\) denotes the probit (cumulative normal) distribution function. Note that if \\(\\psi_2=0\\), the missing data are MAR, while if \\(\\psi_2 \\neq 0\\) the missing data are MNAR since missingness in \\(Y\\) depends on the unobserved value of \\(Y\\). This model can be estimated using either a two-step least squares method, ML in combination with an EM algorithm, or a Bayesian approach. The main issue is the lack of information about \\(\\psi_2\\), which can be partly identified through the specific assumptions about the distribution of the observed data of \\(Y\\). This, however, makes the implicit assumption that the assumed distribution can well described the distribution of the complete (observed and missing) data which can never be tested or checked. An alternative approach is to use a PMM factorisation and model:\n\\[\ny_i \\mid m_i=m,x_i,\\xi,\\omega \\sim N(\\beta_0^m + \\beta_1^mx_i, \\sigma^{2m})\\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,\\xi,\\omega \\sim Bern\\left(\\Phi(\\omega_0 + \\omega_1x_i) \\right),\n\\]\nwhere \\(\\xi=(\\beta_0^m,\\beta_1^m,\\sigma^{2m},\\;\\;\\; m=0,1)\\). This model implies that the distribution of \\(y_i\\) given \\(x_i\\) in the population is a mixture of two normal distributions with mean\n\\[\n\\left[1 - \\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^0 + \\beta_1^0 x_i \\right] + \\left[\\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^1 + \\beta_1^1 x_i \\right].\n\\]\nThe parameters \\((\\beta_0^0,\\beta_1^0,\\sigma^{20},\\omega)\\) can be estimated from the data but the parameters \\((\\beta_0^1,\\beta_1^1,\\sigma^{21})\\) are not estimable because \\(y_i\\) is missing when \\(m_i=1\\). Under MAR, the distribution of \\(Y\\) given \\(X\\) is the same for units with \\(Y\\) observed and missing, such that \\(\\beta_0^0=\\beta_0^1=\\beta_0\\) (as well as for \\(\\beta_1\\) and \\(\\sigma^2\\)). Under MNAR, other assumptions are needed to esitmate the parameters indexed by \\(m=1\\).\nSome final considerations:\n\nBoth SM and PMM model the joint distribution of \\(Y\\) and \\(M\\).\nThe SM formulation is more natural when the substantive interest concerns the relationship between \\(Y\\) and \\(X\\) in the population. However, these parameters can also be derived in PMM by averaging the patterns specific parameters over the missingness patterns.\nThe PMM factorisation is more transparent in terms of the underlying assumptions about the unidentified parameters of the model, while SM tends to impose some obscure constraints in order to identify these parameters, which are also difficult to interpret.\nGiven specific assumptions to identify all the parameters in the model, PMMs are often easier to fit than SMs. In addition, imputations of the missing values are based on the predictive distribution of \\(Y\\) given \\(X\\) and \\(M=0\\).\n\nThese considerations seem to favour PMM over SM as MNAR approaches, especially when considering sensitivity analysis. Bayesian approaches can also be used to identify these models, by assigning prior distributions which can be used to identify those parameters which cannot be estimated from the data. Justifications for the choice of these priors are therefore necessary to ensure the plausibility of the assumptions assessed and the impact of these assumptions on the posterior inference."
  },
  {
    "objectID": "missing_data/pmm/pmm.html",
    "href": "missing_data/pmm/pmm.html",
    "title": "Pattern Mixture Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Pattern Mixture Models(PMM)."
  },
  {
    "objectID": "missing_data/pmm/pmm.html#pattern-mixture-models",
    "href": "missing_data/pmm/pmm.html#pattern-mixture-models",
    "title": "Pattern Mixture Models",
    "section": "Pattern Mixture Models",
    "text": "Pattern Mixture Models\nThe pattern mixture model approach factors the full data distribution as\n\\[\nf(y,r \\mid \\omega) = f(y \\mid r, \\phi) f(r \\mid y,\\chi),\n\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\phi,\\chi)\\). Thus, under the PMM approach, the response model \\(f(y \\mid \\theta)\\) can be retrieved as a mixture of the pattern specific distributions\n\\[\nf(y \\mid \\theta) = \\sum_{r}f(y \\mid r, \\phi)f(r \\mid \\chi),\n\\]\nwith weights given by the corresponding probabilities of the different patterns. The missingness mechanism \\(f(r \\mid y, \\psi)\\) can also be obtained using Bayes’ rule\n\\[\nf(y \\mid r, \\psi) = \\frac{f(y \\mid r, \\phi)f(r\\mid \\chi)}{f(y \\mid \\theta)}.\n\\]\nThe construction of PMMs requires the specification of the full data distribution conditional on different missingness patterns, which may be cumbersome when the number of patterns is large, but with the advantage of making explicit the parameters that cannot be identified by the observed data. In particular, PMMs are well suited to show that the distribution of the response within each pattern can be decomposed as\n\\[\nf(y_{obs},y_{mis} \\mid r, \\phi)= f(y_{mis} \\mid y_{obs},r,\\phi_{E})f(y_{obs}\\mid r,\\phi_{O}),\n\\]\nwhere \\(\\phi_E = \\lambda_1(\\phi)\\) and \\(\\phi_O=\\lambda_2(\\phi)\\) are functions of the mixture component parameter \\(\\phi\\). The former subset of parameters indexes the so called extrapolation distribution and cannot be identified from the data, i.e. the distribution of the missing values given the observed values, while the latter indexes the observed data distribution and is typically identifiable from the data. Assuming there exists a partition such that \\(\\phi_E=(\\phi_{EI},\\phi_{ENI})\\) and the observed data distribution is a function of \\(\\phi_{EI}\\) but not of \\(\\phi_{ENI}\\), then \\(\\phi_{ENI}\\) is a senstivity parameter in that it can only be identified using information from sources other than the observed data and thus makes a suitable basis to formulate sensitivity analysis using informative priors.\n\nExample of PMM for bivariate normal data\nConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A PMM factors the full data distribution as\n\\[\nf(y_1,y_2,r \\mid \\omega) = f(y_1, y_2 \\mid r, \\phi)f(r \\mid ,\\chi),\n\\]\nwhere, for example, we may have \\(Y \\mid R=1 \\sim N(\\mu^1,\\Sigma^1)\\), \\(Y \\mid R=0 \\sim N(\\mu^0,\\Sigma^0)\\) and \\(R \\sim Bern(\\chi)\\). We define \\(\\mu^r=(\\mu^r_1)\\), while \\(\\Sigma^r\\) has elements \\(\\sigma^r=(\\sigma^r_{11},\\sigma^r_{12},\\sigma^r_{22})\\). Similarly, we can define the parameters \\(\\beta^r_0\\), \\(\\beta^r_1\\) and \\(\\sigma^r_{2\\mid 1}\\) as the intercept, slope and residual variance of the regression of \\(Y_2\\) on \\(Y_1\\) for each pattern \\(r\\). Under this reparameterisation, the full data model parameters are\n\\[\n\\phi=\\{\\mu^r_1,\\sigma^r_{11},\\beta^r_0,\\beta^1_1,\\sigma^r_{2\\mid 1}\\}.\n\\]\nThe extrapolation and observed data distributions, with associated parameters, are then\n\\[\nf(y_{mis}\\mid y_{obs},\\phi_{E}) \\rightarrow \\phi_{E}=(\\beta^0_0, \\beta^0_1,\\sigma^0_{2\\mid1})\n\\]\nand\n\\[\nf(y_{obs}\\mid \\phi_{O}) \\rightarrow \\phi_{O}=(\\mu^1,\\beta^1,\\sigma^1_{11},\\mu^0_0,\\sigma^1_{11}).\n\\]\nIt can be shown that, in this specific example, the observed data distribution does not depend on the parameters indexing the extrapolation distribtuon \\(\\phi_{ENI}=(\\beta^0_0,\\beta^0_1,\\sigma^0_{2\\mid 1})\\). It is possible to set \\(\\beta^0=\\beta=1\\) and \\(\\sigma^0_{2\\mid1}=\\sigma^1_{2\\mid1}\\) to yield a Missing At Random(MAR) assumption. Hence, a function that maps identified parameters and sensitivity parameters \\(\\Delta\\) to the space of unidentified parameters can be used to quantify departures from MAR. For example, assume we impose\n\\[\n\\beta^0_0=\\beta^1_0+\\Delta,\n\\]\nthen assigning a point mass prior at \\(\\Delta=0\\) implies MAR, while fixing \\(\\Delta \\neq 0\\) or using any type of inofrmative prior on this parameter implies a Missing Not At Random(MNAR) assumption."
  },
  {
    "objectID": "missing_data/pmm/pmm.html#conlcusions",
    "href": "missing_data/pmm/pmm.html#conlcusions",
    "title": "Pattern Mixture Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, PMMs have the advantage of being able to find full data parameters indexing the distribution of the missing data that are not identified from the observed data, making inference more transparent. A potential downside is the practical implementation of these models which becomes more difficult as the number of patterns and unidentified parameters grows."
  },
  {
    "objectID": "missing_data/spm/spm.html",
    "href": "missing_data/spm/spm.html",
    "title": "Shared Parameter Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Shared Parameter Models(SPM)."
  },
  {
    "objectID": "missing_data/spm/spm.html#shared-parameter-models",
    "href": "missing_data/spm/spm.html#shared-parameter-models",
    "title": "Shared Parameter Models",
    "section": "Shared Parameter Models",
    "text": "Shared Parameter Models\nThe shared parameter model approach consists in an explicit multilevel specification, where random effects \\(b\\) are modelled jointly with \\(Y\\) and \\(R\\) (Wu and Carroll (1988)). The general form of the full data modelling using a SPM approach is\n\\[\nf(y,r \\mid \\omega) = \\int f(y, r, b \\mid \\omega)db.\n\\]\nNext, specific SPMs are formulated by making assumptions about the joint distribution under the integral sign. Main advantages of this models is that they are quite easy to specify and that, through the use of random effects, high-dimensional or multilevel data modelling is relatively easy to accomplish. The main drawback is that the underlying missingness mechanism is often difficult to understand and may not have even a closed form.\n\nExample random coefficients selection model\nWu and Carroll (1988) specified a SPM assuming the response follow a linear random effects model\n\\[\nY_i \\mid x_i,b_i \\sim N(x_i\\beta + w_ib_i, \\Sigma_i(\\phi)),\n\\]\nwhere \\(w_i\\) are the random effects covariates with rows \\(w_i=(1,t_{ij})\\), therefore implying that each individual has a random slope and intercept. The random effects \\(b_i=(b_{i1},b_{i2})\\) are assumed to follow a bivariate normal distribution\n\\[\nb_i \\sim N(0,\\Omega),\n\\]\nwhile the hazard of dropout is Bernoulli with\n\\[\nR_{ij} \\mid R_{ij-1}=1,b_i \\sim Bern(\\pi_{ij}),\n\\]\nwhich depends on the random effects via\n\\[\ng(\\pi_{ij}) = \\psi_0 + \\psi_1b_{i1} + \\psi_2b_{i2}.\n\\]\nThe model can be seen as a special case of the general SPM formulation by noticing that the joint distribution under the integral sign can be factored as\n\\[\nf(y,r,b \\mid x, \\omega) = f(r \\mid b,x,\\psi)f(y \\mid b,x,\\beta,\\phi)f(b \\mid \\Omega)\n\\]\nunder the assumption that \\(R\\) is independent of both \\(Y_{obs}\\) and \\(Y_{mis}\\), conditionally on \\(b\\). However, integrating over the random effects, dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\), is induced and therefore the model characterises a Missing Not At Random(MNAR) mechanism.\nThe conditional linear model (Wu and Bailey (1989)) can also be seen as a version of the SPM, which is formulated as\n\\[\nf(y,r,b \\mid x) = f(y \\mid r,b,x)f(b \\mid r,x)f(r \\mid x).\n\\]"
  },
  {
    "objectID": "missing_data/spm/spm.html#conlcusions",
    "href": "missing_data/spm/spm.html#conlcusions",
    "title": "Shared Parameter Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, shared parameter models are very useful for characterizing joint distributions of repeated measures and event times, and can be particularly useful as a method of data reduction when the dimension of \\(Y\\) is high. Nonetheless, their application to the problem of making full data inference from incomplete longitudinal data should be made with caution and with an eye toward justifying the required assumptions. Sensitivity analysis is an open area of research for these models."
  },
  {
    "objectID": "posts/2019-08-03-my-blog-post/index.html",
    "href": "posts/2019-08-03-my-blog-post/index.html",
    "title": "The P value fallacy",
    "section": "",
    "text": "Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a deductive inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a inductive approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the P value proposed by Fisher and the hypothesis testing developed by Neyman and Pearson.\nThe P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.\nHypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive (\\(\\alpha\\)) and type II error or false-negative (\\(\\beta\\)) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this objectivity is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.\nOver time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting \\(\\alpha\\) and power \\(\\beta\\) before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The P value fallacy is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.\n\n\n\n\n\nI personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.\nI feel that, since the two methods are perceived as “objective”, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as “scientific”.\nThis misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as “less reliable” or “more prone to mistakes” compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed."
  },
  {
    "objectID": "posts/2019-09-25-my-blog-post/index.html",
    "href": "posts/2019-09-25-my-blog-post/index.html",
    "title": "MissingHE 1.2.1",
    "section": "",
    "text": "I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function “pattern”. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.\nSecond, I added a new accessory function called “ppc”, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model. The function implements a relatively large number of checks, mostly taken from the R package bayesplot, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention). For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.\n\n\n\nDensity plots for the observed and replicated data\n\n\nI feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution functions, statistics of interest and many others. In addition, these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.\n\n\n\n\n\nOf course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this check is only partial since the fit to the unobserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption because this is based on information which is not directly available from the data at hand and thus impossible to check."
  },
  {
    "objectID": "posts/2019-10-28-my-blog-post/index.html",
    "href": "posts/2019-10-28-my-blog-post/index.html",
    "title": "Copenhagen, I am coming …",
    "section": "",
    "text": "Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some “applied statistics”-related discussions or the attention is more placed on “economics and clinical” matters. From what I heard by other people who routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.\nI know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available here), but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.\nApart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some routine work for some trial analyses at PRIMENT, which by the way is advertising a new health economist job vacancy for those who might be interested. Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervision for a new PhD student at stats and, after I can find some free time, do some research work on my beloved missing data. Am I ready? not sure about that …"
  },
  {
    "objectID": "posts/2019-12-09-my-blog-post/index.html",
    "href": "posts/2019-12-09-my-blog-post/index.html",
    "title": "Not a very good start…",
    "section": "",
    "text": "After some nice holiday break, I came back to work ready for an exciting 2020 … or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.\n\n\n\n\n\nGoing back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my missingHE package, which is available both on my GitHub page and on the CRAN repository. Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.\nAnother good news is that the last paper written with Michael about missing data handling in economic evaluations will soon be published in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.\nFinally, an announcement about the one-day course I am holding together with my mates from the HEART group about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free …). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.\nNow I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let’s try again 2020."
  },
  {
    "objectID": "posts/2020-02-01-my-blog-post/index.html",
    "href": "posts/2020-02-01-my-blog-post/index.html",
    "title": "Finally here …",
    "section": "",
    "text": "The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course “understanding health economics in clinical trials” that me and the rest of our research team HEART have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials. I look forward to deliver this one-day short course together with my colleagues from the UCL PCPH department which will be structured into different sessions during the day of Feb 11th at the UCL CCTU - 2nd Floor, 90 High Holborn, London. The course is specifically intended for those who would like to know more about health economics, which has become an important component in the design, analysis and most crucially, for the funding approval of clinical trials. The course will focus on the following aspects:\n\nA short intorduction to the basic concepts of health economics and why these can be relevent to different people\nA review of different types of intruments and tools used to collect health economic data in clinical trials\nA quick look at decision models with some examples\nA summary of the typical results from health economic analyses and how to interpret them\n\nThe course is still in its pilot form and therefore it is free of charge. If there are still places available, you are very welcome to join and give us your feedback!.\n\n\n\n\n\nSecond, I am happy to announce that my recent paper about the use of Bayesian Hierarchical Models for the Prediction of Volleyball Results has finally been published on the Journal of Applied Statistics. I am really proud of this paper as it is my first solo paper published and because I have always been very invested in the general topic of predicting sport results using probability models. To be able to publish something about this based on my own efforts is very rewarding in terms of the (small) contribution to research that I hope I was able to provide.\n\n\n\n\n\nFinally, I have submitted an abstract to the 2020 European Health Economics Association Conference, which this year will be held in Oslo, Norway. I have now to patiently wait for the review of the abstracts and see if my work made it, either as an oral presentation or as a poster. Fingers crossed!."
  },
  {
    "objectID": "posts/2020-03-01-my-blog-post/index.html",
    "href": "posts/2020-03-01-my-blog-post/index.html",
    "title": "Lockdown",
    "section": "",
    "text": "I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding us, especially in my homecountry (Italy) and particularly in my parents’ region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time. This is pretty scary to all the people living in those territories, who are now forbidden to have any sort of public meeting and are strongly recommended to stay at home. I am afraid this will not be enough to stop the virus from spreading but of course it is useful as it is the only way we have if we want to contain it. The hope is that by summer time the heath will reduce the ability of the virus to spread and give us some time to come up with a possible vaccine in the next months.\nThere have already been attempts to estimate the fatality ratio of the virus using statistical methods. Here I post the tweet from Andrew Gelman\n\nwhich refers to epidemiologists who tried to use Stan for achieving this objective, although an additional reference to another work based on the use of differential equation analysis is also made. However, results are still preliminary and subject to limitations for the type of data and assumptions used. From a statistical perspective I am sure this new epidemic will be interesting to study and I guess lots of funding will be devoted to analyse the upcoming data to get a better idea of the actual threat it represents for the people. I am not an epidemiologist, so I do not have a big statistical interest in this, although I am pretty much worried as any other person. Hopefully, this will become clearer as time passes and let us just hope the number of deaths will not be very high.\nSorry about talking about this here, but from time to time I would also like to highlight was is currently happening around me. As for my research, nothing has changed much for me at the moment and life as usual continues with another busy upcoming period with lots of boring meetings, reports and standard analyses to do, but hopefully I can also save some time to do some methodological work. I am also waiting for the decision about my abstract which I submitted to the EuHEA conference 2020 and which is supposed to be held in Oslo this July. I really hope I have a chance to presenting my work there as I have never been to this specific health economic conference. Fingers crossed! Of course, nobody knows what will happen from here till July and much is to be discussed also with respect to how the spreading of the virus may affect everyone’s schedule in the next months."
  },
  {
    "objectID": "posts/2020-04-01-my-blog-post/index.html",
    "href": "posts/2020-04-01-my-blog-post/index.html",
    "title": "So much time but also not really",
    "section": "",
    "text": "The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.\nAlthough I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do. This months is particularly busy as I am trying to submit a revision for a paper which hanted me for quite a lot of time now and which I must finish by the first days of May. I am also working on side projects but these have been slowed down due to the current situation. I hope I can find the time (and the will) to do some more updates to my website by adding more tutorials and similar stuff. I do have some nice ideas about possible projects and collaboration but I need to wait until after this weird period.\nI am also planning to prepare a new version for my R package missingHE to add some nice additional features for post-processing the results of Bayesian models and to implement new types of models. These, however, will take time, which at the moment is one thing that I do have but that I also do not have.\n\n\n\n\n\nAnyway, not much of an update this one, but I hope things will move quicker in the next couple of months or so."
  },
  {
    "objectID": "posts/2020-05-18-my-blog-post/index.html",
    "href": "posts/2020-05-18-my-blog-post/index.html",
    "title": "Sorry, an error occurred",
    "section": "",
    "text": "It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.\nIn the past few days, I had an extremely interesting email correspondence with one guy (don’t want to say the name for privacy) interested in using my R package missingHE to do some trial-based CEA. Awesome, I thought. He was looking for some advice for how to customise some models in the package and how to get the results he wanted. Unfortunately for my pride, but fortunately for my package, we discovered a small bug in the code when trying to specify a hurdle model (two-part model) to handle the zero costs when not including any covariate inside the logit model to estimate the probability of having a zero cost. Essentially, under these specific circumstances, the function did not correctly backtransformed the estimate of the mean costs on the appropriate scale and the results provided were incorrect. Sorry about that!\n\n\n\n\n\nTo be honest, you can get away by simply including some baseline covariate into the logit model for the structural zero costs, in which case the estimates produced by the function are correct. I have immediately updated the package version to correct this bug on my GitHub page, where you can find the most up to date version (1.4.1). The version on CRAN will be updated at the next iteration as I have recently uploaded the 1.4.0 version in May. In the meantime, if you want to avoid having that issue above, you can download and install the updated version from my GitHub page.\nI think this is also a good chance to tell that I have updated an old paper on my on my Arxiv account. Both the content and title of the paper have changed considerably, but overall I feel that the overall message and quality of the article has improved. It is still an on-going version, but I am quite satisfied with its current status given all the effort I put into it. Have a lool in case you are interested. New title : “Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations”.\nI believe that is all for this update. Not much going on due to the whole lockdown situation here in the UK but hopefully things are improving a little in a two-months time we will be able to at least go to the office. Let’s see."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html",
    "href": "posts/2020-07-07-my-blog-post/index.html",
    "title": "Why be Bayesian",
    "section": "",
    "text": "Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: why not?\nNow, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of probability of a certain event. In fact, although there is general consensus about the rules of probability, that there is no universal concept of probability, and two quite different definitions come from the frequentist and Bayesian approach:\nRather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of both approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.\nFor the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.\nLet us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on Bayes’s Theorem, which tells us how to update prior beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If \\(\\theta\\) is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally revise prior beliefs about \\(\\theta\\), \\(p(\\theta)\\), in light of the data \\(y\\), to yield posterior beliefs \\(p(y \\mid \\theta)\\). In this way Bayes Theorem provides a solution to the general problem of induction, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of how to learn from data. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.\nAs I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:\nThe result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a family of statistical models.\nNote that the posterior probability statements produced by a Bayesian analysis are probability statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.\nThe prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density \\(p(\\theta)\\). We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters \\(\\theta\\) might vary across groups \\(j = 1,..., J\\) , perhaps as a function of observable characteristics of the groups; e.g., \\(\\theta_j \\sim f (z_j, \\gamma )\\), where now \\(\\gamma\\) is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit \\(j\\), \\(y_j\\), are modeled as a function of covariates and parameters \\(\\theta_j\\) , while cross-unit heterogeneity in the \\(\\theta_j\\) is modeled as function of unit-specific covariates \\(z_j\\) and hyperparameters \\(\\gamma\\). Models of this sort are known to Bayesians as hierarchical models, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of Markov chain Monte Carlo (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture models, and flexible semi-and non-parametric models.\nFrequentist inference asks assuming hypothesis \\(H_0\\) is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis \\(H_0\\) (e.g. generate a sufficiently small p value), then we conclude that \\(H_0\\) is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a statistically significant result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic. In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis \\(H_0\\) to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under \\(H_0\\). But what about cases where repeated sampling makes no sense, even as a thought experiment?\nRecall that in the frequentist approach, parameters are fixed characteristics of populations, so \\(\\mu\\) either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a \\(95\\%\\) confidence interval, the correct frequentist interpretation is that \\(95\\%\\) of the \\(95\\%\\) confidence intervals one would draw in repeated samples will include \\(\\mu\\). Now, is the \\(95\\%\\) confidence interval that one constructs from the data set at hand one of the lucky \\(95\\%\\) that actually contains \\(\\mu\\), or not? No ones knows.\nFinally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am \\(95\\%\\) sure that \\(\\mu\\) is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "title": "Why be Bayesian",
    "section": "Conclusions",
    "text": "Conclusions\nThe mathematics and computation underlying Bayesian analysis has been dramatically simplified via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast increases in the computing power available to social scientists means that Bayesian analysis is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.\nThis time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying philosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.\nSo what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!"
  },
  {
    "objectID": "posts/2020-09-10-my-blog-post/index.html",
    "href": "posts/2020-09-10-my-blog-post/index.html",
    "title": "Starting a new adventure!",
    "section": "",
    "text": "Hello dear readers, I have some exciting news about myself and my future which I am eager to communicate on this blog. I know that it is not exactly the most interesting news for everybody but I have recently joined a new research team in the Department of methodology and statistics at Maastricht University, in the Netherlands.\nI must say, it was not easy for me to leave UCL, where I have studied and obtained my PhD degree and spent quite a few years of my life. I am glad that during this time I met so many fantastic people and colleagues from whom I have leared so much and which I would like to thank from the bottom of my heart. I must say thank you to all my previous PhD supervisors, namely Gianluca Baio, Alexina J. Mason and Rachael Hunter, who were incredibly supportive throughout my entire PhD and with whom I also collaborated during my experience as a research fellow at the department of statistical sciences and PCPH. Particularly during my first post-doc position, I came to know so many colleagues and share so many ideas, which became an essential part of my academic activity and personal growth. Among others, special thanks are also due to my colleagues and friends from the HEART team at PCPH with whom I am still collaborating on some interesting research projects.\nAlthough both my personal and working experience at UCL were amazing, after almost 4 years of PhD and 2 years as a reasearch fellow, I felt that it was time for a change in my life. London can be a quite stressfull city to live in and I wanted to see if I could take some new opportunity to enhance my personal growth by taking a new perspective. This is why I decided to accept a new position as assistant professor in statistics at UM, which I have officially joined a few days ago. During my interview for the position, I was really intrigued by the prospect of this job in terms of both increasing my teaching experience at the university level as well as obtaining more independence in regard to the research topics I would like to explore. Don’t get me wrong, I will still work on stats methods for CEA as this will still be the main focus of my research activity for the next years to come. I love it so much! However, it will also be exciting to see how I can use the experience and knowledge acquired at UCL to contribute (and hopefully improve) the current approach in CEA in the Netherlands.\nI already know that, especially at the beginning, it will not be easy. From studying a new language (yes it something that I would like to do!), to getting acquainted with the new place and rules, the new teaching duties, new colleagues, etc… There is much to learn and to do and I am really excited to see where this new adventure will lead me in a few months/years from now. In the meantime I have to do my best to adapt this big change in my life. My new colleagues at the stats department have been very nice to welcome me during these difficult times where interaction with people must be limited to avoid the spread of the virus. I hope this situation will change in the next years and that we can gradually go back to “normality”, if you want to call it that way.\nTo conclude, I would still thank all the people who have supported me during these exciting but also quite challenging times, first among all my parents to whom my unconditional love goes. I will try to post my future updates in a regular way on my website but, especially at the beginning, I will probably need some time to adapt to the new changes and I will be quite busy. I still hope to continue my on-going collaborations with all my previous colleagues from UCL and the UK while also being able to meet new people and start new relationships here in Maastricht and the Netherlands.\n\n\n\n\n\nThank you and see you soon !\nDank u en tot ziens!"
  },
  {
    "objectID": "posts/2020-11-05-my-blog-post/index.html",
    "href": "posts/2020-11-05-my-blog-post/index.html",
    "title": "A couple of updates",
    "section": "",
    "text": "Finally some exciting updates! I really need some good news after all that happened this year. So, first of all I have recently found out that one of the paper I co-authored got published early this year but I actually forgot to check it. The work is an interesting long-term CEA for a new drug in prostate cancer patients, extrapolating the data from an RCT named STAMPEDE. I was only partially involved with this work, which has been mostly done by the talented and always cheerful Caroline Clarke with whom I collaborated during my post-doc experience at UCL. My contribution mostly resolved about double checking the R code for the model which, I have to say, was pretty sophisticated and not always clear given that was originally done by someone else (I we know how understanding someone else’s code can be an hard task). Overall, I am really happy for this paper which I hope may be of interest to someone specialised in that type of analysis and population.\nNext, I am also proud of having contributed to the third round for the course “Understanding Health Economics in Clinical Trials”, which I delivered together with my ex-colleagues and co-members of the HEART team from UCL. This is the third time we have delivered the course in the past two years and I was really impressed by how far we have come since our first attempt. The structure of the course is now very nice and most of the people attending out last edition (in online format of course, thanks 2020!) provided very nice feedback. This, I believe, was the last time we offered the course for free and from the next time we will start charging a small fee to the participants. We have some minor adjustments to make in a couple of sessions, but overall I feel pretty good about it. I would like to thank all my HEART buddies, with a special mention to Ekaterina Bordea, with whom I share the delivery of the final session of the course. Although we were a bit strict on time, and I ended up taking most of it (sorry Ekaterina!), we both got very positive comments from the attendants. I hope I will still be involved with these guys for the future editions of the course as it was really fun.\nAslo, a quick update about my acadmic work. I have written an abstract for a paper together with some very nice people involved in missing data analysis in CEA, including the amazing Baptiste Leurent and Catrin Plumpton. We submitted the abstract to HESG 2021 which has been accepted as a presentation. We still need to write up the actual paper which we need to submit by the end of this month but I am confident we will make it. This work is again about missing data in CEA but based on a different perspective compared with the standard imputation approach and instead explores the use of mixed models as a possible alternative under some assumptions. I really enjoyed working on this, especially with Baptiste with whom I started working on this quite a few months ago. I am not sure I will be able to be present at HESG as I have some heavy teaching duties in Maastircht in that period but I look forward receiving some feedback for our idea.\nI have also found some time to upload on ArXiv a drafted version for a paper I strated working on about one year ago at UCL. The work is a sort of experiment for me where I tried to apply some Bayesian methods for jointly modelling patient-level partitioned survival cost-utility data. The idea is pretty nice, I think, but I will probably need to polish off a few things before finalising the paper. I was also excited to implement something in STAN which I have started using more frequently in the last months. Although in my experience other Bayesian software seem to be pretty good for different analyses, I think STAN has a great potential in the future, especially thanks to all the support and community posting online solutions and code for different types of problems and analyses.\nFinally, I have been involved in some stats teaching for a master course in epidemiology at my affiliated faculty FHML at UM. Everyting was done online, but overall I was happy about how I delivered my sessions of the course and with the feedback I received from the sudents. This was my first teaching experience at UM and, considering the special circumstances most of us are in this year, I think I managed pretty good. Next month I will involved in the marking of the exams for this course while from Jan 2021 I will be quite busy with lots of courses and a quite frightening time schedule. Good luck to me!\n\n\n\n\n\nSo, lots of news but this is also becuase I did not find much time in the past weeks to update my blog. I hope to be more consistent in the future but you never know, especially this year. One thing I am looking for is to explore and visit Maastricht for which I haven’t had a chance since I started my contract last September. Everyone says that the Christmas market is particularly beautiful but this year there won’t be one because of \\(\\ldots\\) well 2020!"
  },
  {
    "objectID": "posts/2021-01-15-my-blog-post/index.html",
    "href": "posts/2021-01-15-my-blog-post/index.html",
    "title": "What is a Bayesian credible interval?",
    "section": "",
    "text": "Happy new year everybody! Yeah, I know it already almost February but I have been incredibly busy the past few weeks after the Xmas break. From getting familiar with the courses I am teaching this term to providing consultancy advice on statistical problems for students from FHML at UM. This last task has been particularly challenging as I did not have a clear idea of what was the statistical background of the students I has to work with and, in fact, the level of statistical knowledge varied considerably depending on the specific cases considered and I had to adjust my way of interacting with the clients on a case-by-case basis. Overall, I have learned a lot in these past weeks, from doing lots of online teaching and consultations, and I believe I am now well prepared to deal with more students, which is something I am looking for now that I have this experience. So, come at me students!\nNow, leaving aside this little off-topic with respect to the content of this post, let us go back to the key aspect which I glossed over last month. The question that was asked by Dr. Morris was how do you interpret a Bayesian credible interval?. In his comment he argued that the interpretation of such interval was not clear when compared, for example, to the frequentist interpretation of confidence intervals, i.e. the range of likely values for the population parameter of interest based on a \\((1-\\alpha)\\%100\\) confidence level.\n\n\n\nHow CIs are computed\n\n\nThe definition relies on the typical frequentist concept of treating sample data as random quantities that can be hypothetically drawn many times from the population and for each of these data a different confidence interval can be computed based on the sample statistics. This procedure ensures that out of all possible samples that I can draw from the population (and therefore out of all CIs I can compute), about \\(95\\%\\) of these intervals will contain the true (unknown) population parameters of interest which are considered fixed quantities. This, however, means that if I consider the specific CI that I computed over my collected sample data, I have no guarantee that it contains the true parameter value. Indeed, it is always possible that I got particularly unlucky with my sample and my CI belongs to that \\(5\\%\\) (out of the many possible that I could compute based on many drawn samples) that will not contain the true parameter value. This means that, although very powerful on a theoretical level, the frequentist argument may not be that useful when I have to focus on the available data that I have, rather than relying on long-term justifications which may not be applicable to my specific situation.\nSo, the natural question in situations in which this approach does not answer ver well at my research question is, is there another way?. The answer is of course yes and one of these alternative approaches is the Bayesian perspective. In contrast to the frequentist argument, the Bayesian approach treats the statistical problem under examination under a different perspective. First, the (unknown) parameters of interest are not fixed but are random quantities generated according to some probability distribution (the prior distribution). Since this distribution is assigned to the parameter before looking at the data, it corresponds to a mathematical representation of the degree of belief that one has about the possible values of the parameter of interest which must be formulated prior to observing the available data (it may be based on previous research or completely subjective opinions, for example). At this point, the Bayesian procedure simply consists in using the available data, which are treated as fixed, to update our initial belief about the possible parameter values through the Bayesian theorem, i.e. \\(p(\\theta|y)=\\frac{p(y\\mid \\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta)d\\theta}\\).\nThis updated state of belief for the parameter values is itself a probability distribution, known as the posterior distribution \\(p(\\theta \\mid y)\\). This distribution represents how, following a rational and coherent procedure intrinsic to the Bayesian theorem, the initial belief about \\(\\theta\\) (expressed by the prior \\(p(\\theta)\\)) is updated based on the available data (expressed by the likelihood \\(p(y\\mid \\theta)\\)). The posterior is always a compromise between prior and likelihood but since typically the contribution to the likelihood is provided by \\(n\\) data points, the weight of the likelihood is often much larger (equal to \\(n\\)) compared with that of the prior (equal to \\(1\\)). Thus, the more data become available the less is the weight of the prior on the posterior and when \\(n \\rightarrow \\infty\\) the posterior is completely dominated by the likelihood. In addition, since the posterior is a probability distribution, it is always possible to directly summarise the uncertainty around certain posterior quantities of interest (e.g.~mean or mode). Thus, we can calculate the range of values within which our estimate lies with a given probability of say \\(95%\\), corresponding to our credible interval.\nSo the key difference between the frequentist and Bayesian approach is from a theoretical perspective of how probability is conceived: a long-run frequency vs degree of belief definition. This has also implications on how uncertainty around our estimates of interest is expressed: a general rule that applies across all possible cases but that does not guarantee the the validity of the conclusions in each specific case vs probabilistic approach whose results specifically applies to the case under consideration but that does not guarantee that its conclusions are valid across all possible cases.\nA hardcore frequenstist may also point out that Bayesian inference requires prior distributions that are reasonable as otherwise they may drive your results away from the truth. However, my reply would be yes, it is always important that priors are defined based on some available and plausible information (or simply use very loosely defined priors in case such knowledge is not available), but the question is how you define the truth. Is it a single value? In my opinion it is more reasonable to think at the truth as a distribution in that models are only used as a way to simply the reality and not to precisely represent it. Not rejecting a true null hypothesis may be ok but what then? should I continue testing for many different values for what I believe is the true population parameter? Instead, I would prefer to make assumptions about what are possible values for the parameters (therefore accounting for uncertainty around which values are more/less likely a priori) and then use the data I collect to either support/contrast those assumptions.\nThe appealing of the Bayesian approach is precisely to be very practical: combine the current data and an initial assumption about parameter values that generated the data to update my initial belief (in a rational and coherent way) and quantify how the uncertainty around the likely values for the parameter has changed after observing the data. This straightforward interpretation of how to deal with expressing the uncertainty for any unobserved quantity (parameter, prediction, missing value) makes the Bayesian approach optimal for handling decision-making problems where the quantification of the uncertainty around the conclusions is a fundamental element in the decision process."
  },
  {
    "objectID": "posts/2021-03-25-my-blog-post/index.html",
    "href": "posts/2021-03-25-my-blog-post/index.html",
    "title": "Sunny days which I cannot fully enjoy",
    "section": "",
    "text": "Here we are again. Except now it feels like a very nice spring time here in Maastricht with beautiful sunny days an warm weather. The picture does not really represent the environment in this region of the Netherlands (called Limburg), but I thought it was a very nice picture to put as thumbnail.\nI am sorry for this very short and boring posts but this period was a bit busy with lots of work. Hopefully, I will be able to enjoy a short break for a couple of weeks until the next round is up. Anyway, no big updates from me as in this period I have considerably enhanced my teaching and tutoring skills for different groups of student in a undergraduate epidemiological and statistical course here at UM. At first it was a fun exercise and the topics is always nice to review. However, after a couple of months of seminars and lectures I needed some time off from teaching as I was depleting my daily batteries so bad that I only had the energy to survive during the last period. The next course I will teach will start in middle April, so I hope I will have enough time to recover and be ready for the new round.\nDue to my involvement with the teaching I did not have much time for pretty much anything else. I continued doing some consultancy work for the students from other departments who required some assistance in their statistical analysis. Nothing too complex but sometimes it is necessary to go in details with the students in order to understand what they really want as I pretty sure most of the times this is unclear to them as well. At the moment I have a constant number of consultancy sessions per week and so far I had generally positive experiences. In one of these I also had the luck to be involved as a co-author in one health economic analysis done by one medical PhD student from UM. I assisted the student throughout a series of meetings where we went through some quite advanced statistical modelling approach, including multiple imputation, mixed models and bootstrapping. Luckily I was able to teach these methods in R as otherwise things could have become quite long and slow using other software such as SPSS. I believe the analysis is now towards its conclusion and I think the student demonstrated some very nice abilities and skills for doing some advanced coding and interpret the results from the analysis.\nIn the meantime I am still involved in a couple of other reserach projects which however are slowing down due to my lack of time. The ideas I wanted to explore and discuss with other colleagues are cool and potentially interesting but the number of hours in a day are not enough.\n\n\n\n\n\nOne happy note comes from one of the paper I had previously submitted to a journal which has been now officially approved for publication. Here you can find the ArXiv version of the paper which focuses on the implementation of Bayesian methods for the analysis of trial-based partitioned surivival cost-utility data as possible way to deal with some complexities of these types of analyses. This was I side project I decided to explore while I was still in London and then I finished it while I had some free time after my enrollment here at UM More info about this in the next weeks/months.\nFinally, I have received the prestigious title of honorary lecturer at the stats department at UCL, which is essentially just an official requirement that I need to possess in order to continue working with my colleagues from London and to continue supervising a PhD student for a speicfic project (of course about missing data!). Anyway, now I feel like a very important person (not really but hey that’s a cool title to add to the CV!)."
  },
  {
    "objectID": "posts/2021-05-15-my-blog-post/index.html",
    "href": "posts/2021-05-15-my-blog-post/index.html",
    "title": "The good, the bad and the ugly",
    "section": "",
    "text": "A quick update from me about a couple of things, some good, some bad and other \\(\\ldots\\) ugly.\n\nLet’s start with the good. I have found the time to work on some old projects which are finally progressing in their late stages. The most important news is that my article on Bayesian methods for partitioned survival cost-utility analyses has been published in pre-print form and is available for free here. Soon the offical version will become available as open access which is really nice if anyone would be interested in my methods. As for the other projects, at the moment I cannot say too much but they are all very interesting and involve the collaboration with some cool people I had the privilege to work with during my past experience at UCL. Hopefully, in the next couple of months some concrete proof of these collaborations will become publicly available and I am excited to talk about these projects which I think have a huge potential. I also have a couple of ideas for some future projects and I hope during the summer to have some extra time to dedicate to them. I would particularly like to start coding up some Bayesian statistical methods into a new R package to make these more accessible to everybody and I have some coll ideas in mind. It is only matter to find the time to put them into practice!\nNow the bad. The weather in the last period has not been great here in the Netherlands with lots of raining days which, coupled with some extensive teaching duties, did not really help to put me into a good mood for doing some research. I managed to work on some on-going projects but I could not find the time to start new projects or going back to some old ideas which I still have in mind. I am also haunted by thesis consultations with some master students which take lots of time and in this period I cannot find the energy to do anything else apart from routine activities. In addition, the situation of the vaccines does not seem to improve very quickly as my age group will likely receive the vaccine only towards the end of June which means that it may not be possible for me to go back to Italy to see my parents and friends during the summer as I might be asked to be back for the second dose. I hope this situation will be solved before the start of the new academic year. I really miss traveling!\nFinally, the ugly. I have some harsh deadlines which are coming up with respect to some joint projects on some applied works. Nothing particularly complex but they tend to be time consuming and require my attendance to many meetings which I do not find extremely interesting, let’s just say that. The next couple of weeks will be crucial and I really need to find some time to be ready and focus on finishing off these projects which I have dragged from a long time. I have also received confirmation that my invited talk R-HTA 2021 workshop this July will be totally online which makes me a bit sad as I find it really difficult to discuss my research without a physical audience in front of me. Next month I am also volunteering for presenting my work at my department and I need to come up with some good ideas for summarise my entire research in a way which does not completely bore my new colleagues. Seems like a mission impossible if you ask me but I will give it a shot as soon as I have some time.\n\n\n\n\n\n\nSo, many good things are in development but there are also bad things which I must first attend as well as some ugly things from which I cannot really escape. Wish me good luck!"
  },
  {
    "objectID": "posts/2021-07-02-my-blog-post/index.html",
    "href": "posts/2021-07-02-my-blog-post/index.html",
    "title": "A nice workshop",
    "section": "",
    "text": "I have recently been invited to give a talk about my R package missingHE at the R for HTA summer workshop 2021, and I would like to spend a couple of words to describe what was my experience at this event which for me was a first-time. Here you can see one of my tweets about the event (I will try to use Twitter more often in the future, promise!)\n\nIn general, I found it to be a very interesting conference with a crazy amount of people coming from all over the world who were interested, in one way or another, in the use and application of R for health economic assessment. To be honest, the fact that the workshop was fully online, for obvious reasons, may have encouraged lots of people to join compared to what would have been a standard in person attendance. Even so, I am quite impressed by the many people engaging in discussion and the very interesting topics raised. I felt that my presenation was very appreciated and I had a ton of fun discussing some missing data things with some of my colleagues at the workshop, including Manuel Gomes and Gianluca Baio who I know very well from my past at UCL. However, missing data analysis can be really hard to grasp from a mere presentation and I hope I simply gave some intuitions to some of the people in the audience about the importance that missing data assumptions cover in any analysis.\nAside from my personal stuff, which I really love despite not being very appealing to everybody, I have to pay my compliments to the organisers of the the event which ran smoothly and with some very interesting back and forth discussions between people coming from different places and positions but all with the common interest in HTA analyses. In particular, the discussion panel following my presentation was very interesting as it opened up a “hot” argument in regard to the use of R for HTA in the industry. All points discussed were valid and I felt that two of the main concerns which hold people in the HTA industry from using R in their analyses are:\n\nClients do not like going through the R code in order to understand the model structure and results. They have been used to a standardised way to report the model which is often based on an Excel spreadsheet in which they can play around with the cells and see the results for themselves. Since they are unfamiliar with R, they do not want to spend extra time trying to figure out what the model is like based on the R code. However, they are well aware that Excel is not able to perform any advanced statistical modelling and that all models are nowadays implemented in software such as R.\nThere is a general concern about the “quality” of the R packages available on CRAN or on the individual GitHub repositories of different developers. Many people made the comparison with software such as STATA or SAS which cannot be freely updated by the users but where there is a reference body who is in charge of making all the testing and checks before a new version or update is released to the public. On the contrary, R packages do not have such checks as the only requirement for a package to be uploaded on CRAN is that is does not crash when called.\n\nI agree that both points are valid and in fact this is something which should be seriously taken into account in order to encourage people to use R more often. If I may say my humble opinion, what I would argue is that the presence of a “unique” and “validated” reference for the checking and updating of the software routines is not necessarily an advantage in all situations. For example, if someone develops a new model which has not been yet implemented in any current functions, then it will be very difficult that a new version of the software is released simply to include this new model but it will be more likely that a certain amount of time will pass (so that more updates can be included all together) before the new commands become available to everybody. In this case, a software like R gives the chance to any developer to implement their method within a new package which can be uploaded and made available to everybody in a very short time. Of course, it also important that proper testing is carried out to ensure to minimise the amount of bugs or issues that may arise from using this new functions. Although these problems occur more frequently within a software framework such as R, the free-user nature of the software allows everybody encountering an issue in the use of the package to contact the developer, point out the problem and ask for a solution which may become available to everybody in a matter or days or even hours! I personally think that going back to a unique controller for checking the quality of all packages goes against the spirit behind the use of R as a free-user software that anyone can use to create, update and extend packages so to make them available to everybody without the need to wait for an external and impartial controller to do the checks. People will always find some issues or bugs for a new package and, in time, the more people use it the more the corresponding functions will be tested and will ensure a high quality resource for any newcomer.\nAs for the “problem” with the clients, I think this is a very delicate issue as, like in any private sector, it is important to try to match the needs of the client as much as possible. At the moment I think the standard approach is to implement the model in R and then “copy” the results into an Excel spreadsheet giving some powers to the client in terms of changing some inputs and see how this affects the results. This is of course very time consuming and also frustrating at times. A possible solution would be to use the web version of R called R Shiny which is a sort of user-friendly interface which allows people to play around with a model developed in R in a fashion which resembles the familiar Excel output clients are used to see. It is not perfect as, most of the times, the amount of modifications which are allowed to the clients is quite small and it is not really possible to perform any serious debugging of the code unless looking directly at the R code itself which raises the same problem as above. Personally, I think that it will be a matter of time until Excel outputs will gradually go away from the international landscape of HTA. This has already happened in most of academia, particularly in the UK, where statistical analyses are performed using statistical software. Soon the need to implement and improve more complex models in order to comply with the regulations and guidelines of decision-makers will make the simple idea of using Excel as something obsolete and both consultants and clients will need to adapt to the new standard, which is likely to be a statistical software such as R.\nMaybe it is still not the time, but we are getting there I am confident in this!"
  },
  {
    "objectID": "posts/2021-09-20-my-blog-post/index.html",
    "href": "posts/2021-09-20-my-blog-post/index.html",
    "title": "Back to work",
    "section": "",
    "text": "Well those holidays were pretty short in my opinion!\nAnyway, time to get back to work and, in fact, it has been already a while since the normal routine started. I try to keep my posts updated as much as I can but with the approaching of the next academic year things are a bit hectic. New and old courses, tutorials, preparation of new teaching material, coordination with other lecturers, consultation with students and researchers, etc… I just need to get back into the mix but sometimes it feels a bit overwhelming. Not that I complain but I just need a bit more time to find the right balance between all these “required” activities and my research which has been left a bit behind if I want to be honest. Hopefully, in a couple of months things will slow down and I will have some free to time to continue doing some nice research work. I really miss that now !\nEnough about my lamentations, let’s see if there is someting interesting that I can talk about in this, otherwise, plain post. Well I always wanted to give my opinion on a very interesting, although now a bit dated (2001), health economics article by the emeritus professor Antony O’Hagan, to my knowledge still working at the University of Sheffield. This I think the first general health economics publicaton addressing the topic of the need to develop a consistent and flexible statistical framework for the analysis of health economic individual-level data. The article is entitled “A framework for cost-effectiveness analysis from clinical trial data” and is a marvellous example of what many people have replicated and expanded over the following years (including myself), namely the call for a comprehensive analysis framework which can be generally applied for the analysis of cost-effectiveness data which takes into account the typical statistical issues that characterise these types of data.\nIndeed, for many years health economists did not rely on a solid general approach for the modelling and analysis of the data collected from clinical trials as in most cases focus of health economics literature was on decision modelling or aggregated data analysis, with little methodological emphasis on how practitioners involved in clinical studies should handle the specific problems that affect these data, i.e. from missingness, bivariate outcome, covariate adjustment, etc… Antony’s paper was for sure the first reading I was exposed which provided an idea on how things could be done in a more consistent way through the introduction of a Bayesian analysis framework and a clear justification for the adoption of this statistical approach in the field of health economics. This was already explored in the past by other authors (e.g. Claxton for sure was one of the first ones recognising the advantages of using a Bayesian approach) but no clear guidelines or implementation modelling strategy was available for standard analysts to look at.\nIn my opinion this is a very underrated publication and someone may disagree with me when I say that I find this to be a milestone in the statistical analysis literature of health economic data. I rarely see this paper cited in modern articles or publications, although many of these propose a similar concept to the one Antony gave in his paper, i.e. the call for a statistically-sound and reliable analysis framework that analysts may replicate in their analyses to obtain more consistency in the methodology used as well as taking into account the possible problems affecting the data that may lead to biased results. Of course, today things have changed and new publications provide more advanced and alternative approaches to deal with these problems, but the general concept behind this stil remains the same and I think we all owe, at least partially, Antony for what he tried to achieve with his paper. I am not sure how much his work influenced current health economics practice but it has for sure affected me and my research considerably.\nI would encourage any health eocnomist to read this paper as it gives a nice picture of why a statistical framework for the analysis of cost-effectiveness data is something that is desirable and that people should try to implement in thier analyses. Key aspects include the possibility to:\n\nSpecify the model for effect and cost data in a flexible way by expressing the mean population parameters for the two outcomes as functions various model parameters\nThe advantages provided by the Bayesian framework to implement this framework while also naturally accommodating the propagation and quantification of uncertainty surranding the parameters of interest\nThe convenience of using this framework to handle statistical issues such as correlation between outcomes, covariate adjustment, and provision of standard output such as cost-effectiveness planes and curves\n\nThere is also a clear “defense” of the use of prior information in regard to the modelling of these parameters within a Bayesian framework. Long story short, appropriate use of informative priors does not bias the inferences in the sense of favouring one treatment over another in a “subjective” way. Prior information is essentially an extra tool available to analysts which may also be the only way to incorporate some external information (i.e. expert opinion) into the model which would otherwise be discarded therefore resulting in an effective loss of information. Of course, it is important that the way this information is elicited into the model is reasonable and that it actually reflects the information available from external sources. However, there are plenty of ways to perform these assessments to check this in a rigorous way. I find it annoying that people would discard the use of prior information simply because they are not “sure” it is correct to use it. As any source of information, external information is information and as such it would inefficient, whenever this is available, to ignore it. Sometimes ignoring this information may lead to biased conclusions as well, e.g. in the case of missing data where observed data information may not be enough to obtain reliable results.\nIn such cases, wouldn’t be more reasonable, although I agree more difficult, to think about what type of other information is available to us and how this can be incorporated into our analysis in order to better assess the robustness of our results compared to, say, simply cover our eyes and pretend no information exists?"
  },
  {
    "objectID": "posts/2021-11-20-my-blog-post/index.html",
    "href": "posts/2021-11-20-my-blog-post/index.html",
    "title": "And now what?",
    "section": "",
    "text": "As the spectre of a new lockdown approaches the Netherlands, I find myself wondering what are the prospects for the upcoming months. Well, hopefully, things will be better than last year given that the amount of vaccinations is pretty high and there should be no big problems in terms of mobility for those who were vaccinated. This is good as I intend to pass Xmas together with my family and friends back in Italy, unlike last year. However, the fact that the spread of the virus continuos in spite of the high number of vaccinated people poses a big question of what will happen in terms of work. Does it remain fully online or a mix between in-person and online research/teaching will be required from now on? going back to fully in-person work seems pretty unlikely, at least for now and for the next few months. I must say that, although I feel quite ok in working from home in terms of research, I think students really need an in-person discussion and feedback in order to improve the efficacy of what we want them to learn. I experienced this myself by comparing the teaching I did last year with respect to this year, and I noticed how students get so much more from an in-person lecture/tutorial compared to an online one. The context and environment is not the same and I am pretty sure attention is quickly lost when people can have access to easy distraction elements, especially when they attend from the comfort of their own house. With this I simply hope that at least the teaching part of my job will remain on-site as I believe this is the most effective way to teach students and capture their attention, of course if this does not pose any real threat to their health. Personally, I also feel much better knowing that I need to go to an office at least once a week where I have the chance to chat with colleagues and fuly focus on my work and duties but I would also understand who prefers to work from home because of family or other more important issues.\nAt the moment I have a bit of a break from my teaching and I am focussing on my research (finally!). I need to take advantage of this moment as much as possible as form January things will become again a bit hazy due to the many courses I will be involved with. I have started and continued projects with different people both here in Maastricht but also with some old colleagues I met in London with whom I still have contact. Hopefully, I will be able to finalise and submit at least a couple of papers before the end of the year and enjoy my break afterwards. I will also need to look up for some grant applications for the upcoming year as this is an important (although often annoying) part of my job. The unfortunate thing is that, in the Netherlands, there is no national grant specifically dedicated to statistics but applications should be made with respect to some specific research field such as medicine, engineering, neuroscience, etc… I am not a big fan of this type of applications as the lack of statistical background in the people sitting in the evaluating committees is often the reason why applications too focussed on statistical aspects end up being rejected. I suspect this is what happened to my application at the VENI research scheme last year, despite the fact I received a good evaluation of my CV. It also makes sense as if people are not going to understand how your work can be applied in practice to enhance people’s lives, then there is no way they are going to decide in favour of your proposal compared to some applied research work in an area they are familiar with, for example. I need to figure out a smart way to combine my interest in developing new stats methods with some interesting applied work, at least from the perspective of the examiners. This is not easy as competition is also strong for these types of grants and I need to ask for some help from more experienced colleagues to learn how to write things in a more appealing and interesting way for someone who does not know much about statistics.\nFinally, I have also started looking for some future conferences to attend (hopefully in person) in the upcoming year. I found a couple related to HE that I would eager to attend.\n\nThe first is a Dutch-specific HE conference called lola HESG, which this year will be held here in Maastricht. From my understanding this was established in a similar way to what HESG has been organised in the UK, i.e. as a way to share knowledge and build connections among people involved in the analysis of HE data and problems. The format is also very alike as authors are asked to present and discuss the work of colleagues, rather than their own, and to discuss this with someone else. Based on my experience at HESG this format can be really helpful, especially for people just starting doing reasearch to find some possible collaborations or new ideas to work on. Given that it is here, I would have basically zero costs to sustain and therefore I hope I will have the chance to join this conference and meet some new faces.\nThe second is the EuHEA conference 2022, which will be held in Oslo. I have never been to this type of conference but I know that it mostly deals with HE problems and appications for UK and EU countries. I would like to attend this one too as it will provide the chance to receive some nice feedback from expert authors in the area of the statistical analysis of HE data. Although I am quite experienced with it, there is always room for improvement and personal development and I believe this could be the perfect occasion to have an update about what other people in this sector are up to (I have been left a bit behind since last year given that I missed most of the online conferences). This issue is that this is typically not a cheap conference and I find myself in troubles in the event the department will not be able to fund it (given that I might also attend the other conference in the same year).\n\nSo, overall lots to look forward to in the upcoming months where I hope I can start working again on my research projects that were put aside. There are still lots of things which are uncertain but I need to start planning my work since now to avoid potential on the go. And in the event such problems arise, as always, we will do the only thing we can do. We will adapt!"
  },
  {
    "objectID": "posts/2022-02-05-my-blog-post/index.html",
    "href": "posts/2022-02-05-my-blog-post/index.html",
    "title": "Studying Item Response Theory",
    "section": "",
    "text": "Hi everybody, I am happy to be back on my blog to talk about something new and interesting, at least to me it is! Recently I have been so busy with routine works that I had little time to focus on my personal projects, which is something I definitely need to change in the upcoming weeks. Taking by remorse, I decided to force myself to start looking at some new and interesting research lines which could be linked to my current expertise.\nWell, it was since last year that I wanted to dedicate more time to study and learn about the theoretical statistical framework of Item Response Theory, which I find really fascinating. So, last week I started giving myself a bit of time to review some of major handbooks and manuals on the topic, which I only knew in a partial way. Specifically, given my involvement in trial-based analayses, I was quite familiar with concepts such as validated multi-item questionnaires, latent constructs, Likert scales and so on, but I never delved into the whole IRT topic due to time constraints and other priorities until now. I am still reviewing lots of literature as I feel there are concepts/notions which I do not grasp very well yet, but recently I have been reading the Handbook of modern IRT by Wim J. van der Linden, as a general manual providing the basics of the theory and most popular modelling approaches.\nVery briefly, what I learned so far is that IRT theory was born as a way to theoretically and, most importantly, mathematically link the probability for a respondent to give a specific answer to an item within a questionnaire to an underlying latent construct or ability. There is a variety of model classes, each associated to a specific type of item question (e.g. binary, ordinal, categorical) and, within these classes, a there is a range of different model specifications each associated with different assumptions to provide an estimate of some latent abilities of interests while also trying to control for the influence of some other nuisance parameters (e.g. item difficulty) through a rigorous specification of the mathematical functions linking these two types of parameters. I have still lot to cover, but the premises so far are very exciting to me as this can be genuinely seen as a latent analysis problem where an unobserved parameter is estimated by assuming an inherent inductive approach in which observed data (i.e. individual item-answers) and are used to learn something about the latent parameter. Does this sound familiar to you? well to me damn yes, this is the basis of the Bayesian analysis! In fact, Bayesian analysis of IRT data is very popular as their inherent hierarchical structure perfectly matches the flexible Bayesian modelling framework where parameters are random variables to which distributions are assigned to represent their level of uncertainty. This is done prior to observing the data, which can be extremely helpful to incorporate external assumptions about these parameters in the model, as well as after observing the data, after updating the prior in a rational and coherent way (i.e. through Bayes’ theorem). This natural advantage of the Bayesian approach and the notorious difficulties that standard frequentist approaches encounter when running very complex and hierarchical models make the adoption of the Bayesian philosophy not only intuitive but also very practical.\nI think that there is so much potential to work on this topic and to see if there is room to expand the current methodology in some new directions. Of course, I am aware that many different Bayesian models have been applied to these types of data but I feel that there could be space for opening up some new research opportunities that have been only rarely touched before. For example, when collecting questionnaire data, especially self-reported ones, missing values are common and almost inevitable. This further complicates the analysis since assumptions about these unobserved answers must be made when running any type of model. In most cases I see that standard and simplifying assumptions, such as missing completely at random (MCAR), are made to avoid any terrible increase in model specification. Although certainly possible in some cases, such assumptions are not very likely to hold in realistic scenarios (e.g. it is unlikely that people who did not answer at some items are not systematically different from those who completed the questionnaire). So, what if I would like to explicitly state the missing data assumptions within the IRT framework? for example including a selection or pattern mixture model to assess the impact of informative missingness assumptions on the results? This for sure becomes a nightmare from a methodological perspective but I really think it is our duty to evolve this field to improve the methods.\nPossible new research work? grant proposal? could be. I just need to find the time to look at this with calm and critical thinking. Is there anyone out who might be interested in a joint work on this? I am always available!"
  },
  {
    "objectID": "posts/2022-04-10-my-blog-post/index.html",
    "href": "posts/2022-04-10-my-blog-post/index.html",
    "title": "New paper out",
    "section": "",
    "text": "Hello there! (do you get the movie reference?)\nJust a quick update from this month as I have recently received confirmation that a new paper I was involved with as first author has finally been published on Health Economics!!! Hurray! This is a work I am quite proud of as I was the guy who had the initial idea behind the concept of the paper and I was lucky enough to find some very helpful colleagues from the UK, most notably the very nice Baptiste Leurent and Catrin Plumpton who helped me out with laying down my ideas and give it an interesting appeal for the general health economist’s audience.\nThe main objective of the paper is nothing revolutionary or extremely innovative but rather on the importance of spreading awareness about the use of a specific type of modelling approach that has been rarely adopted in the context of health economic evaluation based on trial data, namely linear mixed effects models (LMMs). Indeed, most of the references to this approach in the health economic literature is related to the use of LMM to deal with clustering at the level of centres/countries which, do not get me wrong, is totally correct. However, I have noticed over time that LMMs have almost never been adopted in trial-based CEAs for the standard analysis of utility and cost data collected at different time points. This I think is a big gap in the literature as many analysts may simply ignore the potential use of LMMs for analysing these data given their lack of implementation in the literature.\nAfter reaching out to my colleguaes to have their opinion on the matter, we all agreed that it could be nice to lay down the coding and rationale for using LMMs to perform standard cost-effectiveness analyses based on trial data. Of course, as any method, there are advantages and drawbacks when using LMMs compared to the standard OLS models fitted at the level of aggregated variables (e.g. QALYs) instead of modelling utilities at each time. I will not go into much detail about these pros and cons (perhaps have a look at the paper if you are intrigued) but I will highlight two main points:\n\nWhen dealing with missing outcome values (i.e. always) modelling the disaggregated longitudinal data has the practical advantage the information about the reason for missingness can be more intuitively incorporated into the analysis and additionally allows to avoid some potential loss of power or even bias compared to standard complete case analysis approaches. The nice thing is that LMMs are also valid under standard Missing At Random (MAR) assumptions with the additional benefit that estimates can be derived without the need of using multiple imputation procedures unless some auxiliary variables are also considered into the analysis that are related to missingness. Thus, when then performing bootstrapping, no practical issues arises in terms of choosing the way to combine multiply imputed data and bootstrap replications which instead commonly occurs in standard analyses.\nLMMs allows for a quite flexible specification of the covariance structure of the data over time which can be tested from the observed data. Retrieval of the effects if interest (e.g. mean QALYs or total costs per arm) can be easily obtained by linearly combining the parameter estimates from the fixed effects part of the model depending on the parameterisation chosen. This can also be done in a straightforward way using common software packages (e.g. R or STATA) and does not require more than one or two lines of code.\n\nOf course, there are also downsides to be considered. For example, the use of multiple imputation to account for auxiliary variables or bivariate modelling to deal with the correlation between utilities and costs is not so easy to do. This is perhaps an idea for future development of the methods in the context of CEA. Well, my general solution is quite simple, just go Bayesian and get rid of all your headaches! However, I am not sure all practitioners will agree with me on that. Anyway, I invite you to have a look at the paper if you are involved in CEAs as it may provide some interesting thoughts for you to explore in relation to the methodology to use when analysing trial data and the potential implications of ignoring missing values in a context where the longitudinal nature of the data should be taken into account.\nHave a nice read!"
  },
  {
    "objectID": "posts/2022-06-15-my-blog-post/index.html",
    "href": "posts/2022-06-15-my-blog-post/index.html",
    "title": "Engaging with the HTA community",
    "section": "",
    "text": "Hello folks, it has been a while since I updated my posts but it has been a very busy period I am afraid. Enough with excuses let’s talk about the news!\nFirst, last month I had the chance to participate to one of the most important HTA congress in the Netherlands, called lolaHEG, where researchers and workers in the field of health economics meet up to discuss the novel advances and research ideas in relation to HTA and health economics in general. This is a annual event here in the Netherlands that was created based on the inspiration taken from the UK HESG format, which I had already the pleasure the attend a few times in the past. This year was held here in Maastricht (very fortunate for me!) and attendance was over the roof, probably because of being the first one after corona.\nWhat I really like of this format is that it is unlike any other congress I have been so for, in that participants are encouraged to read and present the work of someone else during the congress, which at first might seems a bit weird. But it works really well. Since you are forced to read the work that you did not do, it is much eaier to catch potential issues or ask critical questions that provide some nice and constructive feedback. Of course the authors of the papers have the chance to reply to the presentation of their work and to explain something that was missed, but the main point of this conference is to engage in reading the work of others rather than simply remembering your own work that probably nobody else is able to deeply understand. Thanks to this format then there is also the chance for the public to intervene in the discussion and ask questions on the basis of those moved by the presenter. I have participated in this type of congress since I was in the UK and I must say that its peculiarity makes it very suited for constructive discussions and developing new ideas.\nThis is what happened to me this year as well. I presented some new work on a statistical methodology review for HTA analsyes in the Netherlands in the past few years, see my pre-print version here, and I received lots of feedback from a varied audience who demonstrated interest in my work, from practitioners using these methods, to HTA directors, people working in regulatory agencies and scholars. I must seriously thank in particular a few people: Manuel Joore, Mohamed El Alili, and Geert Frederix. They all provided lots of feedback and suggested possible changes to my research work to correct some misinterpretation I had when starting working on this project by myself due to my ignorance of how things work here in the HTA regulatory framework in the Netherlands. I will definitely take their suggestions into account and I would even like to invite them to join my project to make sure I can adjust my work and avoid any further misinterpretation about this work which simply started as a standard review but which seems to be expanded towards a more general objective of recommendations and critical analysis of the current state of play for the HTA analyses in the Netherlands. I also had the chance to meet some amazing PhD students who came from all over the county to join this yearly appointment. The event only lasted two days but I had some real fun and met many new faces with whom I really hope I can still keep in contact. If this was not enough, the entire event was brilliantly organised by Maastricht University and in particular by the leader of the Health Service Research department Mikael hiligsmann and the recently appointed director of CAPHRI Silvia Evers who I really need to thank for setting up an amazing congress experience (beautiful location, delicious food, crazy party). I mean, look at the place of the conference, top!\n\n\n\nThe amazing Kasteel Vaeshartelt in Maastricht\n\n\nThe conference was also the perfect opportunity to enter in contact for the first time with many different researchers involved in HTA in the Netherlands and with whom I really hope I will be able to collaborate in the future. I have plenty of ideas of what and how to develop new stuff, now it only a problem of finding someone who shares my objectives and most importantly the time to put them into practice.\nAfter my coming back from the conference the crude reality of previous open projects hit me quite hard but hopefully they will be done in the next month or so. In the meantime, I am preparing myself for a new adventure. Yes, I am going to attend and present my work at the EuHEA 2022 conference that will be held in Oslo this year. This is the first time I am attending this conference and I am eager to join another interesting conference and meet up new and old people working in my research field. And if there is another amazing party, that’s even better!"
  },
  {
    "objectID": "posts/2022-08-20-my-blog-post/index.html",
    "href": "posts/2022-08-20-my-blog-post/index.html",
    "title": "Back from holidays",
    "section": "",
    "text": "Hi folks, it is good to be back on the blog after some nice vacation spent with my family and friends. This is one of the best parts of the year as people have not all come back to work and I have a bit of time to do my own things without being bothered with crazy requests. However, it is also a very anxious part of the year as you can definitely count the number of days this will last, which are not many. Once the machine starts, it will be usual work once more and I really need to avoid thinking about that if I want to survive.\nAs for my news, not too much going on due to my vacation since I came back from my EuHEA 2022 conference in Oslo (nice times those!). I have been invited to give a talk on September 5 at a health economics seminar at King’s College London by the lovely Lily, which I am eager to give in order to receive some feedback and understand what kind of audience attends these types of seminars. I will present my new and ongoing work about missing data analysis in the context of health economic evaluations, which I already presented this year at another seminar at Health economics and health technology assessment department at VU and which had a very nice impact on the audience. The more feedback I am able to gather to develop this idea, the better it will be for finalising the project.\nWhat else? well, I still need to catch up with some old colleagues for existing collaborations and projects that I need to dive into once again. I have a few meetings already planned in the upcoming days and I hope to catch with these things as soon as possible (also because I will risk to forget some of these if I don’t!). I am also finalising my VENI pre-proposal call that I would like to submit before the start of September. Last time it did not go very well for my application but this year I have gathered much more feedback and inputs from different colleagues on my research idea and I hope i will be able to make it through the pre-proposal phase, but let’s see because these grants applications can be very annoying sometimes! I am really convinced that my idea has some strong potential for the future of statistical research in the context of health economics and I hope the referees and the committee members will be able to see it too. However, the final outcome may also depend on the amount of projects submitted which is typically a lot, so competition is really harsh for these grants. Fingers crossed!\nLastly, I also need to catch up with my teaching material which I started developing before leaving for holidays. I am at a good point but at the beginning of September I will need to coordinate a new bachelor course on a subject that I need to re-study a bit since it has been a while since last time I reviewed it. In the upcoming days I will make sure to catch up with these things. The annoying part is that the new semester will start soon and with that a lot of routine things which take a lot of time from my research, such as answering emails, set up meetings, etc … Ehhhhh (sigh)."
  },
  {
    "objectID": "posts/2022-10-10-my-blog-post/index.html",
    "href": "posts/2022-10-10-my-blog-post/index.html",
    "title": "How to handle longitudinal data in economic evaluations",
    "section": "",
    "text": "Hello and welcome back for a new update on my blog. Today we go through another “practical” post where I provide some suggestions and examples on how to fit health economic evaluations and learn how to us the appropriate statistical methods to get the results you want. Last post we had a look at how bootstrapping could be implemented for analysing aggregated health economics outcome data (i.e. QALYs and Total costs) collected and computed using data from a trial. Now, we take a step back as calculation of such aggregated outcomes is typically done on the basis of some longitudinal data that were collected during the duration of the trial at different time points. For example, it is common that QALYs are compute using an Area Under the Curve approach as a weighted function of quality of life utility scores:\n\\[\n\\text{QALY}_{i} = \\sum_{j=1}^J \\frac{(u_{ij-1}+u_{ij})}{2}\\delta_j ,\n\\]\nwhere \\(u_{ij}\\) is the utility score evaluated for the \\(i\\)-th patient at the \\(j\\)-th time point in the trial (with \\(J\\) being the last time), while \\(\\delta_j\\) is the fraction of total time period (usually 1 year) between two consecutive collected measurements (e.g. if 6 months, then \\(\\delta_j=0.5\\)). The individual utility scores \\(u_{ij}\\) are obtained after applying some national tariff system values to patients’ answers to some validated questionnaires, e.g. EQ-5D-5L. For each individual, utility scores are calculated based on their answers to the questionnaire items at each time point and the tariff system (usually specific to each country) is used to numerically “value” their health states according to some pre-specified and assumed population model.\nSimilarly, Total costs are computed for each individual as the sum of all corresponding cost components collected at different times during the trial\n\\[\n\\text{Total costs}_i=\\sum_j^{J}c_{ij},\n\\]\nwhere \\(c_{ij}\\) denotes the cost component collected for patient \\(i\\)-th at the \\(j\\)-th time in the trial. These components are typically calculated out of resource use data obtained from either self-reported questionnaires administered to the patients, their clinic records, or a combination of these. Resource use data are then combined with national unit prices for each type of healthcare service used to derive the individual costs associated with each cost component (i.e. \\(\\text{cost}=\\text{resource use}\\times \\text{unit price}\\)) for each individual at each time point.\nAs perhaps you noticed, it is possible to simply ignore the longitudinal nature of the data by first computing the aggregated outcome measures (QALYs and Total costs) and fit the statistical models directly to these quantities to derive the treatment effect of interest, e.g. mean incremental QALYs and Total costs between a new intervention (\\(t=1\\)) and a control (\\(t=0\\)) group. This also simplified the modelling task substantially since the complexities associated with the intrinsic dependence between outcome observations collected from the same individuals over time can be ignored (i.e. it is already summarised into the aggregated measures). However, there are situations in which it would be preferable to fit the model using the original longitudinal data collected throughout the trial and use the estimated quantitis from this model to derive the main treatment effects of interest, i.e. mean incremental QALYs and Total costs between treatment groups. See reference paper here.\nFirst, let’s simulate some longitudinal health economics data. For the sake of this exercise I will create a balanced design in which utilities and costs are collected at three equally spaced time points (baseline, 6 and 12 months) on \\(n=300\\) individuals randomised to two interventions (new and old group) over a period of 1 year.\n\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nlibrary(mvtnorm)\nmean_ut0 &lt;- c(0.3,0.3,0.3)\nmean_ut1 &lt;- c(0.3,0.4,0.5)\nsigma_u &lt;- diag(3)\ndiag(sigma_u) &lt;- 0.05\nu_t0 &lt;- rmvnorm(n/2,mean = mean_ut0, sigma = sigma_u)\nu_t1 &lt;- rmvnorm(n/2,mean = mean_ut1, sigma = sigma_u)\nmean_ct0 &lt;- c(500,500,500)\nmean_ct1 &lt;- c(500,1300,1500)\nsigma_c &lt;- diag(3)\ndiag(sigma_c) &lt;- 2000\nc_t0 &lt;- rmvnorm(n/2,mean = mean_ct0, sigma = sigma_c)\nc_t1 &lt;- rmvnorm(n/2,mean = mean_ct1, sigma = sigma_c)\nu_1 &lt;- c(u_t0[,1],u_t1[,1])\nc_1 &lt;- c(c_t0[,1],c_t1[,1])\nu_2 &lt;- c(u_t0[,2],u_t1[,2])\nc_2 &lt;- c(c_t0[,2],c_t1[,2])\nu_3 &lt;- c(u_t0[,3],u_t1[,3])\nc_3 &lt;- c(c_t0[,3],c_t1[,3])\ndata_sim_uc &lt;- data.frame(id, trt, u_1, c_1, u_2, c_2, u_3, c_3)\ndata_sim_uc &lt;- data_sim_uc[sample(1:nrow(data_sim_uc)), ]\n\nWith the above commands I have generated, separately for each treatment group, utility and cost data at three different time points assuming a multivariate normal distribution assuming independence over time. Although this is a rather strong assumption in real scenarios here I merely focus on the implementation of the models for longitudinal data. In future posts I will provide more realistic examples to show how changing the data structure actually requires an accurate choice of the methods to deal with them to avoid bias or misleading inferences.\nWe can inspect the generated data using histograms:\n\nlibrary(ggplot2)\ndata_sim_uc$trtf &lt;- factor(data_sim_uc$trt)\nlevels(data_sim_uc$trtf) &lt;- c(\"old\",\"new\")\nu1_hist &lt;- ggplot(data_sim_uc, aes(x=u_1))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nu2_hist &lt;- ggplot(data_sim_uc, aes(x=u_2))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nu3_hist &lt;- ggplot(data_sim_uc, aes(x=u_3))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nc1_hist &lt;- ggplot(data_sim_uc, aes(x=c_1))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nc2_hist &lt;- ggplot(data_sim_uc, aes(x=c_2))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nc3_hist &lt;- ggplot(data_sim_uc, aes(x=c_3))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\n\ngridExtra::grid.arrange(u1_hist, c1_hist, u2_hist, c2_hist, u3_hist, c3_hist, nrow = 3, ncol = 2)\n\n\n\n\n\n\n\n\nFrom the graphs it is apparent that the new intervention is associated with higher costs compared to the old one when focussing on the follow-up period. We can then inspect some summary statistics to have a better idea about these differences:\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\ndata_sim_uc_stats &lt;- data_sim_uc[,c(\"u_1\",\"c_1\",\"u_2\",\"c_2\",\"u_3\",\"c_3\",\"trtf\")]\nd.summary &lt;- data_sim_uc_stats %&gt;%                               \n  group_by(trtf) %&gt;% \n  summarize(meanu1 = mean(u_1), sdu1 = sd(u_1), \n            meanc1 = mean(c_1), sdc1 = sd(c_1),\n            meanu2 = mean(u_2), sdu2 = sd(u_3), \n            meanc2 = mean(c_2), sdc2 = sd(c_2),\n            meanu3 = mean(u_3), sdu3 = sd(u_3), \n            meanc3 = mean(c_3), sdc3 = sd(c_3)\n)\nkable(d.summary, caption = \"Summary statistics\", format = \"html\", digits = 1)\n\nSummary statistics\n\n\ntrtf\nmeanu1\nsdu1\nmeanc1\nsdc1\nmeanu2\nsdu2\nmeanc2\nsdc2\nmeanu3\nsdu3\nmeanc3\nsdc3\n\n\n\n\nold\n0.3\n0.2\n499.9\n42.2\n0.3\n0.2\n507.4\n39.5\n0.3\n0.2\n503.4\n43.8\n\n\nnew\n0.3\n0.2\n505.2\n41.6\n0.4\n0.2\n1304.9\n43.8\n0.5\n0.2\n1498.0\n47.4\n\n\n\n\n\nWe can see that both groups have similar sd while the means show some noticeable differences with \\(t=1\\) being associated with both higher utility and cost post-baseline values (\\(j&gt;1\\)). How do we proceed from here? Normally the approach consists in computing QALYs and Total costs and fit models to these data (see previous post). However, it is also possible to fit models directly to the utility and cost data in the form of linear mixed or random-effects regression models. The main idea behind this models is to treat individuals as random effects so that the observations collected at each time can be assigned a two-level multilevel structure in which individuals are nested within time points. In this way dependence between observations for the same individual is taken into account by means of random effects specification, thus preserving the dependence relationships present in the longitudinal data.\nBefore fitting the model it is necessary to re-arrange out current dataset (wide-format) into a longitudinal format where the utility and cost outcome variables consists in single vectors each of length \\(n \\times J\\) and for each outcome value create an indicator variable associated with the different time at which each value refers to (i.e. taking value 1, 2 or 3). This can be achieved as follows:\n\nlibrary(reshape)\ndata_long_uc&lt;-reshape(data_sim_uc, varying = c(\"u_1\",\"u_2\",\"u_3\",\"c_1\",\"c_2\",\"c_3\"),\n                        direction = \"long\",idvar = \"id\",sep = \"_\")\ndata_long_uc$time_2&lt;-ifelse(data_long_uc$time == 2,1,0)\ndata_long_uc$time_3&lt;-ifelse(data_long_uc$time == 3,1,0)\ndata_long_uc$trt_time_2&lt;-data_long_uc$trt*data_long_uc$time_2\ndata_long_uc$trt_time_3&lt;-data_long_uc$trt*data_long_uc$time_3\ndata_long_uc$timef &lt;- factor(data_long_uc$time)\n\nIn the code above, in addition to the creation of the variable time, additional indicator variables have been created to denote each individual time point (time_2,time_3) and interaction between time point and treatment assignment (trt_time_2,trt_time_3). Although not necessary, these additional indicators can simply the interpretation of the model specification in terms of retrieving estimates for the main effects of interest, e.g. mean utility and cost at each time point by treatment group. After converting our dataset into long format we can now fit the longitudinal random effects model for the two outcomes separately.\n\\[\nu_{ij} = \\alpha_1\\times\\text{time}_j + \\alpha_2 \\times \\text{trt}\\times\\text{time}_2 + \\alpha_3 \\times \\text{trt}\\times\\text{time}_3 + \\omega^u_i + \\varepsilon^u_{ij},\n\\]\n\\[\nc_{ij} = \\beta_1\\times\\text{time}_j + \\beta_2 \\times \\text{trt}\\times\\text{time}_2 + \\beta_3 \\times \\text{trt}\\times\\text{time}_3 + \\omega^c_{i} + \\varepsilon^c_{ij},\n\\]\nwhere the set of regression coefficients \\(\\boldsymbol \\alpha\\) and \\(\\boldsymbol \\beta\\) capture the association between each predictor included into the model and the outcome, while the terms \\(\\omega_i\\) and \\(\\varepsilon_{ij}\\) denote the random effects and residual error term of the model, respectively. Key aspects to highlight from the model specifications are:\n\nThe models do not include a fixed constant as removal of the intercept term allows to interpret the regression parameters and linear combination thereof as functions of the mean utility and cost for each treatment group and time point. For example, when considering time = 1 (baseline), it is clear how the first regression coefficient (\\(\\alpha_1\\) or \\(\\beta_1\\)) represents the mean outcome in the old group at that time point (i.e. setting time=1 and trt=0 gets rid of all other terms into the models).\nDifferent assumptions about the covariance structure of the data can be encoded into the model by specifying a given structure for the error terms. Here, for simplicity, we focus on the simplest case where the error terms are associated with a constant variance of \\(\\sigma^2_{\\varepsilon}\\). More sophisticated assumptions can be incorporated by assuming for example a compound symmetry, autocorrelation or fully unstructured covariance matrix for the error terms.\n\nFinally, we note that here the models are fitted assuming independence between utilities and costs to make easier to interpret the results from each model, while in reality correlation between outcomes should be taken into account even at the modelling stage. The model can be fitted in R using different packages and functions. Here we use the nlme package.\n\nlibrary(nlme)\nLMM_u &lt;- lme(u ~ -1 + timef + trt_time_2 + trt_time_3, random = ~ 1 | id, data = data_long_uc, method = \"ML\")\nLMM_c &lt;- lme(c ~ -1 + timef + trt_time_2 + trt_time_3, random = ~ 1 | id, data = data_long_uc, method = \"ML\")\n\nWe can look at summary results for the fixed effects components of the model (i.e. regression parameters) by typing:\n\nlibrary(nlme)\nsummary(LMM_u)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: data_long_uc \n       AIC       BIC   logLik\n  -139.812 -106.1953 76.90602\n\nRandom effects:\n Formula: ~1 | id\n         (Intercept)  Residual\nStdDev: 3.737738e-05 0.2221528\n\nFixed effects:  u ~ -1 + timef + trt_time_2 + trt_time_3 \n               Value  Std.Error  DF   t-value p-value\ntimef1     0.2952213 0.01286178 596 22.953386  0.0000\ntimef2     0.3275948 0.01818930 596 18.010304  0.0000\ntimef3     0.3122918 0.01818930 596 17.168985  0.0000\ntrt_time_2 0.0822815 0.02572355 596  3.198682  0.0015\ntrt_time_3 0.1931791 0.02572355 596  7.509813  0.0000\n Correlation: \n           timef1 timef2 timef3 trt__2\ntimef2      0.000                     \ntimef3      0.000  0.000              \ntrt_time_2  0.000 -0.707  0.000       \ntrt_time_3  0.000  0.000 -0.707  0.000\n\nStandardized Within-Group Residuals:\n         Min           Q1          Med           Q3          Max \n-3.331350319 -0.695732577  0.008706237  0.671322522  3.136293527 \n\nNumber of Observations: 900\nNumber of Groups: 300 \n\nintervals(LMM_u, level = 0.95, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                lower       est.     upper\ntimef1     0.27003168 0.29522134 0.3204110\ntimef2     0.29197127 0.32759482 0.3632184\ntimef3     0.27666827 0.31229182 0.3479154\ntrt_time_2 0.03190217 0.08228147 0.1326608\ntrt_time_3 0.14279979 0.19317910 0.2435584\n\nsummary(LMM_c)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: data_long_uc \n       AIC      BIC    logLik\n  9338.145 9371.761 -4662.072\n\nRandom effects:\n Formula: ~1 | id\n        (Intercept) Residual\nStdDev:  0.01878518 42.99749\n\nFixed effects:  c ~ -1 + timef + trt_time_2 + trt_time_3 \n              Value Std.Error  DF  t-value p-value\ntimef1     502.5422  2.489386 596 201.8740       0\ntimef2     507.4234  3.520524 596 144.1329       0\ntimef3     503.3765  3.520524 596 142.9834       0\ntrt_time_2 797.4610  4.978772 596 160.1722       0\ntrt_time_3 994.5890  4.978772 596 199.7659       0\n Correlation: \n           timef1 timef2 timef3 trt__2\ntimef2      0.000                     \ntimef3      0.000  0.000              \ntrt_time_2  0.000 -0.707  0.000       \ntrt_time_3  0.000  0.000 -0.707  0.000\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.34872571 -0.70290153 -0.03622616  0.70767638  2.95503948 \n\nNumber of Observations: 900\nNumber of Groups: 300 \n\nintervals(LMM_c, level = 0.95, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n              lower     est.     upper\ntimef1     497.6668 502.5422  507.4177\ntimef2     500.5285 507.4234  514.3183\ntimef3     496.4816 503.3765  510.2714\ntrt_time_2 787.7102 797.4610  807.2119\ntrt_time_3 984.8381 994.5890 1004.3399\n\n\nand we can derive the marginal mean estimates (and corresponding CIs) for the utility and cost at each time point by treatment group using the emmeans function included in the emmeans package:\n\nlibrary(emmeans)\nmu_u &lt;- emmeans(LMM_u, ~ -1 + timef + trt_time_2 + trt_time_3)\nmu_u\n\n timef trt_time_2 trt_time_3 emmean     SE  df lower.CL upper.CL\n 1              0          0  0.295 0.0129 596    0.270    0.320\n 2              0          0  0.328 0.0182 596    0.292    0.363\n 3              0          0  0.312 0.0182 596    0.277    0.348\n 1              1          0  0.378 0.0288 596    0.321    0.434\n 2              1          0  0.410 0.0182 596    0.374    0.446\n 3              1          0  0.395 0.0315 596    0.333    0.456\n 1              0          1  0.488 0.0288 596    0.432    0.545\n 2              0          1  0.521 0.0315 596    0.459    0.583\n 3              0          1  0.505 0.0182 596    0.470    0.541\n 1              1          1  0.571 0.0386 596    0.495    0.646\n 2              1          1  0.603 0.0315 596    0.541    0.665\n 3              1          1  0.588 0.0315 596    0.526    0.650\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nmu_c &lt;- emmeans(LMM_c, ~ -1 + timef + trt_time_2 + trt_time_3)\nmu_c\n\n timef trt_time_2 trt_time_3 emmean    SE  df lower.CL upper.CL\n 1              0          0  502.5 2.489 596    497.7    507.4\n 2              0          0  507.4 3.521 596    500.5    514.3\n 3              0          0  503.4 3.521 596    496.5    510.3\n 1              1          0 1300.0 5.566 596   1289.1   1310.9\n 2              1          0 1304.9 3.521 596   1298.0   1311.8\n 3              1          0 1300.8 6.098 596   1288.9   1312.8\n 1              0          1 1497.1 5.566 596   1486.2   1508.1\n 2              0          1 1502.0 6.098 596   1490.0   1514.0\n 3              0          1 1498.0 3.521 596   1491.1   1504.9\n 1              1          1 2294.6 7.468 596   2279.9   2309.3\n 2              1          1 2299.5 6.098 596   2287.5   2311.4\n 3              1          1 2295.4 6.098 596   2283.5   2307.4\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nFinally, we can compute a linear combination of these marginal mean estimates in order to derive the main effects of interest for the analysis, that is the mean QALYs and Total cost estimates evaluated over the whole trial period. Indeed, it suffices to apply the usual formulae used for computing the aggregated quantities to the mean estimates instead, so to derive the corresponding mean estimates for such quantities by group. We can do this as follows:\n\nmu_QALYs &lt;- contrast(mu_u, list(mu_e_old = c(0.25,0.25+0.25,0.25,0,0,0,0,0,0,0,0,0),\n                                mu_e_new = c(0.25,0,0,0,0.25+0.25,0,0,0,0.25,0,0,0)))\nmu_QALYs\n\n contrast estimate     SE  df t.ratio p.value\n mu_e_old    0.316 0.0107 596  29.601  &lt;.0001\n mu_e_new    0.405 0.0107 596  37.987  &lt;.0001\n\nDegrees-of-freedom method: containment \n\nconfint(mu_QALYs)\n\n contrast estimate     SE  df lower.CL upper.CL\n mu_e_old    0.316 0.0107 596    0.295    0.337\n mu_e_new    0.405 0.0107 596    0.384    0.426\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\nmu_Totalcosts &lt;- contrast(mu_c, list(mu_tc_old = c(0,1,1,0,0,0,0,0,0,0,0,0),\n                                mu_tc_new = c(0,0,0,0,1,0,0,0,1,0,0,0)))\nmu_Totalcosts\n\n contrast  estimate    SE  df t.ratio p.value\n mu_tc_old     1011 4.979 596 203.022  &lt;.0001\n mu_tc_new     2803 4.979 596 562.960  &lt;.0001\n\nDegrees-of-freedom method: containment \n\nconfint(mu_Totalcosts)\n\n contrast  estimate    SE  df lower.CL upper.CL\n mu_tc_old     1011 4.979 596     1001     1021\n mu_tc_new     2803 4.979 596     2793     2813\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nand, in a similar way, we can also compute the incremental means between treatment groups by taking a linear combination of these estimates:\n\ndelta_QALYs &lt;- contrast(mu_u, list(QALYs = c(0,-0.25-0.25,-0.25,0,0.25+0.25,0,0,0,0.25,0,0,0)))\ndelta_QALYs\n\n contrast estimate     SE  df t.ratio p.value\n QALYs      0.0894 0.0144 596   6.219  &lt;.0001\n\nDegrees-of-freedom method: containment \n\nconfint(delta_QALYs)\n\n contrast estimate     SE  df lower.CL upper.CL\n QALYs      0.0894 0.0144 596   0.0612    0.118\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\ndelta_Totalcosts &lt;- contrast(mu_c, list(TCs = c(0,-1,-1,0,1,0,0,0,1,0,0,0)))\ndelta_Totalcosts\n\n contrast estimate   SE  df t.ratio p.value\n TCs          1792 7.04 596 254.515  &lt;.0001\n\nDegrees-of-freedom method: containment \n\nconfint(delta_Totalcosts)\n\n contrast estimate   SE  df lower.CL upper.CL\n TCs          1792 7.04 596     1778     1806\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nAfter fitting the model and having retrieved these estimates, it is possible apply bootstrap methods to resample the estimates over a large number of bootstrap samples so to quantify the uncertainty surrounding these estimates and obtain different replications that can be plotted to produce standard graphical tools such as the cost-effectiveness plane and acceptability curve to assess cost-effectiveness. We refer to the previous post to see how the code looks like for implementing bootstrapping. I will show examples on how to use bootstrap methods in combination with mixed models in future posts.\nSatisfied? well perhaps you are asking yourself: why do I need to bother with this stuff if I can simply compute my aggregated variables and fit simpler models to those instead of complicating my life with longitudinal models? Well you are certainly right that in theory there is no practical advantage of using these models over cross-sectional models fitted at the level of QALYs and Total costs. However, in practice there is always a problem with some people dropping out from the study and thus being unable to observe all utility and cost data that were intended to be measured at each time point. As I will show in future posts, this poses a huge threat to the reliability of the estimates based on the available data (e.g. if only people with worse conditions drop out) which may lead in turn to misleading cost-effectiveness conclusions. Well, it turns out that by fitting models at the original level at which data were collected, i.e. longitudinally, you avoid getting rid of precious information from the data and use it in order to make your results more robust to less restrictive missing data assumptions.\nThis is a huge topic for which I will not start a new discussion right now. However, it is enough to say that if you simply ignore the unobserved cases you may come up with completely wrong answers that are based on very specific and often unrealistic assumptions about these unobserved data!!!"
  },
  {
    "objectID": "posts/2022-12-11-my-blog-post/index.html",
    "href": "posts/2022-12-11-my-blog-post/index.html",
    "title": "More health economics updates",
    "section": "",
    "text": "Hello everybody, I just wanted to take a break from the current series of posts about doing HTA in R and talk a bit about some interesting news related to the international HTA community and events.\nMore specifically, I have recently bumped into an update from the Dutch national health care institute or ZorgInstituut Nederland (ZIN) with respect to the provision of an update about HTA and cost-effectiveness reimbursement application guidelines within the country. You can find here all the relevant information (although the site is in Dutch the documents are in English so rest assured). I would like to talk about this update as I found it really interesting and I think it provides an incredibly welcome push from an institutional body towards the use of appropriate statistical software for reimbursement application procedures that companies are asked to comply in order for their proposed healthcare technologies to be evaluated.\nOne of the key points of the document is related to warn the audience about the intrinsic limitations of standard non-statistical software, e.g. Excel above all, for performing health economic evaluations which increasingly require the use of advanced statistical methods. Indeed, although Excel as certainly great advantages and merits when it comes down to spreadsheet purposes, it was also not specifically designed to perform statistical analyses or complex modelling tasks. In reality, due to its ease of access and popularity among private companies involved in HTA submissions, Excel is still currently one of the most used software for conducting HTA, at least in the private sector. However, as also the updated guidelines from ZIN highlight, the landscape of HTA modelling and statistical techniques is rapidly changing and the use of more appropriate software such as R becomes a necessary requirement (in the sense that these were developed with the objective of conducting advanced statistical analyses) in order to ensure a certain degree of consistency and quality in the statistical methods used within HTA submissions.\nOf course, the guidelines also warn about the possible drawbacks of using R, such as the wider flexibility and free-source nature of the software, which may lead to complications that are difficult to spot if analysts are not familiar with the software (e.g. always using default commands relying on assumptions that are “hidden” instead of properly customise the command based on the current needs). In addition, due to its open-source framework, R packages and their functions are very susceptible to repeated updates and changes which may throw people off gard, especially if they do not keep up the pace with the different updates of the software or packages; similarly, this also implies that no overall and formal check from a responsible institution is done about the quality of the commands, package structures or even help files. This means that analysts should either be able to look into the code to check if everything is done correctly (which kind of eliminates the need to having a package) or blindly trust the author of the package on their commands. I know this can be quite unsettling for analysts used to fixed and stable commands in other software but I believe that, with the acquisition of familiarity with the software and standard (and typically more reliable) packages will allow analysts to acquire a much larger level of flexibility when it comes down to statistical modelling in HTA and will provide them with the necessary technical skills to modify or even create their own packages to solve their specific analysis problems.\nThe document then goes into more detail about the structure required for any reimbursement submission dossier in terms of data, tables, figures, code, etc … They also refer to the use of Markdown (I love it!) for structuring the R code provided with the submission with special care dedicated to the different aspects related to coding clearness (e.g. comments, packages used, testing methods, etc…). They also go all their way to provide a somewhat “comprehensive” list of recommended packages that ZIN suggests using for performing HTA modelling, among which even my own package appears, missingHE, urrah!!!\nOverall, I find this news extremely positive in that I can see that in the Netherlands a strong interest is developing towards the use of R for ensuring a more coherent and quality wise more robust coding framework for conducting, assessing, reporting and comparing statistical analyses within HTA. I highly praise the effort that ZIN’s workers are putting into developing such framework and recognising how, by using a more transparent, appropriate and easy to check modelling technique, more fair economic evaluation assessments and recommendations for reimbursement decisions can be obtained. I really hope that even other national and international organisations and bodies involved in the process of HTA will follow ZIN’s example in adopting guidelines that take into account the importance of requiring a given level of quality for the statistical methodsused and a more transparent framework for their assessment.\nFinally, and a bit unrelated to what I said so far, I would like to put here a recent tweet from Silvia Evers with regard to the next edition of lolaHESG, which this time will be held at EsCHER (Erasmus Centre for Health Economics Rotterdam) between 25-26th May.\nLast edition I had a blast in presenting my work at this conference and meeting a bunch of amazing people involved in HTA in the Netherlands. I sincerely hope that I can replicate my experience in the upcoming year!"
  },
  {
    "objectID": "posts/2023-02-08-my-blog-post/index.html",
    "href": "posts/2023-02-08-my-blog-post/index.html",
    "title": "Structural values in health economics data",
    "section": "",
    "text": "Hello everybody and welcome to my second post of the year. I try to be as consistent as possible with these monthly updates as I would like to keep my blog alive and hopefully post something interesting for somebody involved in data analysis tasks. So, continuing the thread of the past posts, I would like today to deal with a common problem affecting health economics outcome data (e.g. QALYs and Total costs), especially in the context of trial-based cost-effectiveness analysis. The last few times we saw why statistical idiosyncrasies, such as correlation and skewness, represent a possible issue when modelling the data and require the adoption of specific methods to deal with them. Today we take a step further and look at a specific problem which often occurs with individual health economics data: the presence of structural values for the outcome variables, i.e. there is some ceiling effects at one of the boundary of the observed data distributions.\nTypical examples of such values are people associated with zero total costs or one QALY (perfect health status) which are located at the boundaries of the range of possible values that such variables may take, namely lower bound for costs and upper bound for QALYs. When the proportions of the individuals associated with these specific values is small, then hey typically do not cause large problems in that they can be simply treated as other values (e.g. using Normal distributions) or some simple tricks can be implemented to ensure they do not cause an issue when fitting the model (e.g. add/subtract small constant to all individual outcomes to ensure the fitting of alternative parametric distributions). However, it is often the case that the proportions of individuals associated with these values is considerable (e.g. above \\(20-30\\%\\)), therefore implying the existence in the data of a “subset” of cases that have clearly distinct outcome values from all the others. They can then be defined as “structural” in the sense that they relate to specific sub-population conditions (e.g. those in perfect health or experiencing no costs) that are clearly different form the others and whose typical characteristics need to be properly recognised and taken into account at the modelling stage of the analysis (they cannot be treated as the others!).\nToday we will see some examples of these structural values and possible ways to deal with them, although I have to say that in current practice they are rarely addressed via appropriate methods and are instead often treated as other individuals in the samples. For example, let’s reproduce the example in the previous post by simulating some non-Normal bivariate cost and QALY data for a total of \\(300\\) patients assigned to two competing intervention groups (\\(t=0,1\\)). When generating the data, we can try to mimic the typical skewness features of the outcome data by using alternative distributions such as Gamma for costs and Beta for QALYs. However, in contrast to what done before, we now also generate indicator variables that are used in order to determine which individuals should be assigned “structural values”, namely zero costs and one QALYs. The proportions of individuals assigned to these values is obtained by setting the probability of the Bernoulli distribution used to create the indicators.\n\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nmean_e1 &lt;- c(0.5)\nmean_e2 &lt;- c(0.7)\nsigma_e &lt;- 0.15\ntau1_e &lt;- ((mean_e1*(1-mean_e1))/(sigma_e^2)-1)\ntau2_e &lt;- ((mean_e2*(1-mean_e2))/(sigma_e^2)-1)\nalpha1_beta &lt;- tau1_e*mean_e1\nbeta1_beta &lt;- tau1_e*(1-mean_e1)\nalpha2_beta &lt;- tau2_e*mean_e2\nbeta2_beta &lt;- tau2_e*(1-mean_e2)\ne1 &lt;- rbeta(n/2, alpha1_beta, beta1_beta)\ne2 &lt;- rbeta(n/2, alpha2_beta, beta2_beta)\n\nmean_c1 &lt;- 500\nmean_c2 &lt;- 1000\nsigma_c &lt;- 300\ntau1_c &lt;- mean_c1/(sigma_c^2)\ntau2_c &lt;- mean_c2/(sigma_c^2)\nln.mean_c1 &lt;- log(500) + 5*(e1-mean(e1)) \nc1 &lt;- rgamma(n/2, (exp(ln.mean_c1)/sigma_c)^2, exp(ln.mean_c1)/(sigma_c^2))\nln.mean_c2 &lt;- log(1000) + 5*(e2-mean(e2)) + rgamma(n/2,0,tau2_c)\nc2 &lt;- rgamma(n/2, (exp(ln.mean_c2)/sigma_c)^2, exp(ln.mean_c2)/(sigma_c^2))\n\nQALYs &lt;- c(e1,e2)\nCosts &lt;- c(c1,c2)\n\np_zeros &lt;- 0.25\nd_zeros &lt;- rbinom(n, 1, p_zeros)\np_ones &lt;- 0.25\nd_ones &lt;- rbinom(n, 1, p_ones)\n\nQALYs &lt;- ifelse(d_ones==1,1,QALYs)\nCosts &lt;- ifelse(d_zeros==1,0,Costs)\n\ndata_sim_ec &lt;- data.frame(id, trt, QALYs, Costs, d_zeros, d_ones)\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\n\nIn the code above, after simulating QALY and Cost data using Beta and Gamma distribution, indicator variables for the zero and one values were generated for each individual in the sample from a Bernoulli distribution. Whenever the indicator takes value 1, it denotes the presence of a structural value and the corresponding outcome value is then set equal to zero (Costs) or one (QALYs). We can now compute the correlation between variables and plot the two outcome variables against each other to show how the presence of these structural values affect their corresponding association pattern.\n\n#empirical correlation between e and c (across groups)\ncor(data_sim_ec$QALYs,data_sim_ec$Costs)\n\n[1] 0.3582116\n\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nggplot(data_sim_ec, aes(x=QALYs, y=Costs)) +\n  geom_point(size=2, shape=16) + theme_classic() +\n  facet_wrap(~trtf)\n\n\n\n\n\n\n\n\nIn addition, we can also produce histograms of the distribution of the outcomes by treatment group to have a rough idea of the amount of structural values by type of outcome and treatment group in our sample.\n\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nQALY_hist &lt;- ggplot(data_sim_ec, aes(x=QALYs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist &lt;- ggplot(data_sim_ec, aes(x=Costs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\nIt is clear from the graphs above that the considerable proportions of these structural values in both outcomes cannot be simply ignored and must be addressed properly. When conducting the main analysis, two possible approaches can be used to deal with such situation without relying on Normality assumptions of standard methods:\n\nSimply add/subtract some small constant (e.g. \\(0.001\\)) to all individuals in the sample to ensure that no zero or one values occur, therefore allowing the implementation of methods relying on alternative parametric distributions (e.g. Gamma or Beta regression) that are not defined at such values. Although very simple and easy to implement, this method is a sort of a “trick” which is used in order to dodge the problem of these values therefore not recognising their systematic difference with respect to all other individuals in the sample. It is usually reasonable to use only when the proportion of such values is small, thus likely not affecting the overall conclusions of the analysis (i.e. estimate of mean difference in the outcomes between groups). In R we can do this using the following commands:\n\n\ndata_sim_ec$QALYs_new &lt;- data_sim_ec$QALYs - 0.001\ndata_sim_ec$Costs_new &lt;- data_sim_ec$Costs + 0.001\n\nlibrary(betareg) \nmodel_qaly_new &lt;- betareg(QALYs_new ~ trtf, data = data_sim_ec, link = c(\"log\")) \nsummary(model_qaly_new) \n\n\nCall:\nbetareg(formula = QALYs_new ~ trtf, data = data_sim_ec, link = c(\"log\"))\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-1.2361 -0.7496 -0.5277  1.3470  1.7154 \n\nCoefficients (mean model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.30589    0.02674 -11.438   &lt;2e-16 ***\ntrtfnew      0.05196    0.03219   1.614    0.106    \n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    1.601      0.124    12.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   230 on 3 Df\nPseudo R-squared: 0.07084\nNumber of iterations: 14 (BFGS) + 1 (Fisher scoring) \n\nexp(coef(model_qaly_new))[1:2]\n\n(Intercept)     trtfnew \n  0.7364712   1.0533356 \n\nconfint(model_qaly_new)[1:2,1:2] \n\n                  2.5 %     97.5 %\n(Intercept) -0.35829941 -0.2534708\ntrtfnew     -0.01113072  0.1150545\n\nexp(confint(model_qaly_new))[1:2,1:2]\n\n                2.5 %    97.5 %\n(Intercept) 0.6988638 0.7761024\ntrtfnew     0.9889310 1.1219346\n\nmodel_tcost_new &lt;- glm(Costs_new ~ trtf, data = data_sim_ec, family = Gamma(link=\"log\")) \nsummary(model_tcost_new) \n\n\nCall:\nglm(formula = Costs_new ~ trtf, family = Gamma(link = \"log\"), \n    data = data_sim_ec)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.23102    0.07826  79.622  &lt; 2e-16 ***\ntrtfnew      0.74653    0.11067   6.745 7.93e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.9186255)\n\n    Null deviance: 1876.3  on 299  degrees of freedom\nResidual deviance: 1835.4  on 298  degrees of freedom\nAIC: 3751.2\n\nNumber of Fisher Scoring iterations: 25\n\nexp(coef(model_tcost_new))[1:2]\n\n(Intercept)     trtfnew \n 508.272377    2.109675 \n\nconfint(model_tcost_new)[1:2,1:2]\n\n                2.5 %    97.5 %\n(Intercept) 6.0814632 6.3884275\ntrtfnew     0.5294067 0.9636612\n\nexp(confint(model_tcost_new))[1:2,1:2]\n\n                 2.5 %     97.5 %\n(Intercept) 437.669122 594.920319\ntrtfnew       1.697925   2.621276\n\n\nThe code above proceeds to fist add/subtract the small constant to all outcome values, followed by fitting a Beta regression and a Gamma regression models with logarithmic link function (for the mean) to estimate the main treatment effect of interest for the two outcomes, namely mean total costs and QALYs difference between groups (converted on the original scale using the exponential function and with related \\(95\\%\\)CI). In the Beta regression output, an estimate for the parameters \\(\\phi\\) is also provided as this precision parameter (\\(\\text{variance}=\\mu(1-\\mu)/(1-\\phi)\\)) is linked to the mean and variance of the distribution and must be estimated. However, its interpretation is not usually of interest and is mostly provided in order to check for potential estimation problems in case the Beta distribution has a poor fit to the data. 2. Properly account for the existence of the structural values by fitting two separate models for the structural and non-structural component in the data, also referred to two-part regression models. The idea is to separate the two sets of observations: fit standard non-Normal distributions to the non-structural values (to handle skewness) and; fit logistic regression models using as outcomes the indicators for the structural values for each outcome type. The first type of models will provide an estimate for the treatment effect (mean Cost and QALY difference between groups) among the non-structural components of the data (\\(\\Delta^e_{ns}, \\Delta^c_{ns}\\)), while the second type of models will provide estimates for the proportions of structural zeros/ones (\\(\\pi^e,\\pi^c\\)). As a final step, it is possible to combine the results from the two types of models in order to obtain an overall estimate of treatment effect which takes into account the presence of the structural values. This is achieved for the QALY analysis by computing:\n\\[\n\\Delta^e = \\Delta^e_{ns}\\times(1-\\pi^e),\n\\]\nthat is the overall mean QALY difference between groups \\(\\Delta^e\\) will be equal to the product of the mean difference among the non-structural component \\(\\Delta^e_{ns}\\) and the probability of having non-structural values \\((1-\\pi^e)\\). Note that here we assume that the proportion of structural values \\(\\pi^e\\) is estimated across groups but in theory also a different estimate per group can be obtained, for example by including treatment as a predictor in the logistic regression model. In such case, then also the difference between \\(\\pi^e\\) in the two groups should also be incuded in the above formula to derive the overall estimate of \\(\\Delta^e\\).\nSimilarly for costs, we can do:\n\\[\n\\Delta^c = \\Delta^c_{ns}\\times(1-\\pi^c),\n\\]\nthat is the overall mean Cost difference between groups \\(\\Delta^c\\) will be equal to the product of the mean difference among the non-structural component \\(\\Delta^c_{ns}\\) and the probability of having non-structural values \\((1-\\pi^c)\\).\nIn R we can fit these models as follows:\n\ndata_sim_e_ns &lt;- data_sim_ec[data_sim_ec$QALYs&lt;1,]\ndata_sim_c_ns &lt;- data_sim_ec[data_sim_ec$Costs&gt;0,]\n\nmodel_qaly_ns &lt;- betareg(QALYs ~ trtf, data = data_sim_e_ns, link = c(\"log\")) \nsummary(model_qaly_ns) \n\n\nCall:\nbetareg(formula = QALYs ~ trtf, data = data_sim_e_ns, link = c(\"log\"))\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-2.7512 -0.7313  0.0488  0.6366  3.3486 \n\nCoefficients (mean model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.67733    0.02977  -22.75  &lt; 2e-16 ***\ntrtfnew      0.28786    0.03625    7.94 2.02e-15 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)   9.3358     0.8696   10.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 104.3 on 3 Df\nPseudo R-squared: 0.1959\nNumber of iterations: 8 (BFGS) + 2 (Fisher scoring) \n\nexp(coef(model_qaly_ns))[1:2]\n\n(Intercept)     trtfnew \n  0.5079699   1.3335700 \n\nconfint(model_qaly_ns)[1:2,1:2] \n\n                 2.5 %     97.5 %\n(Intercept) -0.7356872 -0.6189791\ntrtfnew      0.2168019  0.3589171\n\nexp(confint(model_qaly_ns))[1:2,1:2]\n\n                2.5 %    97.5 %\n(Intercept) 0.4791761 0.5384939\ntrtfnew     1.2420981 1.4317781\n\nmue_ns &lt;- exp(coef(model_qaly_ns))[2]\n\nlibrary(boot)\nmodel_qaly_ones &lt;- glm(d_ones ~ 1, data = data_sim_ec, family = binomial(link = logit)) \nsummary(model_qaly_ones) \n\n\nCall:\nglm(formula = d_ones ~ 1, family = binomial(link = logit), data = data_sim_ec)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.8473     0.1260  -6.725 1.75e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 366.52  on 299  degrees of freedom\nResidual deviance: 366.52  on 299  degrees of freedom\nAIC: 368.52\n\nNumber of Fisher Scoring iterations: 4\n\npe_ones &lt;- inv.logit(coef(model_qaly_ones))\n\nmue_all &lt;- mue_ns*(1-pe_ones)\n\n\nmodel_tcost_ns &lt;- glm(Costs ~ trtf, data = data_sim_c_ns, family = Gamma(link=\"log\")) \nsummary(model_tcost_ns) \n\n\nCall:\nglm(formula = Costs ~ trtf, family = Gamma(link = \"log\"), data = data_sim_c_ns)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.47948    0.06673  97.106  &lt; 2e-16 ***\ntrtfnew      0.71292    0.09358   7.618  6.2e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.5209232)\n\n    Null deviance: 367.57  on 237  degrees of freedom\nResidual deviance: 338.07  on 236  degrees of freedom\nAIC: 3737.3\n\nNumber of Fisher Scoring iterations: 6\n\nexp(coef(model_tcost_ns))[1:2]\n\n(Intercept)     trtfnew \n 651.629970    2.039936 \n\nconfint(model_tcost_ns)[1:2,1:2]\n\n                2.5 %    97.5 %\n(Intercept) 6.3514894 6.6131753\ntrtfnew     0.5292785 0.8963693\n\nexp(confint(model_tcost_ns))[1:2,1:2]\n\n                 2.5 %     97.5 %\n(Intercept) 573.346008 744.844382\ntrtfnew       1.697707   2.450689\n\nmuc_ns &lt;- exp(coef(model_tcost_ns))[2]\n\nmodel_tcost_zeros &lt;- glm(d_zeros ~ 1, data = data_sim_ec, family = binomial(link = logit)) \nsummary(model_tcost_zeros) \n\n\nCall:\nglm(formula = d_zeros ~ 1, family = binomial(link = logit), data = data_sim_ec)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.3451     0.1426  -9.434   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 305.7  on 299  degrees of freedom\nResidual deviance: 305.7  on 299  degrees of freedom\nAIC: 307.7\n\nNumber of Fisher Scoring iterations: 4\n\npc_zeros &lt;- exp(coef(model_tcost_zeros))\n\nmuc_all &lt;- muc_ns*(1-pc_zeros)\n\nIt is clear how, although this approach allows to take into account for the structural component of the data, it suffers from practical limitations: the non-structural regression models are effectively fitted to a smaller sample compared to the original one since all individuals with structural values are removed from the analysis and are only used to estimate the proportion of values in the second step. In case there is a considerable mismatch between the individuals with structural ones and zeros in the data, then the validity of the estimates for the non-structural components may become questionable, e.g. if the sample size becomes too small. In addition, standard regression output such as p-values, CIs or other summary statistics obtained from the non-structural regression are only applicable to the specific subset of non-structural values and it is generally difficult to derive similar estimates for the overall mean difference (only point estimate). This last limitation can be overcome by nesting the two-part procedure within a bootstrap method, therefore re-fitting the models to a series of bootstrap samples sampled with replacement from the original sample a large number of times so to obtain at the end a distribution of values for the overall treatment effect, which can then be used to generate standard CEA output, e.g. CEAC, CEP, CIs based on percentile method, etc …\nRegardless of the approach used, I believe it is important to account for ceiling effects in the data if the structural values are present in a substantial way. Simply ignoring the presence of structural values can be very dangerous in that these values suggest how these people are systematically different from the others with respect to their outcomes and should therefore be treated differently form all the others in the analysis stage. When only a few values are present, then simply adding/subtracting some constant values may be sufficient. However, when the proportion of such values becomes considerable, more advanced approaches should be pursued to deal with them. The typical problem is that such values occur in combination with other complexities that characterise the data, e.g. skewness, making even more important to use methods that are tailored to address multiple data idiosyncrasies at the same time. In future posts we will see how the existence of these complex features in HE data make the use of standard frequentist methods often difficult and instead favour the use of more flexible Bayesian approaches as more suited to model and quantify uncertainty associated with CEAs."
  },
  {
    "objectID": "posts/2023-04-10-my-blog-post/index.html",
    "href": "posts/2023-04-10-my-blog-post/index.html",
    "title": "Some updates …",
    "section": "",
    "text": "Hello guys, I hope everything is well with you and that you had some nice Easter break (at least I did). Personally, I really needed this break as I was quite tired and without energy due to the heavy workload in the past months. Now I feel refreshed and ready to start anew (let’s see how long this can last). Anyway, today I wanted to give some updates about what is up for me in the upcoming months as it has been a while that I have not made a post like this.\nFirst of all, I am happy to announce that my working paper has been accepted fot discussion at lolaHESG, which is the corresponding Dutch conference to the popular Health Economics Study Group conference in the UK, dedicated to researchers involved in health economics’ topics. I already attened last year’s edition, held in Maastricht, which was a blast. Amazing people and colleagues to talk to, nice food and accommodation, and most importantly lots of different presentations to attend and opening up of new connections and work relationships. Similarly, to last year, also for this year I am going to present some on-going work focussed on the modelling of health economics trial-based data which takes inspriation from one of my most recently published papers about the use of longitudinal linear mixed models for the analysis of health economics trial data. The paper was a simple introduction to practitioners to the use and specification of mixed models for the analysis of trial-based health economics data, with a practical illustration on how to specify the models in software such as STATA or R, full code available on my GitHub. In the work I am going to present at lolaHESG, I hope can start from this introductory and applied paper and perform some simulations to assess the performance of the proposed methods across different missingness and data structure scenarios. It takes a bit of time but I think it will definitely be worth it, at least so that I can get some feedback form the other attendees on how to extend the work further (perhaps for a future paper). I am finalising the work in these days but I need to hurry up as the deadline for the submission of the full paper is the end of this week, I hope I can make it! I also have some nice ideas to further extend the work but I need to have some feedback on what I have done so far first.\nSecond, and a bit unrelated to what I said, I wanted to breifly comment a recent tweet from Gelman\n\nwhich I find it extremely interesting from a purely statistical perspective (sorry health economist audience!). The tweet links back to his blog on a post about a discussion on the concept and definition of the term p-value under a classical statistical framework which, unlike what we are told in basic statistics course, is not so clear-cut across all scenarios. Indeed, he refers to four different alternative definitions of p-values, which vary depending on the specific quantity the researcher refers to with the concept of p-value. In some scenarios, and typically these are the cases encountered in basic analysis problems, these definitions coincide and we have our usual description of a p-value. However, when we go outside such simple scenarios and focus more on what we mean by the term p-value, e.g. based on an test statistic or family of hypothesis tests. This is very interesting to me as the already quite confusing concept of a p-value and its relevance in the context of hypothesis testing framework is further subjected to easy misunderstandings. The whole discussion is about how can we define in an unique way the concept of p-value and the answer, as it usually is in these cases, is that there is no unique definition as it depends on what we mean by the quantity that we define with p-value, e.g. probability statement or a general summary of the data without any probabilistic meaning attached to it. As also Gelman points out in his post, also my thinking about p-values has changed over time. From necessary evils to be used to achieve some statistical conclusions about the likely existence of an effect to until confusing terms that people tend to misinterpret and based on which no critical decision should be based blindly. Regardless of this rumble of thoughts about p-values, I too think that it is not a problem that p-values are used in their common interpretation even though a general an unique definition for such term may not be possible. What is important I feel is that people are aware of the possibly confusing term that a p-value is and that they need to not blindly trust and interpret such values as if they were true statements, as what is meant by such statement may not be fully clear from the start.\nLet’s think careful when we do data analysis!"
  },
  {
    "objectID": "posts/2023-06-11-my-blog-post/index.html",
    "href": "posts/2023-06-11-my-blog-post/index.html",
    "title": "Back from lola",
    "section": "",
    "text": "Hello folks, I am back with my usual update and today I have some good news to report! Last month I went to the annual lolaHESG 2023 conference meeting here in the Netherlands which, this year, was organised by the Erasmus Centre for Health Economics Rotterdam in the beautiful sea town of Egmond aan Zee in North Holland. This was my second time attending this Dutch conference (last year was in Maasticht) and I have to say I was quite impressed with how the conference was grown compared to that time. Not only more attendees in general but also more talks and panel sessions with researchers coming from all over the world to discuss about many different topics related to health economics.\nPersonally, I went there to talk about an on-going project I have been working on since last year, still about missing data methods for economic evaluations, whose idea was kind of related to one of my past papers which I tried to expand a bit. My aim was to receive feedback from the audience in terms of the future directions for this work and I must admit I was not expecting such interest in my work, especially since my talk was the last day of the conference after the party of the night before (which was quite nice). The entirety of my panel session was dedicated to methodological works in the field of health economics with some interesting works from a few other researchers. Unfortunately, I had to leave just right after my talk as I needed to be back in Maastricht before night (it is a long way to travel by train!), but I received very valuable suggestions and I had nice discussions with a few people attending my session. Among these I must mention the great team from VU Amsterdam with whom I also started some collaborations the past months on some interesting work. Special thanks go to the members of the Health Economics and Health Technology Assessment group at VU, lead by the great Judith Bosmans, the lovely [Hanneke van Dongen], and the talented PhD student Jonas Esser with whom I had some nice chats during the whole conference.\nIn terms of organisation, the conference was really well structured within the huge building of the Hotel Zuiderduin with plenty of activities in between-sessions and after them in the two days of the conference. I arrived a day earlier due to the long way from Maastricht and I had the pleasure to enjoy the beautiful landscape of the sea and dunes of Egmond since my arrival. The wehather was really nice and sunny the whole time which also gave us opportunity at the end of the first day of the conference to gather ourselves at the beach right close to the conference building with someone even playing some games!\nAnd after dinner we also had some fun time with some pub quiz games with people randomly assigned to different groups to try to guess some answers to some general and health-economics specific quizzes. It was a lot of fun and my group ended up even in second place (mostly not thanks to me I must say). After that, there was the usual party of lolaHESG which I heard lasted until late night. I could join this one as the day after in the morning I had my talk but I am also getting a bit older now, so not sure if I would have been awake the day after otherwise.\nIn terms of work, I attended a few talks which I found interesting and that I did not now much about before joining them. However, I must say that, for my tastes, there were too few methodological talks related to statistical methods in HE. This might be my bias but I really hoped I could see my statistical works although I recognise the spectrum of topics covered by the general definition of “Health Economics” makes it really hard to have multiple talks about a specific research field. Nevertheless, I saw a great interest in my work and, for what I could attend, also for the other more methodological works presented in my session. Looking forward to next year’s conference which is expected to be even bigger and interesting.\nAs a final note, a bit unrelated to this, I wanted to draw your attention to a nice live speech that professor Gelman gave recently at UCL. I am not sure the live is still on YT but it was a very interesting talk I was the pleasure to follow. The topic was about Gelman’s take on teaching statistics to applied Master and Bachelor students (e.g. political science, medicine, etc…). The argumnet touches me very closely as this is something I am confronted with basically every day at UM, with sometimes non-trivial difficulties in terms of engaging and motivating the students to study something they find it difficult to appreciate. Gelman provided some examples about how he tried to make his sessions about basic statistics more interesting to the students by means of group work, relatable examples through some physical games in class, demonstration via software simulation, etc. Although I defintely agree with him with the imprtance of trying something novel that could motivate the students to study statistics, I am also concerned about the time spent on preparing these exercises. Given the limited amount of time teachers are paid to prepare material, sometimes it is really hard to find a right balance between effort spent on preparing teaching material, grading, feedback and reserach time, which already suffers quite a lot. I will try to make treasure of his talk and think about how to apply some of the ideas I think could be interesting in my own teaching as much as possible starting from next academic year. The new course I will coordinate is the perfect occasion to try something new. We will see how it goes but at least now I am motivated to try my best! Let us see for how long it lasts!"
  },
  {
    "objectID": "posts/2023-09-02-my-blog-post/index.html",
    "href": "posts/2023-09-02-my-blog-post/index.html",
    "title": "Bayesian statistics in health economic evaluations",
    "section": "",
    "text": "Hello folks, I hope you had some break time during summer as surely I did! After a whole year of stress and work it was nice to have some vacation period and to clear my mind for a while. Now that I am back to work I feel recharged and I am ready for a new year. One of my objective for this academic year is to find more time to dedicate to some new research projects as last year I only managed to do very little as most of my research energies went into the writing up of a research grant application. This year I hope to find more time to do something different, at least research wise.\nSo, with that spirit in mind, let’s start from today’s post where I follow-up from a past post introducing the concept of how to perform economic evaluations using standard statistical methods and power it up to what I normally do in this field, use Bayesian statistics! Perhaps some of you will not believe me but over time I am really sure that using Bayesian statistics made my life much easier when coming down to fit relatively complex models to health economics data. Since nowadays this seems to be very common in the literature, there is even more reason to go fully Bayesian when doing these analyses as the degree of flexibility it grants is so much more compared to what standard methods can typically achieve. Of course this is the opinion of someone totally biased! But before raising your finger, please try to come to the end of this post.\nAs I already mentioned in previous posts, the usual analysis task in economic evaluation based on individual-level data (e.g. QALYs and Total costs computed over a trial period) can be quite challenging due to the presence of a series of complexities that affect the data that need to be taken into account when choosing the statistical methods to use in the analysis; examples include: correlation between effects and costs, skewness of the outcome data, presence of structural values in the data. We saw before that different types of methods exist to deal with each of these problems but the general challenge comes from the fact that often these elements are present jointly in a single dataset and therefore the different methods used to handle each of them need to be combined in some way to perform the analysis. This, however, is easier said than done since, particularly under a frequentist framework, the complexity of fitting all these methods in combination with the need to quantify the impact of uncertainty on the results (e.g. via bootstrapping methods) can lead to extremely difficult-to-fit or expensive-to-implement models. This I believe the key reason why analysts often pretend to ignore some of these problems and prefer to implement easier methods in the hope that results will not be too much affected. Despite understanding their point, I feel that if they knew models that can account for all these problems together, then they would also like to fit them to improve the reliability of the results they obtain. Well, that is why today I talk about fitting the model under a Bayesian framework, which allows to achieve this task at the cost of learning a bit about Bayesian inference and how to interpret it.\nLet’s start by simulating some non-Normal bivariate cost and QALY data from an hypothetical study for a total of \\(300\\) patients assigned to two competing intervention groups (\\(t=0,1\\)). When generating the data, we can try to mimic the typical skewness features of the outcome data by using alternative distributions such as Gamma for costs and Beta for QALYs. We also generate indicator variables that are used in order to determine which individuals should be assigned “structural values”, namely zero costs and one QALYs. The proportions of individuals assigned to these values is obtained by setting the probability of the Bernoulli distribution used to create the indicators.\nset.seed(768)\nn &lt;- 300\nid &lt;- seq(1:n)\ntrt &lt;- c(rep(0, n/2),rep(1, n/2))\nmean_e1 &lt;- c(0.5)\nmean_e2 &lt;- c(0.7)\nsigma_e &lt;- 0.15\ntau1_e &lt;- ((mean_e1*(1-mean_e1))/(sigma_e^2)-1)\ntau2_e &lt;- ((mean_e2*(1-mean_e2))/(sigma_e^2)-1)\nalpha1_beta &lt;- tau1_e*mean_e1\nbeta1_beta &lt;- tau1_e*(1-mean_e1)\nalpha2_beta &lt;- tau2_e*mean_e2\nbeta2_beta &lt;- tau2_e*(1-mean_e2)\ne1 &lt;- rbeta(n/2, alpha1_beta, beta1_beta)\ne2 &lt;- rbeta(n/2, alpha2_beta, beta2_beta)\n\nmean_c1 &lt;- 500\nmean_c2 &lt;- 1000\nsigma_c &lt;- 300\ntau1_c &lt;- mean_c1/(sigma_c^2)\ntau2_c &lt;- mean_c2/(sigma_c^2)\nln.mean_c1 &lt;- log(500) + 5*(e1-mean(e1)) \nc1 &lt;- rgamma(n/2, (exp(ln.mean_c1)/sigma_c)^2, exp(ln.mean_c1)/(sigma_c^2))\nln.mean_c2 &lt;- log(1000) + 5*(e2-mean(e2)) + rgamma(n/2,0,tau2_c)\nc2 &lt;- rgamma(n/2, (exp(ln.mean_c2)/sigma_c)^2, exp(ln.mean_c2)/(sigma_c^2))\n\nQALYs &lt;- c(e1,e2)\nCosts &lt;- c(c1,c2)\n\np_zeros &lt;- 0.25\nd_zeros &lt;- rbinom(n, 1, p_zeros)\np_ones &lt;- 0.25\nd_ones &lt;- rbinom(n, 1, p_ones)\n\nQALYs &lt;- ifelse(d_ones==1,1,QALYs)\nCosts &lt;- ifelse(d_zeros==1,0,Costs)\n\ndata_sim_ec &lt;- data.frame(id, trt, QALYs, Costs, d_zeros, d_ones)\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\nIn the code above, after simulating QALY and Cost data using Beta and Gamma distribution, indicator variables for the zero and one values were generated for each individual in the sample from a Bernoulli distribution. Whenever the indicator takes value 1, it denotes the presence of a structural value and the corresponding outcome value is then set equal to zero (Costs) or one (QALYs). We can now compute the correlation between variables and plot the two outcome variables against each other to show how the presence of these structural values affect their corresponding association pattern.\n#empirical correlation between e and c (across groups)\ncor(data_sim_ec$QALYs,data_sim_ec$Costs)\n\n[1] 0.3582116\n\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nggplot(data_sim_ec, aes(x=QALYs, y=Costs)) +\n  geom_point(size=2, shape=16) + theme_classic() +\n  facet_wrap(~trtf)\nIn addition, we can also produce histograms of the distribution of the outcomes by treatment group to have a rough idea of the amount of structural values by type of outcome and treatment group in our sample.\ndata_sim_ec$trtf &lt;- factor(data_sim_ec$trt)\nlevels(data_sim_ec$trtf) &lt;- c(\"old\",\"new\")\nQALY_hist &lt;- ggplot(data_sim_ec, aes(x=QALYs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\nTcost_hist &lt;- ggplot(data_sim_ec, aes(x=Costs))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(trtf ~ .) + theme_classic()\ngridExtra::grid.arrange(QALY_hist, Tcost_hist, nrow = 1, ncol = 2)"
  },
  {
    "objectID": "posts/2023-09-02-my-blog-post/index.html#step-1-fit-a-standard-normal-model",
    "href": "posts/2023-09-02-my-blog-post/index.html#step-1-fit-a-standard-normal-model",
    "title": "Bayesian statistics in health economic evaluations",
    "section": "Step 1: fit a standard normal model",
    "text": "Step 1: fit a standard normal model\nIn order to explain the basics of how to fit a Bayesian model, let’s start by considering a (kind of) standard model based on Normal distributions for both QALYs and Total costs. However, I will slightly modify the model to allow for the correlation between the two outcomes, that is we fit a bivariate normal model \\(p(e,c\\mid  \\boldsymbol \\theta)\\), where \\(e\\) and \\(c\\) denote the QALYs and Total cost variables measured for each patient in the trial while \\(\\boldsymbol \\theta\\) denote the set of parameters indexing the model, including the key quantities of interest for the economic evaluations, i.e. the treatment-specific mean effect and cost \\(\\mu_{et}\\) and \\(\\mu_{ct}\\). To ease the task of modelling the data, we can re-express the joint distribution as:\n\\[\np(e,c\\mid \\boldsymbol \\theta) = p(e\\mid \\boldsymbol \\theta_e) p(c \\mid e, \\boldsymbol \\theta_c),\n\\]\nwhere \\(p(e\\mid \\boldsymbol \\theta_e)\\) is the marginal distribution of the effects and \\(p(c \\mid e, \\boldsymbol \\theta_c)\\) is the conditional distribution of the cost given the effects, each indexed by corresponding set of parameters. The main reason for factoring the joint distribution into this product is the possibility to specify univariate distributions for \\(e\\) and \\(c\\), rather than a single bivariate distribution. This can be helpful when, for example, different covariates are considered for the two outcomes as it allows a higher degree of flexibility in specifying the model for each variable. But how are we going to fit the model? well, for that we can rely on freely-available Bayesian software which allows model fitting in a relatively simple way at the cost of learning how to code up the model in this new language. In this post I will consider the JAGS software although this is only one of the many that can be used. For the sake of making things clearer I will not focus here on the details of how the software works and which types of algorithms it uses to implement the model, but I will jump straight into the coding part. First, we need to write the code of the model into a txt file that will then be read by the program after providing the data as input. We can do all this in R.\n\nmodel_bn &lt;- \"\nmodel {\n\n#model specification\nfor(i in 1:n){\nQALYs[i] ~ dnorm(nu_e[i],tau_e)\nnu_e[i] &lt;- beta0 + beta1*trt[i]\n\nCosts[i] ~ dnorm(nu_c[i],tau_c)\nnu_c[i] &lt;- gamma0 + gamma1*trt[i] + gamma2*QALYs[i]\n}\n\n#prior specification\ntau_e &lt;- 1/ss_e\nss_e &lt;- s_e*s_e\ntau_c &lt;- 1/ss_c\nss_c &lt;- s_c*s_c\n\ns_c ~ dunif(0,1000)\ns_e ~ dunif(0,1000)\nbeta0 ~ dnorm(0,0.000001)\nbeta1 ~ dnorm(0,0.000001)\ngamma0 ~ dnorm(0,0.000001)\ngamma1 ~ dnorm(0,0.000001)\ngamma2 ~ dnorm(0,0.000001)\n\n}\n\"\nwriteLines(model_bn, con = \"model_bn.txt\")\n\nIn the code above I first specify the model structure, i.e. assign normal distributions to QLAYs and Costs variable indexed by two parameters, the means \\(\\nu\\) and precisions \\(\\tau\\) (note that precisions correspond to inverse of the variance \\(\\tau=1/\\sigma^2\\)) since these are the default parameters used by JAGS to specify a normal distribution. For each outcome then I specify the mean structure, i.e. the mean of \\(e\\) depennds only on the treatment indicator while the mean of \\(c\\) depend both on treatment indicator and \\(e\\) (this is a conditional cost model!). Next, I specify the priors for non-deterministic parameters, namely using uniform distributions for standard deviations and normal distributions for regression coefficients. Finally, I save the model as a txt file in the current wd using the writeLines function. The model is now written and we can fit it by calling the JAGS software directly from R through dedicated functions. Before that, we need to convert the data as input for the software. Then, we load the package R2jags which allows to call the software from R through the function jags and after providing some technical parameters needed to run the model.\n\n#save data input\nn &lt;- dim(data_sim_ec)[1]\nQALYs &lt;- data_sim_ec$QALYs\nCosts &lt;- data_sim_ec$Costs\ntrt &lt;- data_sim_ec$trt\n\n#load package and provide algorithm parameters\nlibrary(R2jags)\nset.seed(2345) #set seed for reproducibility\ndatalist&lt;-list(\"n\",\"QALYs\",\"Costs\",\"trt\") #pass data into a list\n#set up initial values for algorithm\ninits1 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 1)\ninits2 &lt;- list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 2)\n#set parameter easimates to save\nparams&lt;-c(\"beta0\",\"beta1\",\"gamma0\",\"gamma1\",\"gamma2\",\"s_c\",\"s_e\",\"nu_c\",\"nu_e\")\nfilein&lt;-\"model_bn.txt\" #name of model file\nn.iter&lt;-20000 #n of iterations\n\nWe are now ready to fit the model, which we can do by typing\n\n#fit model\njmodel_bn&lt;-jags(data=datalist,inits=list(inits1,inits2),\n                parameters.to.save=params,model.file=filein,\n                n.chains=2,n.iter=n.iter,n.thin=1)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 600\n   Unobserved stochastic nodes: 7\n   Total graph size: 1345\n\nInitializing model\n\n\nAfter some time needed for the model to run, we end up with something like this\n\n#posterior results\nprint(jmodel_bn)\n\nInference for Bugs model at \"model_bn.txt\", fit using jags,\n 2 chains, each with 20000 iterations (first 10000 discarded)\n n.sims = 20000 iterations saved\n           mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\nbeta0        0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nbeta1        0.116   0.027    0.063    0.098    0.116    0.134    0.168 1.001\ngamma0    -147.064 136.100 -414.191 -238.684 -146.655  -56.422  120.626 1.001\ngamma1     443.594  89.617  268.643  383.247  443.401  504.381  618.307 1.001\ngamma2    1003.566 185.780  642.338  878.610 1002.303 1128.245 1368.741 1.001\nnu_c[1]    653.790  99.155  458.610  587.817  653.583  719.540  847.442 1.001\nnu_c[2]   1019.968  62.799  897.609  977.709 1019.448 1062.243 1143.497 1.001\nnu_c[3]    345.232  68.708  211.853  298.661  344.974  391.561  480.094 1.001\nnu_c[4]   1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[5]    625.502  65.446  495.707  581.253  625.514  669.387  754.077 1.001\nnu_c[6]   1194.620  66.124 1065.903 1150.333 1194.031 1238.974 1323.582 1.001\nnu_c[7]    460.143  62.371  338.056  418.112  460.125  502.151  583.002 1.001\nnu_c[8]    856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[9]   1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[10]   563.317  62.574  440.263  521.301  563.189  605.310  686.902 1.001\nnu_c[11]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[12]  1191.867  65.950 1063.432 1147.673 1191.312 1235.984 1320.751 1.001\nnu_c[13]   994.028  63.705  870.006  951.189  993.596 1036.803 1120.059 1.001\nnu_c[14]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[15]  1165.410  64.460 1039.591 1122.082 1164.928 1208.722 1291.399 1.001\nnu_c[16]   809.178  78.808  654.435  756.291  809.094  861.036  964.868 1.001\nnu_c[17]   340.903  69.063  206.924  294.139  340.636  387.481  476.537 1.001\nnu_c[18]   872.921  72.138  732.591  824.388  872.264  920.863 1015.917 1.001\nnu_c[19]   500.837  61.750  379.933  459.165  500.833  542.352  622.304 1.001\nnu_c[20]   973.213  64.682  847.399  929.984  972.492 1016.510 1100.975 1.001\nnu_c[21]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[22]   974.684  64.606  848.977  931.523  973.961 1017.918 1102.260 1.001\nnu_c[23]   837.468  75.693  690.179  786.761  837.409  887.411  987.207 1.001\nnu_c[24]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[25]   806.961  79.061  651.919  753.944  806.819  859.092  963.011 1.001\nnu_c[26]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[27]   361.790  67.419  230.804  316.064  361.775  407.072  494.146 1.001\nnu_c[28]  1024.197  62.685  901.741  981.990 1023.696 1066.397 1147.427 1.001\nnu_c[29]   383.746  65.890  255.443  338.978  383.877  428.377  513.959 1.001\nnu_c[30]   227.424  80.696   70.418  173.207  227.225  282.027  385.480 1.001\nnu_c[31]   929.086  67.436  797.581  883.929  928.507  974.166 1062.601 1.001\nnu_c[32]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[33]   823.970  77.151  673.371  772.360  823.975  874.825  976.251 1.001\nnu_c[34]  1132.331  63.085 1009.282 1089.397 1132.140 1174.378 1255.022 1.001\nnu_c[35]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[36]  1118.608  62.681  996.209 1076.318 1118.460 1160.549 1240.764 1.001\nnu_c[37]   359.446  67.594  228.237  313.609  359.371  404.935  492.136 1.001\nnu_c[38]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[39]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[40]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[41]   588.930 108.778  374.370  516.785  588.994  661.198  802.627 1.001\nnu_c[42]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[43]   455.381  62.503  332.926  413.332  455.433  497.558  578.525 1.001\nnu_c[44]   740.735  87.187  570.340  682.575  740.532  798.336  912.542 1.001\nnu_c[45]   430.231  63.398  306.488  387.377  430.247  473.506  555.669 1.001\nnu_c[46]   441.056  62.972  317.871  398.451  441.096  483.796  565.390 1.001\nnu_c[47]  1169.891  64.689 1043.692 1126.451 1169.458 1213.259 1296.198 1.001\nnu_c[48]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[49]   288.014  73.972  144.226  238.023  287.678  338.282  432.055 1.001\nnu_c[50]   928.271  67.496  796.571  883.110  927.761  973.420 1061.858 1.001\nnu_c[51]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[52]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[53]  1072.145  62.071  950.668 1030.223 1071.914 1113.477 1193.582 1.001\nnu_c[54]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[55]   988.362  63.950  863.777  945.424  987.777 1031.150 1114.910 1.001\nnu_c[56]   128.235  93.581  -53.831   65.024  127.986  190.842  312.124 1.001\nnu_c[57]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[58]   146.811  91.025  -29.725   85.618  146.778  208.016  325.036 1.001\nnu_c[59]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[60]   607.099 106.033  398.304  536.753  606.724  677.340  815.537 1.001\nnu_c[61]   152.831  90.209  -21.966   92.077  152.643  213.540  329.462 1.001\nnu_c[62]   501.200  61.748  380.299  459.536  501.200  542.677  622.629 1.001\nnu_c[63]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[64]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[65]   972.467  64.721  846.548  929.167  971.758 1015.811 1100.624 1.001\nnu_c[66]   556.390  62.378  433.787  514.490  556.439  598.405  679.073 1.001\nnu_c[67]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[68]   611.944 105.307  404.612  542.031  611.649  681.797  818.630 1.001\nnu_c[69]  1135.728  63.200 1012.346 1092.687 1135.406 1177.868 1258.411 1.001\nnu_c[70]  1055.716  62.140  934.313 1013.709 1055.420 1097.361 1177.248 1.001\nnu_c[71]   831.932  76.284  683.435  780.881  831.983  882.173  982.980 1.001\nnu_c[72]   778.404  82.440  617.205  723.291  778.203  832.481  940.706 1.001\nnu_c[73]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[74]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[75]   865.762  72.822  723.928  816.706  865.138  914.183 1010.231 1.001\nnu_c[76]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[77]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[78]  1084.918  62.121  963.388 1043.058 1084.629 1126.439 1206.235 1.001\nnu_c[79]   200.283  84.019   37.557  143.579  200.108  257.303  364.512 1.001\nnu_c[80]   331.705  69.843  196.626  284.343  331.304  378.963  469.184 1.001\nnu_c[81]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[82]   300.374  72.737  159.373  251.200  300.150  349.635  442.519 1.001\nnu_c[83]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[84]  1180.882  65.291 1053.657 1137.075 1180.348 1224.545 1308.583 1.001\nnu_c[85]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[86]  1035.128  62.435  912.887  993.050 1034.908 1077.069 1157.367 1.001\nnu_c[87]  1049.819  62.200  928.122 1007.790 1049.655 1091.573 1171.365 1.001\nnu_c[88]   480.979  61.940  359.345  439.130  480.732  522.537  602.747 1.001\nnu_c[89]  1129.820  63.003 1006.979 1087.061 1129.528 1171.899 1252.262 1.001\nnu_c[90]   972.579  64.715  846.671  929.290  971.854 1015.907 1100.674 1.001\nnu_c[91]   918.919  68.194  785.895  873.028  918.408  964.483 1053.651 1.001\nnu_c[92]   431.336  63.351  307.756  388.563  431.312  474.563  556.622 1.001\nnu_c[93]  1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[94]   449.700  62.676  326.853  407.428  449.703  492.178  573.204 1.001\nnu_c[95]  1060.214  62.106  938.643 1018.300 1059.854 1101.722 1181.800 1.001\nnu_c[96]   453.978  62.544  331.473  411.816  453.999  496.181  577.207 1.001\nnu_c[97]   775.863  82.750  614.126  720.477  775.750  830.194  939.044 1.001\nnu_c[98]   856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[99]  1075.338  62.075  953.857 1033.428 1075.046 1116.783 1197.246 1.001\nnu_c[100] 1265.739  71.736 1126.516 1217.434 1265.248 1314.108 1407.430 1.001\nnu_c[101] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[102]  567.378  62.701  444.136  525.268  567.360  609.454  691.153 1.001\nnu_c[103]  471.741  62.102  350.168  430.014  471.659  513.439  593.879 1.001\nnu_c[104]  390.904  65.439  263.424  346.660  391.057  435.257  520.019 1.001\nnu_c[105] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[106]  545.692  62.125  423.762  503.696  545.709  587.546  667.842 1.001\nnu_c[107] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[108] 1246.361  70.006 1110.454 1199.244 1245.694 1293.687 1384.062 1.001\nnu_c[109]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[110]  453.798  62.550  331.319  411.643  453.823  496.027  577.068 1.001\nnu_c[111]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[112]  829.187  76.581  679.831  777.979  829.247  879.665  980.609 1.001\nnu_c[113] 1173.521  64.882 1047.074 1129.969 1173.028 1216.966 1300.152 1.001\nnu_c[114]  579.849  63.146  455.542  537.246  579.870  622.206  704.546 1.001\nnu_c[115]  878.268  71.639  739.032  830.161  877.622  926.056 1020.183 1.001\nnu_c[116]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[117] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[118] 1087.045  62.138  965.397 1045.166 1086.754 1128.640 1208.452 1.001\nnu_c[119]  269.480  75.916  121.941  218.248  269.373  320.905  417.606 1.001\nnu_c[120]  217.486  81.893   58.804  162.411  217.173  272.976  377.719 1.001\nnu_c[121] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[122]  411.898  64.255  286.333  368.495  411.972  455.562  538.746 1.001\nnu_c[123]  340.754  69.076  206.779  293.997  340.491  387.344  476.440 1.001\nnu_c[124]  393.508  65.281  266.302  349.330  393.631  437.807  522.578 1.001\nnu_c[125]  993.860  63.712  869.841  951.039  993.414 1036.649 1119.920 1.001\nnu_c[126]  918.847  68.200  785.814  872.950  918.333  964.407 1053.593 1.001\nnu_c[127]  942.708  66.491  813.072  898.199  941.958  987.311 1074.899 1.001\nnu_c[128]  600.639  64.066  474.121  557.393  600.442  643.701  726.447 1.001\nnu_c[129]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[130]  238.751  79.362   84.821  185.310  238.637  292.281  394.532 1.001\nnu_c[131]  291.722  73.596  148.600  241.944  291.345  341.674  435.143 1.001\nnu_c[132]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[133]  272.240  75.620  125.369  221.126  272.136  323.435  419.896 1.001\nnu_c[134]  862.012  73.188  719.604  812.768  861.429  910.596 1007.066 1.001\nnu_c[135]  527.533  61.839  406.251  485.702  527.602  569.191  649.584 1.001\nnu_c[136] 1109.593  62.471  987.488 1067.352 1109.397 1151.373 1231.314 1.001\nnu_c[137]  357.166  67.767  225.565  311.261  357.093  402.750  490.291 1.001\nnu_c[138] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[139] 1014.176  62.971  891.107  971.770 1013.619 1056.509 1138.054 1.001\nnu_c[140] 1096.916  62.249  975.076 1054.895 1096.604 1138.436 1218.545 1.001\nnu_c[141]  837.029  75.740  689.575  786.289  836.974  886.966  986.871 1.001\nnu_c[142] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[143]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[144]  440.123  63.006  316.866  397.468  440.150  482.925  564.565 1.001\nnu_c[145]  401.262  64.828  274.856  357.348  401.464  445.279  529.296 1.001\nnu_c[146] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[147]  364.962  67.185  234.535  319.386  365.043  410.060  497.141 1.001\nnu_c[148] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[149]  463.035  62.297  341.267  420.976  463.007  504.950  585.569 1.001\nnu_c[150] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[151]  165.754  88.480   -5.480  106.133  165.719  225.351  339.038 1.001\nnu_c[152]  377.056  66.333  247.901  331.963  377.180  421.751  507.572 1.001\nnu_c[153]  507.359  61.735  386.562  465.664  507.384  548.947  628.816 1.001\nnu_c[154]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[155] 1029.794  62.549  907.183  987.624 1029.433 1071.778 1152.676 1.001\nnu_c[156]  676.339  95.935  487.461  612.554  676.129  739.855  863.658 1.001\nnu_c[157]  452.035  62.603  329.448  409.841  452.074  494.344  575.421 1.001\nnu_c[158] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[159] 1037.890  62.382  915.811  995.760 1037.673 1079.745 1159.808 1.001\nnu_c[160] 1097.436  62.257  975.554 1055.434 1097.111 1138.986 1219.053 1.001\nnu_c[161]  992.475  63.771  868.361  949.591  991.984 1035.225 1118.779 1.001\nnu_c[162]  197.107  84.419   33.531  140.025  197.024  254.196  361.972 1.001\nnu_c[163] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[164]  432.218  63.315  308.621  389.404  432.183  475.332  557.546 1.001\nnu_c[165]  604.453  64.258  477.469  561.030  604.294  647.567  730.810 1.001\nnu_c[166] 1108.516  62.448  986.372 1066.250 1108.262 1150.275 1230.153 1.001\nnu_c[167]  490.674  61.820  369.481  449.052  490.515  532.279  612.184 1.001\nnu_c[168]  297.238  73.045  155.364  247.821  296.911  346.776  439.597 1.001\nnu_c[169]  757.527  85.033  591.281  700.792  757.319  813.510  924.869 1.001\nnu_c[170] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[171]  769.315  83.557  606.126  713.484  769.219  824.229  933.923 1.001\nnu_c[172]  503.620  61.741  382.632  461.915  503.588  545.131  624.894 1.001\nnu_c[173]  130.676  93.242  -50.660   67.756  130.524  193.125  313.898 1.001\nnu_c[174]  959.564  65.437  832.146  915.742  958.767 1003.510 1089.361 1.001\nnu_c[175] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[176]  170.081  87.908   -0.093  110.791  169.977  229.324  342.003 1.001\nnu_c[177] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[178]  305.910  72.200  166.109  257.031  305.625  354.740  447.209 1.001\nnu_c[179]  481.343  61.934  359.708  439.493  481.098  522.917  603.097 1.001\nnu_c[180]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[181]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[182]  534.462  61.927  412.766  492.529  534.620  576.220  656.674 1.001\nnu_c[183]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[184] 1036.865  62.401  914.755  994.690 1036.621 1078.762 1158.836 1.001\nnu_c[185]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[186] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[187]  228.208  80.602   71.421  174.065  227.969  282.654  386.077 1.001\nnu_c[188]  453.709  62.552  331.241  411.562  453.718  495.945  576.989 1.001\nnu_c[189] 1079.333  62.088  958.114 1037.493 1078.952 1120.800 1200.873 1.001\nnu_c[190] 1072.557  62.071  951.115 1030.611 1072.321 1113.916 1193.961 1.001\nnu_c[191] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[192]  436.323  63.151  313.002  393.679  436.323  479.210  561.095 1.001\nnu_c[193] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[194]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[195] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[196] 1017.705  62.864  895.085  975.433 1017.095 1060.072 1141.432 1.001\nnu_c[197]  162.162  88.957  -10.108  102.143  162.094  222.163  335.939 1.001\nnu_c[198] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[199] 1067.667  62.075  945.754 1025.813 1067.496 1109.081 1188.903 1.001\nnu_c[200] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[201]  323.111  70.601  186.414  275.174  322.813  371.022  462.002 1.001\nnu_c[202] 1100.266  62.299  978.213 1058.213 1100.001 1141.787 1221.926 1.001\nnu_c[203]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[204]  923.430  67.853  790.929  877.931  922.885  968.765 1057.648 1.001\nnu_c[205]  808.787  78.852  653.982  755.927  808.727  860.683  964.561 1.001\nnu_c[206]   73.696 101.388 -123.777    5.095   73.187  141.186  273.770 1.001\nnu_c[207]  148.163  90.841  -27.996   87.123  148.121  209.274  325.924 1.001\nnu_c[208] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[209]  780.940  82.132  620.262  726.041  780.662  834.767  942.495 1.001\nnu_c[210]  772.066  83.217  609.404  716.391  771.970  826.645  936.222 1.001\nnu_c[211]  566.636  62.677  443.372  524.561  566.575  608.673  690.331 1.001\nnu_c[212] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[213] 1014.910  62.948  891.849  972.478 1014.298 1057.259 1138.751 1.001\nnu_c[214]  712.363  90.953  533.536  651.553  712.275  772.596  890.455 1.001\nnu_c[215]  992.563  63.767  868.445  949.670  992.060 1035.320 1118.845 1.001\nnu_c[216]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[217]  259.008  77.060  109.562  207.075  258.796  310.995  409.677 1.001\nnu_c[218]  910.403  68.862  776.619  864.265  909.869  956.347 1046.502 1.001\nnu_c[219]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[220]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[221] 1135.591  63.195 1012.223 1092.577 1135.274 1177.724 1258.271 1.001\nnu_c[222]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[223]  314.743  71.366  176.571  266.342  314.410  363.168  454.796 1.001\nnu_c[224]  166.929  88.324   -4.136  107.394  166.891  226.387  339.874 1.001\nnu_c[225] 1137.322  63.256 1013.909 1094.325 1137.065 1179.437 1260.155 1.001\nnu_c[226]  902.146  69.538  766.438  855.432  901.529  948.629 1039.498 1.001\nnu_c[227] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[228]  339.275  69.199  205.103  292.389  339.044  385.902  475.157 1.001\nnu_c[229]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[230] 1183.029  65.416 1055.654 1139.089 1182.527 1226.752 1310.951 1.001\nnu_c[231] 1048.167  62.221  926.479 1006.160 1047.966 1089.878 1169.762 1.001\nnu_c[232]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[233]  471.388  62.109  349.843  429.703  471.298  513.122  593.505 1.001\nnu_c[234]  756.718  85.136  590.368  699.890  756.510  812.821  924.330 1.001\nnu_c[235] 1015.861  62.919  892.961  973.494 1015.225 1058.222 1139.715 1.001\nnu_c[236]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[237]  433.514  63.262  310.113  390.746  433.533  476.546  558.628 1.001\nnu_c[238]  354.821  67.947  222.710  308.802  354.675  400.591  488.379 1.001\nnu_c[239] 1191.369  65.919 1062.956 1147.190 1190.825 1235.472 1320.154 1.001\nnu_c[240] 1008.980  63.140  885.696  966.597 1008.513 1051.395 1133.476 1.001\nnu_c[241]  710.029  91.269  530.568  649.105  709.985  770.399  888.743 1.001\nnu_c[242] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[243]  379.830  66.147  251.073  334.858  380.032  424.445  510.398 1.001\nnu_c[244]  472.134  62.094  350.590  430.446  472.051  513.847  594.245 1.001\nnu_c[245]  568.522  62.739  445.109  526.389  568.511  610.636  692.440 1.001\nnu_c[246]  525.065 118.679  290.495  445.865  525.356  604.315  757.512 1.001\nnu_c[247]  355.505  67.895  223.552  309.538  355.419  401.219  488.949 1.001\nnu_c[248] 1224.452  68.225 1091.572 1178.466 1223.865 1270.420 1358.569 1.001\nnu_c[249]  307.717  72.027  168.248  259.022  307.392  356.456  448.801 1.001\nnu_c[250] 1093.948  62.210  972.201 1052.079 1093.639 1135.454 1215.576 1.001\nnu_c[251]  313.416  71.489  174.968  264.926  313.081  361.911  453.697 1.001\nnu_c[252] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[253]  217.555  81.884   58.885  162.485  217.240  273.041  377.784 1.001\nnu_c[254] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[255] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[256]  274.688  75.359  128.526  223.717  274.514  325.774  421.700 1.001\nnu_c[257] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[258]  577.648  63.062  453.270  535.044  577.676  619.974  702.294 1.001\nnu_c[259]  288.377  73.935  144.676  238.407  288.093  338.608  432.318 1.001\nnu_c[260]  962.194  65.285  834.977  918.512  961.499 1005.948 1091.694 1.001\nnu_c[261]  289.305  73.841  145.714  239.348  289.004  339.481  433.145 1.001\nnu_c[262] 1081.546  62.099  960.130 1039.688 1081.260 1123.012 1202.804 1.001\nnu_c[263]  188.313  85.538   22.210  130.477  188.244  246.045  355.528 1.001\nnu_c[264]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[265]   56.582 103.919 -145.814  -14.036   56.083  125.730  261.937 1.001\nnu_c[266]  793.987  80.571  636.301  739.769  793.778  847.068  952.291 1.001\nnu_c[267] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[268] 1144.670  63.532 1020.502 1101.573 1144.416 1187.147 1268.349 1.001\nnu_c[269]  231.478  80.215   75.627  177.611  231.251  285.529  388.696 1.001\nnu_c[270]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[271]  327.033  70.252  191.121  279.389  326.818  374.628  465.338 1.001\nnu_c[272]  946.117  66.268  817.019  901.739  945.340  990.589 1077.745 1.001\nnu_c[273]  573.242  62.900  449.396  530.871  573.247  615.448  697.549 1.001\nnu_c[274]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[275] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[276]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[277] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[278] 1232.160  68.830 1098.163 1185.729 1231.552 1278.396 1367.565 1.001\nnu_c[279] 1009.228  63.132  885.931  966.839 1008.768 1051.619 1133.730 1.001\nnu_c[280]  164.753  88.613   -6.774  105.008  164.741  224.484  338.222 1.001\nnu_c[281] 1300.096  75.126 1155.050 1249.149 1299.742 1350.649 1449.069 1.001\nnu_c[282]  626.000  65.477  496.128  581.724  626.001  669.931  754.663 1.001\nnu_c[283]  257.657  77.210  107.917  205.546  257.496  309.786  408.557 1.001\nnu_c[284]  372.097  66.675  242.342  326.834  372.134  416.931  503.345 1.001\nnu_c[285]  715.856  90.481  537.992  655.407  715.729  775.835  893.483 1.001\nnu_c[286]  403.727  64.691  277.568  359.971  403.867  447.772  531.564 1.001\nnu_c[287] 1152.203  63.844 1027.452 1109.080 1151.926 1195.022 1276.850 1.001\nnu_c[288] 1004.235  63.307  880.782  961.825 1003.827 1046.725 1129.082 1.001\nnu_c[289]  426.669  63.551  302.407  383.738  426.792  469.977  552.243 1.001\nnu_c[290]  782.928  81.891  622.579  728.116  782.675  836.602  944.040 1.001\nnu_c[291]   95.061  98.280  -96.099   28.409   94.726  160.560  288.836 1.001\nnu_c[292]  398.652  64.977  271.841  354.557  398.829  442.788  526.913 1.001\nnu_c[293] 1142.963  63.466 1019.007 1099.823 1142.655 1185.382 1266.339 1.001\nnu_c[294]  397.086  65.069  270.034  352.930  397.223  441.330  525.547 1.001\nnu_c[295]  416.723  64.013  291.533  373.423  416.783  460.279  543.401 1.001\nnu_c[296]  155.756  89.815  -18.145   95.239  155.576  216.315  331.500 1.001\nnu_c[297]  988.642  63.937  864.104  945.693  988.055 1031.457 1115.224 1.001\nnu_c[298]  139.710  91.995  -38.810   77.675  139.478  201.461  320.181 1.001\nnu_c[299]  856.502  89.274  679.911  796.575  856.125  916.588 1031.153 1.001\nnu_c[300] 1116.904  62.638  994.629 1074.631 1116.739 1158.830 1238.954 1.001\nnu_e[1]      0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[2]      0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[3]      0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[4]      0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[5]      0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[6]      0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[7]      0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[8]      0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[9]      0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[10]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[11]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[12]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[13]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[14]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[15]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[16]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[17]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[18]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[19]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[20]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[21]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[22]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[23]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[24]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[25]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[26]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[27]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[28]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[29]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[30]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[31]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[32]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[33]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[34]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[35]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[36]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[37]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[38]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[39]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[40]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[41]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[42]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[43]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[44]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[45]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[46]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[47]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[48]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[49]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[50]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[51]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[52]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[53]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[54]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[55]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[56]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[57]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[58]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[59]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[60]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[61]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[62]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[63]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[64]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[65]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[66]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[67]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[68]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[69]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[70]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[71]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[72]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[73]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[74]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[75]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[76]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[77]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[78]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[79]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[80]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[81]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[82]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[83]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[84]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[85]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[86]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[87]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[88]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[89]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[90]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[91]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[92]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[93]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[94]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[95]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[96]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[97]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[98]     0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[99]     0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[100]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[101]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[102]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[103]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[104]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[105]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[106]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[107]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[108]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[109]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[110]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[111]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[112]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[113]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[114]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[115]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[116]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[117]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[118]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[119]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[120]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[121]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[122]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[123]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[124]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[125]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[126]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[127]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[128]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[129]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[130]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[131]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[132]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[133]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[134]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[135]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[136]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[137]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[138]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[139]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[140]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[141]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[142]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[143]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[144]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[145]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[146]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[147]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[148]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[149]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[150]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[151]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[152]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[153]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[154]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[155]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[156]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[157]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[158]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[159]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[160]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[161]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[162]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[163]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[164]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[165]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[166]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[167]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[168]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[169]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[170]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[171]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[172]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[173]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[174]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[175]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[176]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[177]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[178]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[179]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[180]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[181]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[182]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[183]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[184]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[185]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[186]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[187]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[188]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[189]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[190]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[191]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[192]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[193]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[194]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[195]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[196]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[197]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[198]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[199]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[200]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[201]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[202]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[203]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[204]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[205]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[206]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[207]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[208]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[209]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[210]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[211]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[212]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[213]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[214]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[215]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[216]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[217]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[218]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[219]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[220]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[221]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[222]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[223]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[224]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[225]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[226]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[227]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[228]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[229]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[230]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[231]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[232]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[233]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[234]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[235]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[236]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[237]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[238]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[239]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[240]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[241]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[242]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[243]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[244]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[245]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[246]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[247]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[248]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[249]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[250]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[251]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[252]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[253]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[254]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[255]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[256]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[257]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[258]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[259]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[260]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[261]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[262]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[263]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[264]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[265]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[266]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[267]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[268]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[269]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[270]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[271]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[272]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[273]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[274]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[275]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[276]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[277]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[278]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[279]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[280]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[281]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[282]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[283]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[284]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[285]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[286]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[287]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[288]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[289]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[290]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[291]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[292]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[293]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[294]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[295]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[296]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[297]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\nnu_e[298]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[299]    0.656   0.019    0.619    0.643    0.656    0.669    0.693 1.001\nnu_e[300]    0.772   0.019    0.735    0.759    0.772    0.785    0.809 1.001\ns_c        759.603  31.405  701.198  737.789  758.687  780.114  824.233 1.001\ns_e          0.231   0.009    0.214    0.225    0.231    0.237    0.251 1.001\ndeviance  4800.101   3.774 4794.804 4797.326 4799.423 4802.136 4809.117 1.001\n          n.eff\nbeta0     20000\nbeta1     20000\ngamma0    20000\ngamma1    20000\ngamma2    20000\nnu_c[1]   13000\nnu_c[2]    6900\nnu_c[3]    9900\nnu_c[4]   12000\nnu_c[5]   12000\nnu_c[6]    8800\nnu_c[7]    9500\nnu_c[8]   20000\nnu_c[9]   12000\nnu_c[10]  11000\nnu_c[11]  12000\nnu_c[12]   8800\nnu_c[13]   7000\nnu_c[14]  20000\nnu_c[15]   8200\nnu_c[16]   9300\nnu_c[17]   9900\nnu_c[18]   8100\nnu_c[19]   9700\nnu_c[20]   7100\nnu_c[21]  12000\nnu_c[22]   7100\nnu_c[23]   8700\nnu_c[24]  20000\nnu_c[25]   9300\nnu_c[26]  20000\nnu_c[27]   9800\nnu_c[28]   6900\nnu_c[29]   9700\nnu_c[30]  12000\nnu_c[31]   7400\nnu_c[32]  20000\nnu_c[33]   9000\nnu_c[34]   7600\nnu_c[35]  20000\nnu_c[36]   7500\nnu_c[37]   9800\nnu_c[38]  12000\nnu_c[39]  20000\nnu_c[40]  12000\nnu_c[41]  14000\nnu_c[42]  20000\nnu_c[43]   9400\nnu_c[44]  11000\nnu_c[45]   9500\nnu_c[46]   9400\nnu_c[47]   8300\nnu_c[48]  12000\nnu_c[49]  11000\nnu_c[50]   7400\nnu_c[51]  12000\nnu_c[52]  20000\nnu_c[53]   7000\nnu_c[54]  20000\nnu_c[55]   7000\nnu_c[56]  15000\nnu_c[57]  20000\nnu_c[58]  14000\nnu_c[59]  20000\nnu_c[60]  14000\nnu_c[61]  14000\nnu_c[62]   9700\nnu_c[63]  20000\nnu_c[64]  20000\nnu_c[65]   7100\nnu_c[66]  10000\nnu_c[67]  20000\nnu_c[68]  14000\nnu_c[69]   7700\nnu_c[70]   7000\nnu_c[71]   8800\nnu_c[72]   9900\nnu_c[73]  20000\nnu_c[74]  20000\nnu_c[75]   8300\nnu_c[76]  20000\nnu_c[77]  20000\nnu_c[78]   7100\nnu_c[79]  13000\nnu_c[80]   9900\nnu_c[81]  12000\nnu_c[82]  10000\nnu_c[83]  12000\nnu_c[84]   8500\nnu_c[85]  12000\nnu_c[86]   6900\nnu_c[87]   6900\nnu_c[88]   9500\nnu_c[89]   7600\nnu_c[90]   7100\nnu_c[91]   7500\nnu_c[92]   9500\nnu_c[93]  12000\nnu_c[94]   9400\nnu_c[95]   7000\nnu_c[96]   9400\nnu_c[97]   9900\nnu_c[98]  20000\nnu_c[99]   7100\nnu_c[100] 11000\nnu_c[101] 12000\nnu_c[102] 11000\nnu_c[103]  9500\nnu_c[104]  9600\nnu_c[105] 12000\nnu_c[106] 10000\nnu_c[107] 12000\nnu_c[108] 10000\nnu_c[109] 20000\nnu_c[110]  9400\nnu_c[111] 20000\nnu_c[112]  8900\nnu_c[113]  8400\nnu_c[114] 11000\nnu_c[115]  8100\nnu_c[116] 20000\nnu_c[117] 12000\nnu_c[118]  7100\nnu_c[119] 11000\nnu_c[120] 12000\nnu_c[121] 12000\nnu_c[122]  9500\nnu_c[123]  9900\nnu_c[124]  9600\nnu_c[125]  7000\nnu_c[126]  7500\nnu_c[127]  7300\nnu_c[128] 12000\nnu_c[129] 20000\nnu_c[130] 12000\nnu_c[131] 11000\nnu_c[132] 20000\nnu_c[133] 11000\nnu_c[134]  8300\nnu_c[135] 10000\nnu_c[136]  7400\nnu_c[137]  9800\nnu_c[138] 12000\nnu_c[139]  6900\nnu_c[140]  7200\nnu_c[141]  8700\nnu_c[142] 12000\nnu_c[143] 20000\nnu_c[144]  9400\nnu_c[145]  9600\nnu_c[146] 12000\nnu_c[147]  9800\nnu_c[148] 12000\nnu_c[149]  9500\nnu_c[150] 12000\nnu_c[151] 13000\nnu_c[152]  9700\nnu_c[153]  9700\nnu_c[154] 20000\nnu_c[155]  6900\nnu_c[156] 12000\nnu_c[157]  9400\nnu_c[158] 12000\nnu_c[159]  6900\nnu_c[160]  7200\nnu_c[161]  7000\nnu_c[162] 13000\nnu_c[163] 12000\nnu_c[164]  9400\nnu_c[165] 12000\nnu_c[166]  7300\nnu_c[167]  9600\nnu_c[168] 10000\nnu_c[169] 10000\nnu_c[170] 12000\nnu_c[171] 10000\nnu_c[172]  9700\nnu_c[173] 14000\nnu_c[174]  7200\nnu_c[175] 12000\nnu_c[176] 13000\nnu_c[177] 12000\nnu_c[178]  9300\nnu_c[179]  9500\nnu_c[180] 20000\nnu_c[181] 20000\nnu_c[182] 10000\nnu_c[183] 20000\nnu_c[184]  6900\nnu_c[185] 20000\nnu_c[186] 12000\nnu_c[187] 12000\nnu_c[188]  9400\nnu_c[189]  7100\nnu_c[190]  7000\nnu_c[191] 12000\nnu_c[192]  9400\nnu_c[193] 12000\nnu_c[194] 20000\nnu_c[195] 12000\nnu_c[196]  6900\nnu_c[197] 14000\nnu_c[198] 12000\nnu_c[199]  7000\nnu_c[200] 12000\nnu_c[201]  9800\nnu_c[202]  7300\nnu_c[203] 20000\nnu_c[204]  7500\nnu_c[205]  9300\nnu_c[206] 16000\nnu_c[207] 14000\nnu_c[208] 12000\nnu_c[209]  9800\nnu_c[210] 10000\nnu_c[211] 11000\nnu_c[212] 12000\nnu_c[213]  6900\nnu_c[214] 11000\nnu_c[215]  7000\nnu_c[216] 20000\nnu_c[217] 11000\nnu_c[218]  7600\nnu_c[219] 20000\nnu_c[220] 20000\nnu_c[221]  7700\nnu_c[222] 20000\nnu_c[223]  9700\nnu_c[224] 13000\nnu_c[225]  7700\nnu_c[226]  7700\nnu_c[227] 12000\nnu_c[228]  9900\nnu_c[229] 20000\nnu_c[230]  8600\nnu_c[231]  6900\nnu_c[232] 20000\nnu_c[233]  9500\nnu_c[234] 10000\nnu_c[235]  6900\nnu_c[236] 20000\nnu_c[237]  9400\nnu_c[238]  9800\nnu_c[239]  8800\nnu_c[240]  6900\nnu_c[241] 11000\nnu_c[242] 12000\nnu_c[243]  9700\nnu_c[244]  9500\nnu_c[245] 11000\nnu_c[246] 15000\nnu_c[247]  9800\nnu_c[248]  9600\nnu_c[249]  9400\nnu_c[250]  7200\nnu_c[251]  9600\nnu_c[252] 12000\nnu_c[253] 12000\nnu_c[254] 12000\nnu_c[255] 12000\nnu_c[256] 11000\nnu_c[257] 12000\nnu_c[258] 11000\nnu_c[259] 11000\nnu_c[260]  7100\nnu_c[261] 11000\nnu_c[262]  7100\nnu_c[263] 13000\nnu_c[264] 20000\nnu_c[265] 17000\nnu_c[266]  9600\nnu_c[267] 12000\nnu_c[268]  7800\nnu_c[269] 12000\nnu_c[270] 20000\nnu_c[271]  9800\nnu_c[272]  7300\nnu_c[273] 11000\nnu_c[274] 20000\nnu_c[275] 12000\nnu_c[276] 20000\nnu_c[277] 12000\nnu_c[278]  9900\nnu_c[279]  6900\nnu_c[280] 14000\nnu_c[281] 12000\nnu_c[282] 12000\nnu_c[283] 11000\nnu_c[284]  9700\nnu_c[285] 11000\nnu_c[286]  9500\nnu_c[287]  7900\nnu_c[288]  6900\nnu_c[289]  9500\nnu_c[290]  9800\nnu_c[291] 16000\nnu_c[292]  9600\nnu_c[293]  7800\nnu_c[294]  9600\nnu_c[295]  9500\nnu_c[296] 14000\nnu_c[297]  7000\nnu_c[298] 14000\nnu_c[299] 20000\nnu_c[300]  7400\nnu_e[1]   20000\nnu_e[2]   20000\nnu_e[3]   20000\nnu_e[4]   20000\nnu_e[5]   20000\nnu_e[6]   20000\nnu_e[7]   20000\nnu_e[8]   20000\nnu_e[9]   20000\nnu_e[10]  20000\nnu_e[11]  20000\nnu_e[12]  20000\nnu_e[13]  20000\nnu_e[14]  20000\nnu_e[15]  20000\nnu_e[16]  20000\nnu_e[17]  20000\nnu_e[18]  20000\nnu_e[19]  20000\nnu_e[20]  20000\nnu_e[21]  20000\nnu_e[22]  20000\nnu_e[23]  20000\nnu_e[24]  20000\nnu_e[25]  20000\nnu_e[26]  20000\nnu_e[27]  20000\nnu_e[28]  20000\nnu_e[29]  20000\nnu_e[30]  20000\nnu_e[31]  20000\nnu_e[32]  20000\nnu_e[33]  20000\nnu_e[34]  20000\nnu_e[35]  20000\nnu_e[36]  20000\nnu_e[37]  20000\nnu_e[38]  20000\nnu_e[39]  20000\nnu_e[40]  20000\nnu_e[41]  20000\nnu_e[42]  20000\nnu_e[43]  20000\nnu_e[44]  20000\nnu_e[45]  20000\nnu_e[46]  20000\nnu_e[47]  20000\nnu_e[48]  20000\nnu_e[49]  20000\nnu_e[50]  20000\nnu_e[51]  20000\nnu_e[52]  20000\nnu_e[53]  20000\nnu_e[54]  20000\nnu_e[55]  20000\nnu_e[56]  20000\nnu_e[57]  20000\nnu_e[58]  20000\nnu_e[59]  20000\nnu_e[60]  20000\nnu_e[61]  20000\nnu_e[62]  20000\nnu_e[63]  20000\nnu_e[64]  20000\nnu_e[65]  20000\nnu_e[66]  20000\nnu_e[67]  20000\nnu_e[68]  20000\nnu_e[69]  20000\nnu_e[70]  20000\nnu_e[71]  20000\nnu_e[72]  20000\nnu_e[73]  20000\nnu_e[74]  20000\nnu_e[75]  20000\nnu_e[76]  20000\nnu_e[77]  20000\nnu_e[78]  20000\nnu_e[79]  20000\nnu_e[80]  20000\nnu_e[81]  20000\nnu_e[82]  20000\nnu_e[83]  20000\nnu_e[84]  20000\nnu_e[85]  20000\nnu_e[86]  20000\nnu_e[87]  20000\nnu_e[88]  20000\nnu_e[89]  20000\nnu_e[90]  20000\nnu_e[91]  20000\nnu_e[92]  20000\nnu_e[93]  20000\nnu_e[94]  20000\nnu_e[95]  20000\nnu_e[96]  20000\nnu_e[97]  20000\nnu_e[98]  20000\nnu_e[99]  20000\nnu_e[100] 20000\nnu_e[101] 20000\nnu_e[102] 20000\nnu_e[103] 20000\nnu_e[104] 20000\nnu_e[105] 20000\nnu_e[106] 20000\nnu_e[107] 20000\nnu_e[108] 20000\nnu_e[109] 20000\nnu_e[110] 20000\nnu_e[111] 20000\nnu_e[112] 20000\nnu_e[113] 20000\nnu_e[114] 20000\nnu_e[115] 20000\nnu_e[116] 20000\nnu_e[117] 20000\nnu_e[118] 20000\nnu_e[119] 20000\nnu_e[120] 20000\nnu_e[121] 20000\nnu_e[122] 20000\nnu_e[123] 20000\nnu_e[124] 20000\nnu_e[125] 20000\nnu_e[126] 20000\nnu_e[127] 20000\nnu_e[128] 20000\nnu_e[129] 20000\nnu_e[130] 20000\nnu_e[131] 20000\nnu_e[132] 20000\nnu_e[133] 20000\nnu_e[134] 20000\nnu_e[135] 20000\nnu_e[136] 20000\nnu_e[137] 20000\nnu_e[138] 20000\nnu_e[139] 20000\nnu_e[140] 20000\nnu_e[141] 20000\nnu_e[142] 20000\nnu_e[143] 20000\nnu_e[144] 20000\nnu_e[145] 20000\nnu_e[146] 20000\nnu_e[147] 20000\nnu_e[148] 20000\nnu_e[149] 20000\nnu_e[150] 20000\nnu_e[151] 20000\nnu_e[152] 20000\nnu_e[153] 20000\nnu_e[154] 20000\nnu_e[155] 20000\nnu_e[156] 20000\nnu_e[157] 20000\nnu_e[158] 20000\nnu_e[159] 20000\nnu_e[160] 20000\nnu_e[161] 20000\nnu_e[162] 20000\nnu_e[163] 20000\nnu_e[164] 20000\nnu_e[165] 20000\nnu_e[166] 20000\nnu_e[167] 20000\nnu_e[168] 20000\nnu_e[169] 20000\nnu_e[170] 20000\nnu_e[171] 20000\nnu_e[172] 20000\nnu_e[173] 20000\nnu_e[174] 20000\nnu_e[175] 20000\nnu_e[176] 20000\nnu_e[177] 20000\nnu_e[178] 20000\nnu_e[179] 20000\nnu_e[180] 20000\nnu_e[181] 20000\nnu_e[182] 20000\nnu_e[183] 20000\nnu_e[184] 20000\nnu_e[185] 20000\nnu_e[186] 20000\nnu_e[187] 20000\nnu_e[188] 20000\nnu_e[189] 20000\nnu_e[190] 20000\nnu_e[191] 20000\nnu_e[192] 20000\nnu_e[193] 20000\nnu_e[194] 20000\nnu_e[195] 20000\nnu_e[196] 20000\nnu_e[197] 20000\nnu_e[198] 20000\nnu_e[199] 20000\nnu_e[200] 20000\nnu_e[201] 20000\nnu_e[202] 20000\nnu_e[203] 20000\nnu_e[204] 20000\nnu_e[205] 20000\nnu_e[206] 20000\nnu_e[207] 20000\nnu_e[208] 20000\nnu_e[209] 20000\nnu_e[210] 20000\nnu_e[211] 20000\nnu_e[212] 20000\nnu_e[213] 20000\nnu_e[214] 20000\nnu_e[215] 20000\nnu_e[216] 20000\nnu_e[217] 20000\nnu_e[218] 20000\nnu_e[219] 20000\nnu_e[220] 20000\nnu_e[221] 20000\nnu_e[222] 20000\nnu_e[223] 20000\nnu_e[224] 20000\nnu_e[225] 20000\nnu_e[226] 20000\nnu_e[227] 20000\nnu_e[228] 20000\nnu_e[229] 20000\nnu_e[230] 20000\nnu_e[231] 20000\nnu_e[232] 20000\nnu_e[233] 20000\nnu_e[234] 20000\nnu_e[235] 20000\nnu_e[236] 20000\nnu_e[237] 20000\nnu_e[238] 20000\nnu_e[239] 20000\nnu_e[240] 20000\nnu_e[241] 20000\nnu_e[242] 20000\nnu_e[243] 20000\nnu_e[244] 20000\nnu_e[245] 20000\nnu_e[246] 20000\nnu_e[247] 20000\nnu_e[248] 20000\nnu_e[249] 20000\nnu_e[250] 20000\nnu_e[251] 20000\nnu_e[252] 20000\nnu_e[253] 20000\nnu_e[254] 20000\nnu_e[255] 20000\nnu_e[256] 20000\nnu_e[257] 20000\nnu_e[258] 20000\nnu_e[259] 20000\nnu_e[260] 20000\nnu_e[261] 20000\nnu_e[262] 20000\nnu_e[263] 20000\nnu_e[264] 20000\nnu_e[265] 20000\nnu_e[266] 20000\nnu_e[267] 20000\nnu_e[268] 20000\nnu_e[269] 20000\nnu_e[270] 20000\nnu_e[271] 20000\nnu_e[272] 20000\nnu_e[273] 20000\nnu_e[274] 20000\nnu_e[275] 20000\nnu_e[276] 20000\nnu_e[277] 20000\nnu_e[278] 20000\nnu_e[279] 20000\nnu_e[280] 20000\nnu_e[281] 20000\nnu_e[282] 20000\nnu_e[283] 20000\nnu_e[284] 20000\nnu_e[285] 20000\nnu_e[286] 20000\nnu_e[287] 20000\nnu_e[288] 20000\nnu_e[289] 20000\nnu_e[290] 20000\nnu_e[291] 20000\nnu_e[292] 20000\nnu_e[293] 20000\nnu_e[294] 20000\nnu_e[295] 20000\nnu_e[296] 20000\nnu_e[297] 20000\nnu_e[298] 20000\nnu_e[299] 20000\nnu_e[300] 20000\ns_c       20000\ns_e       15000\ndeviance  20000\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 7.1 and DIC = 4807.2\nDIC is an estimate of expected predictive error (lower deviance is better).\n\n\nThe print function allows to see key posterior summaries for all parameters saved from the model, including values for posterior mean estimates, different quantiles of the posterior distribution for each parameter and diagnostic statistics such as potential scale-reduction factor or Rhat and the number of effective sample size or n.eff. Here I will not go into details about these quantities but it is enough to say that they can be used to check whether some problems occurred in the algorithm. From a first look everything seems ok. Additional checks should also be done to ensure the model behaves somewhat reasonably (e.g. no incorrect prior specification), such as posterior predictive checks, but given the simplicity of the setting I will not go into that now.\nAt this point however you should be asking, but what about the quantities I want to estimate, i.e. the mean QALYs and Total costs per treatment arm? how can I obtain these? Well a possible way to retrieve these is to post-process the results of the model. In particular, we can use our estimates for the conditional means \\(\\nu_e, \\nu_c\\) and standard deviations \\(s_e,s_c\\) in order to generate, through simulation methods, estimates for the marginal means \\(\\mu_e,\\mu_c\\) we are looking for. Although this process may seem quite complicated it is relatively simple to implement and, most importantly, can be done for most of the models we will fit, even the most complicated ones. So, in R, we do this by typing.\n\n#obtain estimates of means by arm\n\n#extract estimates for each mean parameter by trt group\nnu_e0 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==0]\nnu_e1 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_e[,trt==1]\nnu_c0 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==0]\nnu_c1 &lt;- jmodel_bn$BUGSoutput$sims.list$nu_c[,trt==1]\n#extract estimates for std\ns_e &lt;- jmodel_bn$BUGSoutput$sims.list$s_e\ns_c &lt;- jmodel_bn$BUGSoutput$sims.list$s_c\n\n#create empty vectors to contain results for means by trt group\nmu_e0 &lt;- mu_c0 &lt;- c()\nmu_e1 &lt;- mu_c1 &lt;- c()\n\n#set number of replications\nL &lt;- 5000\n\nset.seed(2345) #set seed for reproducibility\n#generate replications and take mean at each iteration of the posterior\nfor(i in 1:n.iter){\n mu_e0[i] &lt;- mean(rnorm(L,nu_e0[i,],s_e[i])) \n mu_e1[i] &lt;- mean(rnorm(L,nu_e1[i,],s_e[i]))\n mu_c0[i] &lt;- mean(rnorm(L,nu_c0[i,],s_c[i])) \n mu_c1[i] &lt;- mean(rnorm(L,nu_c1[i,],s_c[i])) \n}\n\nAt this point we obtained the final posterior estimates for our desired quantities, namely \\(\\mu_{ct}\\) and \\(\\mu_{et}\\), which can be summarised as usual, or we can even compute the incremental quantities \\(\\Delta_e=\\mu_{e1}-\\mu_{e0}\\) and \\(\\Delta_c=\\mu_{c1}-\\mu_{c0}\\) to see the distribution of the differences between mean outcomes by trt group (i.e. we look at the usual Cost-Effectivenss Plane).\n\n#compute differences by arm\nDelta_e &lt;- mu_e1 - mu_e0\nDelta_c &lt;- mu_c1 - mu_c0\n\n#plot the differences against each other\ndata_delta_ec &lt;- data.frame(Delta_e,Delta_c)\nggplot(data_delta_ec, aes(x=Delta_e, y=Delta_c)) +\n  geom_point(size=2, shape=16) + theme_classic()\n\n\n\n\n\n\n\n\nWe can then produce all standard CEA output, e.g. CEAC or CE Plane, by post-processing these posterior distributions. If you want to skip the fun, we can take advantage of the R package BCEA which is dedicated to post-processing the results from a Bayesian CEA model.\n\n#load package and provide means e and c by group as input \nlibrary(BCEA)\nmu_e &lt;- cbind(mu_e0,mu_e1)\nmu_c &lt;- cbind(mu_c0,mu_c1)\n#produce CEA output\ncea_res &lt;- bcea(eff = mu_e, cost = mu_c, ref = 2)\n\n#CE Plane (set wtp value)\nceplane.plot(cea_res, graph = \"ggplot2\", wtp = 10000)\n\n\n\n\n\n\n\n#CEAC \nceac.plot(cea_res, graph = \"ggplot2\")\n\n\n\n\n\n\n\n#other output\nsummary(cea_res)\n\n\nCost-effectiveness analysis summary \n\nReference intervention:  intervention 2\nComparator intervention: intervention 1\n\nOptimal decision: choose intervention 1 for k &lt; 4900 and intervention 2 for k &gt;= 4900\n\n\nAnalysis for willingness to pay parameter k = 25000\n\n               Expected net benefit\nintervention 1                15889\nintervention 2                18223\n\n                                    EIB   CEAC   ICER\nintervention 2 vs intervention 1 2334.1 0.9996 4827.2\n\nOptimal intervention (max expected net benefit) for k = 25000: intervention 2\n             \nEVPI 0.088877\n\n\nSo, what you think? pretty cool…. Today we only scratch the surface of fitting Bayesian models for CEA with a very simple example based on normal distributions. In next posts I will show how these models can be tailored in a way to handle all problems of CEA data without the need to become crazy to figure out a way to fit the model or how to quantify the impact of uncertainty on the CEA results.I hope that I was able to catch your attention!"
  },
  {
    "objectID": "posts/2023-11-05-my-blog-post/index.html",
    "href": "posts/2023-11-05-my-blog-post/index.html",
    "title": "A tutorial on using R to conduct trial-based CEA",
    "section": "",
    "text": "Hello dear readers, here I am with a new post about using statistics in health economic evaluations. Today I want to take a break from the quick examples I gave on using Bayesian methods in CEA and instead focus on the research work of Ben at al. (2023) entitled Conducting Trial‑Based Economic Evaluations Using R: A Tutorial, which has been recently published on PharmacoEconomics. I was very intrigued by this publication as last year I had the pleasure to meet and discuss in person with the main author of the paper, who seemed to be well aware and knowledgeable about the importance of using adequate statistical methods in trial-based CEA. From a first look at the paper I can already tell the the content is super relevant in that it provides some concrete examples and lines of code to implement what is now considered the “standard” approach to perform trial-based CEA, taking into account most of the typical complexities that affect CEA data. However, I thought having a quick review of the paper here would be the perfect chance for me to dive into the details of the methods proposed by the author and discuss their perspective and key points.\nThe paper is constructed in the form of a step-by-step tutorial on performing trial-based CEA using a variety of methods to account for possible issues affecting the data. To demonstrate how the methods can be applied in practice, they simulate a fake RCT dataset of 200 people (106 in the control and 94 in the intervention) together with other baseline variables that include age, sex, utility scores and costs. The construct a treatment variable Tr denoting whether individuals were assigned to the control (0) or intervention (1) group, and generate follow-up outcome variables for both utilities and costs for 3, 6, 9 and 12 months. Finally, they introduce missingness in all outcome follow-up variables according to a missing at random (MAR) assumption, where the chance of missingness is made dependent on the observed baseline variables age and sex. Key characteristics of the data that they outlined as important to take into account, and which are typical of most CEAs, include: presence of missing outcome data in both effectiveness and cost post-randomisation variables, high degrees of skewness for all outcome variables, presence of a correlation between utilities and costs, and presence of imbalances between the groups in some baseline variables. To tackle these issues within a single analysis, as it often occurs in practice, they suggest the use of well-known approaches in the literature, including the use of multiple imputation to handle MAR missingness, non-parametric bootstrapping to handle non-normality, seemingly unrelated regression methods to capture correlations, and regression adjustment approaches to account for baseline imbalances. Although all these methods are well known in the literature, they are hardly used jointly in a real analysis, even in the presence of all these data features, possibly because of the lack of knowledge of expertise needed in order to implement such approaches in standard software. Because of this, I really think this paper provides some useful and very heplful indications and guidelines to readers interested in these analyses and warn them about the pitfalls of relying on the use of simpler but likely inadequate methods (e.g. simply discarding missing cases or ad-hoc imputing missing values instead of accounting for properly missingness uncertainty).\nFor the purpose of this post, I will simulate my own dataset and apply the methods the author proposed to the newly generated data, so keep in mind that results will not be the same as those obtained by the authors using their own simulated data. So, let’s start with simulating a multivariate non-normal outcome variables (utilities and costs) for 200 people at multiple time points (time 0, 1, 2, 3, 4). To simulate these types of outcomes, for example, I can rely on Gamma distributions and link these distributions to each other through a regression to the mean approach in order to ensure some degree of correlation between each pair of variables. As for the temporal association, we can assume a first-order autoregressive structure to simplify the simulation of the data.\nset.seed(768)\nn &lt;- 200\nid &lt;- seq(1:n)\nTr &lt;- c(rep(0, 106),rep(1, 94))\nage &lt;- rnorm(n, 50, 17)\nsex &lt;- rbinom(n, 1, 0.4)\nbeta0_u0 &lt;- 0.3 \nbeta1_u0 &lt;- 0.1\nu0 &lt;- 1 - (beta0_u0 + beta1_u0*Tr + rgamma(n, 0.1, 2.5))\nbeta0_c0 &lt;- 0.1 \nbeta1_c0 &lt;- 0.25\nbeta2_c0 &lt;- -0.2\nc0 &lt;- beta0_c0 + beta1_c0*Tr + beta2_c0*(u0 - mean(u0)) + rgamma(n, 0.1, 2.5)\nbeta0_u1 &lt;- 0.6 \nbeta1_u1 &lt;- 0.1\nbeta2_u1 &lt;- -0.6 \nbeta3_u1 &lt;- 0.2\nu1 &lt;- 1 - (beta0_u1 + beta1_u1*Tr + beta2_u1*(c0 - mean(c0)) + beta3_u1*(u0 - mean(u0)) + rgamma(n, 0.5, 2))\nbeta0_c1 &lt;- 0.2 \nbeta1_c1 &lt;- 0.25\nbeta2_c1 &lt;- -0.2\nbeta3_c1 &lt;- 0.2\nc1 &lt;- beta0_c1 + beta1_c1*Tr + beta2_c1*(u1 - mean(u1)) + beta2_c1*(c0 - mean(c0)) + rgamma(n, 0.1, 2.5)\nbeta0_u2 &lt;- 0.6 \nbeta1_u2 &lt;- 0.1\nbeta2_u2 &lt;- -0.6 \nbeta3_u2 &lt;- 0.2\nu2 &lt;- 1 - (beta0_u2 + beta1_u2*Tr + beta2_u2*(c1 - mean(c1)) + beta3_u2*(u1 - mean(u1)) + rgamma(n, 0.5, 2))\nbeta0_c2 &lt;- 0.2 \nbeta1_c2 &lt;- 0.25\nbeta2_c2 &lt;- -0.2\nbeta3_c2 &lt;- 0.2\nc2 &lt;- beta0_c2 + beta1_c2*Tr + beta2_c2*(u2 - mean(u2)) + beta2_c2*(c1 - mean(c1)) + rgamma(n, 0.1, 2.5)\nbeta0_u3 &lt;- 0.6 \nbeta1_u3 &lt;- 0.1\nbeta2_u3 &lt;- -0.6 \nbeta3_u3 &lt;- 0.2\nu3 &lt;- 1 - (beta0_u3 + beta1_u3*Tr + beta2_u3*(c2 - mean(c2)) + beta3_u3*(u2 - mean(u2)) + rgamma(n, 0.5, 2))\nbeta0_c3 &lt;- 0.3 \nbeta1_c3 &lt;- 0.25\nbeta2_c3 &lt;- -0.2\nbeta3_c3 &lt;- 0.2\nc3 &lt;- beta0_c3 + beta1_c3*Tr + beta2_c3*(u3 - mean(u3)) + beta2_c3*(c2 - mean(c2)) + rgamma(n, 0.1, 2.5)\nbeta0_u4 &lt;- 0.8 \nbeta1_u4 &lt;- 0.1\nbeta2_u4 &lt;- -0.4 \nbeta3_u4 &lt;- 0.3\nu4 &lt;- 1 - (beta0_u4 + beta1_u4*Tr + beta2_u4*(c3 - mean(c3)) + beta3_u4*(u3 - mean(u3)) + rgamma(n, 0.01, 1.5))\nbeta0_c4 &lt;- 0.3 \nbeta1_c4 &lt;- 0.25\nbeta2_c4 &lt;- -0.2\nbeta3_c4 &lt;- 0.2\nc4 &lt;- beta0_c3 + beta1_c3*Tr + beta2_c3*(u4 - mean(u4)) + beta2_c3*(c3 - mean(c3)) + rgamma(n, 0.1, 2.5)\n\ndata_sim_ec &lt;- data.frame(id, age, sex, Tr, u0, c0, u1, c1, u2, c2, u3, c3, u4, c4)\ndata_sim_ec$c0 &lt;- data_sim_ec$c0*1000\ndata_sim_ec$c1 &lt;- data_sim_ec$c1*1000\ndata_sim_ec$c2 &lt;- data_sim_ec$c2*1000\ndata_sim_ec$c3 &lt;- data_sim_ec$c3*1000\ndata_sim_ec$c4 &lt;- data_sim_ec$c4*1000\ndata_sim_ec &lt;- data_sim_ec[sample(1:nrow(data_sim_ec)), ]\nWe can now inspect the empirical distributions of the two outcomes by treatment group to have an idea of the level of the associated skewness and examine correlation coefficients to see what type of association we should expect to find. For example, let’s examine the pair of u and c variables at time 4:\n#scatterplot of e and c data by group\nlibrary(ggplot2)\ndata_sim_ec$Trf &lt;- factor(data_sim_ec$Tr)\nlevels(data_sim_ec$Trf) &lt;- c(\"old\",\"new\")\n\ndata_sim_ec$Trf &lt;- factor(data_sim_ec$Tr)\nlevels(data_sim_ec$Trf) &lt;- c(\"old\",\"new\")\nu4_hist &lt;- ggplot(data_sim_ec, aes(x=u4))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(Trf ~ .) + theme_classic()\nc4_hist &lt;- ggplot(data_sim_ec, aes(x=c4))+\n  geom_histogram(color=\"black\", fill=\"grey\")+\n  facet_grid(Trf ~ .) + theme_classic()\ngridExtra::grid.arrange(u4_hist, c4_hist, nrow = 1, ncol = 2)\nWith a correlation coefficient between the two variables of -0.1962025. Next, we need to introduce some missing values at all follow-up times, using a MAR mechanism dependent on age and sex. We can accomplish this using a series of logistic regression using as outcome the missingness indicator for the corresponding outcome variable and including into those regressions age and sex as predictors. We then need to play around with the regression coefficients to obtain desired amount of missing values for each variable.\nset.seed(768)\nlibrary(boot)\nn &lt;- 200\ngamma0_u1 &lt;- -1.3\ngamma1_u1 &lt;- 0.5\ngamma2_u1 &lt;- 0.5\npi_u1 &lt;- inv.logit(gamma0_u1 + gamma1_u1*(age/100) + gamma2_u1*sex)\nm_u1 &lt;- rbinom(n, 1, pi_u1)\ngamma0_c1 &lt;- -1.3\ngamma1_c1 &lt;- 0.5\ngamma2_c1 &lt;- 0.5\npi_c1 &lt;- inv.logit(gamma0_c1 + gamma1_c1*(age/100) + gamma2_c1*sex)\nm_c1 &lt;- rbinom(n, 1, pi_c1)\ngamma0_u2 &lt;- -1\ngamma1_u2 &lt;- 0.5\ngamma2_u2 &lt;- 0.5\npi_u2 &lt;- inv.logit(gamma0_u2 + gamma1_u2*(age/100) + gamma2_u2*sex)\nm_u2 &lt;- rbinom(n, 1, pi_u1)\ngamma0_c2 &lt;- -1\ngamma1_c2 &lt;- 0.5\ngamma2_c2 &lt;- 0.5\npi_c2 &lt;- inv.logit(gamma0_c2 + gamma1_c2*(age/100) + gamma2_c2*sex)\nm_c2 &lt;- rbinom(n, 1, pi_c2)\ngamma0_u3 &lt;- -0.5\ngamma1_u3 &lt;- 0.5\ngamma2_u3 &lt;- 0.5\npi_u3 &lt;- inv.logit(gamma0_u3 + gamma1_u3*(age/100) + gamma2_u3*sex)\nm_u3 &lt;- rbinom(n, 1, pi_u3)\ngamma0_c3 &lt;- -0.5\ngamma1_c3 &lt;- 0.5\ngamma2_c3 &lt;- 0.5\npi_c3 &lt;- inv.logit(gamma0_c3 + gamma1_c3*(age/100) + gamma2_c3*sex)\nm_c3 &lt;- rbinom(n, 1, pi_c3)\ngamma0_u4 &lt;- 0\ngamma1_u4 &lt;- 0.5\ngamma2_u4 &lt;- 0.5\npi_u4 &lt;- inv.logit(gamma0_u4 + gamma1_u4*(age/100) + gamma2_u4*sex)\nm_u4 &lt;- rbinom(n, 1, pi_u4)\ngamma0_c4 &lt;- 0\ngamma1_c4 &lt;- 0.5\ngamma2_c4 &lt;- 0.5\npi_c4 &lt;- inv.logit(gamma0_c4 + gamma1_c4*(age/100) + gamma2_c4*sex)\nm_c4 &lt;- rbinom(n, 1, pi_c4)\n\nu1 &lt;- ifelse(m_u1==1,NA,u1)\nc1 &lt;- ifelse(m_c1==1,NA,c1)\nu2 &lt;- ifelse(m_u2==1,NA,u2)\nc2 &lt;- ifelse(m_c2==1,NA,c2)\nu3 &lt;- ifelse(m_u3==1,NA,u3)\nc3 &lt;- ifelse(m_c3==1,NA,c3)\nu4 &lt;- ifelse(m_u4==1,NA,u4)\nc4 &lt;- ifelse(m_c4==1,NA,c4)\n\ndata_sim_ec_obs &lt;- data.frame(id, Tr, age, sex, u0, c0, u1, c1, u2, c2, u3, c3, u4, c4)\ndata_sim_ec_obs$c0 &lt;- data_sim_ec_obs$c0*1000\ndata_sim_ec_obs$c1 &lt;- data_sim_ec_obs$c1*1000\ndata_sim_ec_obs$c2 &lt;- data_sim_ec_obs$c2*1000\ndata_sim_ec_obs$c3 &lt;- data_sim_ec_obs$c3*1000\ndata_sim_ec_obs$c4 &lt;- data_sim_ec_obs$c4*1000\nWe can inspect the data through the command “summary” in order to have an idea about the number and proportions of missing values for each variable in our dataset.\nsummary(data_sim_ec_obs)\n\n       id               Tr            age               sex       \n Min.   :  1.00   Min.   :0.00   Min.   :  5.697   Min.   :0.000  \n 1st Qu.: 50.75   1st Qu.:0.00   1st Qu.: 40.261   1st Qu.:0.000  \n Median :100.50   Median :0.00   Median : 50.613   Median :0.000  \n Mean   :100.50   Mean   :0.47   Mean   : 50.345   Mean   :0.405  \n 3rd Qu.:150.25   3rd Qu.:1.00   3rd Qu.: 60.196   3rd Qu.:1.000  \n Max.   :200.00   Max.   :1.00   Max.   :102.607   Max.   :1.000  \n                                                                  \n       u0                c0                u1                 c1         \n Min.   :-0.4017   Min.   :  83.03   Min.   :-1.45341   Min.   : -94.59  \n 1st Qu.: 0.5993   1st Qu.:  85.08   1st Qu.:-0.02676   1st Qu.: 211.69  \n Median : 0.6000   Median : 323.86   Median : 0.24521   Median : 363.23  \n Mean   : 0.6152   Mean   : 248.09   Mean   : 0.11125   Mean   : 351.79  \n 3rd Qu.: 0.6997   3rd Qu.: 356.89   3rd Qu.: 0.32320   3rd Qu.: 438.14  \n Max.   : 0.7000   Max.   :1131.00   Max.   : 0.69507   Max.   :1059.01  \n                                     NA's   :68         NA's   :47       \n       u2                 c2               u3                 c3        \n Min.   :-1.69971   Min.   :-346.4   Min.   :-1.82613   Min.   : 110.7  \n 1st Qu.:-0.11202   1st Qu.: 208.7   1st Qu.:-0.12191   1st Qu.: 327.5  \n Median : 0.17524   Median : 331.9   Median : 0.15356   Median : 447.7  \n Mean   : 0.04501   Mean   : 348.4   Mean   : 0.03382   Mean   : 454.9  \n 3rd Qu.: 0.27519   3rd Qu.: 441.6   3rd Qu.: 0.27746   3rd Qu.: 529.4  \n Max.   : 1.20925   Max.   :1173.7   Max.   : 0.92052   Max.   :1528.8  \n NA's   :72         NA's   :81       NA's   :96         NA's   :99      \n       u4                 c4        \n Min.   :-0.26983   Min.   : 121.7  \n 1st Qu.: 0.05507   1st Qu.: 340.5  \n Median : 0.13413   Median : 422.7  \n Mean   : 0.14381   Mean   : 463.7  \n 3rd Qu.: 0.20350   3rd Qu.: 572.4  \n Max.   : 0.84580   Max.   :1153.8  \n NA's   :124        NA's   :123\nWe have finally generated our partially-observed CEA dataset and we proceed now to implement the methods described in Ben at el. (2023) to perform the economic evaluation."
  },
  {
    "objectID": "posts/2023-11-05-my-blog-post/index.html#step-1-imputation",
    "href": "posts/2023-11-05-my-blog-post/index.html#step-1-imputation",
    "title": "A tutorial on using R to conduct trial-based CEA",
    "section": "Step 1 Imputation",
    "text": "Step 1 Imputation\nFirst, the problem of missing values is handled via multiple imputation techniques separately by treatment arm, with particular reference to its multiple imputation by chained equations version. This is typically the type of MI methods used due to the simplicity of implementation (based on univariate regression approaches) and its flexibility in dealing with data-specific features such as high levels of skewness. Indeed, at the imputation step, an imputation model is created using the predictive mean matching approach which allows to preserves the features of the data you are trying to impute (the basic idea is that imputations are generated based on some sampling with replacement approach from the observed data). I this way, M=5 imputed datasets are created and then aggregated quantities in terms of effectiveness (e.g. QALYs) and costs are generated by combining the imputations for each imputed outcome variable across datasets. In my example, since missingness only depends on age and sex, I remove all outcome variables as predictors from the imputation models to simplify the implementation. In general, since MAR cannot be tested, it is always wise to include as many observed variables as possible as predictors in the imputation model.\n\nlibrary(mice)\nTr0 &lt;- subset(data_sim_ec_obs, Tr==0)\nTr1 &lt;- subset(data_sim_ec_obs, Tr==1)\npred.Mat &lt;- make.predictorMatrix(data_sim_ec_obs)\npred.Mat[,c(\"Tr\",\"id\",\"u1\",\"c1\",\"u2\",\"c2\",\"u3\",\"c3\",\"u4\",\"c4\")] &lt;- 0\nimp.Tr0 &lt;- mice(Tr0, m=5, method = \"pmm\", predictorMatrix = pred.Mat, seed = 678)\n\n\n iter imp variable\n  1   1  u1  c1  u2  c2  u3  c3  u4  c4\n  1   2  u1  c1  u2  c2  u3  c3  u4  c4\n  1   3  u1  c1  u2  c2  u3  c3  u4  c4\n  1   4  u1  c1  u2  c2  u3  c3  u4  c4\n  1   5  u1  c1  u2  c2  u3  c3  u4  c4\n  2   1  u1  c1  u2  c2  u3  c3  u4  c4\n  2   2  u1  c1  u2  c2  u3  c3  u4  c4\n  2   3  u1  c1  u2  c2  u3  c3  u4  c4\n  2   4  u1  c1  u2  c2  u3  c3  u4  c4\n  2   5  u1  c1  u2  c2  u3  c3  u4  c4\n  3   1  u1  c1  u2  c2  u3  c3  u4  c4\n  3   2  u1  c1  u2  c2  u3  c3  u4  c4\n  3   3  u1  c1  u2  c2  u3  c3  u4  c4\n  3   4  u1  c1  u2  c2  u3  c3  u4  c4\n  3   5  u1  c1  u2  c2  u3  c3  u4  c4\n  4   1  u1  c1  u2  c2  u3  c3  u4  c4\n  4   2  u1  c1  u2  c2  u3  c3  u4  c4\n  4   3  u1  c1  u2  c2  u3  c3  u4  c4\n  4   4  u1  c1  u2  c2  u3  c3  u4  c4\n  4   5  u1  c1  u2  c2  u3  c3  u4  c4\n  5   1  u1  c1  u2  c2  u3  c3  u4  c4\n  5   2  u1  c1  u2  c2  u3  c3  u4  c4\n  5   3  u1  c1  u2  c2  u3  c3  u4  c4\n  5   4  u1  c1  u2  c2  u3  c3  u4  c4\n  5   5  u1  c1  u2  c2  u3  c3  u4  c4\n\nimp.Tr1 &lt;- mice(Tr1, m=5, method = \"pmm\", predictorMatrix = pred.Mat, seed = 678)\n\n\n iter imp variable\n  1   1  u1  c1  u2  c2  u3  c3  u4  c4\n  1   2  u1  c1  u2  c2  u3  c3  u4  c4\n  1   3  u1  c1  u2  c2  u3  c3  u4  c4\n  1   4  u1  c1  u2  c2  u3  c3  u4  c4\n  1   5  u1  c1  u2  c2  u3  c3  u4  c4\n  2   1  u1  c1  u2  c2  u3  c3  u4  c4\n  2   2  u1  c1  u2  c2  u3  c3  u4  c4\n  2   3  u1  c1  u2  c2  u3  c3  u4  c4\n  2   4  u1  c1  u2  c2  u3  c3  u4  c4\n  2   5  u1  c1  u2  c2  u3  c3  u4  c4\n  3   1  u1  c1  u2  c2  u3  c3  u4  c4\n  3   2  u1  c1  u2  c2  u3  c3  u4  c4\n  3   3  u1  c1  u2  c2  u3  c3  u4  c4\n  3   4  u1  c1  u2  c2  u3  c3  u4  c4\n  3   5  u1  c1  u2  c2  u3  c3  u4  c4\n  4   1  u1  c1  u2  c2  u3  c3  u4  c4\n  4   2  u1  c1  u2  c2  u3  c3  u4  c4\n  4   3  u1  c1  u2  c2  u3  c3  u4  c4\n  4   4  u1  c1  u2  c2  u3  c3  u4  c4\n  4   5  u1  c1  u2  c2  u3  c3  u4  c4\n  5   1  u1  c1  u2  c2  u3  c3  u4  c4\n  5   2  u1  c1  u2  c2  u3  c3  u4  c4\n  5   3  u1  c1  u2  c2  u3  c3  u4  c4\n  5   4  u1  c1  u2  c2  u3  c3  u4  c4\n  5   5  u1  c1  u2  c2  u3  c3  u4  c4\n\nimp &lt;- rbind(imp.Tr0,imp.Tr1)\nimpdat &lt;- complete(imp, action = \"long\", include = FALSE)\nM &lt;- imp[[\"m\"]]\nimpdat$Tcosts &lt;- (impdat$c1 + impdat$c2 + impdat$c3 + impdat$c4)\nimpdat$QALY &lt;- 1/2*(((impdat$u0 + impdat$u1)*0.25) + (impdat$u1 + impdat$u2)*0.25 + (impdat$u2 + impdat$u3)*0.25 + ((impdat$u3 + impdat$u4)*0.25))\nimpdata &lt;- split(impdat, f = impdat$.imp)\n\nOne thing that the authors do not mention at this point, which however I think can be quite important, is the need to check convergence of the algorithm behind mice since this is essentially based on the same iterative approach of Markov Chain Monte Carlo methods used for Bayesian inference. The main point being that, in case of potential issues in convergence, then results may change depending on how many iterations you decide to run the model before “sampling” from it the imputations. See here for a detailed explanation from the creator of mice, prof. Stef van Buuren, of why this is important to do and the different tools that can be used to do these checks. Some examples of diagnostic tools available in mice to check possible issues in the convergence of the algorithm: the plot function, by default showing the mean and variance of the imputations as a hint of the behaviour of the algorithm; the stripplot and densityplot function, respectively dedicated to compare the distributions and the kernel density estimates of imputed vs observed data.\n\nplot(imp.Tr0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstripplot(imp.Tr0)\n\n\n\n\n\n\n\ndensityplot(imp.Tr0)\n\n\n\n\n\n\n\n\nTypically, when using imputation methods such as pmm and not too many predictors in the imputation model, these checks are more a formality since the algorithm should not have big problems in the convergence or in generating weird imputations. However, I would say it is always a good idea to checks these before looking at the results based on the imputed data."
  },
  {
    "objectID": "posts/2023-11-05-my-blog-post/index.html#step-2-model-fitting",
    "href": "posts/2023-11-05-my-blog-post/index.html#step-2-model-fitting",
    "title": "A tutorial on using R to conduct trial-based CEA",
    "section": "Step 2 Model fitting",
    "text": "Step 2 Model fitting\nNext, we proceed to use a non-parametric bootstrap procedure in combination with a SUR approach fitted at the level of the imputed QALY and TCost variables. These methods, implemented through the boot and systemfit package, account for the skewness of the imputed/observed data as well as the correlation between the outcome variables. These are very typical features of trial-based CEA data which need to be taken into account. In addition, the modelling framework granted by SUR allows the inclusion of predictors into the analysis by means of a regression adjustment approach, therefore allowing us to include important variables as predictors into the model to control for possible baseline imbalances between treatment groups.\nWe start by creating a function to fit SUR models to the imputed QALY and TCost data and to extract the model estimates of interest, and then proceed to apply the boot function (here I choose to set the number of bootstrap replications to 1000) to each imputed dataset to retrieve the estimates of interest from each analysis.\n\nlibrary(boot)\nlibrary(systemfit)\nfsur &lt;- function(x,i){\n dataset &lt;- x[i,]\n r1 &lt;- Tcosts ~ Tr + c0 + age + sex\n r2 &lt;- QALY ~ Tr + u0 + age + sex\n fitsur &lt;- systemfit(list(costreg = r1, effectreg = r2), \"SUR\", data = dataset)\n betas &lt;- fitsur$coefficients\n return(c(betas[[\"costreg_Tr\"]], betas[[\"effectreg_Tr\"]]))\n}\n\n#ff &lt;- boot(data = impdata[[1]], statistic = fsur, R=1000)\nbootce &lt;- lapply(impdata, function(x) boot(data = x, statistic = fsur, R=1000))\n\nThe main idea is to fir SUR models to each imputed dataset and, within each of these, run the analysis a sufficiently large amount of times (here 1000), each time sampling with replacement so to obtain a “distribution” of the estimates of interest for each analysis fitted to each of the imputed datasets. You can imagine that this procedure can become quite computationally expensive as soon as either the number of imputations or the number of bootstrap replications are increased. Unfortunately, this is neede in order to retrieve replications of the estimates of interest that capture both sampling uncertainty (by means of boostrapping) and missing data uncertainty (by means of multiple imputations). A quick note I would like to make here is that, despite this being the most easy way to implement this approach, it is not the only one. Indeed, it is also possible to: first generate bootstrap replications of the observed data alone leaving missing values as missing, and only then apply the multiple imputation procedure within each of the replicated bootstrapped observed dataset. See for reference the paper from Brand at al. (2018)). In general, the latter is the one that makes ore sense to me since you are not generating bootstrap replications of imputed data which may or may not be an accurate representations of the actual underlying missing values, but only generate imputations after the observed data have been already bootstrapped. However, this procedure has the drawback of being much more challenging to implement due to the fact that, within each of the R boostrapped datasets you need to run M imputations. In addition, no strong evidence was found in the literature about the overperformance of one approach in comparison to the other in terms of estimates validity and efficiency, provided the model and missingness mechanism are correctly specified. In truth, there is no clear theoretical reasoning behind the performance of these approaches and conclusions on their appropriateness are mostly based on simulation results which seem to indicate no substantial difference between the two methods. Personally, I am always a bit hesitant to say that one method is better than the other given that no clear evidence as been found in the literature but I can understand that analysts always prefer the first approach due to its implementation simplicity.\nMoving on, we now proceed to extract the statistics of interest from each imputed dataset as well as from the overall bootstrapped list object. I believe it is important to note that you need to load the packages dplyr and purrr in order to obtain the output described by the author.\n\nlibrary(dplyr)\nlibrary(purrr)\nimputed &lt;- lapply(bootce, function(x)((x[[\"t0\"]])))\nimputed &lt;- lapply(imputed, setNames, c(\"cost_diff\",\"effect_diff\"))\nimputed &lt;- as.matrix(reduce(imputed, bind_rows))\npostboot &lt;- lapply(bootce, function(x) as.data.frame((x[[\"t\"]])))\npostboot &lt;- lapply(postboot, setNames, c(\"bootcost_diff\",\"booteffect_diff\"))\n\nNext, we pooled statistics of interest across imputed datasets via Rubin’rules and get additional information about imutations such as the fraction of missing information and loss of efficiency for both effectiveness and cost differential estimates.\n\npooled &lt;- apply(imputed, 2, mean)\ncost_diff_pooled &lt;- pooled[[\"cost_diff\"]]\neffect_diff_pooled &lt;- pooled[[\"effect_diff\"]]\nICER &lt;- cost_diff_pooled/effect_diff_pooled\ncov &lt;- lapply(postboot, function(x) cov(x))\nW &lt;- 1/M*(cov[[\"1\"]]+cov[[\"2\"]]+cov[[\"3\"]]+cov[[\"4\"]]+cov[[\"5\"]])\nB &lt;- matrix(0,ncol = 2, nrow = 2)\nfor(i in 1:M){\n  B &lt;- B + (matrix(imputed[i,],nrow = 2) - pooled) %*% (matrix(imputed[i,],nrow = 1) - pooled)\n}\nB &lt;- 1/(M-1)*B\ncov_pooled &lt;- (1+1/M)*B + W\n\nZa &lt;- qnorm(0.975)\nLL_cost_pooled &lt;- cost_diff_pooled - (Za*sqrt(cov_pooled[1,1]))\nUL_cost_pooled &lt;- cost_diff_pooled + (Za*sqrt(cov_pooled[1,1]))\nLL_effect_pooled &lt;- effect_diff_pooled - (Za*sqrt(cov_pooled[2,2]))\nUL_effect_pooled &lt;- effect_diff_pooled + (Za*sqrt(cov_pooled[2,2]))\n\nFMI &lt;- B/(B+W)\nLE &lt;- FMI/M"
  },
  {
    "objectID": "posts/2023-11-05-my-blog-post/index.html#step-3-cea",
    "href": "posts/2023-11-05-my-blog-post/index.html#step-3-cea",
    "title": "A tutorial on using R to conduct trial-based CEA",
    "section": "Step 3 CEA",
    "text": "Step 3 CEA\nFinally, we conclude the analysis by creating standard summaries of CE results, using the CE plane, net monetary benefit (NMB), and CEA curve. The authors write down their own code for getting this plot and I agree this is a nice idea to show how these are generated. However, for the purpose of analysts I think they can also rely on default package functions, such as bcea in the package BCEA, in order to automatically generate these plots. In case customisation of these is desired, then of course writing down your own code for the plotting is very useful.\n\nlibrary(ggplot2)\nlibrary(ggpointdensity)\npoint &lt;- data.frame(imputed)\nboot &lt;- reduce(postboot, bind_rows)\nggplot(data = boot, aes(x=booteffect_diff, y=bootcost_diff)) +\n  geom_pointdensity(aes(booteffect_diff,bootcost_diff), size=2, alpha=0.75, show.legend = FALSE, adjust = 0.05) +\n  geom_point(data = data.frame(x=mean(point$effect_diff), y = mean(point$cost_diff)), aes(x,y), color = \"red\", size=2) +\n  labs(x=\"Diff in QALY\") +\n  labs(y=\"Diff in TCost\") +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  scale_x_continuous(breaks = seq(0,1,by=0.02)) +\n  scale_y_continuous(breaks = seq(0,4500,by=1000)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nwtp &lt;- seq(0,80000,1000)\nCEAC &lt;- as.data.frame(wtp)\nNB &lt;- (wtp*effect_diff_pooled) - cost_diff_pooled\nvarNB &lt;- wtp^2*cov_pooled[2,2] + cov_pooled[1,1] -2*wtp*cov_pooled[1,2]\nCEAC$prob &lt;- pnorm(NB/sqrt(varNB))\n\nggplot(data = CEAC, aes(x=wtp, y=prob)) +\n  geom_line(colour=\"black\",linewidth=1) +\n  ylim(0,1) +\n  labs(x=\"WTP\") +\n  labs(y=\"CEAC\") +\n  geom_vline(xintercept = 0)+\n  geom_hline(yintercept = 0)+\n  theme_minimal()"
  },
  {
    "objectID": "posts/2023-11-05-my-blog-post/index.html#conclusions",
    "href": "posts/2023-11-05-my-blog-post/index.html#conclusions",
    "title": "A tutorial on using R to conduct trial-based CEA",
    "section": "Conclusions",
    "text": "Conclusions\nThe authors go in detail on the potential limitations/alternatives to the approaches used, namely the choice of the order between bootstrapping and imputing, the reliance on a norm-based approach to estimate confidence intervals, use of linear mixed models rather than cross-section SUR models, implementation of a Bayesian framework, etc. I think it is wise to mention the limitations of the approaches used to ensure transparency and in some cases these certainly apply and the anlaysts may need to think on how to solve these problems. However, I believe there is merit in showing the way they did so that others may be able, if desired, to use their methods in their own analysis, rather than just keeping using simple but likely incorrect methods which often ignore key features of CEA data.\nIt was certainly a pleasant reading for me and I hope we see more of these types of tutorials popping up in the CEA literature as they provide a strong support to practitioners in order to implement the methods advocated in the literature while also relying on some help on how to implement them in standard software. A final note which, as a Bayesian, I would like to make is that, often people complain about using a Bayesian approach in light of the complexity of building up and fitting a model with respect to a frequentist approach. Well, looking at all the steps needed in order to fit an adequate CEA model, it seems to me that the Bayesian counterpart is more more intuitive and easy to implement !!! no issues about how to combine bootstrapping with imputations, no issues to check congeniality between imputation and analysis model, no issues in the choice of the methods to derive CI bounds and parameters’ “distributions”.\nIn addition, while plots like the CEAC are commonly interpreted as showing the probability of cost-effectiveness, this makes sense only under a Bayesian approach since otherwise NB or any other parameter should not be attached any probabilistic statements! There is also a nice paper from Fenwick et. al(2004) pointing this out which is something we should keep in mind when interpreting these outputs."
  },
  {
    "objectID": "posts/2024-01-10-my-blog-post/index.html",
    "href": "posts/2024-01-10-my-blog-post/index.html",
    "title": "Why we cannot interpret relative risks in case-control studies",
    "section": "",
    "text": "Hello folks and happy new year! Back from my winter holidays, and after a couple of weeks of feeling quite sick, I am ready to resume my work both in teaching and research. With regard to this, today I would like to deviate from my usual focus on economic evaluations and focus on a more general statistical topic related to the calculation and interpretation of summary statistics for categorical data, particularly the Relative Risk and the Odds Ratio, in different contexts. Indeed, these are common measures computed within clinical trials with some binary outcome of interest (e.g. having or not having a disease) and represent also one of the simplest type of summary statistics students are introduced to in a basic statistics course. I bring this up since I am currently teaching in one such courses and I have found quite difficult to explain how to interpret these measures to students who lack a solid mathematical background. Leaving aside the actual calculation of the measures, which is trivial, it happens sometimes that students ask questions on why odds ratios can be interpreted in basically any study design context while this is not true for relative risks. Of course the answer is relatively straightforward to people who are familiar with such designs but for students I have realised myself that this is not automatic and in some cases I feel that my explanation does not reach them fully, although I am still at a loss of what I am missing. Thus, my idea to write on this blog a general example where I simulate some data to show the issues in interpreting relative risks in case-cohort study in the plainest and simplest way that I can possible think of. The hope is that, by forcing myself to write this down carefully, I will be able to identify a best way to explain and show this concept.\nSo, without further delay and with my apologies to all health economists who hoped for another post on CEA (sorry!), let me begin with presenting my example which is mostly inspired from another publicly available simulation example that can be found here.\n\nRisks and Relative Risks\nLet me start with setting up the notation I will use. Let’s imagine that we have a study, either a case-control or cohort study, in which the researchers are interested in estimating for a given patient population (of size \\(N\\)) a Relative Risk or Odds Ratio of a specific disease condition \\(Y\\) (\\(0\\)=absent,\\(1\\)=present) given the exposure status \\(X\\) (\\(0\\)=not exposed,\\(1\\)=exposed).\nThen, we can compute an estimate for the Risk of having the disease given exposure for this population as:\n\\[\n\\text{Risk}=P(Y=1 \\mid X=1) = \\frac{\\sum_i \\mathbb{I}(Y_i=1 \\mid X_i=1)}{\\sum_i \\mathbb{I}(Y_i=1 \\mid X_i=1) + \\sum_i \\mathbb{I}(Y_i=0 \\mid X_i=1)},\n\\]\nthat is the sum of all outcome values (remember that \\(Y=1\\) means presence of the disease) for all patients \\(i=1,\\ldots,N\\) who are exposed (\\(X=1\\)) over the total number of exposed patients. Now, if we want the Relative Risk of having the disease for being exposed vs not exposed, then we need to compute:\n\\[\n\\text{RR}=\\frac{P(Y=1 \\mid X=1)}{P(Y=1 \\mid X=0)},\n\\]\nwhich corresponds to dividing the risk of having the disease when exposed to the risk when not exposed (\\(X\\)=0). Now, here things get interesting as how you interpret this quantity depends on the design of the study. Let’s assume that we have a cohort study, that is patients are sampled first based on their exposure status \\(X\\) and then we observe their outcome status \\(Y\\). This means that we can directly compute the two risks of having the disease when \\(X=1\\) and when \\(X=0\\) since we already know their exposure status. Thus, the quantity:\n\\[\n\\text{Risk}_{\\text{cohort}}=P(Y=1 \\mid X=1),\n\\]\neffectively corresponds to the risk or probability of having the disease in the study given that exposure is observed (and similarly the same applies for \\(P(Y=1 \\mid X=0)\\)). Since the Relative Risk is simply the ratio of the two risks, then also RR can be computed and interpreted as the ratio of the two risks or, alternatively, how much more likely are exposed patients to have the disease compared to unexposed patients.\nThings however change when we consider a different design of the study, such as a case-control study, where patients are sampled first based on their outcome status \\(Y\\) and then we observe their exposure status \\(X\\). Indeed, under this sampling scheme, you cannot compute \\(P(Y=1 \\mid X=1)\\) or \\(P(Y=1 \\mid X=0)\\) since you can only know the exposure status of each patient after the outcome has been observed. What you can estimate is instead the risk of having exposure when \\(Y=1\\) and when \\(Y=0\\). For example, the risk of being exposed given that patients have the disease is:\n\\[\n\\text{Risk}_{\\text{case-control}}=P(X=1 \\mid Y=1) = \\frac{\\sum_i \\mathbb{I}(X_i=1 \\mid Y_i=1)}{\\sum_i \\mathbb{I}(X_i=1 \\mid Y_i=1) + \\sum_i \\mathbb{I}(X_i=0 \\mid Y_i=1)},\n\\]\nwhich of course is different from \\(P(Y=1 \\mid X=1)\\), and similarly for \\(P(X=1 \\mid Y=0) \\neq P(Y=1 \\mid X=0)\\). Thus, also the Relative Risk derived based on these probabilities:\n\\[\n\\text{RR}=\\frac{P(X=1 \\mid Y=1)}{P(X=1 \\mid Y=0)},\n\\]\ndoes not correspond to \\(\\frac{P(Y=1 \\mid X=1)}{P(Y=1 \\mid X=0)}\\).\n\n\nOdds and Odds Ratios\nWhen referring to odds and Odds Ratios, instead, the situation is different due to the different nature of the computed measures. Indeed, the Odds of having the disease when exposed is the ratio between the probability of having the disease and the probability of not having the disease when exposed. In formulae, this is expressed as:\n\\[\n\\text{Odds}=\\frac{P(Y=1 \\mid X=1)}{P(Y=0 \\mid X=1)},\n\\]\nwhich is always interpreted in relative terms, that is as how much more chance you have of having the disease compared to not having the disease given that you are exposed. From this, we can derive the formula of the Odds Ratio of having the disease which corresponds to the ratio of the odds for having the disease when exposed vs when unexposed:\n\\[\n\\text{OR}=\\frac{\\frac{P(Y=1 \\mid X=1)}{P(Y=0 \\mid X=1)}}{\\frac{P(Y=1 \\mid X=0)}{P(Y=0 \\mid X=0)}},\n\\]\nwhich is interpreted as how many more odds you have of having the disease when exposed compared to when unexposed.\nIn the context of a cohort study, we can immediately see that, given that we first sample patients based on their exposure status \\(X\\), the OR can be directly interpreted without any issues since we can estimate all odds within the ratio. In the context of a case-control study, let us consider the odds of having the disease given that patients are exposed:\n\\[\n\\text{Odds}_{\\text{case-control}}=\\frac{P(Y=1 \\mid X=1)}{P(Y=0 \\mid X=1)},\n\\]\nwhose numerator and denominator can also be re-expressed using the conditional probability rule as:\n\\[\nP(Y=1 \\mid X=1)= \\frac{P(Y=1)P(X=1 \\mid Y=1)}{P(X=1)} \\;\\;\\; \\text{and} \\;\\;\\; P(Y=0 \\mid X=1)= \\frac{P(Y=0)P(X=1 \\mid Y=0)}{P(X=1)}\n\\]\nwhich leads to:\n\\[\n\\text{Odds(X=1)}_{\\text{case-control}}=\\frac{\\frac{P(Y=1)P(X=1 \\mid Y=1)}{P(X=1)}}{\\frac{P(Y=0)P(X=1 \\mid Y=0)}{P(X=1)}},\n\\]\nSimilarly, for the odds of being exposed given that the patients did not have the disease are:\n\\[\n\\text{Odds(X=0)}_{\\text{case-control}}=\\frac{\\frac{P(Y=1)P(X=0 \\mid Y=1)}{P(X=0)}}{\\frac{P(Y=0)P(X=0 \\mid Y=0)}{P(X=0)}},\n\\]\nIf we then calculate the Odds Ratio we get:\n\\[\n\\text{OR}_{\\text{case-control}}=\\frac{\\frac{P(Y=1 \\mid X=1)}{P(Y=0 \\mid X=1)}}{\\frac{P(Y=1 \\mid X=0)}{P(Y=0 \\mid X=0)}}=\\frac{\\frac{\\frac{P(Y=1)P(X=1 \\mid Y=1)}{P(X=1)}}{\\frac{P(Y=0)P(X=1 \\mid Y=0)}{P(X=1)}}}{\\frac{\\frac{P(Y=1)P(X=0 \\mid Y=1)}{P(X=0)}}{\\frac{P(Y=0)P(X=0 \\mid Y=0)}{P(X=0)}}} = \\frac{\\frac{P(X=1 \\mid Y=1)}{P(X=1 \\mid Y=0)}}{\\frac{P(X=0 \\mid Y=1)}{P(X=0 \\mid Y=0)}} = \\frac{\\frac{P(X=1 \\mid Y=1)}{P(X=0 \\mid Y=1)}}{\\frac{P(X=1 \\mid Y=0)}{P(X=0 \\mid Y=0)}}.\n\\]\nThe above formula shows how the OR can also be calculated based on \\(P(X=1 \\mid Y=1)\\), \\(P(X=0 \\mid Y=1)\\), \\(P(X=1 \\mid Y=0)\\) and \\(P(X=0 \\mid Y=0)\\), that is the probabilities of having a specific exposure status \\(X\\) given the outcome status \\(Y\\), which are directly available in a case-control study. Therefore, the OR is an effect size measure that is adequate for both case-control and cohort designs, because they all measure the same thing.\n\n\nExample\nHere I will try to empirically show the differences between RR and OR in a hypothetical scenario. First I generate data for a population of 1 million people, and this population will be divided in \\(25\\%\\) who smoke and \\(75\\%\\) who do not smoke, where smoking is the exposure variable \\(X\\). I set a RR of 2 when considering the probability of someone who smokes to get cancer, the disease variable \\(Y\\). Thus, I set \\(5\\%\\) of the smoking population to have cancer, and \\(2.5\\%\\) of the nonsmoking population to have cancer.\n\nset.seed(1234)\npop &lt;- data.frame(smoke = sample(c(\"Smokes\", \"NeverSmoked\"), 1e6, prob = c(0.25, 0.75), rep= T))\npop[which(pop$smoke==\"Smokes\"), \"cancer\"] &lt;- sample(c(\"Cancer\", \"Healthy\"), sum(pop$smoke==\"Smokes\"), prob = c(0.05, 0.95), rep= T)\npop[which(pop$smoke==\"NeverSmoked\"), \"cancer\"] &lt;- sample(c(\"Cancer\", \"Healthy\"), sum(pop$smoke==\"NeverSmoked\"), prob = c(0.025, 0.975), rep= T)\n\n# Plot the simulated population\nlibrary(ggplot2)\npop2 &lt;- unique(pop)\nfor(i in 1:nrow(pop2)){\n  pop2[i, \"counts\"] &lt;- sum(pop$smoke==pop2[i, \"smoke\"] & pop$cancer==pop2[i, \"cancer\"])\n}\nggplot(pop2, aes(x = cancer, y = counts, fill = smoke)) +\ngeom_bar(stat = \"identity\") +\ntheme_bw() \n\n\n\n\n\n\n\n\nTo simulate a cohort study we need to draw a sample from this population.\n\n# Determine sample size with alpha = 5% and power = 80%\nsample.size &lt;- power.prop.test(p1 = 0.02, p2 = 0.01, power = 0.8)\n\n# Now let's draw this amount of patients from each condition in our dataset.\n## Draw a sample from smokers (we still don't know if they will get cancer or not)\nsample.smokes &lt;- pop[which(pop$smoke==\"Smokes\"),][sample(c(1:sum(pop$smoke==\"Smokes\")), sample.size$n, replace = F),]\n## Draw a sample from nonsmokers (we still don't know if they will get cancer or not)\nsample.neversmoked &lt;- pop[which(pop$smoke==\"NeverSmoked\"),][sample(c(1:sum(pop$smoke==\"NeverSmoked\")), sample.size$n, replace = F),]\n\n# Check our RR\na &lt;- sum(sample.smokes$cancer == \"Cancer\")/sum(nrow(sample.smokes))\nb &lt;- sum(sample.neversmoked$cancer == \"Cancer\")/sum(nrow(sample.neversmoked))\na/b \n\n[1] 1.758621\n\n\nWe see that everytime this script is run, it returns a different value for RR. This is because sampling error occurs, that is why alpha is \\(5\\%\\) and power is \\(80\\%\\). We expect a rate of false positives and false negatives. We can plot the differences in RR after 100 different measurements (which is equivalent to 100 different studies looking at the same population, each one finding a different RR value.\n\n# Real RR\na &lt;- sum(pop$smoke==\"Smokes\" & pop$cancer==\"Cancer\")/sum(pop$smoke==\"Smokes\")\nb &lt;- sum(pop$smoke==\"NeverSmoked\" & pop$cancer==\"Cancer\")/sum(pop$smoke==\"NeverSmoked\")\nRealRR &lt;- a/b\n\n# Sample RR\nSampleRR &lt;- c()\nfor(i in 1:100){\n  sample.smokes &lt;- pop[which(pop$smoke==\"Smokes\"),][sample(c(1:sum(pop$smoke==\"Smokes\")), sample.size$n, replace = F),]\n  sample.neversmoked &lt;- pop[which(pop$smoke==\"NeverSmoked\"),][sample(c(1:sum(pop$smoke==\"NeverSmoked\")), sample.size$n, replace = F),]\n  a &lt;- sum(sample.smokes$cancer == \"Cancer\")/sum(nrow(sample.smokes))\n  b &lt;- sum(sample.neversmoked$cancer == \"Cancer\")/sum(nrow(sample.neversmoked))\n  SampleRR[i] &lt;- a/b\n}\n\nggplot(data = data.frame(RR = c(RealRR, SampleRR), \n                         Group = c(\"Real\", rep(\"Sample\", 100))), aes(x = Group, y = RR))+ geom_boxplot(aes(colour=Group)) +\ngeom_point(size = 3, aes(colour=Group)) +\ntheme_bw()\n\n\n\n\n\n\n\n\nNow, let’s proceed to get the estimate of the OR in a case-control simulation. For a case-control study, we draw a sample from random people who have cancer, and the same number of people who do not have cancer, and check if they have smoked or not in the past.\n\n# Define the sample size for a case-control study\nlibrary(epiR)\n\nsample.size.cc &lt;- epi.sscc(OR = 2, p0 = 0.2, power = 0.8, n = NA)$n.case\n\n# Draw samples of people who have cancer or not.\nsample.cancer &lt;- pop[which(pop$cancer==\"Cancer\"),][sample(c(1:sum(pop$cancer==\"Cancer\")), sample.size.cc, replace = F),]\nsample.healthy &lt;- pop[which(pop$cancer==\"Healthy\"),][sample(c(1:sum(pop$cancer==\"Healthy\")), sample.size.cc, replace = F),]\n\n# Determine 100 OR calculations\nOR80 &lt;- c()\nfor(i in 1:100){\n  sample.cancer &lt;- pop[which(pop$cancer==\"Cancer\"),][sample(c(1:sum(pop$cancer==\"Cancer\")), sample.size.cc, replace = F),]\nsample.healthy &lt;- pop[which(pop$cancer==\"Healthy\"),][sample(c(1:sum(pop$cancer==\"Healthy\")), sample.size.cc, replace = F),]\n  a &lt;- sum(sample.cancer$smoke == \"Smokes\")/sum(sample.healthy$smoke == \"Smokes\")\n  b &lt;- sum(sample.cancer$smoke == \"NeverSmoked\")/sum(sample.healthy$smoke == \"NeverSmoked\")\n  OR80[i] &lt;- a/b\n}\n\n# Plot differences\nggplot(data = data.frame(RR = c(RealRR, OR80), \n                         Group = c(\"Real\", rep(\"Odds Ratio\", 100))),aes(x = Group, y = RR)) +\ngeom_boxplot(aes(colour=Group)) +\ngeom_point(size = 3, aes(colour=Group)) +\ntheme_bw()\n\n\n\n\n\n\n\n\nFinally, let’s compare the OR and the RR obtained previously.\n\nggplot(data = data.frame(RR = c(RealRR, SampleRR, OR80), \n      Group = c(\"Real\", rep(\"Risk Ratio\", 100), rep(\"Odds Ratio\", 100))),\n      aes(x = Group, y = RR)) +\ngeom_boxplot(aes(colour=Group)) +\ngeom_point(size = 3, aes(colour=Group))+\ntheme_bw()\n\n\n\n\n\n\n\n\nWe can see very clearly than under optimal circumstances, the OR is very close to the RR, which in turn is a good, but far from perfect, estimate of the true risk. Not let’s try something forbidden by the rules of statistics. The Risk Ratio should not be calculated using a case-control design, but let’s do it here to show what it produces. Additionally, I will calculate an OR from the cohort study as well.\n\n# Determine 100 forbidden RR calculations from case-control studies\nforbiddenRR &lt;- c()\nfor(i in 1:100){\n  sample.cancer &lt;- pop[which(pop$cancer==\"Cancer\"),][sample(c(1:sum(pop$cancer==\"Cancer\")), sample.size.cc, replace = F),]\nsample.healthy &lt;- pop[which(pop$cancer==\"Healthy\"),][sample(c(1:sum(pop$cancer==\"Healthy\")), sample.size.cc, replace = F),]\n  smoked &lt;- sum(sample.cancer$smoke == \"Smokes\") + sum(sample.healthy$smoke == \"Smokes\")\n  neversmoked &lt;- sum(sample.cancer$smoke == \"NeverSmoked\") + sum(sample.healthy$smoke == \"NeverSmoked\")\n  a &lt;- sum(sample.cancer$smoke == \"Smokes\")/smoked\n  b &lt;- sum(sample.cancer$smoke == \"NeverSmoked\")/neversmoked\n  forbiddenRR[i] &lt;- a/b\n}\n\n# Determine 100 OR calculations from cohort studies\nallowedOR &lt;- c()\nfor(i in 1:100){\n  sample.smokes &lt;- pop[which(pop$smoke==\"Smokes\"),][sample(c(1:sum(pop$smoke==\"Smokes\")), sample.size$n, replace = F),]\n  sample.neversmoked &lt;- pop[which(pop$smoke==\"NeverSmoked\"),][sample(c(1:sum(pop$smoke==\"NeverSmoked\")), sample.size$n, replace = F),]\n  a &lt;- sum(sample.smokes$cancer == \"Cancer\")/sum(sample.smokes$cancer == \"Healthy\")\n  b &lt;- sum(sample.neversmoked$cancer == \"Cancer\")/sum(sample.neversmoked$cancer == \"Healthy\")\n  allowedOR[i] &lt;- a/b\n}\n\n# Plot differences\nggplot(data = data.frame(RR = c(RealRR, OR80, forbiddenRR, SampleRR, allowedOR), \n                         Effect.size = c(\"Real\", \n                                   rep(\"OR\", 100), \n                                   rep(\"RR\", 100), \n                                   rep(\"RR\", 100), \n                                   rep(\"OR\", 100)),\n                         Study.type = c(\"Real\", \n                                   rep(\"case-control\", 100),\n                                   rep(\"case-control\", 100), \n                                   rep(\"cohort\", 100), \n                                   rep(\"cohort\", 100))),\n       aes(x = Study.type, y = RR)) +\ngeom_boxplot(aes(colour=Effect.size)) +\ngeom_point(position = position_dodge(width=0.75), aes(colour=Effect.size))+\ntheme_bw()\n\n\n\n\n\n\n\n\nThis shows something really interesting. We can see that the distributions of the “allowed” calculations are all similar, and they wander around the true risk figure. However, the “forbidden” calculation, which is the RR in a case-control study, has a really narrow distribution of values that never get close to the true figure.\n\n\nConclusion\nHere I tried to explain in a visual and a theoretical way why the OR is an effect size measurement that can be calculated in either a cohort or a case-control study, because they are mathematically the same. However, the RR can only be calculated using a cohort study design, while a case-control will only be able to offer an OR, and that is mathematically true.\nHopefully this will make things easier to understand for my students!"
  },
  {
    "objectID": "posts/2024-03-13-my-blog-post/index.html",
    "href": "posts/2024-03-13-my-blog-post/index.html",
    "title": "Rasch Model",
    "section": "",
    "text": "Hello everyone, and welcome back to my blog. Today I would like to resume a topic that I promised myself I will study more but which I kind of left alone for quite some time now. I initially introduced the topic a few months ago in another post, using a primary reference the book of Jean-Paul Fox called Bayesian Item Response Modelling. With this post I would like to take over from where I left and continue talking a bit about Item Response Theory (IRT).\nLast time I focussed on IRT models, I introduced the simplest type of model for binary IRT response data, called the Rasch Model or one-parameter logistic response model, in which the probability of a correct response is given by:\n\\[\nP(Y_{ik}=1 | \\theta_i,b_k) = \\frac{\\text{exp}(\\theta_i - b_k)}{1+\\text{exp}(\\theta_i-b_k)},\n\\]\nfor individual \\(i\\) with ability level \\(\\theta_i\\) and item \\(k\\) with item-difficulty parameter \\(b_k\\). Now, let’s try to simulate some data according to the Rasch model.\nLet’s start by considering a simple questionnaire example formed by \\(K=2\\) dichotomous items, and for each of these the \\(i\\)-th respondent in the dataset may provide either a negative (e.g. wrong/failure) or positive (e.g. correct/success) response \\(Y_{ik}=0\\) or \\(Y_{ik}=1\\) according to some probability which in turn depends on some person-specific ability level \\(\\theta_i\\) and item-specific difficulty level \\(b_k\\). We proceed as follows:\n\nSimulate item difficulties \\(b_k\\) using a uniform distribution, i.e. \\(b_k\\sim \\text{Uniform}(a_b,b_b)\\) for \\(k=1,K=2\\).\nSimulate the person abilities \\(\\theta_i\\) using a normal distribution, i.e. \\(\\theta_i \\sim \\text{Normal}(\\mu_{\\theta},\\sigma_{\\theta})\\) for \\(i=1,\\ldots,N=100\\).\nUse the generated values of \\(\\theta_i\\) and \\(b_k\\) to obtain \\(P(Y_{ik}=1 | \\theta_i,b_k)\\), i.e. the probability of giving the correct response for the \\(i\\)-th person on the \\(k\\)-th item using the Item Characteristic Curve (ICC) equation of the Rasch model.\n\n\nset.seed(7689)\n\nK &lt;- 2\nb &lt;- runif(K,-1,1) \nN &lt;- 10\ntheta &lt;- rnorm(N,0,2)\ntemp &lt;- matrix( rep( theta, length( b ) ) , ncol = length( b ) )\np_resp &lt;- matrix(NA, nrow = N, ncol = K)\nfor(i in 1:N){\n for(k in 1:K){\n  p_resp[i,k] &lt;- (exp(theta[i] - b[k])) / (1 + exp(theta[i] - b[k]))\n }\n}\nobs_resp &lt;- matrix( sapply( c(p_resp), rbinom, n = 1, size = 1), ncol = length(b) )\n\n#put everything into a function\nsim_rasch &lt;- function(N,K,a_b=-1,b_b=1,mu_theta=0,sigma_theta=2){\nb &lt;- runif(K,a_b,b_b) \ntheta &lt;- rnorm(N,mu_theta,sigma_theta)\ntemp &lt;- matrix( rep( theta, length( b ) ) , ncol = length( b ) )\np_resp &lt;- matrix(NA, nrow = N, ncol = K)\nfor(i in 1:N){\n for(k in 1:K){\n  p_resp[i,k] &lt;- (exp(theta[i] - b[k])) / (1 + exp(theta[i] - b[k]))\n }\n}\n obs_resp &lt;- matrix( sapply( c(p_resp), rbinom, n = 1, size = 1), ncol = length(b) )\n output &lt;- list(\"y\"=obs_resp, \"p\"=p_resp, \"theta\"=theta, \"b\"=b)\n return(output)\n}\n\nobs_resp\n\n      [,1] [,2]\n [1,]    0    0\n [2,]    0    0\n [3,]    0    0\n [4,]    1    1\n [5,]    1    0\n [6,]    1    1\n [7,]    0    1\n [8,]    0    0\n [9,]    0    0\n[10,]    0    1\n\n\nNow, let’s put everything we have done into a function so that we can customise the output as much as we like, for example considering a sample of \\(N=25\\) people who answer a set of \\(K=5\\) dichotomous items:\n\nset.seed(7689)\n\n#put everything into a function\nsim_rasch &lt;- function(N,K,a_b=-1,b_b=1,mu_theta=0,sigma_theta=2){\nb &lt;- runif(K,a_b,b_b) \ntheta &lt;- rnorm(N,mu_theta,sigma_theta)\ntemp &lt;- matrix( rep( theta, length( b ) ) , ncol = length( b ) )\np_resp &lt;- matrix(NA, nrow = N, ncol = K)\nfor(i in 1:N){\n for(k in 1:K){\n  p_resp[i,k] &lt;- (exp(theta[i] - b[k])) / (1 + exp(theta[i] - b[k]))\n }\n}\n obs_resp &lt;- matrix( sapply( c(p_resp), rbinom, n = 1, size = 1), ncol = length(b) )\n output &lt;- list(\"y\"=obs_resp, \"p\"=p_resp, \"theta\"=theta, \"b\"=b)\n return(output)\n}\n\ndata_sim &lt;- sim_rasch(N=50,K=5)\n\n#extract observed data\ndata_sim$y\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    1    1\n [2,]    0    0    1    0    1\n [3,]    0    0    0    0    0\n [4,]    1    1    1    1    1\n [5,]    1    1    0    1    1\n [6,]    1    1    1    1    1\n [7,]    0    0    1    0    0\n [8,]    1    1    1    1    1\n [9,]    1    1    1    1    1\n[10,]    1    0    1    0    1\n[11,]    0    0    0    0    0\n[12,]    1    0    0    0    0\n[13,]    0    0    0    1    0\n[14,]    1    1    1    1    1\n[15,]    1    1    1    1    1\n[16,]    0    1    0    0    1\n[17,]    0    0    0    0    0\n[18,]    0    1    0    0    1\n[19,]    1    1    1    1    1\n[20,]    0    0    0    0    1\n[21,]    1    1    0    1    0\n[22,]    0    0    0    0    0\n[23,]    0    0    1    0    0\n[24,]    0    0    0    1    0\n[25,]    0    0    1    1    1\n[26,]    0    0    0    0    0\n[27,]    0    0    1    1    1\n[28,]    0    0    1    0    0\n[29,]    1    1    1    1    1\n[30,]    0    1    0    0    0\n[31,]    1    1    1    1    1\n[32,]    0    0    1    0    1\n[33,]    0    0    0    0    1\n[34,]    0    0    0    1    1\n[35,]    1    0    1    0    1\n[36,]    1    0    1    1    1\n[37,]    1    0    1    0    1\n[38,]    1    1    1    1    1\n[39,]    0    0    0    1    1\n[40,]    1    1    1    1    1\n[41,]    0    0    0    0    0\n[42,]    0    0    0    0    1\n[43,]    0    0    1    0    0\n[44,]    1    1    0    0    1\n[45,]    0    0    0    1    0\n[46,]    1    1    1    1    1\n[47,]    1    0    1    0    1\n[48,]    1    1    1    1    1\n[49,]    0    1    0    1    0\n[50,]    0    0    1    1    0\n\n\nOk cool, now that we simulated the data, let’s try to use one of the many R packages to fit a Rasch model to the data and see whether the model fits them as it should be. For this purpose, I will use the eRm package, specifically dedicated to the fitting and checking of Rasch models to the data. I refer to this webpage for a more in depth explanation of the package and its function, from which most of the stuff I will show is taken from.\n\n#load package\nlibrary(eRm)\n\n#fit model to simulated item responses\nitems &lt;- data_sim$y\nK &lt;- ncol(items)\nN &lt;- nrow(items)\ncolnames(items) &lt;- c(sprintf(\"item%01d\", seq(1,K)))\n#fit the model\nraschfit &lt;- RM(items)\n\nAfter fitting the model and saving the output in the object raschfit, we can inspect the coefficient estimates of the model both in terms of person and item-specific parameters:\n\n#print estimates\n\ni.param &lt;- coef(raschfit)\n#item parameters/difficulties\ni.param\n\nbeta item1 beta item2 beta item3 beta item4 beta item5 \n-0.4359822 -0.7641137  0.2419883  0.1154749  0.8426327 \n\np.param &lt;- person.parameter(raschfit)\n#person parameters/abilities\np.param\n\n\nPerson Parameters:\n\n Raw Score   Estimate Std.Error\n         0 -2.5773973        NA\n         1 -1.4777665 1.1455225\n         2 -0.4349030 0.9460435\n         3  0.4371723 0.9454384\n         4  1.4776048 1.1439625\n         5  2.5743185        NA\n\n#plot item and persons together\nplotPImap(raschfit)\n\n\n\n\n\n\n\n\nWe can also obtain the ICC function for each item. For example, let’s say we want to show the ICC for item \\(2\\) and \\(4\\):\n\n#item 2\nplotICC(raschfit, item.subset = c(2), ask=F)\n\n\n\n\n\n\n\n#item 4\nplotICC(raschfit, item.subset = c(4), ask=F)\n\n\n\n\n\n\n\n\nNext, it may be useful to test some of the key assumptions of the Rasch model to see whether the model seems to be reasonable for the data at hand. Among the most popular test procedures that are available in in the package, we can consider the following:\n\nAndersen’s conditional Likelihood Ratio Test.\n\nIt is a surprising result that we can take our items and then get the correct estimates of all the item parameters \\(\\boldsymbol b=(b_1,\\ldots,b_K)\\) in the any of these sub samples. This only work because we use conditional inference. It works if we divide the sample into two or more groups using any splitting criterion. We can do it in eRm:\n\ngr &lt;- cut(\n  rowSums(items),\n  breaks=c(0,2,5),\n  include.lowest = TRUE)\nLRtest(raschfit, splitcr=gr)\n\n\nAndersen LR-test: \nLR-value: 8.896 \nChi-square df: 4 \np-value:  0.064 \n\n#graphically\nlr &lt;- LRtest(raschfit, splitcr = gr, se = T)\nplotGOF(lr)\n\n\n\n\n\n\n\nplotGOF(lr, conf=list(), xlim=c(-4,4), ylim=c(-5,5))\n\n\n\n\n\n\n\n\n\nWald test\n\nThe test allows for testing each item \\(k\\). The idea is the same: divide sample into two subgroups. Item parameters should be invariant\n\nWaldtest(raschfit, splitcr = gr)\n\n\nWald test on item level (z-values):\n\n           z-statistic p-value\nbeta item1      -2.081   0.037\nbeta item2       1.385   0.166\nbeta item3       0.494   0.622\nbeta item4       1.729   0.084\nbeta item5      -0.391   0.696\n\n\n\nInfit and Outfit test statistics\n\nExpected response matrix obtained from \\(\\pi_{ij}=\\frac{\\text{exp}(\\hat{b}_k+\\hat{\\theta}_i)}{1+\\text{exp}(\\hat{b}_k+\\hat{\\theta}_i)}\\), while residuals are defined as \\(e_{ik}=y_{ik}-\\pi_{ik}\\). From these quantities, we can retrieve the two statistics: \\(\\text{INFIT}_k=w_k=\\frac{1}{N}\\frac{\\sum_i e^2_{ik}}{\\sum_i \\nu_{ik}}\\) and \\(\\text{OUTFIT}_k=u_k=\\frac{1}{N}\\sum_i \\frac{e^2_{ik}}{\\nu_{ik}}\\), where \\(\\nu_{ik}=\\pi_{ik}\\times (1-\\pi_{ik})\\). The INFIT and OUTFIT item fit test statistics have expected value \\(1\\). Informal evaluation: \\(0.7\\) to \\(1.3\\) is fine (\\(0.5\\) to \\(1.5\\) is OK). The interpretation of the OUTFIT statistic is sensitive against outlying observations e.g. when a very able person gets an easy item wrong. To calculate in eRm we have to use the p.param object:\n\nitemfit(p.param)\n\n\nItemfit Statistics: \n       Chisq df p-value Outfit MSQ Infit MSQ Outfit t Infit t Discrim\nitem1 20.869 30   0.892      0.673     0.689   -1.297  -1.771   2.353\nitem2 31.971 30   0.369      1.031     1.053    0.204   0.308  -0.228\nitem3 37.423 30   0.165      1.207     1.137    1.118   0.897  -0.921\nitem4 34.630 30   0.256      1.117     1.136    0.650   0.875  -1.691\nitem5 23.833 30   0.780      0.769     0.806   -1.198  -1.364   0.715\n\n#use iarm package to shows p-values less than 0.05\nlibrary(iarm)\nout_infit(raschfit)\n\n\n      Outfit se    pvalue padj sig  Infit se    pvalue padj  sig \nitem1 0.705  0.299 0.324  1         0.716 0.205 0.165  0.772     \nitem2 1.137  0.38  0.718  1         1.124 0.244 0.611  1         \nitem3 1.253  0.195 0.195  1         1.165 0.162 0.309  0.772     \nitem4 1.155  0.206 0.452  1         1.174 0.167 0.298  0.772     \nitem5 0.779  0.223 0.321  1         0.819 0.15  0.227  0.772     \n\nP value adjustment: BH\n\n\nItem-total correlations and item-score correlations are routinely reported in classical test theory. We can use the simple structure in the Rasch model to compute the expected values of the item-score correlation:\n\nitem_restscore(raschfit)\n\n      observed expected se     pvalue padj.BH sig\nitem1 0.8768   0.7233   0.0851 0.0714 0.3568     \nitem2 0.7057   0.7603   0.1295 0.6732 0.6732     \nitem3 0.5632   0.6538   0.1507 0.5476 0.6732     \nitem4 0.5770   0.6661   0.1446 0.5378 0.6732     \nitem5 0.7328   0.5992   0.1174 0.2550 0.6374     \n\n\n\nICC plots compared to empirical ICC\n\nData with no missing values - score distribution:\n\ns &lt;- rowSums(items)\ntable(s)\n\ns\n 0  1  2  3  4  5 \n 6 12  9  8  2 13 \n\nhist(s)\n\n\n\n\n\n\n\nfit&lt;-RM(items)\nppar &lt;- person.parameter(raschfit)\n\n#one to one correspondence between y and theta\nplot(ppar)\n\n\n\n\n\n\n\n#we look at the item 2\nplotICC(fit,\nitem.subset = c(2),\nempICC = list(\"raw\"),\nempCI = list(gamma=0.95, col=\"blue\")\n)\n\n\n\n\n\n\n\n\nThis plot shows the ICC for the selected item. The x-axis shows the ability continuum, the y-axis the response probability. The continuous line describes the probability to respond correctly to the problem given a level of ability. The difficulty of the item is where the probability of a correct response equals \\(0.5\\). The option empICC equal to “raw” also plots the relative frequencies of positive responses for each rawscore group at the position of the corresponding ability level. The blue dotted lines represent the \\(95\\%\\) confidence level for the relative frequencies and are shown if options are provided if the optional argument empCI is specified.\n\ntests for local dependence\n\nTesting for local dependence can be done by removing an item, fitting the Rasch model to the remaining items, splitting with respect to the removed item. The general method for testing local dependence is to compute Yens \\(Q_3\\) statistic, which proceeds as follows. Estimate \\(\\boldsymbol b\\) and \\(\\boldsymbol \\theta\\); compute the expected data matrix \\(\\boldsymbol E=E_{ik}=E(Y_{ik}\\mid \\theta_i=\\hat{\\theta}_i)=P(Y_{ik}=1\\mid \\theta_i=\\hat{\\theta}_i)\\); compute the matrix of residuals \\(\\boldsymbol R=R_{ik}=\\frac{Y_{ik} - E_{ik}}{\\text{Var}(Y_{ik})}\\); evaluate correlation between residuals. We use the sirt package for this\n\nmod &lt;- sirt::rasch.mml2(items)\n\n------------------------------------------------------------\nSemiparametric Marginal Maximum Likelihood Estimation \nRaschtype Model with generalized logistic link function: alpha1= 0 , alpha2= 0  \n------------------------------------------------------------\n...........................................................\nIteration 1     2024-07-22 15:35:17.027667 \n   Deviance=302.8245\n    Maximum b parameter change=0.149448 \n...........................................................\nIteration 2     2024-07-22 15:35:17.029991 \n   Deviance=299.1961 | Deviance change=3.628433\n    Maximum b parameter change=0.036572 \n...........................................................\nIteration 3     2024-07-22 15:35:17.030829 \n   Deviance=296.505 | Deviance change=2.691082\n    Maximum b parameter change=0.036111 \n...........................................................\nIteration 4     2024-07-22 15:35:17.031601 \n   Deviance=294.5184 | Deviance change=1.986675\n    Maximum b parameter change=0.034245 \n...........................................................\nIteration 5     2024-07-22 15:35:17.032313 \n   Deviance=293.147 | Deviance change=1.371326\n    Maximum b parameter change=0.031046 \n...........................................................\nIteration 6     2024-07-22 15:35:17.03324 \n   Deviance=292.2506 | Deviance change=0.896385\n    Maximum b parameter change=0.027082 \n...........................................................\nIteration 7     2024-07-22 15:35:17.034008 \n   Deviance=291.6894 | Deviance change=0.561218\n    Maximum b parameter change=0.022895 \n...........................................................\nIteration 8     2024-07-22 15:35:17.034674 \n   Deviance=291.3492 | Deviance change=0.340225\n    Maximum b parameter change=0.018872 \n...........................................................\nIteration 9     2024-07-22 15:35:17.035317 \n   Deviance=291.1472 | Deviance change=0.202026\n    Maximum b parameter change=0.01525 \n...........................................................\nIteration 10     2024-07-22 15:35:17.035976 \n   Deviance=291.0282 | Deviance change=0.11894\n    Maximum b parameter change=0.012139 \n...........................................................\nIteration 11     2024-07-22 15:35:17.036595 \n   Deviance=290.958 | Deviance change=0.070237\n    Maximum b parameter change=0.009558 \n...........................................................\nIteration 12     2024-07-22 15:35:17.037218 \n   Deviance=290.916 | Deviance change=0.042003\n    Maximum b parameter change=0.00747 \n...........................................................\nIteration 13     2024-07-22 15:35:17.037848 \n   Deviance=290.8904 | Deviance change=0.025604\n    Maximum b parameter change=0.005808 \n...........................................................\nIteration 14     2024-07-22 15:35:17.038479 \n   Deviance=290.8744 | Deviance change=0.015958\n    Maximum b parameter change=0.004501 \n...........................................................\nIteration 15     2024-07-22 15:35:17.039254 \n   Deviance=290.8643 | Deviance change=0.010172\n    Maximum b parameter change=0.003481 \n...........................................................\nIteration 16     2024-07-22 15:35:17.039984 \n   Deviance=290.8576 | Deviance change=0.00662\n    Maximum b parameter change=0.002688 \n...........................................................\nIteration 17     2024-07-22 15:35:17.040703 \n   Deviance=290.8533 | Deviance change=0.004387\n    Maximum b parameter change=0.002073 \n...........................................................\nIteration 18     2024-07-22 15:35:17.041456 \n   Deviance=290.8503 | Deviance change=0.002952\n    Maximum b parameter change=0.001598 \n...........................................................\nIteration 19     2024-07-22 15:35:17.042538 \n   Deviance=290.8483 | Deviance change=0.002011\n    Maximum b parameter change=0.001231 \n...........................................................\nIteration 20     2024-07-22 15:35:17.043476 \n   Deviance=290.8469 | Deviance change=0.001384\n    Maximum b parameter change=0.000948 \n...........................................................\nIteration 21     2024-07-22 15:35:17.044301 \n   Deviance=290.8459 | Deviance change=0.000961\n    Maximum b parameter change=0.000729 \n...........................................................\nIteration 22     2024-07-22 15:35:17.045088 \n   Deviance=290.8453 | Deviance change=0.000673\n    Maximum b parameter change=0.000561 \n...........................................................\nIteration 23     2024-07-22 15:35:17.04583 \n   Deviance=290.8448 | Deviance change=0.000474\n    Maximum b parameter change=0.000431 \n...........................................................\nIteration 24     2024-07-22 15:35:17.046605 \n   Deviance=290.8445 | Deviance change=0.000336\n    Maximum b parameter change=0.000331 \n...........................................................\nIteration 25     2024-07-22 15:35:17.047385 \n   Deviance=290.8442 | Deviance change=0.000239\n    Maximum b parameter change=0.000255 \n...........................................................\nIteration 26     2024-07-22 15:35:17.048099 \n   Deviance=290.8441 | Deviance change=0.000171\n    Maximum b parameter change=0.000196 \n...........................................................\nIteration 27     2024-07-22 15:35:17.048813 \n   Deviance=290.8439 | Deviance change=0.000123\n    Maximum b parameter change=0.00015 \n...........................................................\nIteration 28     2024-07-22 15:35:17.049523 \n   Deviance=290.8438 | Deviance change=8.9e-05\n    Maximum b parameter change=0.000115 \n...........................................................\nIteration 29     2024-07-22 15:35:17.050303 \n   Deviance=290.8438 | Deviance change=6.5e-05\n    Maximum b parameter change=8.8e-05 \n------------------------------------------------------------\nStart: 2024-07-22 15:35:17.025159 \nEnd: 2024-07-22 15:35:17.05309 \nTime difference of 0.02793121 secs\nDifference: 0.02793121 \n------------------------------------------------------------\n\nbeta &lt;- mod$item$b\nmod.wle &lt;- sirt::wle.rasch(dat= items , b = beta)\n\n\nWLE Reliability= 0.54 \n\neta &lt;- mod.wle$theta\n\n#and now we can calculate Yen’s Q3 statistic\nq3 &lt;- sirt::Q3(dat = items, theta = eta , b = beta)\n\nYen's Q3 Statistic based on an estimated theta score \n*** 5 Items | 10 item pairs\n*** Q3 Descriptives\n     M     SD    Min    10%    25%    50%    75%    90%    Max \n-0.210  0.179 -0.540 -0.397 -0.336 -0.169 -0.073 -0.026  0.002 \n\n\nThe conventional interpretation is that correlations should be close to zero. A large value is evidence of a problem with the scale, but since we do not know the asymptotic distribution we have to rely on a rule of thumb to decide when to reject model fit. Based on simulation studies, a value of \\(0.2\\) is considered above the average and works well in many situations.\nSo, what do you think? pretty fun, isn’t it? Next time I will delve into this a bit more and check the model fit. Another excuse to keep studying this super cool topic!"
  },
  {
    "objectID": "posts/2024-05-10-my-blog-post/index.html",
    "href": "posts/2024-05-10-my-blog-post/index.html",
    "title": "Two-Parameter Logistic Model",
    "section": "",
    "text": "Hello everyone and welcome back to another post about Item Response Theory models, a topic which has caught my interest in recent times and that I am trying to learn bit by bit both from a theoretical perspective and a practical perspective. Today, I would like to continue the topic started last month with the two-parameter logistic model as a more sophisticated version of the simple Rasch model for multi-item binary response questionnaire data. Last time I focussed mostly on the theoretical introduction of the model and its differences compared to the standard one-parameter logistic model. Today, I would like instead to focus on how to implement such model in R using some pre-defined software packages and some example data as well as to see how to interpret the results from such analysis.\nAs a quick recap about the modelling framework, we may define the item response function or item characteristic curve (ICC) for the two-parameter logistic IRT model as:\n\\[\nP(Y_{ik}=1 | \\theta_i,a_k,b_k) = \\frac{\\text{exp}(a_k\\theta_i-b_k)}{1+\\text{exp}(a_k\\theta_i-b_k)},\n\\]\nwhere \\(Y_{ik}\\) denotes the response on item \\(k\\) for individual \\(i\\), whose probability of being correct (i.e. equal to \\(1\\)) is determined on the logistic scale as a function of some individual latent ability parameter \\(\\theta_i\\), an item difficulty parameter \\(b_k\\) (determining the position of the ICC), and an item discriminability parameter \\(a_k\\) (determining the steepness of slope of the ICC).\nNow, let’s try to see how we can implement this model to analyse some multi-item questionnaire data in R and, more specifically, using functions from the packages ltm and psych. As motivating example we use the dataset from the ltm package named LSAT which is a classical example in educational testing for measuring ability traits, consisting in a dataframe with the responses of \\(1000\\) individuals to \\(5\\) questions. After loading the dataset, we proceed to fit the two-parameter logistic IRT model using the function ltm\n\nlibrary(ltm)\nlibrary(psych)\n\n#load data\nlsat.df &lt;- LSAT\n\n#fit 2-PLM via ltm\nPL2.rasch &lt;- ltm(lsat.df~z1)\nsummary(PL2.rasch)\n\n\nCall:\nltm(formula = lsat.df ~ z1)\n\nModel Summary:\n   log.Lik      AIC      BIC\n -2466.653 4953.307 5002.384\n\nCoefficients:\n                value std.err  z.vals\nDffclt.Item 1 -3.3597  0.8669 -3.8754\nDffclt.Item 2 -1.3696  0.3073 -4.4565\nDffclt.Item 3 -0.2799  0.0997 -2.8083\nDffclt.Item 4 -1.8659  0.4341 -4.2982\nDffclt.Item 5 -3.1236  0.8700 -3.5904\nDscrmn.Item 1  0.8254  0.2581  3.1983\nDscrmn.Item 2  0.7229  0.1867  3.8721\nDscrmn.Item 3  0.8905  0.2326  3.8281\nDscrmn.Item 4  0.6886  0.1852  3.7186\nDscrmn.Item 5  0.6575  0.2100  3.1306\n\nIntegration:\nmethod: Gauss-Hermite\nquadrature points: 21 \n\nOptimization:\nConvergence: 0 \nmax(|grad|): 0.024 \nquasi-Newton: BFGS \n\n\nThe summary function allows to display key results from fitting the model, namely the different estimates for the item discrimination and difficulty parameters \\(a_k\\) and \\(b_k\\) based on the observed data. Higher difficulty values indicate that the item is harder (i.e. require a higher latent ability to answer correctly); higher discriminability estimates indicate that the item has better ability to tell the difference between different levels of latent ability. These can both be made clearer in the ICC plots, which we can draw using the function plot.\n\nlibrary(ltm)\nlibrary(psych)\n\n#plot ICC\nplot(PL2.rasch,type=c(\"ICC\"))\n\n\n\n\n\n\n\n\nUnlike the ICCs for the one-parameter logistic IRT model, the ICCs for the two-parameter logistic IRT model do not all have the same shape. Item curves which are more “spread out” or flat indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Conversely, an item with high discriminability (steep slope) shows that for such item we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong.\nA note about difficulty: because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item 3 is generally the most difficult item (i.e. lowest probability of getting correct for most latent trait values), while items 1 and 5 are roughly the easiest.\nNext, we may display the item information curve (IIC) using again the plot function. IICs show how much “information” about the latent trait ability an item gives. Mathematically, these are the first derivatives of the ICCs. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practically speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\n#plot ICC\nplot(PL2.rasch,type=c(\"IIC\"))\n\n\n\n\n\n\n\n\nSimilar to the ICCs, we see that item 3 provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item 1 and 5 provides the most information about lower ability levels (the peak of their IICs is farthest to the left). While all ICCs and IICs for the items have the same shape in the one-parameter model (i.e. all items are equally good at providing information about the latent trait), instead, in the two-parameter model this does not have to be the case.\nNext, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good coverage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.\n\n#plot IC test\nplot(PL2.rasch,type=c(\"IIC\"),items=c(0))\n\n\n\n\n\n\n\n\nThe IIC for the whole test shows that the test provides the most information for lower-than average ability levels (about \\(\\theta=-2\\)), but does not provide much information about higher ability levels. Next, we test how well the two-parameter model fits the data using the item.fit function to test whether individual items fit the two-parameter logistic IRT model.\n\n#fit of the model\nitem.fit(PL2.rasch,simulate.p.value=T)\n\n\nItem-Fit Statistics and P-values\n\nCall:\nltm(formula = lsat.df ~ z1)\n\nAlternative: Items do not fit the model\nAbility Categories: 10\nMonte Carlo samples: 100 \n\n            X^2 Pr(&gt;X^2)\nItem 1 276.1857    0.198\nItem 2 253.5272   0.7129\nItem 3 437.0737   0.2574\nItem 4 216.3252   0.7624\nItem 5 400.3201   0.1188\n\n\nWe see from this all items fit the model well (large p-values). We then estimate the individual latent ability scores using the factor.scores function and plot the density curve of the estimated ability scores.\n\n#estimate theta and plot density curve of estimated theta\ntheta.rasch &lt;- ltm::factor.scores(PL2.rasch)\nsummary(theta.rasch$score.dat$z1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.8953 -1.0026 -0.5397 -0.6629 -0.3572  0.6064 \n\nplot(theta.rasch)\n\n\n\n\n\n\n\n\nWe see that the estimated ability scores are roughly normally distributed, with mean \\(-0.7\\) and standard deviation \\(0.5\\). We test for unidimensionality (i.e. is there a single trait \\(\\theta\\) being measured here?) using the unidimTest function.\n\n#test unidim\nunidimTest(PL2.rasch,lsat.df)\n\n\nUnidimensionality Check using Modified Parallel Analysis\n\nCall:\nltm(formula = lsat.df ~ z1)\n\nMatrix of tertachoric correlations\n       Item 1 Item 2 Item 3 Item 4 Item 5\nItem 1 1.0000 0.1703 0.2275 0.1072 0.0665\nItem 2 0.1703 1.0000 0.1891 0.1111 0.1724\nItem 3 0.2275 0.1891 1.0000 0.1867 0.1055\nItem 4 0.1072 0.1111 0.1867 1.0000 0.2009\nItem 5 0.0665 0.1724 0.1055 0.2009 1.0000\n\nAlternative hypothesis: the second eigenvalue of the observed data is substantially larger \n            than the second eigenvalue of data under the assumed IRT model\n\nSecond eigenvalue in the observed data: 0.2254\nAverage of second eigenvalues in Monte Carlo samples: 0.2595\nMonte Carlo samples: 100\np-value: 0.6634\n\n\nThe test is not significant, hence unidimensionality is not rejected.\nNice, now I am also able to fit the two-parameter logistic IRT model. This was not that different from the one-parameter model but with the additional complexity of capturing an additional item-level parameters in \\(a_k\\) which before was assumed fixed to be the same across all items. Interpretation of results and parametric assumptions underneath the model are basically the same. Next time it will be the turn for the three-parameter model, which I am looking forward to learn! I find these topics really interesting and I am eager to learn more about them. See you next time!"
  },
  {
    "objectID": "posts/2024-07-10-my-blog-post/index.html",
    "href": "posts/2024-07-10-my-blog-post/index.html",
    "title": "Some time off …",
    "section": "",
    "text": "Dear readers, unfortunately I have some bad news to communicate which relate to my private life as one person very close to me passed away a couple of weeks ago. This has left and will leave a strong impact on my life as it was something quite unexpected. Because of this I also had to cancel my participation to EuHEA 2024 and probably will also need to cancel my attendance at ICTMC 2024 later this autumn.\nI want to take some time off to think carefully on how to move on from this point. Of course, routine work must go on but at the moment I am a bit deprived of will to talk happily and enthusiastically about new ideas or things to do. I know and hope this is simply a transition period and that things will become “easier” as tie passes but I also think it is important to take some time off and do a check of my life and take the time needed to re-assess and mentally prepare myself to continue my life in a quite different mindset and using a new perspective.\nI know I talked quite broadly and perhaps in a way that is a bit empty and confusing but I am convinced that I will soon be able to go back to my usual self and give you new updates on my research and ideas. I just ask you to bear with me for a little time, as it is something that I really need before re-starting the usual posting and sharing of news.\nThank you very much for your understanding and I hope to be able to get back to you soon enough."
  },
  {
    "objectID": "posts/2024-09-14-my-blog-post/index.html",
    "href": "posts/2024-09-14-my-blog-post/index.html",
    "title": "In which Dutch province do people travel the most?",
    "section": "",
    "text": "Hello dear readers, I hope everything is well with you and welcome back for another post on my site. Today I would like to continue the little exercise I started last time where I played a bit using some Dutch data related to the different provinces of the country to make some comparisons between them. Again, I apologise to my readers who are looking forward to some more health economics topic, but I really want to continue the previous topic with some additional fun examples.\nWhat I am focussing today is still about using some Dutch-province level data, this time taken directly from the CBS website and related to the average number of travels per person per year. My basic idea is very simple. Which province has the largest average number of travels per year? and can we also identify some temporal trend? in order to answer this question I have taken from the above link information about the average number of travels per person per year for each of the \\(12\\) provinces of the Netherlands over a period of \\(6\\) years, between 2018 and 2023.\nAs usual, let’s start by drawing a map of the country divided into provinces:\n\nlibrary(tmap)\ntmap_mode(\"plot\")\ndata(NLD_prov)\ntm_shape(NLD_prov) +\n  tm_borders(lwd=2) + \n  tm_fill(\"name\") + \n  tm_format(\"NLD\", title=\"Dutch provinces\", bg.color=\"white\")\n\n\n\n\n\n\n\n\nNext, let’s download the CBS data we are interested as a .csv file, import it into R and let’s explore the data.\n\nNLD_prov_travel &lt;- read.csv(\"per_person__travel_modes__travel_purpose_16092024_155133.csv\", header = T, sep = \";\")\n\n#create a subset of the dataset and rename the variables\nNLD_prov_travel &lt;- NLD_prov_travel[,5:7]\nnames(NLD_prov_travel) &lt;- c(\"Province\",\"Year\",\"Travels\")\n\n#let's extract the travel data for all provinces for 1 year first, say 2023\nNLD_prov_travel_2023 &lt;- NLD_prov_travel[NLD_prov_travel$Year==2023,]\n\n#see the data\nNLD_prov_travel_2023\n\n             Province Year Travels\n6      Groningen (PV) 2023     994\n12       Fryslân (PV) 2023     995\n18       Drenthe (PV) 2023     988\n24    Overijssel (PV) 2023    1054\n30     Flevoland (PV) 2023     936\n36    Gelderland (PV) 2023    1036\n42       Utrecht (PV) 2023    1029\n48 Noord-Holland (PV) 2023     954\n54  Zuid-Holland (PV) 2023     945\n60       Zeeland (PV) 2023     974\n66 Noord-Brabant (PV) 2023    1023\n72       Limburg (PV) 2023    1011\n\n\nWe can see that in 2023 it appears that Overijssel, Gelderland, Utrecht, and Noord-Brabant were the provinces associated with most average number of travels per person (above \\(1020\\) travels). It could interesting to get some information about how the number of travels has changed over time for each province. To achieve this we can try to compute some summary statistics:\n\n#load package\nlibrary(data.table)\n#create custom summary function\nmy.summary &lt;- function(x, na.rm=TRUE){list(n= length(x),\n                                  Mean=mean(x),\n                                  SD=sd(x),\n                                  Median=median(x), \n                                  Min=min(x),\n                                  Max=max(x))}\n#summarise data\nsum_data_travel &lt;- setDT(NLD_prov_travel)[, unlist(lapply(.SD, my.summary),recursive=FALSE), Province]\n\n#extract and display relevant data\nsum_data_travel.dt &lt;- sum_data_travel[,c(1,8:13)] \nsum_data_travel.dt\n\n              Province Travels.n Travels.Mean Travels.SD Travels.Median\n                &lt;char&gt;     &lt;int&gt;        &lt;num&gt;      &lt;num&gt;          &lt;num&gt;\n 1:     Groningen (PV)         6     941.3333   72.39797          959.5\n 2:       Fryslân (PV)         6     960.5000   39.53606          972.0\n 3:       Drenthe (PV)         6     976.1667   55.27537          985.0\n 4:    Overijssel (PV)         6    1002.1667   67.49049         1034.0\n 5:     Flevoland (PV)         6     905.0000   58.61058          920.5\n 6:    Gelderland (PV)         6     999.0000   59.04574         1026.0\n 7:       Utrecht (PV)         6     992.8333   65.26689         1023.5\n 8: Noord-Holland (PV)         6     930.1667   65.12885          950.5\n 9:  Zuid-Holland (PV)         6     923.0000   64.91225          942.5\n10:       Zeeland (PV)         6     962.0000   55.20507          979.0\n11: Noord-Brabant (PV)         6     983.0000   49.52979         1005.0\n12:       Limburg (PV)         6     963.1667   56.86973          985.5\n    Travels.Min Travels.Max\n          &lt;int&gt;       &lt;int&gt;\n 1:         831        1024\n 2:         908         998\n 3:         880        1028\n 4:         886        1054\n 5:         820         980\n 6:         905        1046\n 7:         888        1056\n 8:         820        1006\n 9:         814         989\n10:         858        1015\n11:         904        1023\n12:         872        1011\n\n\nCreating summary statistics per Province over a period of 6 years (2018-2023), we can see that on average Overijssel still places at the top with a mean number of travels per person of \\(1002\\), followed by Gelderland (\\(999\\)) and Utrecht (\\(992\\)), while all other Provinces seem to fall a bit behind with very similar values between \\(900\\) and \\(976\\). However, we also notice that, for some Provinces, median values are a bit different from means (typically higher), with also some minimum and maximum values that can be quite extreme in some cases. So, let’s have a look at the shape of the data distribution. For example, we may plot via histograms the travel data by Province over the 6-year period considered.\n\nlibrary(ggplot2)\nggplot(data = NLD_prov_travel, aes(x = Travels)) + geom_histogram(binwidth = 30, fill=\"white\", color = \"black\") + facet_wrap(~Province, scales = \"free\") + theme_classic()\n\n\n\n\n\n\n\n\nAlthough 6 data points per Province is not a lot to look at, we can notice that for some Provinces the distribution of the number of travels is skewed, therefore suggesting how the mean perhaps is not a good indicator for these data.\nWhat at this point if we want to try and make a prediction of what the number of travels would be for each Province in 2024? and what about the expected number of travels over the 6 year period across all Provinces? Well, for that we need some inferential statistics! For example, taking into account the time dependence of the 6 observations per Province, let’s try to fit a linear mixed effects model including a random error term to capture the clustering nature of the data at the level of the Provinces:\n\\[\n\\text{Travel}_{ij} = \\beta_0 + \\beta_1 \\times \\text{Year} + \\omega_j + \\varepsilon_{ij}\n\\]\nwhere: \\(\\text{Travel}_{ij}\\) refers to the number of travels per person for Province \\(i\\) in Year \\(j\\), while \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the year regression coefficient, \\(\\varepsilon_{ij}\\) and \\(\\omega_j\\) are the random error term and the Province-specific random effect term.\n\n#load package\nlibrary(lme4)\nfm &lt;- lmer(Travels ~ Year + (1 | Province), data = NLD_prov_travel)\n#define new data for prediction\nnewdata &lt;- expand.grid(\n  Year = c(2024),\n  Province = unique(NLD_prov_travel$Province)\n)\n#predict\nfm_pred &lt;- predict(fm, newdata = newdata)\nnames(fm_pred) &lt;- unique(NLD_prov_travel$Province)\n\n#look at predictions\nfm_pred\n\n    Groningen (PV)       Fryslân (PV)       Drenthe (PV)    Overijssel (PV) \n          948.6030           955.9821           962.0137           972.0237 \n    Flevoland (PV)    Gelderland (PV)       Utrecht (PV) Noord-Holland (PV) \n          934.6147           970.8045           968.4304           944.3038 \n Zuid-Holland (PV)       Zeeland (PV) Noord-Brabant (PV)       Limburg (PV) \n          941.5447           956.5596           964.6445           957.0087 \n\n\nWe can see that, according to the model predictions Overijssel still is associated with the largest number of travels even in 2024, followed by Gelderland and Utrecht, although the difference now seems a little bit smaller compared to what observed from the descriptive analysis of the data. Finally, we can also try to obtain an estimate of the average number of travels across Years and Provinces. This can be done, if we consider Year as a continuous variable, by setting its value equal to the average across the years considered (\\(\\bar{\\text{Year}}=\\text{E}[\\text{Year}]_i\\), for \\(i=,2018,2019,\\ldots,2023\\)), generate the predictions by Province, and then take the expectation across them.\n\n#define new data for prediction\nnewdata &lt;- expand.grid(\n  Year = c(mean(2018:2023)),\n  Province = unique(NLD_prov_travel$Province)\n)\n#predict\nfm_pred_avg &lt;- predict(fm, newdata = newdata)\nnames(fm_pred_avg) &lt;- unique(NLD_prov_travel$Province)\n\n#look at predictions\nmean(fm_pred_avg)\n\n[1] 961.5278\n\n\nSo, based on the model and data, we would expect an average number of travels per person across Provinces between 2018-2023 of about 961.53.\nI hope you enjoyed this little exercise as much as I did. It is a nice distraction from the usual work I do and a bit of fesh air. But no worries, I will soon come back to talk more about health economics and Bayesian statstics!"
  },
  {
    "objectID": "posts/2024-11-10-my-blog-post/index.html",
    "href": "posts/2024-11-10-my-blog-post/index.html",
    "title": "Markov Models in Economic Evaluations",
    "section": "",
    "text": "Hello dear readers and welcome to a new post of my blog. Today, I would like to continue the discussion initiated with the last post about modelling in health economic evaluations, particularly with respect to the construction and implementation of decision analytic models or DAMs, such as the Decision Tree Models that were introduced last time. Picking up on this trend, in this post I would like to introduce and talk about Markov Models in economic evaluation, mostly drawing inspiration and references from the book Khan (2015) (Chapter 6) which contains a nice introduction and overview on this topic. As usual warning for the casual reader: in this post I may use some terms that are specific to the health economics literature and field without diving into too many explanations; when this is the case, please bear with me and perhaps make a quick online search (or check my previous posts) to check if anything is unclear to you. For a look at how to implement more complex types of markov models, I recommend checking this nice post, although it is mostly focussed on the implementation of such models in Excel, and heemod package vignette for R."
  },
  {
    "objectID": "posts/2024-11-10-my-blog-post/index.html#markov-models",
    "href": "posts/2024-11-10-my-blog-post/index.html#markov-models",
    "title": "Markov Models in Economic Evaluations",
    "section": "Markov Models",
    "text": "Markov Models\nWhen the complexity of the modelling task for disease progression increases considerably, e.g. because of many possible mutually exclusive outcomes or the need to repeat the analysis at given time intervals, the use and implementation of decision tree (explored in the previous post) may become quite burdensome. Rather than creating very complex branches and estimate the costs and effects associated with each possible outcome within each branch of the decision tree, it is often more useful to specify the modelling task with respect to the transition of the patients through different (and more limited) health states via Markov Models. More specifically, a Markov model in economic evaluation aims at modelling the transition of patients from one health state to another, and estimate the costs and effects expected to accrue as a result of transitions between health states over a period of time. The period of time over which these transitions are observed (denoted as cycle) may vary depending on the context and the type of disease being modelled, e.g. a week, a month, a year, or even a lifetime.\nIn Markov Models, the probabilities of moving between health states are captured as a matrix of probabilities called a transition matrix, where chance of moving to a different health state depends only on the current health state (and not on health states prior to the current state). This is called the Markov property: \\(X_{t+1} = X_t + \\varepsilon\\), where \\(X_{t+1}\\) is the health state at time \\(t+1\\) and is dependent only on the current health state \\(X_t\\). An example of a simple Markov model is shown in the thumbnail figure of this post and formed by three health states (well, sick, dead) and arrows denoting the direction of the possible transitions between states. Since Markov Models are mostly simulation-based methods, no individual-level data are used to fit the models which are instead based on some aggregated data obtained from the literature or expert elicitation. This also includes the choice for the values of the transition probabilities, whose identification and selection should go through a rigorous process to ensure its reliability and appropriateness."
  },
  {
    "objectID": "posts/2024-11-10-my-blog-post/index.html#a-simple-example",
    "href": "posts/2024-11-10-my-blog-post/index.html#a-simple-example",
    "title": "Markov Models in Economic Evaluations",
    "section": "A simple example",
    "text": "A simple example\nAs a demonstrative example, let’s consider the transition matrix shown in Table 1, which displays the assumed transition probabilities for three health states (Well, Sick and Dead) from baseline (rows) to 1 month follow-up (columns), for an hypothetical experimental treatment.\n\n\n\n\nTable 1: Example Transition Matrix on Experimental Treatment after 1 Month Post-Randomisation\n\n\n\n\n\n\n\nWell\nSick\nDead\nTotal\n\n\n\n\nWell\n0.45\n0.35\n0.20\n1\n\n\nSick\n0.05\n0.60\n0.35\n1\n\n\nDead\n0.00\n0.00\n1.00\n1\n\n\n\n\n\n\n\n\nLet’s also assume that patients were randomised to one of two treatments, namely Experimental vs Control, and they were considered to be in either a Well or Sick state. From Table 1, we can see that the proportion of patients who started the hypothetical trial at baseline in a Well state and then remained in the same state after treatment was \\(45\\%\\), very few patients (\\(5\\%\\)) who started the trial in a Sick state improved into a Well state at follow up, and \\(60\\%\\) of patients who were in the Sick state did not change states after starting treatment. In order to compute what the transition matrix will look like after \\(1\\) month, we need to know the initial probabilities (or the probabilities at baseline), say equal to the first row of Table 1. After \\(1\\) month of treatment, the transition matrix is updated by multiplying the initial probabilities by the \\(3\\times3\\) transition matrix in Table 1:\n\ntm_update &lt;- matrix(c(0.45,0.05,0,0.35,0.6,0,0.2,0.35,1),3,3) \ntm_init &lt;- c(0.45,0.35,0.2)\ntm_1m &lt;- tm_init%*%tm_update\n\ntm_1m\n\n     [,1]   [,2]   [,3]\n[1,] 0.22 0.3675 0.4125\n\n\nThe above \\(1\\times3\\) “matrix” denotes the probabilities in each of the three health states of the model at 1 month after treatment and becomes the initial matrix needed for further calculations. After \\(2\\) months (second cycle of the process), the updated transition matrix is obtained by multiplying the above matrix again by the \\(3 \\times 3\\) transition matrix in Table 1, and so on. In general, one can compute the proportion of patients for the \\(n+1\\)-th step in any of these three health states by simply multiplying the updated transition matrix at step \\(n\\) by the given transition matrix:\n\\[\nM_n = M_{n-1}\\times P,\n\\] where \\(M_n\\) and \\(M_{n-1}\\) denote the \\(1\\times 3\\) transition matrix at step \\(n\\) and \\(n-1\\), while \\(P\\) denotes the constant \\(3\\times3\\) transition matrix used to update the probabilities at each step. Finally, the last important factor associated with the Markov model is the duration of the interval of time, termed a cycle, set to \\(1\\) month in the example above. In general, the length of the cycle could be longer or shorter, with shorter cycles needing more computations since they imply a more frequent update of the transition probabilities, i.e. more steps to run.\nTo continue our example, let’s now consider a more realistic scenario where the transition matrix and initial probabilities are provided for both an experimental group and a control group form an hypothetical trial assessing the cost-effectiveness of the two treatments. The \\(3\\times3\\) transition matrices associated with the two treatments (\\(P^{exp},P^{ctr}\\)) are shown in Table 2, while the \\(1\\times3\\) matrices of the initial probabilities (\\(M^{exp}_0,M^{ctr}_{0}\\)) of the two groups are assumed to be equal to the first row of their respective transition matrices.\n\n\n\n\nTable 2: Example Transition Matrices on Experimental and Control Treatment after 1 Month Post-Randomisation\n\n\n\n\n\n\n\nWell\nSick\nDead\nTotal\n\n\n\n\nWell\n0.45\n0.35\n0.20\n1\n\n\nSick\n0.05\n0.60\n0.35\n1\n\n\nDead\n0.00\n0.00\n1.00\n1\n\n\n\n\n\n\n\n\n\nWell\nSick\nDead\nTotal\n\n\n\n\nWell\n0.25\n0.25\n0.50\n1\n\n\nSick\n0.10\n0.45\n0.45\n1\n\n\nDead\n0.00\n0.00\n1.00\n1\n\n\n\n\n\n\n\n\nWe can therefore update the transition matrices of the two treatments over \\(5\\) cycles, each of \\(1\\) months, by doing the following\n\n#experimental treatment\ntm_exp_update &lt;- matrix(c(0.45,0.05,0,0.35,0.6,0,0.2,0.35,1),3,3) \ntm_exp_init &lt;- c(0.45,0.35,0.2)\n#update over 5 cycles\ntm_exp_1m &lt;- tm_exp_init%*%tm_exp_update\ntm_exp_2m &lt;- tm_exp_1m%*%tm_exp_update\ntm_exp_3m &lt;- tm_exp_2m%*%tm_exp_update\ntm_exp_4m &lt;- tm_exp_3m%*%tm_exp_update\ntm_exp_5m &lt;- tm_exp_4m%*%tm_exp_update\n\n#control treatment\ntm_ctr_update &lt;- matrix(c(0.25,0.10,0,0.25,0.45,0,0.50,0.45,1),3,3) \ntm_ctr_init &lt;- c(0.25,0.25,0.50)\n#update over 5 cycles\ntm_ctr_1m &lt;- tm_ctr_init%*%tm_ctr_update\ntm_ctr_2m &lt;- tm_ctr_1m%*%tm_ctr_update\ntm_ctr_3m &lt;- tm_ctr_2m%*%tm_ctr_update\ntm_ctr_4m &lt;- tm_ctr_3m%*%tm_ctr_update\ntm_ctr_5m &lt;- tm_ctr_4m%*%tm_ctr_update\n\n#experimental TM at the end of 5th cycle\ntm_exp_5m\n\n           [,1]      [,2]    [,3]\n[1,] 0.02642064 0.1077694 0.86581\n\n#control TM at the end of 5th cycle\ntm_ctr_5m\n\n            [,1]       [,2]      [,3]\n[1,] 0.005600547 0.01602453 0.9783749\n\n\nOnce the transition probabilities over the desired number of cycles are estimated, further data manipulation can be carried out to estimate the expected costs and effects, which can be included in an additional vector and then multiplied with the elements of the matrices after each cycle to generate the expected total costs and effects. As an example, let’s assume that patients were treated for \\(3\\) months but followed up for \\(12\\) months during the trial and that costs were collected from patients files and measured in euros, while effects were collected via self-reported questionnaires (e.g. EQ-5D) and measured via utilities. We need to associate to each health state and treatment in the model a corresponding value for each of the outcomes that we want to measure, i.e. costs and utilities. Thus, we may assume that (monthly) utilities for each health state are \\(u^{well}=0.78\\), \\(u^{sick}=0.40\\), and \\(u^{dead}=0\\), but are constant across treatments. Conversely, we may assume that (monthly) costs are constant across health states but vary between treatment groups, i.e. \\(c_{exp}=954\\) and \\(c^{ctr}=435\\). Next, we need to determine the number of cycles of the model. In this case, we will use \\(12\\) cycles given that we focus over a \\(1\\) year follow-up, with each cycle being \\(1\\) month long. Hence, the expected costs and QALYs for each month will be generated and then added up.\nThe proportion of patients in each of the health states at the start of the model needs to be determined, typically an arbitrary large number, e.g. \\(10000\\), since the idea is to estimate the cumulative costs and effects over the \\(10000\\) patients for each group. We can now proceed to calculate the transition matrices and total costs and QALYs associated with both treatment groups over \\(12\\) months via our Markov Model.\n\n#set up initial values for the model\n\n#initial transition probs by arm\ntm_exp_init &lt;- c(0.45,0.35,0.2)\ntm_ctr_init &lt;- c(0.25,0.25,0.50)\n#transition matrices by arm\ntm_exp_update &lt;- matrix(c(0.45,0.05,0,0.35,0.6,0,0.2,0.35,1),3,3) \ntm_ctr_update &lt;- matrix(c(0.25,0.10,0,0.25,0.45,0,0.50,0.45,1),3,3) \n#utilities by health state\nu_well &lt;- 0.78\nu_sick &lt;- 0.4\nu_dead &lt;- 0\n#costs by arm\nc_exp &lt;- 954\nc_ctr &lt;- 435\n#number of cycles\nn_cycle &lt;- 12\n#initial number of patients in the cohort\nn_pat &lt;- 10000\n\n  #end of cycle 1 TP\n  trans_mat_cycle1_exp &lt;- tm_exp_init%*%tm_exp_update\n  trans_mat_cycle1_ctr &lt;- tm_ctr_init%*%tm_ctr_update\n  #lists to store TP results by cycle\n  trans_mat_cycles_exp &lt;- list()\n  trans_mat_cycles_ctr &lt;- list()\n  \n\n#create function to run the model\nmm_sim &lt;- function(tm_exp_init,tm_ctr_init,tm_exp_update,tm_ctr_update,\n                   u_well,u_sick,u_dead,c_exp,c_ctr,n_cycle,n_pat){\n  \n  res_list &lt;- res_list_exp &lt;- res_list_ctr &lt;- list()\n  #set up matrices to contain results from all TP over each cycle by group\n  tm_exp &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(tm_exp) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  tm_ctr &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(tm_ctr) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  #assign first row to be initial TP\n  tm_exp[1,] &lt;- tm_exp_init\n  tm_ctr[1,] &lt;- tm_ctr_init\n  #loop through cycles and update TM at each cycle\n  for(i in 2:c(n_cycle+1)){\n    tm_exp[i,] &lt;- tm_exp[i-1,]%*%tm_exp_update\n    tm_ctr[i,] &lt;- tm_ctr[i-1,]%*%tm_ctr_update\n  }\n  #remove intial TP from matrices\n  tm_exp &lt;- tm_exp[-1,]\n  tm_ctr &lt;- tm_ctr[-1,]\n  #using obtained TM to compute number of patients in each health state at each cycle\n  npat_cycle_exp &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(npat_cycle_exp) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  npat_cycle_ctr &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(npat_cycle_ctr) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  npat_cycle_exp &lt;- tm_exp*n_pat\n  npat_cycle_ctr &lt;- tm_ctr*n_pat\n  #and number still alive\n  nailive_cycle_exp &lt;- as.matrix(rowSums(npat_cycle_exp[,-3]))\n  colnames(nailive_cycle_exp) &lt;- \"nalive\"\n  nailive_cycle_ctr &lt;- as.matrix(rowSums(npat_cycle_ctr[,-3]))\n  colnames(nailive_cycle_ctr) &lt;- \"nalive\"\n  #use obtained number of patients at each cycle to get the same for costs and QALYs\n  costs_cycle_exp &lt;- as.matrix(rowSums(as.matrix(tm_exp*c_exp)[,-3]))\n  colnames(costs_cycle_exp) &lt;- \"costs\"\n  costs_cycle_ctr &lt;- as.matrix(rowSums(as.matrix(tm_ctr*c_ctr)[,-3]))\n  colnames(costs_cycle_ctr) &lt;- \"costs\"\n  QALY_cycle_exp &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(QALY_cycle_exp) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  QALY_cycle_ctr &lt;- matrix(NA, nrow = n_cycle+1, ncol = 3)\n  colnames(QALY_cycle_ctr) &lt;- c(\"Well\",\"Sick\",\"Dead\")\n  u_well_v &lt;- rep(u_well,n_cycle)\n  u_sick_v &lt;- rep(u_sick,n_cycle)\n  u_dead_v &lt;- rep(u_dead,n_cycle)\n  u_M &lt;- cbind(u_well_v,u_sick_v,u_dead_v)\n  QALY_cycle_exp &lt;- as.matrix(rowSums(as.matrix(tm_exp*u_M)[,-3]))\n  colnames(QALY_cycle_exp) &lt;- c(\"QALYs\")\n  QALY_cycle_ctr &lt;- as.matrix(rowSums(as.matrix(tm_ctr*u_M)[,-3]))\n  colnames(QALY_cycle_ctr) &lt;- c(\"QALYs\")\n  #save output\n  res_list_exp[[1]] &lt;- tm_exp\n  res_list_exp[[2]] &lt;- npat_cycle_exp\n  res_list_exp[[3]] &lt;- nailive_cycle_exp\n  res_list_exp[[4]] &lt;- costs_cycle_exp\n  res_list_exp[[5]] &lt;- QALY_cycle_exp\n  res_list_ctr[[1]] &lt;- tm_ctr\n  res_list_ctr[[2]] &lt;- npat_cycle_ctr\n  res_list_ctr[[3]] &lt;- nailive_cycle_ctr\n  res_list_ctr[[4]] &lt;- costs_cycle_ctr\n  res_list_ctr[[5]] &lt;- QALY_cycle_ctr\n  names(res_list_exp)&lt;-c(\"TM\",\"npatients\",\"nalive\",\"costs\",\"QALYs\")\n  names(res_list_ctr)&lt;-c(\"TM\",\"npatients\",\"nalive\",\"costs\",\"QALYs\")\n  res_list[[1]] &lt;- res_list_exp\n  res_list[[2]] &lt;- res_list_ctr\n  names(res_list)&lt;-c(\"Experimental\",\"Control\")\n  return(res_list)\n}\n\n#run the function and get the output\nmm_res &lt;- mm_sim(tm_exp_init = tm_exp_init, tm_ctr_init = tm_ctr_init, tm_exp_update = tm_exp_update,\n       tm_ctr_update = tm_ctr_update, u_well = u_well, u_sick = u_sick, u_dead = u_dead, c_exp = c_exp,\n       c_ctr = c_ctr, n_cycle = n_cycle, n_pat = n_pat)\n\nFor example, we can extract information regarding the number of patients associated with each health state at each cycle of the model for both intervention groups:\n\n#experimental\nmm_res$Experimental$npatients\n\n            Well       Sick     Dead\n [1,] 2200.00000 3675.00000 4125.000\n [2,] 1173.75000 2975.00000 5851.250\n [3,]  676.93750 2195.81250 7127.250\n [4,]  414.41250 1554.41562 8031.172\n [5,]  264.20641 1077.69375 8658.100\n [6,]  172.77757  739.08849 9088.134\n [7,]  114.70433  503.92524 9381.370\n [8,]   76.81321  342.50166 9580.685\n [9,]   51.69103  232.38562 9715.923\n[10,]   34.88024  157.52323 9807.597\n[11,]   23.57227  106.72203 9869.706\n[12,]   15.94362   72.28351 9911.773\n\n#control\nmm_res$Control$npatients\n\n             Well        Sick     Dead\n [1,] 875.0000000 1750.000000 7375.000\n [2,] 393.7500000 1006.250000 8600.000\n [3,] 199.0625000  551.250000 9249.688\n [4,] 104.8906250  297.828125 9597.281\n [5,]  56.0054687  160.245313 9783.749\n [6,]  30.0258984   86.111758 9883.862\n [7,]  16.1176504   46.256766 9937.626\n [8,]   8.6550892   24.844957 9966.500\n [9,]   4.6482680   13.344003 9982.008\n[10,]   2.4964673    7.166868 9990.337\n[11,]   1.3408037    3.849208 9994.810\n[12,]   0.7201217    2.067344 9997.213\n\n\nor the costs and QALYs accrued at each cycle across the alive patients\n\n#experimental\nmm_res$Experimental$costs\n\n           costs\n [1,] 560.475000\n [2,] 395.790750\n [3,] 274.060350\n [4,] 187.826203\n [5,] 128.017275\n [6,]  86.992022\n [7,]  59.017262\n [8,]  40.002639\n [9,]  27.100912\n[10,]  18.355292\n[11,]  12.430076\n[12,]   8.416869\n\nmm_res$Experimental$QALYs\n\n            QALYs\n [1,] 0.318600000\n [2,] 0.210552500\n [3,] 0.140633625\n [4,] 0.094500800\n [5,] 0.063715850\n [6,] 0.043040190\n [7,] 0.029103948\n [8,] 0.019691497\n [9,] 0.013327325\n[10,] 0.009021588\n[11,] 0.006107518\n[12,] 0.004134943\n\n#control\nmm_res$Control$costs\n\n            costs\n [1,] 114.1875000\n [2,]  60.9000000\n [3,]  32.6385937\n [4,]  17.5182656\n [5,]   9.4069090\n [6,]   5.0519880\n [7,]   2.7132871\n [8,]   1.4572520\n [9,]   0.7826638\n[10,]   0.4203551\n[11,]   0.2257655\n[12,]   0.1212548\n\nmm_res$Control$QALYs\n\n             QALYs\n [1,] 0.1382500000\n [2,] 0.0709625000\n [3,] 0.0375768750\n [4,] 0.0200945938\n [5,] 0.0107782391\n [6,] 0.0057864904\n [7,] 0.0031074474\n [8,] 0.0016688952\n [9,] 0.0008963250\n[10,] 0.0004813992\n[11,] 0.0002585510\n[12,] 0.0001388633\n\n\nfrom which an estimate of the mean costs and QALYs (and the group differences) over the \\(12\\) months period (without discounting) can be obtained by taking the sum across these accrued expected values at each cycle\n\n#experimental\nmu_c_exp &lt;- sum(mm_res$Experimental$costs)\nmu_e_exp &lt;- sum(mm_res$Experimental$QALYs)\n\n#control\nmu_c_ctr &lt;- sum(mm_res$Control$costs)\nmu_e_ctr &lt;- sum(mm_res$Control$QALYs)\n\nmu_c_diff &lt;- mu_c_exp-mu_c_ctr\nmu_e_diff &lt;- mu_e_exp-mu_e_ctr\n\nand form this an estimate of the ICER can be obtained:\n\\[\n\\text{ICER} = \\frac{1798.4846494 - 245.4238347}{0.9524298-0.2900002} =\\frac{1553.0608148}{0.6624296} = 2344.49\n\\]\nI hope you enjoyed today’s topic as it was quite interesting, at least for me. I always want to learn more about this stuff and rarely have the time do so. By going through the coding, I think things become much easier to understand, especially when these types of models can be quite complex and difficult to grasp by just reading it on a paper. Probably next time I will dive into the uncertainty assessment for these models, which at the moment I neglected due to time constraints. Well, then hope to see you back on my next post to learn how to do that as well (if you are interested)."
  },
  {
    "objectID": "publication/2019-a-my-publication/index.html",
    "href": "publication/2019-a-my-publication/index.html",
    "title": "A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data",
    "section": "",
    "text": "Abstract\nEconomic evaluations from individual level data are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. A critical problem in these analyses is that both effectiveness and cost data typically present some complexity (eg, nonnormality, spikes, and missingness) that should be addressed using appropriate methods. However, in routine analyses, standardised approaches are typically used, possibly leading to biassed inferences. We present a general Bayesian framework that can handle the complexity. We show the benefits of using our approach with a motivating example, the MenSS trial, for which there are spikes at one in the effectiveness and missingness in both outcomes. We contrast a set of increasingly complex models and perform sensitivity analysis to assess the robustness of the conclusions to a range of plausible missingness assumptions. We demonstrate the flexibility of our approach with a second example, the PBS trial, and extend the framework to accommodate the characteristics of the data in this study. This paper highlights the importance of adopting a comprehensive modelling approach to economic evaluations and the strategic advantages of building these complex models within a Bayesian framework.\n         \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and J Mason, Alexina and Baio, Gianluca},\n  title = {A {Full} {Bayesian} {Model} to {Handle} {Structural} {Ones}\n    and {Missingness} in {Economic} {Evaluations} from\n    {Individual-Level} {Data}},\n  volume = {28},\n  number = {8},\n  date = {2019-04-01},\n  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8045},\n  doi = {10.1002/sim.8045},\n  langid = {en},\n  abstract = {{[}Economic evaluations from individual level data are an\n    important component of the process of technology appraisal\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Alexina J Mason, and Gianluca Baio. 2019. “A Full\nBayesian Model to Handle Structural Ones and Missingness in Economic\nEvaluations from Individual-Level Data.” Statistics in Medicine.\nApril 1, 2019. https://doi.org/10.1002/sim.8045."
  },
  {
    "objectID": "publication/2019-c-my-publication/index.html",
    "href": "publication/2019-c-my-publication/index.html",
    "title": "A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations",
    "section": "",
    "text": "Abstract\nTrial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study, using methods that ignore the complexities of utility and cost data (e.g. skewness and spikes). We present an alternative and more efficient Bayesian parametric approach to handle missing longitudinal outcomes in economic evaluations, while accounting for the complexities of the data. We specify a flexible parametric model for the observed data and partially identify the distribution of the missing data with partial identifying restrictions and sensitivity parameters. We explore alternative nonignorable scenarios through different priors for the sensitivity parameters, calibrated on the observed data. Our approach is motivated by, and applied to, data from a trial assessing the cost-effectiveness of a new treatment for intellectual disability and challenging behaviour.\n         \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and J Daniels, Michael and Baio, Gianluca},\n  title = {A {Bayesian} {Parametric} {Approach} to {Handle} {Missing}\n    {Longitudinal} {Outcome} {Data} in {Trial-Based} {Health} {Economic}\n    {Evaluations}},\n  volume = {183},\n  number = {2},\n  date = {2019-09-26},\n  url = {https://academic.oup.com/jrsssa/article/183/2/607/7056293},\n  doi = {10.1111/rssa.12522},\n  langid = {en},\n  abstract = {{[}Trial-based economic evaluations are typically\n    performed on cross-sectional variables, derived from the responses\n    for only the completers in the study ...{]}\\{style=“font-size:\n    85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Michael J Daniels, and Gianluca Baio. 2019. “A\nBayesian Parametric Approach to Handle Missing Longitudinal Outcome Data\nin Trial-Based Health Economic Evaluations.” Journal of the Royal\nStatistical Society Series A. September 26, 2019. https://doi.org/10.1111/rssa.12522."
  },
  {
    "objectID": "publication/2020-a-my-publication/index.html",
    "href": "publication/2020-a-my-publication/index.html",
    "title": "Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations",
    "section": "",
    "text": "Abstract\nHealth economic evaluations based on patient-level data collected alongside clinical trials (e.g. health related quality of life and resource use measures) are an important component of the process which informs resource allocation decisions. Almost inevitably, the analysis is complicated by the fact that some individuals drop out from the study, which causes their data to be unobserved at some time point. Current practice performs the evaluation by handling the missing data at the level of aggregated variables (e.g. QALYs), which are obtained by combining the economic data over the duration of the study, and are often conducted under a missing at random (MAR) assumption. However, this approach may lead to incorrect inferences since it ignores the longitudinal nature of the data and may end up discarding a considerable amount of observations from the analysis. We propose the use of joint longitudinal models to extend standard cost-effectiveness analysis methods by taking into account the longitudinal structure and incorporate all available data to improve the estimation of the targeted quantities under MAR. Our approach is compared to popular missingness approaches in trial-based analyses, motivated by an exploratory simulation study, and applied to data from two real case studies.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2020,\n  author = {Gabrio, Andrea and M Hunter, Rachael and J Mason, Alexina\n    and Baio, Gianluca},\n  title = {Joint Longitudinal Models for Dealing with Missing at Random\n    Data in Trial-Based Economic Evaluations},\n  volume = {24},\n  number = {5},\n  date = {2020-05-11},\n  url = {https://www.valueinhealthjournal.com/article/S1098-3015(21)00042-5/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301521000425%3Fshowall%3Dtrue},\n  doi = {10.1016/j.jval.2020.11.018},\n  langid = {en},\n  abstract = {{[}Health economic evaluations based on patient-level data\n    collected alongside clinical trials (e.g. health related quality of\n    life and resource use measures) are an important component\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Rachael M Hunter, Alexina J Mason, and Gianluca Baio.\n2020. “Joint Longitudinal Models for Dealing with Missing at\nRandom Data in Trial-Based Economic Evaluations.” Value in\nHealth. May 11, 2020. https://doi.org/10.1016/j.jval.2020.11.018."
  },
  {
    "objectID": "publication/2022-a-my-publication/index.html",
    "href": "publication/2022-a-my-publication/index.html",
    "title": "A Scoping Review of Item-Level Missing Data in Within-Trial Cost-Effectiveness Analysis",
    "section": "",
    "text": "Abstract\nCost-effectiveness analysis (CEA) alongside randomized controlled trials often relies on self-reported multi-item questionnaires that are invariably prone to missing item-level data. The purpose of this study is to review how missing multi-item questionnaire data are handled in trial-based CEAs. We searched the National Institute for Health Research journals to identify within-trial CEAs published between January 2016 and April 2021 using multi-item instruments to collect costs and quality of life (QOL) data. Information on missing data handling and methods, with a focus on the level and type of imputation, was extracted. A total of 87 trial-based CEAs were included in the review. Complete case analysis or available case analysis and multiple imputation (MI) were the most popular methods, selected by similar numbers of studies, to handle missing costs and QOL in base-case analysis. Nevertheless, complete case analysis or available case analysis dominated sensitivity analysis. Once imputation was chosen, missing costs were widely imputed at item-level via MI, whereas missing QOL was usually imputed at the more aggregated time point level during the follow-up via MI. Missing costs and QOL tend to be imputed at different levels of missingness in current CEAs alongside randomized controlled trials. Given the limited information provided by included studies, the impact of applying different imputation methods at different levels of aggregation on CEA decision making remains unclear.\n\n\n\n\nCitationBibTeX citation:@online{ling2022,\n  author = {Ling, Xiaoxiao and Gabrio, Andrea and Baio, Gianluca},\n  title = {A {Scoping} {Review} of {Item-Level} {Missing} {Data} in\n    {Within-Trial} {Cost-Effectiveness} {Analysis}},\n  volume = {25},\n  number = {9},\n  date = {2022-03-10},\n  url = {https://www.valueinhealthjournal.com/article/S1098-3015(22)00111-5/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301522001115%3Fshowall%3Dtrue},\n  doi = {10.1016/j.jval.2022.02.009},\n  langid = {en},\n  abstract = {{[}Cost-effectiveness analysis (CEA) alongside randomized\n    controlled trials often relies on self-reported multi-item\n    questionnaires ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nLing, Xiaoxiao, Andrea Gabrio, and Gianluca Baio. 2022. “A Scoping\nReview of Item-Level Missing Data in Within-Trial Cost-Effectiveness\nAnalysis.” Value in Health. March 10, 2022. https://doi.org/10.1016/j.jval.2022.02.009."
  },
  {
    "objectID": "publication/2023-a-my-publication/index.html",
    "href": "publication/2023-a-my-publication/index.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Abstract\nEconomic evaluations have been increasingly conducted in different countries to aid national decision-making bodies in resource allocation problems based on current and prospective evidence on costs and effects data for a set of competing health care interventions. In 2016, the Dutch National Health Care Institute issued new guidelines that aggregated and updated previous recommendations on key elements for conducting economic evaluation. However, the impact on standard practice after the introduction of the guidelines in terms of design, methodology and reporting choices, is still uncertain. To assess this impact, we examine and compare key analysis components of economic evaluations conducted in the Netherlands before (2010–2015) and after (2016–2020) the introduction of the recent guidelines. We specifically focus on two aspects of the analysis that are crucial in determining the plausibility of the results: statistical methodology and missing data handling. Our review shows how, over the last period, many components of economic evaluations have changed in accordance with the new recommendations towards more transparent and advanced analytic approaches. However, potential limitations are identified in terms of the use of less advanced statistical software together with rarely satisfactory information to support the choice of missing data methods, especially in sensitivity analysis.\n\n\n\n\n\nCitationBibTeX citation:@online{gabrio2023,\n  author = {Gabrio, Andrea},\n  title = {A Review of Heath Economic Evaluation Practice in the\n    {Netherlands:} Are We Moving Forward?},\n  date = {2023-06-03},\n  url = {https://www.cambridge.org/core/journals/health-economics-policy-and-law/article/abs/review-of-heath-economic-evaluation-practice-in-the-netherlands-are-we-moving-forward/8A4D0D3A9E7EBC83A65B4283D170FCC6},\n  doi = {10.1017/S1744133123000087},\n  langid = {en},\n  abstract = {{[}Economic evaluations have been increasingly conducted\n    in different countries to aid national decision-making bodies in\n    resource allocation problems ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2023. “A Review of Heath Economic Evaluation\nPractice in the Netherlands: Are We Moving Forward?” Health\nEconomics, Policy and Law. June 3, 2023. https://doi.org/10.1017/S1744133123000087."
  },
  {
    "objectID": "research/bookHTA/bookHTA.html",
    "href": "research/bookHTA/bookHTA.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Introduction\nThe type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and \\(P\\)-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n\nwhether the treatments under evaluation are cost-effective given the available evidence and\nwhether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).\n\nThis corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\n\n\nBayesian methods in HTA\nThere are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in Figure 1.\n\n\n\n\n\n\nFigure 1: Diagram representation of the process for health economic evaluation.\n\n\n\nThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g. the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the available evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n\nwhether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and\nwhether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.\n\nWhile the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\n\n\nConclusions\nHTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions."
  },
  {
    "objectID": "research/jointHTA/jointHTA.html",
    "href": "research/jointHTA/jointHTA.html",
    "title": "Joint Longitudinal Models for Dealing With Missing at Random Data in Trial-Based Economic Evaluations",
    "section": "",
    "text": "Introduction\nIn trial-based economic evaluation, some individuals are typically associated with missing data at some time point, so that their corresponding aggregated outcomes (e.g. quality-adjusted life-years) cannot be evaluated. Restricting the analysis to the complete cases is inefficient and can result in biased estimates, while imputation methods are often implemented under a missing at random (MAR) assumption. We propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR.\n\n\nStandard approach in trial-based CEA\nAccording to recent reviews, standard practice in trial-based CEAs handles missingness at the level of the aggregated outcomes and baseline variables. Indeed, estimates of interest are obtained by directly modeling the aggregated outcomes rather than the utility and cost data at each time. This requires the analyst to process the data collected on individual \\(i\\) at time \\(j\\) in treatment \\(t\\), to derive the aggregated measures over the study duration.\nFigure 1 shows a typical data set of trial-based CEA, formed by the sets of utility and cost variables collected at baseline \\(j = 0\\) and some follow-ups \\(j = 1,\\ldots,J\\). The graph represents the standard procedure for processing the data and identifying the variables used in the analysis.\n\n\n\n\n\n\nFigure 1: Schematic representation of the standard procedure for processing trial-based CEA data\n\n\n\nA general limitation of any aggregated method is to ignore the longitudinal nature of the data and discard all follow-up values for partially observed individuals. Conversely, methods that handle missingness at each time point account for the longitudinal structure, incorporate all available evidence, and potentially make the missingness assumptions (e.g. missing at random or MAR) more reasonable.\n\n\nMethods\nWe propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR. We compare the results from methods that handle missingness at an aggregated (case deletion, baseline imputation, and joint aggregated models) and disaggregated (joint longitudinal models) level under MAR. The methods are compared using a simulation study and applied to data from 2 real case studies.\n\n\nConclusions\nJoint longitudinal models provide an alternative and potentially less biased approach for handling missing data with respect to current practice under a missing at random assumption. Methods that ignore some of the available information may be associated with biased results and mislead the decision-making process. This is a potentially serious issue for those who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options."
  },
  {
    "objectID": "research/missingHE/missingHE.html",
    "href": "research/missingHE/missingHE.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variables, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitivity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot can produce graphs, such as those shown in Figure 1, which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\n\n\n\n\n\nFigure 1: Plot of observed (black dots) and imputed (red dots and lines) effectiveness and cost data by treatment group.\n\n\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "research/partsurvHTA/partsurvHTA.html",
    "href": "research/partsurvHTA/partsurvHTA.html",
    "title": "A Bayesian Framework for Patient-Level Partitioned Survival Cost-Utility Analysis",
    "section": "",
    "text": "Modelling Framework\nwe extend the current methods for modelling trial-based partitioned survival cost-utility data, taking advantage of the flexibility of the Bayesian approach, and specify a joint probabilistic model for the health economic outcomes. We propose a general framework that is able to account for the multiple types of complexities affecting individual level data (correlation, missingness, skewness and structural values), while also explicitly modelling the dependence relationships between different types of quality of life and cost components.\nConsider a clinical trial in which patient-level information on a set of suitably defined effectiveness and cost variables is collected at \\(J\\) time points on \\(N\\) individuals, who have been allocated to \\(T\\) intervention groups. Assume that the primary endpoint of the trial is OS, while secondary endpoints include PFS, a self-reported health-related quality of life questionnaire (e.g. EQ-5D) and health records on different types of services (e.g. drug frequency and dosage, hospital visits, etc.). Following standard health economic notation, we denote with \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) the two sets of health economic outcomes (effectiveness and costs) collected for the \\(i\\)-th individual in treatment \\(t\\) of the trial. For simplicity, we define \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) based on the variables used in the analysis.\nThe effectiveness outcomes are represented by pre-progression (\\(e^{PFS}\\_{it}=\\text{QAS}^{\\text{PFS}}\\)) and post-progression (\\(e^{PPS}\\_{it}=\\text{QAS}^{\\text{PPS}}\\)) QAS data calculated using survival and utility data collected up to and beyond progression. We denote the full set of effectiveness variables as \\(\\boldsymbol e_{it}=(e^{\\text{PFS}}\\_{it},e^{\\text{PPS}}\\_{it})\\), formed by the pre and post-progression components. The cost outcomes are represented by a set of \\(K\\) variables (\\(c\\_{it}=c^k\\_{it}\\), for \\(k=1,\\ldots,K\\)) calculated based on \\(K\\) different types of health services and associated unit prices. We denote the full set of cost variables as \\(\\boldsymbol c\\_{it}=(c^1\\_{it},\\ldots,c^K\\_{it})\\), formed by the \\(K\\) different cost components.\nThe objective of the economic evaluation is to perform a patient-level partitioned survival cost-utility analysis by specifying a joint model \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) denotes the full set of model parameters. Among these parameters, interest is in the marginal mean effectiveness and costs \\(\\boldsymbol \\mu=(\\mu\\_{et},\\mu\\_{ct})\\) which are used to inform the decision-making process. Different approaches can be used to specify \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\). Here, we express the joint distribution as\n\\[\np(\\boldsymbol e_{it}, \\boldsymbol c_{it} \\mid \\boldsymbol \\theta) = p(\\boldsymbol e_{it} \\mid \\boldsymbol \\theta_e)p(\\boldsymbol c_{it} \\mid \\boldsymbol  e_{it} , \\boldsymbol  \\theta_c),\n\\tag{1}\\]\nwhere \\(p(\\boldsymbol e_{it} \\mid \\boldsymbol  \\theta_e)\\) is the marginal distribution of the effectiveness and \\(p(\\boldsymbol  c_{it} \\mid \\boldsymbol  e_{it} \\boldsymbol  \\theta_c)\\) is the conditional distribution of the costs given the effectiveness, respectively indexed by \\(\\boldsymbol  \\theta_e\\) and \\(\\boldsymbol  \\theta_c\\), with \\(\\boldsymbol  \\theta=(\\boldsymbol  \\theta_e,\\boldsymbol  \\theta_c)\\). We specify the model in Equation 1 in terms of a marginal distribution for the effectiveness and a conditional distribution for the costs. A key advantage of using a conditional factorisation, compared to a multivariate marginal approach, is that univariate models for each variable can be flexibly specified to tackle the idiosyncrasies of the data (e.g. non-normality ans spikes) while also capturing the potential correlation between the variables. We now describe how the two factors on the right-hand side of the Equation can be specified.\nFigure 1 provides a visual representation of the proposed modelling framework.\n\n\n\n\n\n\nFigure 1: Visual representation of the proposed modelling framework\n\n\n\nThe effectiveness and cost distributions are represented in terms of combined “modules”- the red and blue boxes - in which the random quantities are linked through logical relationships. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in each module.\n\n\nConclusions\nAlthough our approach may not be applicable to all cases, the data analysed are very much representative of the “typical” data used in partitioned survival cost-utility analysis alongside clinical trials. Thus, it is highly likely that the same features apply to other real cases. This is a very important, if somewhat overlooked problem, as methods that do not take into account the complexities affecting patient-level data, while being easier to implement and well established among practitioners, may ultimately mislead cost-effectiveness conclusions and bias the decision-making process."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html",
    "href": "research/reviewQES/reviewQES.html",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "",
    "text": "We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a Quality Evaluation Scheme (QES), providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#descriptive-review",
    "href": "research/reviewQES/reviewQES.html#descriptive-review",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Descriptive Review",
    "text": "Descriptive Review\n\n\n\n\n\n\nFigure 2: Missingness methods by outcome and period.\n\n\n\nFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects (\\(\\approx 45\\%\\) and \\(\\approx 38\\%\\)) and costs ( \\(\\approx 50\\%\\) and \\(\\approx 62\\%\\)). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#quality-assessment",
    "href": "research/reviewQES/reviewQES.html#quality-assessment",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Quality assessment",
    "text": "Quality assessment\nGenerally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the QES. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than \\(7\\%\\) of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making."
  },
  {
    "objectID": "software/missingHE/index.html",
    "href": "software/missingHE/index.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package aimed at providing some useful tools to analysts in order to handle missing outcome data under a Full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS. The package allows to obtain inferences using Markov Chain Monte Carlo (MCMC) methods under a range of modelling approaches and missing data assumptions. The package also contains functions specifically defined to assess model fit and possible issues in model convergence as well as to summarise the main results from the economic analysis.\nMissing data are iteratively imputed using data augmentation methods according to the type of model, distribution and missingness assumptions specified by the user using different arguments in the functions of the package. The posterior distribution of the main quantities of interest (e.g. some suitable measures of costs and clinical benefits) is then summarised to assess the cost-effectiveness of a new intervention (\\(t=2\\)) against a standard intervention (\\(t=1\\)).\nmissingHE produces plots which compares the observed and imputed values for both cost and benefit measures in each treatment intervention considered to detect possible concerns about the plausibility of the imputation methods. In addition, the output of missingHE cab be analysed using different funtions in the R package BCEA which produces a synthesis of the decision process given the current evidence and uncertainty, as well as several indicators that can be used to perform Probabilistic Sensitivity Analysis to parameter and model uncertainty.\n\n\n\nExample of a graphical output from missingHE"
  },
  {
    "objectID": "software/missingHE/index.html#example",
    "href": "software/missingHE/index.html#example",
    "title": "missingHE",
    "section": "Example",
    "text": "Example\n\nlibrary(missingHE)\nmodel.sel &lt;- selection(data = MenSS, model.eff = e ~ u.0, model.cost = c ~ e, model.me = me ~ 1, model.mc = mc ~ 1, type = \"MAR\", n.chains = 2, n.iter = 1000, n.burnin = 100, dist_e = \"norm\", dist_c = \"norm\")\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 410\n   Unobserved stochastic nodes: 242\n   Total graph size: 2399\n\nInitializing model\n\nsummary(model.sel)\n\n\n Cost-effectiveness analysis summary \n \n Comparator intervention: intervention 1 \n Reference intervention: intervention 2 \n \n Parameter estimates under MAR assumption\n \n Comparator intervention \n                        mean     sd      LB     UB\nmean effects (t = 1)   0.874  0.016   0.847  0.901\nmean costs (t = 1)   236.716 49.778 155.903 319.43\n\n Reference intervention \n                        mean     sd      LB      UB\nmean effects (t = 2)   0.917  0.022   0.882   0.952\nmean costs (t = 2)   185.896 40.642 120.007 250.279\n\n Incremental results \n                   mean     sd       LB     UB\ndelta effects     0.043  0.028   -0.002  0.089\ndelta costs      -50.82 63.696 -155.573 55.615\nICER          -1191.646"
  },
  {
    "objectID": "software/missingHE/index.html#news-and-updates-about-missinghe",
    "href": "software/missingHE/index.html#news-and-updates-about-missinghe",
    "title": "missingHE",
    "section": "News and updates about missingHE",
    "text": "News and updates about missingHE\n\nFrom 25/09/2019, the updated version (1.2.1) of missingHE has become available on CRAN, which allows to perform posterior predictive checks for each type of model as a further way to assess the fit of the model to the observed data.\n\nThe checks can be done by first setting the optional argument ppc = TRUE when fitting the model using one of the main function of the package. For example, when using selection to fit selection models you would have something like this:\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age, model.cost = c ~ age + e, model.me = me ~ age, model.mc = mc ~ age, dist_e = \"norm\", dist_c = \"gamma\", type = \"MAR\", n.iter = 1000, ppc = TRUE)\n\nThen you can use the function ppc to perform different types of posterior predictive checks that you can choose among a set of pre-specified types using the type argument. For example, if we want to compare histograms of the empirical and predictive distributions of the effectiveness variable in one arm (e.g. control), then we can type\n\nppc(model.sel, type = \"histogram\", outcome = \"effects_arm1\")\n\nand we get something like this\n\n\n\nExample of posterior predictive checks in missingHE\n\n\n\nFrom 07/01/2020, the updated version (1.3.2) of missingHE has become available on CRAN, which allows to choose among more distributions for the effectiveness measures, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) health outcomes.\n\nFor example, we can choose to specify a selection model assuming a Bernoulli distribution for the effects (if this is a binary outcome) and a LogNormal distribution for the costs\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age, model.cost = c ~ age + e, model.me = me ~ 1, model.mc = mc ~ 1, dist_e = \"bern\", dist_c = \"lnorm\", type = \"MAR\")\n\n\nFrom 30/04/2020, the updated version (1.4.0) of missingHE has become available on CRAN, which allows to perform fit random effects for each type of model implemented. The random terms can be specified using the following notation\n\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age + (age | site), model.cost = c ~ age + e + (age + e | site), model.me = me ~ age + (1 | site), model.mc = mc ~ age + (0 + age | site), dist_e = \"norm\", dist_c = \"gamma\", type = \"MAR\", n.iter = 10000, ppc = TRUE)\n\nI borrowed this notation, alongside with a couple of internal functions, from the lme4 package. The terms inside the brackets on the left of the bar are the terms for which the random effects are assumed (these must also be included as fixed effects). The term on the right of the bar is the clustering variable over which the random effects are specified.\nFor example the formula + (age | site) specifies random effects for the intercept and age across the values of the site variable. It aslo possible to specify random slope only models (i.e. remove the random intercept) by adding the term 0 + inside the brackets on the left of the bar.\nAll functions in the package have been updated to take into account the possibility that random effects are specified and to perform diagnostic and posterior predictive checks based on the random effects if these are included. In addition, a new generic function called coef is now available to extract the fixed or random effect terms from the effectiveness and cost models for each type of model in missingHE. For example, we can extract summary statistics for the fixed effects from the fitted selection model by using the command\n\ncoef(model.sel, random = FALSE)\n\nwhich prints something like this\n\n\n$Comparator\n$Comparator$Effects\n             mean    sd  lower upper\n(Intercept) 0.187 0.138 -0.083 0.457\nu.0         0.779 0.150  0.487 1.074\n\n$Comparator$Costs\n                mean      sd     lower    upper\n(Intercept)  236.716  49.778   142.237  335.056\ne           -963.484 418.694 -1772.698 -149.278\n\n\n$Reference\n$Reference$Effects\n             mean    sd lower upper\n(Intercept) 0.665 0.075 0.520 0.811\nu.0         0.285 0.087 0.115 0.458\n\n$Reference$Costs\n                mean      sd    lower   upper\n(Intercept)  185.896  40.642  103.211 262.101\ne           -187.828 349.812 -871.173 485.674\n\n\nIf we set random = TRUE, then summary statistics for the random effects terms are printed.\n\nFrom 10/06/2020 a new version (1.4.1) of missingHE is available to download from my GitHub page, which includes three vignettes providing some tutorials on how to use the functions of the package. Each vignette is specifically designed to help different types of users:\n\nThe first vignette is named Introduction_to_missingHE and is designed to provide some introductory summary about the use of the functions of the package based on the default settings, what the user needs to specify and how to interpret and extract the results. See the vignette here\nThe second vignette is named Fitting_MNAR_models_in_missingHE and is deisgned to help those who would like to explore MNAR assumptions and how this can be done within each main function of the package. See the vignette here\nThe third vignette is named Model_customisation_in_missingHE and is designed for those who are already familiar with the package but who would like to customise the functions in a more flexible way, for example by including random effects, using different priors or modelling assumptions. See the vignette here\n\nFrom 21/03/2023 a new version (4.2.0) of missingHE is available to download from my GitHub page, which includes an additional vignette providing some tutorials on how to extend the already existing functions within the package to fit longitudinal data. Some functions and options still need to be updated but the default configurations for either selection, pattern, or hurdle functions can now be applied to two-arms within-trial longitudinal data economic evaluations. See the vignette here\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "software/missingHE/index.html#installation",
    "href": "software/missingHE/index.html#installation",
    "title": "missingHE",
    "section": "Installation",
    "text": "Installation\nThere are two ways of installing missingHE. A stable version (currently 4.2.0) is packaged and available from CRAN. You can simply type on your R terminal\n\ninstall.packages(\"missingHE\")\n\nThe second way involves using the development version of missingHE, which is available from GitHub - this will usually be updated more frequently and may be continuously tested. On Windows machines, you need to install a few dependencies, including Rtools first, e.g. by running\n\npkgs &lt;- c(\"R2jags\",\"ggplot2\",\"gridExtra\",\"BCEA\",\"ggmcmc\",\"loo\",\"Rtools\",\"devtools\", \"utils\")\nrepos &lt;- c(\"https://cran.rstudio.com\") \ninstall.packages(pkgs,repos=repos,dependencies = \"Depends\")\n\nbefore installing the package using devtools:\n\ndevtools::install_github(\"AnGabrio/missingHE\", build_vignettes = TRUE)\n\nThe optional argument build_vignettes = TRUE allows to install the vignettes of the package locally on your computer. These consist in brief tutorials to guid the user on how to use and customise the models in missingHE using different functions of the package. Once the package is installed, they can be accessed by using the command\n\nutils::browseVignettes(package = \"missingHE\")\n\nAll models implemented in missingHE are written in the BUGS language using the software JAGS, which needs to be installed from its own repository and instructions for installations under different OS can be found online. Once installed, the software is called in missingHE via the R package R2jags. Note that the missingHE package is currently under active development and therefore it is advisable to reinstall the package directly from GitHub before each use to ensure that you are using the most updated version."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html",
    "href": "tutorials/2019-07-01-intro-jags/index.html",
    "title": "Super basic introduction to JAGS",
    "section": "",
    "text": "The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R.\nPrerequisites:"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#what-is-jags",
    "href": "tutorials/2019-07-01-intro-jags/index.html#what-is-jags",
    "title": "Super basic introduction to JAGS",
    "section": "What is JAGS?",
    "text": "What is JAGS?\nJAGS or Just Another Gibbs Sampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Plummer (2004)). JAGS is a free software based on the Bayesian inference Using Gibbs Sampling (informally BUGS) language at the base of WinBUGS/OpenBUGS but, unlike these programs, it is written in C++ and is platform independent. The latest version of JAGS can be downloaded from Martyn Plummer’s repository and is available for different OS. There are different R packages which function as frontends for JAGS. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the R2jags package (Su et al. (2015)) and show how to fit JAGS models using this package."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#installing-jags-and-r2jags",
    "href": "tutorials/2019-07-01-intro-jags/index.html#installing-jags-and-r2jags",
    "title": "Super basic introduction to JAGS",
    "section": "Installing JAGS and R2jags",
    "text": "Installing JAGS and R2jags\nInstall the latest version of JAGS for your OS. Next, install the package R2jags from within R or Rstudio, via the package installer or by typing in the command line\n\ninstall.packages(\"R2jags\", dependencies = TRUE)\n\nThe dependencies = TRUE option will automatically install all the packages on which the functions in the R2jags package rely."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#simulate-data",
    "href": "tutorials/2019-07-01-intro-jags/index.html#simulate-data",
    "title": "Super basic introduction to JAGS",
    "section": "Simulate data",
    "text": "Simulate data\nFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon  \n\\]\nThe R commands which I use to simulate the data are the following:\n\nn.sim=100; set.seed(123)\nx=rnorm(n.sim, mean = 5, sd = 2)\nepsilon=rnorm(n.sim, mean = 0, sd = 1)\nbeta0=1.5\nbeta1=1.2\ny=beta0 + beta1 * x + epsilon\n\nThen, I define all the data for JAGS in a list object\n\ndatalist=list(\"y\",\"x\",\"n.sim\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#model-file",
    "href": "tutorials/2019-07-01-intro-jags/index.html#model-file",
    "title": "Super basic introduction to JAGS",
    "section": "Model file",
    "text": "Model file\nNow, I write the model for JAGS and save it as a text file named \"basic.mod.txt\" in the current working directory\n\nbasic.mod= \"\nmodel {\n#model\n for(i in 1:n.sim){\n  y[i] ~ dnorm(mu[i], tau)\n  mu[i] = beta0 + beta1 * x[i]\n }\n#priors\nbeta0 ~ dnorm(0, 0.01)\nbeta1 ~ dnorm(0, 0.01)\ntau ~ dgamma(0.01,0.01)\n}\n\"\n\nThe part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean mu and precision tau (where, precision = 1/variance). The covariate x is included at the mean level using a linear regression, which is indexed by the intercept beta0 and slope beta1 terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.\nTo write and save the model as the text file “basic.mod.txt” in the current working directory, I use the writeLines function\n\nwriteLines(basic.mod, \"basic.mod.txt\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#pre-processing",
    "href": "tutorials/2019-07-01-intro-jags/index.html#pre-processing",
    "title": "Super basic introduction to JAGS",
    "section": "Pre-processing",
    "text": "Pre-processing\nDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in JAGS\n\nparams=c(\"beta0\",\"beta1\")\ninits=function(){list(\"beta0\"=rnorm(1), \"beta1\"=rnorm(1))}\n\nThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, JAGS can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting inits=NULL, which is then passed to the jags function in R2jags.\nBefore using R2jags for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable\n\nlibrary(R2jags)\nset.seed(123)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#fit-the-model",
    "href": "tutorials/2019-07-01-intro-jags/index.html#fit-the-model",
    "title": "Super basic introduction to JAGS",
    "section": "Fit the model",
    "text": "Fit the model\nNow, we can fit the model in JAGS using the jags function in the R2jags package and save it in the object basic.mod\n\nbasic.mod=jags(data = datalist, inits = inits,\n  parameters.to.save = params, n.chains = 2, n.iter = 2000, \n  n.burnin = 1000, model.file = \"basic.mod.txt\")\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 406\nNA \nNA Initializing model\n\n\nWhile the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath JAGS, such as number of observed and unobserved nodes and graph size."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#post-processing",
    "href": "tutorials/2019-07-01-intro-jags/index.html#post-processing",
    "title": "Super basic introduction to JAGS",
    "section": "Post-processing",
    "text": "Post-processing\nOnce the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing print(basic.mod) or, alternatively,\n\nprint(basic.mod$BUGSoutput$summary)\n\n\n\nNA           mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff\nNA beta0      1.5 0.294   0.95   1.3   1.5   1.7   2.1    1  2000\nNA beta1      1.2 0.054   1.07   1.1   1.2   1.2   1.3    1  2000\nNA deviance 278.8 2.475 276.03 277.1 278.2 279.9 285.1    1  2000\n\n\nThe posterior distribution of each parameter is summarised in terms of:\n\nThe mean, sd and some percentiles\nPotential scale reduction factor Rhat and effective sample size n.eff (Gelman et al. (2013)). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below \\(1.05\\) for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).\n\nThe deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (Spiegelhalter et al. (2014)), which is a relative measure of model comparison. The DIC of the model can be accessed by typing\n\nbasic.mod$BUGSoutput$DIC\n\nNA [1] 282"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#diagnostics",
    "href": "tutorials/2019-07-01-intro-jags/index.html#diagnostics",
    "title": "Super basic introduction to JAGS",
    "section": "Diagnostics",
    "text": "Diagnostics\nMore diagnostics are available when we convert the model output into an MCMC object using the command\n\nbasic.mod.mcmc=as.mcmc(basic.mod)\n\nDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the mcmcplots package (Curtis (2015)) to obtain graphical diagnostics and results.\n\ninstall.packages(\"mcmcplots\")\nlibrary(mcmcplots)\n\nFor example, density and trace plots can be obtained by typing\n\ndenplot(basic.mod.mcmc, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\ntraplot(basic.mod.mcmc, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\n\nBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html",
    "href": "tutorials/2020-02-01-ancova-stan/index.html",
    "title": "Ancova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages dedicated to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#introduction",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#introduction",
    "title": "Ancova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#null-hypothesis",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#null-hypothesis",
    "title": "Ancova (Stan)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nFactor A: the main treatment effect\n\n\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\n\nThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\n\\(H_0(B):\\beta_1(pooled)=0\\)\n\nThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#linear-models",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#linear-models",
    "title": "Ancova (Stan)",
    "section": "Linear models",
    "text": "Linear models\nOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\nSingle categorical and single covariate\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\nSingle categorical and two covariates\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\nFactorial designs\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nNested designs\n\nLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nPartly nested designs\n\nLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#analysis-of-variance",
    "title": "Ancova (Stan)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\nancova_table\n\nNA           df       MS       F-ratio (A&B fixed) F-ratio (B fixed) \nNA Factor A  \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"   \"(MS A)/(MS res)\" \nNA Factor B  \"1\"      \"MS B\"   \"(MS B)/(MS res)\"   \"(MS B)/(MS res)\" \nNA Factor AB \"a-1\"    \"MS AB\"  \"(MS AB)/(MS res)\"  \"(MS AB)/(MS res)\"\nNA Residual  \"(n-2)a\" \"MS res\" \"\"                  \"\"\n\n\nThe corresponding R syntax is given below.\n\nanova(lm(DV ~ B * A, dataset))\n# OR\nanova(aov(DV ~ B * A, dataset))\n# OR (make sure not using treatment contrasts)\nAnova(lm(DV ~ B * A, dataset), type = \"III\")"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#assumptions",
    "title": "Ancova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\nThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\nThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nThe appropriate residuals are independent of one another.\nThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\nFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\nFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\nHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\n\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\nAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\nUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\nSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis",
    "title": "Ancova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nlibrary(car)\nscatterplot(Y ~ B | A, data = data)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data)\n\n# OR via ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\nanova(lm(Y ~ B * A, data = data))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nNA B          1  989.99  989.99  92.6782 1.027e-09 ***\nNA A          2 2320.05 1160.02 108.5956 9.423e-13 ***\nNA B:A        2   51.36   25.68   2.4041    0.1118    \nNA Residuals 24  256.37   10.68                       \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis-1",
    "title": "Ancova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nscatterplot(Y ~ B | A, data = data1)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data1)\n\n\n\n\n\n\n\n# OR via ggplot\nggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\nanova(lm(Y ~ B * A, data = data1))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nNA B          1  442.02  442.02  41.380 1.187e-06 ***\nNA A          2 2760.60 1380.30 129.217 1.418e-13 ***\nNA B:A        2  285.75  142.87  13.375 0.0001251 ***\nNA Residuals 24  256.37   10.68                      \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is strong evidence to suggest that the assumption of equal slopes is violated."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#fitting-the-model",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#fitting-the-model",
    "title": "Ancova (Stan)",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nmodelString2 = \"\n  data {\n  int&lt;lower=1&gt; n;\n  int&lt;lower=1&gt; nX;\n  vector [n] y;\n  matrix [n,nX] X;\n  }\n  parameters {\n  vector[nX] beta;\n  real&lt;lower=0&gt; sigma;\n  }\n  transformed parameters {\n  vector[n] mu;\n\n  mu = X*beta;\n  }\n  model {\n  // Likelihood\n  y~normal(mu,sigma);\n  \n  // Priors\n  beta ~ normal(0,100);\n  sigma~cauchy(0,5);\n  }\n  generated quantities {\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n  log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); \n  }\n  }\n  \n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"ancovaModel2.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat &lt;- model.matrix(~A * B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = Xmat, nX = ncol(Xmat), n = nrow(data1)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"log_lik\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\ndata1.rstan &lt;- stan(data = data1.list, file = \"ancovaModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 2.9e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.065 seconds (Warm-up)\nNA Chain 1:                0.045 seconds (Sampling)\nNA Chain 1:                0.11 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 4e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.069 seconds (Warm-up)\nNA Chain 2:                0.045 seconds (Sampling)\nNA Chain 2:                0.114 seconds (Total)\nNA Chain 2:\n\nprint(data1.rstan, par = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]  48.04    0.10 1.91  44.19  46.84  48.04  49.29  51.73   375    1\nNA beta[2] -10.41    0.12 2.74 -15.56 -12.18 -10.39  -8.72  -4.83   561    1\nNA beta[3] -26.32    0.12 2.43 -31.11 -27.90 -26.35 -24.79 -21.39   430    1\nNA beta[4]  -0.34    0.00 0.08  -0.50  -0.40  -0.35  -0.29  -0.19   416    1\nNA beta[5]  -0.28    0.00 0.11  -0.49  -0.34  -0.27  -0.21  -0.08   483    1\nNA beta[6]   0.26    0.00 0.11   0.04   0.19   0.26   0.33   0.48   511    1\nNA sigma     3.36    0.02 0.50   2.55   3.01   3.29   3.65   4.50  1090    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 11:50:52 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#mcmc-diagnostics-1",
    "title": "Ancova (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nmcmc &lt;- As.mcmc.list(data1.rstan)\n\ndenplot(mcmc, parms = c(\"beta\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"beta\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\n#Raftery diagnostic\nraftery.diag(mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\n\nThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n\nNA            beta[1]     beta[2]     beta[3]     beta[4]     beta[5]     beta[6]\nNA Lag 0   1.00000000  1.00000000  1.00000000  1.00000000  1.00000000  1.00000000\nNA Lag 1   0.47965891  0.40490097  0.42921394  0.47213590  0.43786125  0.40348216\nNA Lag 5   0.10276601  0.04520477  0.08439589  0.08552624  0.07188554  0.05055728\nNA Lag 10  0.03369686  0.04235285  0.03837580  0.04276107  0.06964443  0.05297790\nNA Lag 50 -0.05533000 -0.03563501 -0.08740181 -0.04617438 -0.01873684 -0.08254539\nNA               sigma  log_lik[1]   log_lik[2]  log_lik[3]   log_lik[4]\nNA Lag 0   1.000000000  1.00000000  1.000000000  1.00000000  1.000000000\nNA Lag 1   0.244539989  0.30295557  0.115863972  0.31646347  0.029392467\nNA Lag 5   0.034914225  0.03692325  0.025476065  0.04472861  0.009289407\nNA Lag 10  0.027104230  0.04375764  0.001071547  0.03530329  0.036276640\nNA Lag 50 -0.005837235 -0.04398093 -0.022416916 -0.01700789 -0.017665756\nNA         log_lik[5]   log_lik[6]   log_lik[7]   log_lik[8]  log_lik[9]\nNA Lag 0   1.00000000  1.000000000  1.000000000  1.000000000  1.00000000\nNA Lag 1   0.05686476  0.279972053  0.333411513  0.393178740  0.26907784\nNA Lag 5   0.01335058  0.009737116  0.056417267 -0.001796254  0.02998479\nNA Lag 10  0.03736092  0.035609098  0.008349122  0.056906159  0.01515109\nNA Lag 50 -0.01570145 -0.012353687 -0.007107282 -0.056604829 -0.03352728\nNA         log_lik[10]  log_lik[11] log_lik[12]  log_lik[13]  log_lik[14]\nNA Lag 0   1.000000000  1.000000000 1.000000000  1.000000000  1.000000000\nNA Lag 1   0.192883022  0.068556254 0.204989891  0.003861059  0.105690603\nNA Lag 5   0.037648652  0.031608422 0.030527908  0.011161059  0.040450947\nNA Lag 10  0.007251782 -0.006369060 0.001890380  0.036208400  0.008267025\nNA Lag 50 -0.028984961 -0.005015408 0.005481482 -0.018976796 -0.004975828\nNA         log_lik[15]  log_lik[16]   log_lik[17] log_lik[18] log_lik[19]\nNA Lag 0   1.000000000  1.000000000  1.0000000000  1.00000000  1.00000000\nNA Lag 1  -0.021918535  0.028936497  0.1274525386  0.03355231 -0.01865606\nNA Lag 5  -0.006083026  0.025989718  0.0299094329  0.02020100  0.01961257\nNA Lag 10  0.008300113  0.014088547 -0.0117960594  0.01141223  0.02524133\nNA Lag 50  0.004556124 -0.009052317 -0.0004925802  0.02661374 -0.01229104\nNA        log_lik[20]  log_lik[21] log_lik[22]  log_lik[23]  log_lik[24]\nNA Lag 0   1.00000000  1.000000000  1.00000000  1.000000000  1.000000000\nNA Lag 1   0.17819054  0.120052667  0.20632108 -0.024461686 -0.052472858\nNA Lag 5   0.03745210 -0.020154436  0.03360932 -0.031301123 -0.009290672\nNA Lag 10  0.03345325  0.004498098  0.02884422 -0.002109614 -0.014376687\nNA Lag 50 -0.01491661 -0.016212205 -0.02709841 -0.017716486  0.004503930\nNA         log_lik[25]  log_lik[26] log_lik[27]  log_lik[28] log_lik[29]\nNA Lag 0   1.000000000  1.000000000  1.00000000  1.000000000  1.00000000\nNA Lag 1  -0.026932812 -0.010753209 -0.03500057 -0.022972322  0.02432922\nNA Lag 5  -0.007038723 -0.018424596 -0.01393167 -0.006548756 -0.03234786\nNA Lag 10  0.001166187  0.002621322  0.01265460  0.014571694 -0.00114465\nNA Lag 50 -0.007274914 -0.014512013  0.01665142 -0.016130612 -0.01750531\nNA         log_lik[30]          lp__\nNA Lag 0   1.000000000  1.000000e+00\nNA Lag 1  -0.015077656  4.530274e-01\nNA Lag 5   0.006166306 -7.071898e-05\nNA Lag 10  0.028726884  8.583829e-03\nNA Lag 50 -0.014572075 -8.625792e-02\n\n\n\nstan_rhat(data1.rstan)\n\n\n\n\n\n\n\nstan_ess(data1.rstan)\n\n\n\n\n\n\n\n\nRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#model-validation-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#model-validation-1",
    "title": "Ancova (Stan)",
    "section": "Model validation",
    "text": "Model validation\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nResiduals against predictors\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = newdata1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nnewdata1 = newdata1 %&gt;% cbind(fit, resid)\nggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\n\n\n\n\n\n\n\n\nAnd now for studentised residuals\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nsresid = resid/sd(resid)\nggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nXmat = model.matrix(~A * B, data1)\n## get median parameter estimates\ncoefs = mcmc[, 1:6]\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\n    ], mcmc[i, \"sigma\"]))\nnewdata1 = data.frame(A = data1$A, B = data1$B, yRep) %&gt;% gather(key = Sample,\n    value = Value, -A, -B)\nggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \"Model\"),\n    alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\n    fill = \"Obs\"), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\n    x = A), position = position_jitter(width = 0.1, height = 0),\n    color = \"black\") + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \"Model\",\n    group = B, color = A), alpha = 0.5) + geom_point(data = data1,\n    aes(y = Y, x = B, group = B, color = A)) + theme_classic()\n\n\n\n\n\n\n\n\nThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\nmcmc_intervals(as.matrix(data1.rstan), regex_pars = \"beta|sigma\")\n\n\n\n\n\n\n\nmcmc_areas(as.matrix(data1.rstan), regex_pars = \"beta|sigma\")"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#parameter-estimates-1",
    "title": "Ancova (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the additive model.\n\nprint(data1.rstan, pars = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]  48.04    0.10 1.91  44.19  46.84  48.04  49.29  51.73   375    1\nNA beta[2] -10.41    0.12 2.74 -15.56 -12.18 -10.39  -8.72  -4.83   561    1\nNA beta[3] -26.32    0.12 2.43 -31.11 -27.90 -26.35 -24.79 -21.39   430    1\nNA beta[4]  -0.34    0.00 0.08  -0.50  -0.40  -0.35  -0.29  -0.19   416    1\nNA beta[5]  -0.28    0.00 0.11  -0.49  -0.34  -0.27  -0.21  -0.08   483    1\nNA beta[6]   0.26    0.00 0.11   0.04   0.19   0.26   0.33   0.48   511    1\nNA sigma     3.36    0.02 0.50   2.55   3.01   3.29   3.65   4.50  1090    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 11:50:52 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n# OR\ntidyMCMC(data1.rstan, conf.int = TRUE, conf.method = \"HPDinterval\", pars = c(\"beta\", \"sigma\"))\n\nNA # A tibble: 7 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   48.0      1.91    44.2      51.8   \nNA 2 beta[2]  -10.4      2.74   -16.0      -5.34  \nNA 3 beta[3]  -26.3      2.43   -30.9     -21.3   \nNA 4 beta[4]   -0.345    0.0767  -0.492    -0.193 \nNA 5 beta[5]   -0.276    0.105   -0.487    -0.0735\nNA 6 beta[6]    0.262    0.110    0.0393    0.479 \nNA 7 sigma      3.36     0.501    2.52      4.46\n\n\nConclusions\n\nThe intercept of the first group (Group A) is \\(48.2\\).\nThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\nThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\nA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\ndifference in slope between Group B and Group A \\(-0.270\\).\ndifference in slope between Group C and Group A \\(0.270\\).\n\nThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\n## since values are less than zero\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[2]\"])  # effect of (B-A = 0)\n\nNA [1] 0.0015\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[3]\"])  # effect of (C-A = 0)\n\nNA [1] 0\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[4]\"])  # effect of (slope = 0)\n\nNA [1] 0\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[5]\"])  # effect of (slopeB - slopeA = 0)\n\nNA [1] 0.0095\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[6]\"])  # effect of (slopeC - slopeA = 0)\n\nNA [1] 0.028\n\nmcmcpvalue(as.matrix(data1.rstan)[, 2:6])  # effect of (model)\n\nNA [1] 0\n\n\nThere is evidence that the response differs between the groups.\n\n(full = loo(extract_log_lik(data1.rstan)))\n\nNA \nNA Computed from 2000 by 30 log-likelihood matrix.\nNA \nNA          Estimate  SE\nNA elpd_loo    -83.0 4.8\nNA p_loo         6.9 2.0\nNA looic       166.0 9.5\nNA ------\nNA MCSE of elpd_loo is NA.\nNA MCSE and ESS estimates assume independent draws (r_eff=1).\nNA \nNA Pareto k diagnostic values:\nNA                          Count Pct.    Min. ESS\nNA (-Inf, 0.7]   (good)     29    96.7%   174     \nNA    (0.7, 1]   (bad)       1     3.3%   &lt;NA&gt;    \nNA    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nNA See help('pareto-k-diagnostic') for details.\n\n# now fit a model without main factor\nmodelString3 = \"\n  data {\n  int&lt;lower=1&gt; n;\n  int&lt;lower=1&gt; nX;\n  vector [n] y;\n  matrix [n,nX] X;\n  }\n  parameters {\n  vector[nX] beta;\n  real&lt;lower=0&gt; sigma;\n  }\n  transformed parameters {\n  vector[n] mu;\n\n  mu = X*beta;\n  }\n  model {\n  // Likelihood\n  y~normal(mu,sigma);\n  \n  // Priors\n  beta ~ normal(0,1000);\n  sigma~cauchy(0,5);\n  }\n  generated quantities {\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n  log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); \n  }\n  }\n  \n  \"\n\n## write the model to a stan file \nwriteLines(modelString3, con = \"ancovaModel3.stan\")\n\nXmat &lt;- model.matrix(~A + B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = Xmat, n = nrow(data1), nX = ncol(Xmat)))\ndata1.rstan.red &lt;- stan(data = data1.list, file = \"ancovaModel3.stan\", chains = nChains,\n    iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 1.8e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.052 seconds (Warm-up)\nNA Chain 1:                0.035 seconds (Sampling)\nNA Chain 1:                0.087 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 5e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.043 seconds (Warm-up)\nNA Chain 2:                0.031 seconds (Sampling)\nNA Chain 2:                0.074 seconds (Total)\nNA Chain 2:\n\n(reduced = loo(extract_log_lik(data1.rstan.red)))\n\nNA \nNA Computed from 2000 by 30 log-likelihood matrix.\nNA \nNA          Estimate  SE\nNA elpd_loo    -92.3 4.8\nNA p_loo         5.7 2.0\nNA looic       184.6 9.7\nNA ------\nNA MCSE of elpd_loo is NA.\nNA MCSE and ESS estimates assume independent draws (r_eff=1).\nNA \nNA Pareto k diagnostic values:\nNA                          Count Pct.    Min. ESS\nNA (-Inf, 0.7]   (good)     29    96.7%   364     \nNA    (0.7, 1]   (bad)       1     3.3%   &lt;NA&gt;    \nNA    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nNA See help('pareto-k-diagnostic') for details.\n\npar(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\nplot(full, label_points = TRUE)\nplot(reduced, label_points = TRUE)\n\n\n\n\n\n\n\n\nThe expected out-of-sample predictive accuracy is substantially lower for the model that includes \\(x\\). This might be used to suggest that the inferential evidence for a general effect of \\(x\\) on \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#graphical-summaries-1",
    "title": "Ancova (Stan)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = as.matrix(data1.rstan)\n## Calculate the fitted values\nnewdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\n    len = 100))\nXmat = model.matrix(~A * B, newdata1)\ncoefs = mcmc[, c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\")]\nfit = coefs %*% t(Xmat)\nnewdata1 = newdata1 %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()\n\n\n\n\n\n\n\n\nAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\n## Calculate partial residuals fitted values\nfdata1 = rdata1 = data1\nfMat = rMat = model.matrix(~A * B, fdata1)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\nrdata1 = rdata1 %&gt;% mutate(partial.resid = resid + fit)\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\n    aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2020-02-01-comparing-two-pop-jags/index.html",
    "href": "tutorials/2020-02-01-comparing-two-pop-jags/index.html",
    "title": "Comparing Two Populations (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to explore differences between two populations. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThe BUGS/JAGS/STAN languages and algorithms are very powerful and flexible. However, the cost of this power and flexibility is complexity and the need for a firm understanding of the model you wish to fit as well as the priors to be used. The algorithms requires the following inputs.\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-comparing-two-pop-jags/index.html#fitting-the-model-in-jags",
    "href": "tutorials/2020-02-01-comparing-two-pop-jags/index.html#fitting-the-model-in-jags",
    "title": "Comparing Two Populations (JAGS)",
    "section": "Fitting the model in JAGS",
    "text": "Fitting the model in JAGS\nBroadly, there are two ways of parameterising (expressing the unknown (to be estimated) components of a model) a model. Either we can estimate the means of each group (Means parameterisation) or we can estimate the mean of one group and the difference between this group and the other group(s) (Effects parameterisation). The latter is commonly used for frequentist null hypothesis testing as its parameters are more consistent with the null hypothesis of interest (that the difference between the two groups equals zero).\n\nEffects parameterisation\n\n\\[\ny_i = \\beta_0 + \\beta_{j}x_i + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n\\]\nEach \\(y_i\\) is modelled by an intercept \\(\\beta_0\\) (mean of group A) plus a difference parameter \\(\\beta_j\\) (difference between mean of group A and group B) multiplied by an indicator of which group the observation came from (\\(x_i\\)), plus a residual drawn from a normal distribution with mean \\(0\\) and standard deviation \\(\\sigma\\). Actually, there are as many \\(\\beta_j\\) parameters as there are groups but one of them (typically the first) is set to be equal to zero (to avoid over-parameterization). Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of effect parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\).\n\nMeans parameterisation\n\n\\[\ny_i = \\beta_{j} + \\epsilon_i, \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma).  \n\\]\nEach \\(y_i\\) is modelled as the mean \\(\\beta_j\\) of each group (\\(j=1,2\\)) plus a residual drawn from a normal distribution with a mean of zero and a standard deviation of \\(\\sigma\\). Actually, \\(\\boldsymbol \\beta\\) is a set of \\(j\\) coefficients corresponding to the \\(j\\) dummy coded factor levels. Expected values of \\(y\\) are modelled assuming they are drawn from a normal distribution whose mean is determined by a linear combination of means parameters and whose variance is defined by the degree of variability in this mean. The parameters are: \\(\\beta_1\\), \\(\\beta_2\\) and \\(\\sigma\\).\nIn JAGS, distributions are defined by their precision \\(\\tau\\) rather than their standard deviation \\(\\sigma\\). Precision is just the inverse of variance (\\(\\tau=\\frac{1}{\\sigma^2}\\)) and are chosen as they permit the gamma distribution to be used as the conjugate prior of the variance of a normal distribution. Bayesian analyses require that priors are specified for all the parameters. We will define vague (non-informative) priors for each of the parameters such that the posterior distributions are almost entirely influenced by the likelihood (and thus the data). Hence, appropriate (conjugate) priors for the effects parameterisation could be:\n\n\\(\\boldsymbol \\beta \\sim \\text{Normal}(0,1.0\\text{E-}6)\\) - a very flat normal distribution centered around zero. Note, \\(1.0\\text{E-}6\\) is scientific notation for \\(0.000001\\).\n\\(\\tau \\sim \\text{Gamma}(0.1,0.1)\\) a vague gamma distribution with a shape parameter close to zero (must be greater than \\(0\\)).\n\nThe JAGS language very closely matches the above model and prior definitions - hence the importance on understanding the model you wish to fit. The JAGS language resembles R in many respects. It basically consists of:\n\nstochastic nodes - those that appear on the left hand side of \\(\\sim\\)\ndeterministic nodes - those that appear on the left hand side of &lt;-\n\\(R\\)-like for loops and functions to transform and summarise the data\n\nThat said, JAGS is based on a declarative language, which means: the order with which statements appear in the model definition are not important; nodes should not be defined more than once (you cannot change a value).We are now in a good position to define the model (Likelihood function and prior distributions).\nEffects Parameterisation\n\nmodelString = \"  \n model {\n  #Likelihood\n  for (i in 1:n) {\n    y[i]~dnorm(mu[i],tau)\n    mu[i] &lt;- beta0+beta[x[i]]\n  }\n \n  #Priors\n  beta0 ~ dnorm(0,1.0E-06)\n  beta[1] &lt;- 0\n  beta[2] ~ dnorm(0,1.0E-06)\n  tau ~ dgamma(0.1,0.1)\n  sigma&lt;-1/sqrt(tau)\n\n  #Other Derived parameters \n  # Group means (note, beta is a vector)\n  Group.means &lt;-beta0+beta  \n }\n \"\n## write the model to a text file\nwriteLines(modelString, con = \"ttestModel.txt\")\n\nMeans Parameterisation\n\nmodelString.means = \"  \n  model {\n   #Likelihood \n   for (i in 1:n) {\n     y[i]~dnorm(mu[i],tau)\n     mu[i] &lt;- beta[x[i]]\n   }\n \n   #Priors\n   for (j in min(x):max(x)) {\n     beta[j] ~ dnorm(0,0.001)\n   }\n \n   tau~dgamma(0.1,0.1)\n   sigma&lt;-1/sqrt(tau)\n \n   #Other Derived parameters \n   effect &lt;-beta[2]-beta[1]\n }\n \"\n\n## write the model to a text file\nwriteLines(modelString.means, con = \"ttestModelMeans.txt\")\n\nArrange the data as a list (as required by JAGS). Note, all variables must be numeric, therefore we use the numeric version of \\(x\\). Furthermore, the first level must be \\(1\\).\n\ndata.list &lt;- with(data, list(y = y, x = xn, n = nrow(data)))\ndata.list.means &lt;- with(data, list(y = y, x = xn, n = nrow(data)))\n\nDefine the initial values for the chain. Reasonable starting points can be gleaned from the data themselves.\n\ninits &lt;- list(beta0 = mean(data$y), beta = c(NA, diff(tapply(data$y,\n    data$x, mean))), sigma = sd(data$y/2))\ninits.means &lt;- list(beta = tapply(data$y, data$x, mean), sigma = sd(data$y/2))\n\nDefine the nodes (parameters and derivatives) to monitor.\n\nparams &lt;- c(\"beta0\", \"beta\", \"sigma\", \"Group.means\")\nparams.means &lt;- c(\"beta\", \"effect\", \"sigma\")\n\nDefine the chain parameters.\n\nadaptSteps = 1000  # the number of steps over which to establish a good stepping distance\nburnInSteps = 2000  # the number of initial samples to discard\nnChains = 2  # the number of independed sampling chains to perform \nnumSavedSteps = 50000  # the total number of samples to store\nthinSteps = 1  # the thinning rate\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nWhen using the jags function (R2jags package), it is not necessary to provide initial values. However, if they are to be supplied, the inital values must be provided as a list of the same length as the number of chains.\nEffects Parameterisation\n\ndata.r2jags &lt;- jags(data=data.list,\ninits=NULL, #or inits=list(inits,inits) # since there are two chains\nparameters.to.save=params,\nmodel.file=\"ttestModel.txt\",\nn.chains=nChains,\nn.iter=nIter,\nn.burnin=burnInSteps,\nn.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 214\nNA \nNA Initializing model\n\n#print results\nprint(data.r2jags)\n\nNA Inference for Bugs model at \"ttestModel.txt\", fit using jags,\nNA  2 chains, each with 25000 iterations (first 2000 discarded)\nNA  n.sims = 46000 iterations saved\nNA                mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat\nNA Group.means[1] 105.200   0.357 104.497 104.959 105.201 105.441 105.900 1.001\nNA Group.means[2]  77.882   0.438  77.018  77.589  77.882  78.174  78.746 1.001\nNA beta[1]          0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000\nNA beta[2]        -27.318   0.563 -28.426 -27.696 -27.315 -26.943 -26.212 1.001\nNA beta0          105.200   0.357 104.497 104.959 105.201 105.441 105.900 1.001\nNA sigma            2.771   0.202   2.408   2.630   2.759   2.900   3.198 1.001\nNA deviance       487.192   2.485 484.376 485.370 486.547 488.331 493.506 1.001\nNA                n.eff\nNA Group.means[1] 46000\nNA Group.means[2] 15000\nNA beta[1]            1\nNA beta[2]        35000\nNA beta0          46000\nNA sigma          46000\nNA deviance       46000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.1 and DIC = 490.3\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nMeans Parameterisation\n\ndata.r2jags.means &lt;- jags(data=data.list.means,\ninits=NULL, #or inits=list(inits.means,inits.means) # since there are two chains\nparameters.to.save=params.means,\nmodel.file=\"ttestModelMeans.txt\",\nn.chains=nChains,\nn.iter=nIter,\nn.burnin=burnInSteps,\nn.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 211\nNA \nNA Initializing model\n\n#print results\nprint(data.r2jags.means)\n\nNA Inference for Bugs model at \"ttestModelMeans.txt\", fit using jags,\nNA  2 chains, each with 25000 iterations (first 2000 discarded)\nNA  n.sims = 46000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]  105.184   0.357 104.481 104.947 105.184 105.423 105.884 1.001 46000\nNA beta[2]   77.867   0.439  77.001  77.575  77.866  78.160  78.736 1.001 39000\nNA effect   -27.317   0.566 -28.433 -27.696 -27.317 -26.940 -26.197 1.001 46000\nNA sigma      2.768   0.201   2.408   2.626   2.755   2.897   3.192 1.001 34000\nNA deviance 487.195   2.498 484.360 485.377 486.540 488.323 493.721 1.001 46000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.1 and DIC = 490.3\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nNotes\n\nIf inits=NULL the jags function will generate vaguely sensible initial values for each chain based on the data.\nIn addition to the mean and quantiles of each of the sample nodes, the jags function will calculate.\n\nThe effective sample size for each sample - if n.eff for a node is substantially less than the number of iterations, then it suggests poor mixing.\nThe Potential scale reduction factor or Rhat values for each sample - these are a convergence diagnostic (values of \\(1\\) indicate full convergence, values greater than \\(1.01\\) are indicative of non-convergence.\nAn information criteria (DIC) for model selection.\n\n\nThe total number samples collected is \\(46000\\). That is, there are \\(46000\\) samples collected from the multidimensional posterior distribution and thus, \\(46000\\) samples collected from the posterior distributions of each parameter. The effective number of samples column indicates the number of independent samples represented in the total. It is clear that for all parameters the chains were well mixed."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html",
    "title": "Simple Linear Regression (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#introduction",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#introduction",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nMany clinicians get a little twitchy and nervous around mathematical and statistical formulae and nomenclature. Whilst it is possible to perform basic statistics without too much regard for the actual equation (model) being employed, as the complexity of the analysis increases, the need to understand the underlying model becomes increasingly important. Moreover, model specification in BUGS/JAGS/STAN (the language used to program Bayesian modelling) aligns very closely to the underlying formulae. Hence a good understanding of the underlying model is vital to be able to create a sensible Bayesian model. Consequently, I will always present the linear model formulae along with the analysis.\nTo introduce the philosophical and mathematical differences between classical (frequentist) and Bayesian statistics, based on previous works, we present a provocative yet compelling trend analysis of two hypothetical populations (A vs B). The temporal trend of population A shows very little variability from a very subtle linear decline (\\(n=10\\), \\(\\text{slope}=-0.10\\), \\(\\text{p-value}=0.048\\)). By contrast, the B population appears to decline more dramatically, yet has substantially more variability (\\(n=10\\), \\(\\text{slope}=-10.23\\), \\(\\text{p-value}=0.058\\)). From a traditional frequentist perspective, we would conclude that there is a “significant” relationship in Population A (\\(p&lt;0.05\\)), yet not in Population B (\\(p&gt;0.05\\)). However, if we consider a third population C which is exactly the same as populstion B but with a higher number of observations, then we may end up with a completely different conclusion compared with that based on population B (\\(n=100\\), \\(\\text{slope}=-10.47\\), \\(\\text{p-value}&lt;0.001\\)).\nThe above illustrates a couple of things:\n\nstatistical significance does not necessarily translate into clinical importance. Indeed, population B is declining at nearly \\(10\\) times the rate of population A. That sounds rather important, yet on the basis of the hypothesis test, we would dismiss the decline in population B.\nthat a p-value is just the probability of detecting an effect or relationship - what is the probability that the sample size is large enough to pick up a difference.\n\nLet us now look at it from a Bayesian perspective, with a focus on population A and B. We would conclude that:\n\nthe mean (plus or minus CI) slopes for Population A and B are \\(-0.1 (-0.21,0)\\) and \\(-10.08 (-20.32,0.57)\\) respectively\nthe Bayesian approach allows us to query the posterior distribution is many other ways in order to ask sensible clinical questions. For example, we might consider that a rate of change of \\(5\\)% or greater represents an important biological impact. For population A and B, the probability that the rate is \\(5\\)% or greater is \\(0\\) and \\(0.85\\) respectively."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#linear-regression",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#linear-regression",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Linear regression",
    "text": "Linear regression\nSimple linear regression is a linear modelling process that models a continuous response against a single continuous predictor. The linear model is expressed as:\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0,\\sigma),\n\\]\nwhere \\(y_i\\) is the response variable for each of the \\(i=1\\ldots,n\\) observations, \\(\\beta_0\\) is the intercept (value when \\(x=0\\)), \\(\\beta_1\\) is the slope (rate of change in \\(y\\) per unit change in \\(x\\)), \\(x_i\\) is the predictor variable, \\(\\epsilon_i\\) is the residual value (difference between the observed value and the value expected by the model). The parameters of the trendline \\(\\boldsymbol \\beta=(\\beta_0,\\beta_1)\\) are determined by Ordinary Least Squares (OLS) in which the sum of the squared residuals is minimized. A non-zero population slope is indicative of a relationship."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#centering-the-data",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#centering-the-data",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Centering the data",
    "text": "Centering the data\nWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\n\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\nIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\nFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\nNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function.\n\ndata &lt;- within(data, {\n    cx &lt;- as.numeric(scale(x, scale = FALSE))\n})\nhead(data)\n\nNA          y x   cx\nNA 1 35.69762 1 -7.5\nNA 2 35.84911 2 -6.5\nNA 3 43.29354 3 -5.5\nNA 4 34.35254 4 -4.5\nNA 5 33.14644 5 -3.5\nNA 6 39.57532 6 -2.5"
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#normality",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#normality",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Normality",
    "text": "Normality\nEstimation and inference testing in linear regression assumes that the response is normally distributed in each of the populations. In this case, the populations are all possible measurements that could be collected at each level of \\(x\\) - hence there are \\(16\\) populations. Typically however, we only collect a single observation from each population (as is also the case here). How then can be evaluate whether each of these populations are likely to have been normal? For a given response, the population distributions should follow much the same distribution shapes. Therefore provided the single samples from each population are unbiased representations of those populations, a boxplot of all observations should reflect the population distributions."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#homogeneity-of-variance",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#homogeneity-of-variance",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Homogeneity of variance",
    "text": "Homogeneity of variance\nSimple linear regression also assumes that each of the populations are equally varied. Actually, it is prospect of a relationship between the mean and variance of \\(y\\)-values across x-values that is of the greatest concern. Strictly the assumption is that the distribution of \\(y\\) values at each \\(x\\) value are equally varied and that there is no relationship between mean and variance. However, as we only have a single \\(y\\)-value for each \\(x\\)-value, it is difficult to directly determine whether the assumption of homogeneity of variance is likely to have been violated (mean of one value is meaningless and variability can’t be assessed from a single value). If we then plot the residuals (difference between observed values and those predicted by the trendline) against the predict values and observe a definite presence of a pattern, then it is indicative of issues with the assumption of homogeneity of variance.\nHence looking at the spread of values around a trendline on a scatterplot of \\(y\\) against \\(x\\) is a useful way of identifying gross violations of homogeneity of variance. Residual plots provide an even better diagnostic. The presence of a wedge shape is indicative that the population mean and variance are related."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#linearity",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#linearity",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Linearity",
    "text": "Linearity\nLinear regression fits a straight (linear) line through the data. Therefore, prior to fitting such a model, it is necessary to establish whether this really is the most sensible way of describing the relationship. That is, does the relationship appear to be linearly related or could some other non-linear function describe the relationship better. Scatterplots and residual plots are useful diagnostics."
  },
  {
    "objectID": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#model-assumptions",
    "href": "tutorials/2020-02-02-simple-linear-regression-jags/index.html#model-assumptions",
    "title": "Simple Linear Regression (JAGS)",
    "section": "Model assumptions",
    "text": "Model assumptions\nThe typical assumptions which need to be checked when fitting a standard linear regression model are:\n\nAll of the observations are independent - this must be addressed at the design and collection stages\nThe response variable (and thus the residuals) should be normally distributed\nThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately)\nThe relationship between the linear predictor (right hand side of the regression formula) and the link function should be linear. A scatterplot with smoother can be useful for identifying possible non-linearity.\n\nSo lets explore normality, homogeneity of variances and linearity by constructing a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\)). We will also include a range of smoothers (linear and lowess) and marginal boxplots on the scatterplot to assist in exploring linearity and normality respectively.\n\n# scatterplot\nlibrary(car)\nscatterplot(y ~ x, data)\n\n\n\n\n\n\n\n\nConclusions:\nThere is no evidence that the response variable is non-normal. The spread of values around the trendline seems fairly even (hence it there is no evidence of non-homogeneity). The data seems well represented by the linear trendline. Furthermore, the lowess smoother does not appear to have a consistent shift trajectory. Obvious violations could be addressed either by:\n\nConsider a non-linear linear predictor (such as a polynomial, spline or other non-linear function)\nTransform the scale of the response variables (e.g. to address normality)"
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html",
    "title": "Multiple Linear Regression (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#introduction",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#introduction",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Introduction",
    "text": "Introduction\nMultiple regression is an extension of simple linear regression whereby a response variable is modelled against a linear combination of two or more simultaneously measured predictor variables. There are two main purposes of multiple linear regression:\n\nTo develop a better predictive model (equation) than is possible from models based on single independent variables.\nTo investigate the relative individual effects of each of the multiple independent variables above and beyond (standardised across) the effects of the other variables.\n\nAlthough the relationship between response variable and the additive effect of all the predictor variables is represented overall by a single multidimensional plane (surface), the individual effects of each of the predictor variables on the response variable (standardised across the other variables) can be depicted by single partial regression lines. The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable. In essence, it is the effect of one predictor variable at one specific level (the means) of all the other predictor variables (i.e. when each of the other predictors are set to their averages).\nMultiple regression models can be constructed additively (containing only the predictor variables themselves) or in a multiplicative design (which incorporate interactions between predictor variables in addition to the predictor variables themselves). Multiplicative models are used primarily for testing inferences about the effects of various predictor variables and their interactions on the response variable. Additive models by contrast are used for generating predictive models and estimating the relative importance of individual predictor variables more so than hypothesis testing."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#additive-model",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#additive-model",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Additive Model",
    "text": "Additive Model\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} +  \\beta_2x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\n\\]\nwhere \\(\\beta_0\\) is the population \\(y\\)-intercept (value of \\(y\\) when all partial slopes equal zero), \\(\\beta_1,\\beta_2,\\ldots,\\beta_{J}\\) are the partial population slopes of \\(Y\\) on \\(X_1,X_2,\\ldots,X_J\\) respectively holding the other \\(X\\) constant. \\(\\epsilon_i\\) is the random unexplained error or residual component. The additive model assumes that the effect of one predictor variable (partial slope) is independent of the levels of the other predictor variables."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#multiplicative-model",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#multiplicative-model",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Multiplicative Model",
    "text": "Multiplicative Model\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} +  \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\ldots + \\beta_Jx_{iJ} + \\epsilon_i,\n\\]\nwhere \\(\\beta_3x_{i1}x_{i2}\\) is the interactive effect of \\(X_1\\) and \\(X_2\\) on \\(Y\\) and it examines the degree to which the effect of one of the predictor variables depends on the levels of the other predictor variable(s)."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#data-generation",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#data-generation",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Data generation",
    "text": "Data generation\nLets say we had set up a natural experiment in which we measured a response (\\(y\\)) from each of \\(20\\) sampling units (\\(n=20\\)) across a landscape. At the same time, we also measured two other continuous covariates (\\(x_1\\) and \\(x_2\\)) from each of the sampling units. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nn = 100\nintercept = 5\ntemp = runif(n)\nnitro = runif(n) + 0.8 * temp\nint.eff = 2\ntemp.eff &lt;- 0.85\nnitro.eff &lt;- 0.5\nres = rnorm(n, 0, 1)\ncoef &lt;- c(int.eff, temp.eff, nitro.eff, int.eff)\nmm &lt;- model.matrix(~temp * nitro)\n\ny &lt;- t(coef %*% t(mm)) + res\ndata &lt;- data.frame(y, x1 = temp, x2 = nitro, cx1 = scale(temp,\n    scale = F), cx2 = scale(nitro, scale = F))\nhead(data)\n\nNA          y        x1        x2         cx1         cx2\nNA 1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\nNA 2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557\nNA 3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\nNA 4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568\nNA 5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415\nNA 6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335\n\n\nWith these sort of data, we are primarily interested in investigating whether there is a relationship between the continuous response variable and the components linear predictor (continuous predictors). We could model the relationship via either:\n\nAn additive model in which the effects of each predictor contribute in an additive way to the response - we do not allow for an interaction as we consider an interaction either not of great importance or likely to be absent.\nA multiplicative model in which the effects of each predictor and their interaction contribute to the response - we allow for the impact of one predictor to vary across the range of the other predictor."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#centering-the-data",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#centering-the-data",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Centering the data",
    "text": "Centering the data\nWhen a linear model contains a covariate (continuous predictor variable) in addition to another predictor (continuous or categorical), it is nearly always advisable that the continuous predictor variables are centered prior to the analysis. Centering is a process by which the mean of a variable is subtracted from each of the values such that the scale of the variable is shifted so as to be centered around \\(0\\). Hence the mean of the new centered variable will be \\(0\\), yet it will retain the same variance.\nThere are multiple reasons for this:\n\nIt provides some clinical meaning to the \\(y\\)-intercept. Recall that the \\(y\\)-intercept is the value of \\(Y\\) when \\(X\\) is equal to zero. If \\(X\\) is centered, then the \\(y\\)-intercept represents the value of \\(Y\\) at the mid-point of the \\(X\\) range. The \\(y\\)-intercept of an uncentered \\(X\\) typically represents a unreal value of \\(Y\\) (as an \\(X\\) of \\(0\\) is often beyond the reasonable range of values).\nIn multiplicative models (in which predictors and their interactions are included), main effects and interaction terms built from centered predictors will not be correlated to one another.\nFor more complex models, centering the covariates can increase the likelihood that the modelling engine converges (arrives at a numerically stable and reliable outcome).\n\nNote, centering will not effect the slope estimates. In R, centering is easily achieved with the scale function, which centers and scales (divides by standard deviation) the data. We only really need to center the data, so we provide the argument scale=FALSE. Also note that the scale function attaches the pre-centered mean (and standard deviation if scaling is performed) as attributes to the scaled data in order to facilitate back-scaling to the original scale. While these attributes are often convenient, they do cause issues for some of the Bayesian routines and so we will strip these attributes using the as.numeric function. Instead, we will create separate scalar variables to store the pre-scaled means.\n\ndata &lt;- within(data, {\n    cx1 &lt;- as.numeric(scale(x1, scale = FALSE))\n    cx2 &lt;- as.numeric(scale(x2, scale = FALSE))\n})\nhead(data)\n\nNA          y        x1        x2         cx1         cx2\nNA 1 2.426468 0.2875775 0.8300510 -0.21098147 -0.08302110\nNA 2 4.927690 0.7883051 0.9634676  0.28974614  0.05039557\nNA 3 3.176118 0.4089769 0.8157946 -0.08958207 -0.09727750\nNA 4 6.166652 0.8830174 1.6608878  0.38445841  0.74781568\nNA 5 4.788890 0.9404673 1.2352762  0.44190829  0.32220415\nNA 6 2.541536 0.0455565 0.9267954 -0.45300249  0.01372335\n\nmean.x1 = mean(data$x1)\nmean.x2 = mean(data$x2)"
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#additive-model-1",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#additive-model-1",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Additive model",
    "text": "Additive model\nWe now translate the likelihood for the additive model into Stan code.\n\nmodelString = \"\n    data { \n    int&lt;lower=1&gt; n;   // total number of observations \n    vector[n] Y;      // response variable \n    int&lt;lower=1&gt; nX;  // number of effects \n    matrix[n, nX] X;   // model matrix \n    } \n  transformed data { \n  matrix[n, nX - 1] Xc;  // centered version of X \n    vector[nX - 1] means_X;  // column means of X before centering \n    \n    for (i in 2:nX) { \n    means_X[i - 1] = mean(X[, i]); \n    Xc[, i - 1] = X[, i] - means_X[i - 1]; \n    }  \n    } \n    parameters { \n    vector[nX-1] beta;  // population-level effects \n    real cbeta0;  // center-scale intercept \n    real&lt;lower=0&gt; sigma;  // residual SD \n    } \n    transformed parameters { \n    } \n    model { \n    vector[n] mu; \n  mu = Xc * beta + cbeta0; \n    // prior specifications \n    beta ~ normal(0, 100); \n    cbeta0 ~ normal(0, 100); \n    sigma ~ cauchy(0, 5); \n    // likelihood contribution \n    Y ~ normal(mu, sigma); \n    } \n    generated quantities {\n    real beta0;  // population-level intercept \n    vector[n] log_lik;\n    beta0 = cbeta0 - dot_product(means_X, beta);\n    for (i in 1:n) {\n    log_lik[i] = normal_lpdf(Y[i] | Xc[i] * beta + cbeta0, sigma);\n    } \n    }\n  \n  \"\n## write the model to a stan file \nwriteLines(modelString, con = \"linregModeladd.stan\")\nwriteLines(modelString, con = \"linregModelmult.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX = model.matrix(~cx1 + cx2, data = data)\ndata.list &lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\nparams &lt;- c(\"beta\",\"beta0\", \"cbeta0\", \"sigma\", \"log_lik\")\nnChains = 2\nburnInSteps = 1000\nthinSteps = 1\nnumSavedSteps = 3000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 2500\n\n\nNow compile and run the Stan code via the rstan interface. Note that the first time stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\nlibrary(rstan)\n\nDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\ndata.rstan.add &lt;- stan(data = data.list, file = \"linregModeladd.stan\", chains = nChains, pars = params,\n    iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 6.3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.035 seconds (Warm-up)\nNA Chain 1:                0.036 seconds (Sampling)\nNA Chain 1:                0.071 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.029 seconds (Warm-up)\nNA Chain 2:                0.038 seconds (Sampling)\nNA Chain 2:                0.067 seconds (Total)\nNA Chain 2:\n\ndata.rstan.add\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2500; warmup=1000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]        2.83    0.01 0.45   1.95   2.52   2.82   3.15   3.70  2213    1\nNA beta[2]        1.58    0.01 0.39   0.82   1.31   1.58   1.85   2.34  2327    1\nNA beta0          3.80    0.00 0.10   3.60   3.73   3.80   3.87   4.00  2900    1\nNA cbeta0         3.80    0.00 0.10   3.60   3.73   3.80   3.87   4.00  2900    1\nNA sigma          1.00    0.00 0.07   0.87   0.95   0.99   1.04   1.14  2636    1\nNA log_lik[1]    -1.13    0.00 0.09  -1.33  -1.19  -1.13  -1.07  -0.96  2576    1\nNA log_lik[2]    -0.95    0.00 0.08  -1.11  -1.00  -0.95  -0.90  -0.80  2646    1\nNA log_lik[3]    -0.94    0.00 0.07  -1.09  -0.99  -0.94  -0.89  -0.80  2538    1\nNA log_lik[4]    -0.95    0.00 0.09  -1.14  -1.00  -0.94  -0.89  -0.79  2020    1\nNA log_lik[5]    -1.23    0.00 0.15  -1.57  -1.33  -1.22  -1.13  -0.98  3457    1\nNA log_lik[6]    -0.94    0.00 0.08  -1.10  -0.99  -0.93  -0.88  -0.79  2161    1\nNA log_lik[7]    -1.27    0.00 0.16  -1.61  -1.36  -1.25  -1.16  -1.00  2505    1\nNA log_lik[8]    -2.01    0.00 0.28  -2.61  -2.18  -1.98  -1.81  -1.52  4226    1\nNA log_lik[9]    -1.00    0.00 0.08  -1.16  -1.05  -1.00  -0.95  -0.85  2454    1\nNA log_lik[10]   -1.42    0.00 0.18  -1.81  -1.53  -1.41  -1.30  -1.12  2632    1\nNA log_lik[11]   -0.95    0.00 0.09  -1.13  -1.00  -0.94  -0.89  -0.79  1822    1\nNA log_lik[12]   -1.14    0.00 0.10  -1.35  -1.20  -1.13  -1.07  -0.96  2827    1\nNA log_lik[13]   -2.49    0.01 0.40  -3.35  -2.74  -2.45  -2.20  -1.81  2401    1\nNA log_lik[14]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2294    1\nNA log_lik[15]   -1.16    0.00 0.13  -1.45  -1.24  -1.14  -1.06  -0.94  2344    1\nNA log_lik[16]   -0.95    0.00 0.09  -1.14  -1.01  -0.95  -0.89  -0.80  2364    1\nNA log_lik[17]   -0.95    0.00 0.08  -1.10  -1.00  -0.95  -0.90  -0.81  2483    1\nNA log_lik[18]   -1.16    0.00 0.17  -1.56  -1.26  -1.14  -1.04  -0.89  2128    1\nNA log_lik[19]   -1.25    0.00 0.10  -1.46  -1.32  -1.25  -1.18  -1.07  2591    1\nNA log_lik[20]   -1.34    0.00 0.18  -1.73  -1.45  -1.32  -1.21  -1.04  3196    1\nNA log_lik[21]   -0.99    0.00 0.10  -1.20  -1.05  -0.99  -0.93  -0.83  2959    1\nNA log_lik[22]   -1.43    0.00 0.14  -1.74  -1.52  -1.42  -1.33  -1.18  2596    1\nNA log_lik[23]   -1.07    0.00 0.09  -1.26  -1.13  -1.07  -1.01  -0.90  2313    1\nNA log_lik[24]   -0.97    0.00 0.10  -1.19  -1.02  -0.96  -0.90  -0.80  2158    1\nNA log_lik[25]   -2.59    0.01 0.29  -3.23  -2.76  -2.57  -2.39  -2.08  2569    1\nNA log_lik[26]   -1.06    0.00 0.13  -1.34  -1.13  -1.04  -0.97  -0.85  2822    1\nNA log_lik[27]   -0.95    0.00 0.08  -1.11  -1.00  -0.95  -0.90  -0.80  2563    1\nNA log_lik[28]   -0.93    0.00 0.08  -1.08  -0.98  -0.93  -0.88  -0.79  2283    1\nNA log_lik[29]   -1.16    0.00 0.14  -1.48  -1.24  -1.14  -1.06  -0.92  2850    1\nNA log_lik[30]   -0.93    0.00 0.08  -1.07  -0.98  -0.93  -0.87  -0.78  2335    1\nNA log_lik[31]   -2.53    0.01 0.39  -3.35  -2.78  -2.50  -2.25  -1.85  4084    1\nNA log_lik[32]   -1.34    0.00 0.22  -1.83  -1.47  -1.32  -1.18  -1.00  4107    1\nNA log_lik[33]   -0.93    0.00 0.07  -1.07  -0.98  -0.92  -0.88  -0.79  2433    1\nNA log_lik[34]   -0.96    0.00 0.08  -1.13  -1.01  -0.95  -0.90  -0.81  2535    1\nNA log_lik[35]   -2.25    0.01 0.34  -2.97  -2.47  -2.24  -2.02  -1.65  3160    1\nNA log_lik[36]   -1.55    0.00 0.13  -1.83  -1.63  -1.54  -1.46  -1.31  2785    1\nNA log_lik[37]   -1.78    0.00 0.25  -2.33  -1.93  -1.76  -1.60  -1.35  3703    1\nNA log_lik[38]   -1.21    0.00 0.13  -1.50  -1.30  -1.20  -1.11  -0.98  2183    1\nNA log_lik[39]   -2.57    0.01 0.42  -3.44  -2.82  -2.54  -2.27  -1.85  2344    1\nNA log_lik[40]   -1.70    0.00 0.18  -2.08  -1.81  -1.70  -1.58  -1.38  3425    1\nNA log_lik[41]   -1.59    0.00 0.21  -2.07  -1.71  -1.57  -1.44  -1.22  3882    1\nNA log_lik[42]   -0.94    0.00 0.07  -1.09  -0.99  -0.94  -0.89  -0.80  2496    1\nNA log_lik[43]   -1.98    0.01 0.33  -2.72  -2.17  -1.94  -1.74  -1.40  2716    1\nNA log_lik[44]   -1.87    0.00 0.24  -2.40  -2.02  -1.85  -1.70  -1.44  2992    1\nNA log_lik[45]   -2.24    0.01 0.34  -2.97  -2.45  -2.22  -1.99  -1.65  2158    1\nNA log_lik[46]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2087    1\nNA log_lik[47]   -1.58    0.00 0.21  -2.05  -1.71  -1.56  -1.44  -1.22  3563    1\nNA log_lik[48]   -1.22    0.00 0.16  -1.58  -1.32  -1.21  -1.11  -0.96  2590    1\nNA log_lik[49]   -3.82    0.01 0.54  -5.01  -4.16  -3.80  -3.44  -2.86  2980    1\nNA log_lik[50]   -1.48    0.00 0.20  -1.91  -1.60  -1.46  -1.34  -1.14  4335    1\nNA log_lik[51]   -1.33    0.00 0.20  -1.79  -1.45  -1.31  -1.18  -1.01  2243    1\nNA log_lik[52]   -1.23    0.00 0.09  -1.41  -1.28  -1.22  -1.17  -1.07  2937    1\nNA log_lik[53]   -0.98    0.00 0.08  -1.16  -1.03  -0.98  -0.92  -0.82  2822    1\nNA log_lik[54]   -1.05    0.00 0.12  -1.32  -1.12  -1.04  -0.97  -0.86  2894    1\nNA log_lik[55]   -0.94    0.00 0.08  -1.11  -0.99  -0.94  -0.89  -0.79  2201    1\nNA log_lik[56]   -0.92    0.00 0.07  -1.07  -0.97  -0.92  -0.87  -0.78  2330    1\nNA log_lik[57]   -1.26    0.00 0.14  -1.56  -1.34  -1.25  -1.16  -1.03  2831    1\nNA log_lik[58]   -1.04    0.00 0.10  -1.25  -1.10  -1.03  -0.97  -0.85  2207    1\nNA log_lik[59]   -1.53    0.00 0.19  -1.95  -1.64  -1.51  -1.39  -1.19  3488    1\nNA log_lik[60]   -0.95    0.00 0.08  -1.12  -1.00  -0.95  -0.90  -0.80  2321    1\nNA log_lik[61]   -1.48    0.00 0.13  -1.74  -1.56  -1.47  -1.39  -1.26  3295    1\nNA log_lik[62]   -1.09    0.00 0.12  -1.37  -1.17  -1.08  -1.01  -0.89  3160    1\nNA log_lik[63]   -1.74    0.00 0.16  -2.08  -1.84  -1.73  -1.63  -1.45  2588    1\nNA log_lik[64]   -6.97    0.02 0.95  -8.99  -7.58  -6.93  -6.32  -5.28  2723    1\nNA log_lik[65]   -1.02    0.00 0.09  -1.21  -1.08  -1.01  -0.96  -0.85  2439    1\nNA log_lik[66]   -0.96    0.00 0.07  -1.11  -1.01  -0.96  -0.91  -0.82  2696    1\nNA log_lik[67]   -1.28    0.00 0.15  -1.63  -1.37  -1.27  -1.17  -1.02  4157    1\nNA log_lik[68]   -1.09    0.00 0.12  -1.35  -1.16  -1.08  -1.01  -0.88  2261    1\nNA log_lik[69]   -1.07    0.00 0.10  -1.27  -1.12  -1.06  -1.00  -0.89  3027    1\nNA log_lik[70]   -1.02    0.00 0.09  -1.19  -1.07  -1.01  -0.96  -0.86  2818    1\nNA log_lik[71]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  2405    1\nNA log_lik[72]   -0.93    0.00 0.07  -1.07  -0.98  -0.92  -0.88  -0.79  2349    1\nNA log_lik[73]   -0.93    0.00 0.08  -1.09  -0.99  -0.93  -0.88  -0.79  2303    1\nNA log_lik[74]   -3.83    0.01 0.62  -5.15  -4.22  -3.78  -3.38  -2.72  2652    1\nNA log_lik[75]   -1.22    0.00 0.10  -1.42  -1.28  -1.21  -1.15  -1.04  2664    1\nNA log_lik[76]   -1.42    0.00 0.14  -1.73  -1.51  -1.41  -1.32  -1.17  2567    1\nNA log_lik[77]   -0.93    0.00 0.07  -1.07  -0.98  -0.93  -0.88  -0.79  2554    1\nNA log_lik[78]   -0.96    0.00 0.07  -1.11  -1.01  -0.96  -0.91  -0.82  2798    1\nNA log_lik[79]   -0.99    0.00 0.09  -1.18  -1.05  -0.99  -0.93  -0.83  2213    1\nNA log_lik[80]   -0.95    0.00 0.08  -1.11  -1.00  -0.94  -0.89  -0.80  2449    1\nNA log_lik[81]   -1.55    0.00 0.20  -2.00  -1.68  -1.54  -1.41  -1.21  2138    1\nNA log_lik[82]   -1.67    0.00 0.18  -2.05  -1.78  -1.66  -1.55  -1.36  2544    1\nNA log_lik[83]   -0.99    0.00 0.08  -1.16  -1.05  -0.99  -0.94  -0.85  2504    1\nNA log_lik[84]   -1.37    0.00 0.16  -1.72  -1.47  -1.36  -1.25  -1.09  2460    1\nNA log_lik[85]   -0.93    0.00 0.08  -1.08  -0.98  -0.93  -0.87  -0.78  2308    1\nNA log_lik[86]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  2524    1\nNA log_lik[87]   -1.61    0.00 0.26  -2.20  -1.77  -1.59  -1.43  -1.18  2779    1\nNA log_lik[88]   -0.96    0.00 0.09  -1.15  -1.02  -0.96  -0.91  -0.81  2691    1\nNA log_lik[89]   -1.65    0.00 0.29  -2.27  -1.82  -1.62  -1.43  -1.17  4157    1\nNA log_lik[90]   -1.09    0.00 0.13  -1.38  -1.16  -1.08  -1.00  -0.88  2161    1\nNA log_lik[91]   -1.18    0.00 0.14  -1.51  -1.26  -1.17  -1.08  -0.95  3654    1\nNA log_lik[92]   -0.99    0.00 0.08  -1.16  -1.04  -0.99  -0.94  -0.84  2339    1\nNA log_lik[93]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2233    1\nNA log_lik[94]   -1.31    0.00 0.11  -1.54  -1.38  -1.31  -1.23  -1.11  3600    1\nNA log_lik[95]   -1.96    0.01 0.30  -2.61  -2.15  -1.94  -1.74  -1.47  2131    1\nNA log_lik[96]   -3.51    0.01 0.46  -4.49  -3.80  -3.48  -3.19  -2.69  2972    1\nNA log_lik[97]   -1.11    0.00 0.10  -1.33  -1.17  -1.10  -1.04  -0.92  3034    1\nNA log_lik[98]   -1.48    0.00 0.19  -1.90  -1.60  -1.46  -1.35  -1.15  2624    1\nNA log_lik[99]   -1.09    0.00 0.11  -1.34  -1.16  -1.08  -1.01  -0.88  2378    1\nNA log_lik[100]  -1.66    0.00 0.13  -1.93  -1.74  -1.65  -1.57  -1.42  2966    1\nNA lp__         -48.87    0.04 1.42 -52.25 -49.62 -48.58 -47.80 -47.03  1096    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:25:36 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-03-multiple-linear-regression/index.html#multiplicative-model-1",
    "href": "tutorials/2020-02-03-multiple-linear-regression/index.html#multiplicative-model-1",
    "title": "Multiple Linear Regression (Stan)",
    "section": "Multiplicative model",
    "text": "Multiplicative model\nWe now translate the likelihood for the multiplicative model into Stan code. Arrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX = model.matrix(~cx1 * cx2, data = data)\ndata.list &lt;- with(data, list(Y = y, X = X, nX = ncol(X), n = nrow(data)))\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\nparams &lt;- c(\"beta\",\"beta0\", \"cbeta0\", \"sigma\", \"log_lik\")\nnChains = 2\nburnInSteps = 1000\nthinSteps = 1\nnumSavedSteps = 3000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 2500\n\n\nNow compile and run the Stan code via the rstan interface.\n\ndata.rstan.mult &lt;- stan(data = data.list, file = \"linregModelmult.stan\", chains = nChains, pars = params,\n    iter = nIter, warmup = burnInSteps, thin = thinSteps, save_dso = TRUE)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 8e-06 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  750 / 2500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1000 / 2500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 1001 / 2500 [ 40%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 1: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 1: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 1: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.035 seconds (Warm-up)\nNA Chain 1:                0.044 seconds (Sampling)\nNA Chain 1:                0.079 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 5e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 2500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  250 / 2500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  500 / 2500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  750 / 2500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1000 / 2500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 1001 / 2500 [ 40%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 2500 [ 50%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 2500 [ 60%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 2500 [ 70%]  (Sampling)\nNA Chain 2: Iteration: 2000 / 2500 [ 80%]  (Sampling)\nNA Chain 2: Iteration: 2250 / 2500 [ 90%]  (Sampling)\nNA Chain 2: Iteration: 2500 / 2500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nNA Chain 2:                0.044 seconds (Sampling)\nNA Chain 2:                0.08 seconds (Total)\nNA Chain 2:\n\ndata.rstan.mult\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2500; warmup=1000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA                mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]        2.80    0.01 0.45   1.94   2.50   2.81   3.10   3.65  2879    1\nNA beta[2]        1.51    0.01 0.39   0.73   1.26   1.51   1.76   2.26  2794    1\nNA beta[3]        1.46    0.02 1.21  -0.92   0.68   1.48   2.25   3.90  3518    1\nNA beta0          3.71    0.00 0.12   3.48   3.63   3.72   3.80   3.94  3606    1\nNA cbeta0         3.80    0.00 0.10   3.61   3.74   3.80   3.86   3.98  3535    1\nNA sigma          0.99    0.00 0.07   0.86   0.94   0.99   1.04   1.14  3084    1\nNA log_lik[1]    -1.10    0.00 0.09  -1.30  -1.16  -1.10  -1.04  -0.94  3313    1\nNA log_lik[2]    -0.97    0.00 0.09  -1.15  -1.03  -0.96  -0.91  -0.81  3250    1\nNA log_lik[3]    -0.93    0.00 0.07  -1.07  -0.98  -0.93  -0.88  -0.79  2816    1\nNA log_lik[4]    -0.98    0.00 0.12  -1.27  -1.04  -0.96  -0.90  -0.80  2492    1\nNA log_lik[5]    -1.31    0.00 0.17  -1.70  -1.41  -1.29  -1.19  -1.02  3654    1\nNA log_lik[6]    -0.94    0.00 0.09  -1.12  -0.99  -0.94  -0.88  -0.79  2160    1\nNA log_lik[7]    -1.19    0.00 0.15  -1.53  -1.28  -1.18  -1.08  -0.93  2891    1\nNA log_lik[8]    -2.18    0.01 0.33  -2.90  -2.38  -2.15  -1.95  -1.61  3666    1\nNA log_lik[9]    -0.97    0.00 0.08  -1.13  -1.02  -0.96  -0.91  -0.82  2997    1\nNA log_lik[10]   -1.45    0.00 0.18  -1.84  -1.56  -1.44  -1.32  -1.14  3035    1\nNA log_lik[11]   -1.06    0.00 0.20  -1.60  -1.13  -1.01  -0.93  -0.81  2858    1\nNA log_lik[12]   -1.17    0.00 0.10  -1.39  -1.24  -1.16  -1.09  -0.98  3299    1\nNA log_lik[13]   -2.25    0.01 0.40  -3.11  -2.49  -2.21  -1.95  -1.57  3513    1\nNA log_lik[14]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2539    1\nNA log_lik[15]   -1.16    0.00 0.13  -1.45  -1.24  -1.14  -1.06  -0.93  3152    1\nNA log_lik[16]   -0.99    0.00 0.11  -1.23  -1.05  -0.98  -0.91  -0.81  2805    1\nNA log_lik[17]   -0.95    0.00 0.08  -1.10  -1.00  -0.94  -0.89  -0.80  2956    1\nNA log_lik[18]   -1.09    0.00 0.16  -1.48  -1.17  -1.06  -0.97  -0.85  2699    1\nNA log_lik[19]   -1.20    0.00 0.10  -1.42  -1.27  -1.20  -1.14  -1.02  3385    1\nNA log_lik[20]   -1.39    0.00 0.19  -1.81  -1.50  -1.37  -1.25  -1.08  3441    1\nNA log_lik[21]   -0.96    0.00 0.09  -1.16  -1.01  -0.95  -0.90  -0.81  3148    1\nNA log_lik[22]   -1.34    0.00 0.15  -1.66  -1.43  -1.33  -1.23  -1.08  3315    1\nNA log_lik[23]   -1.02    0.00 0.09  -1.21  -1.08  -1.01  -0.96  -0.85  3014    1\nNA log_lik[24]   -0.96    0.00 0.09  -1.16  -1.01  -0.95  -0.89  -0.80  2535    1\nNA log_lik[25]   -2.78    0.01 0.34  -3.48  -3.00  -2.76  -2.54  -2.19  3076    1\nNA log_lik[26]   -1.08    0.00 0.14  -1.41  -1.16  -1.07  -0.98  -0.86  3183    1\nNA log_lik[27]   -0.97    0.00 0.09  -1.16  -1.03  -0.96  -0.91  -0.81  2998    1\nNA log_lik[28]   -0.94    0.00 0.08  -1.11  -0.99  -0.93  -0.88  -0.79  2649    1\nNA log_lik[29]   -1.26    0.00 0.18  -1.68  -1.37  -1.24  -1.13  -0.97  3129    1\nNA log_lik[30]   -0.92    0.00 0.08  -1.08  -0.98  -0.92  -0.87  -0.78  2607    1\nNA log_lik[31]   -2.22    0.01 0.42  -3.14  -2.48  -2.17  -1.93  -1.50  3887    1\nNA log_lik[32]   -1.16    0.00 0.22  -1.71  -1.27  -1.11  -1.00  -0.86  3560    1\nNA log_lik[33]   -0.93    0.00 0.07  -1.08  -0.97  -0.92  -0.88  -0.79  3203    1\nNA log_lik[34]   -0.98    0.00 0.09  -1.16  -1.03  -0.97  -0.91  -0.82  3343    1\nNA log_lik[35]   -2.66    0.01 0.53  -3.81  -2.98  -2.62  -2.27  -1.75  3400    1\nNA log_lik[36]   -1.67    0.00 0.18  -2.05  -1.78  -1.66  -1.54  -1.36  3419    1\nNA log_lik[37]   -1.87    0.00 0.27  -2.45  -2.03  -1.85  -1.68  -1.39  3501    1\nNA log_lik[38]   -1.29    0.00 0.17  -1.66  -1.40  -1.28  -1.18  -1.01  3039    1\nNA log_lik[39]   -2.96    0.01 0.58  -4.25  -3.33  -2.90  -2.56  -1.96  2947    1\nNA log_lik[40]   -1.78    0.00 0.21  -2.22  -1.91  -1.77  -1.63  -1.42  3384    1\nNA log_lik[41]   -1.38    0.00 0.25  -1.94  -1.52  -1.34  -1.20  -0.99  3015    1\nNA log_lik[42]   -0.93    0.00 0.07  -1.08  -0.98  -0.93  -0.88  -0.79  2800    1\nNA log_lik[43]   -2.03    0.01 0.34  -2.78  -2.23  -1.98  -1.78  -1.45  3189    1\nNA log_lik[44]   -1.92    0.00 0.26  -2.47  -2.09  -1.90  -1.73  -1.48  3270    1\nNA log_lik[45]   -2.08    0.01 0.34  -2.86  -2.30  -2.05  -1.83  -1.50  2994    1\nNA log_lik[46]   -1.00    0.00 0.13  -1.32  -1.06  -0.98  -0.91  -0.81  2431    1\nNA log_lik[47]   -1.78    0.01 0.29  -2.44  -1.96  -1.75  -1.56  -1.28  3256    1\nNA log_lik[48]   -1.24    0.00 0.16  -1.59  -1.34  -1.23  -1.13  -0.97  2957    1\nNA log_lik[49]   -3.57    0.01 0.53  -4.66  -3.92  -3.53  -3.22  -2.62  3154    1\nNA log_lik[50]   -1.63    0.00 0.26  -2.21  -1.78  -1.61  -1.45  -1.20  3684    1\nNA log_lik[51]   -1.38    0.00 0.21  -1.85  -1.51  -1.36  -1.22  -1.02  3044    1\nNA log_lik[52]   -1.29    0.00 0.10  -1.51  -1.36  -1.29  -1.22  -1.10  3613    1\nNA log_lik[53]   -1.00    0.00 0.09  -1.18  -1.05  -0.99  -0.94  -0.83  3443    1\nNA log_lik[54]   -1.26    0.00 0.25  -1.90  -1.39  -1.21  -1.08  -0.90  3083    1\nNA log_lik[55]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2691    1\nNA log_lik[56]   -0.93    0.00 0.08  -1.09  -0.98  -0.93  -0.88  -0.79  2746    1\nNA log_lik[57]   -1.20    0.00 0.14  -1.51  -1.28  -1.19  -1.10  -0.96  3234    1\nNA log_lik[58]   -0.99    0.00 0.10  -1.20  -1.05  -0.98  -0.92  -0.82  2749    1\nNA log_lik[59]   -1.50    0.00 0.18  -1.89  -1.61  -1.48  -1.36  -1.18  3103    1\nNA log_lik[60]   -0.95    0.00 0.08  -1.12  -1.01  -0.95  -0.90  -0.81  2960    1\nNA log_lik[61]   -1.56    0.00 0.15  -1.86  -1.66  -1.55  -1.46  -1.31  3332    1\nNA log_lik[62]   -1.29    0.00 0.24  -1.89  -1.42  -1.25  -1.12  -0.94  3216    1\nNA log_lik[63]   -1.63    0.00 0.17  -2.01  -1.74  -1.62  -1.51  -1.32  3373    1\nNA log_lik[64]   -6.84    0.02 0.90  -8.72  -7.42  -6.79  -6.22  -5.21  3165    1\nNA log_lik[65]   -0.99    0.00 0.09  -1.17  -1.04  -0.99  -0.93  -0.83  2812    1\nNA log_lik[66]   -0.99    0.00 0.08  -1.15  -1.04  -0.99  -0.94  -0.84  3227    1\nNA log_lik[67]   -1.22    0.00 0.15  -1.56  -1.30  -1.20  -1.11  -0.97  3726    1\nNA log_lik[68]   -1.03    0.00 0.11  -1.29  -1.10  -1.02  -0.95  -0.85  2783    1\nNA log_lik[69]   -1.09    0.00 0.10  -1.31  -1.15  -1.08  -1.02  -0.91  3459    1\nNA log_lik[70]   -1.03    0.00 0.09  -1.21  -1.09  -1.02  -0.96  -0.87  3133    1\nNA log_lik[71]   -0.93    0.00 0.07  -1.08  -0.98  -0.92  -0.88  -0.79  2918    1\nNA log_lik[72]   -0.93    0.00 0.08  -1.09  -0.99  -0.93  -0.88  -0.78  2690    1\nNA log_lik[73]   -0.93    0.00 0.08  -1.09  -0.98  -0.92  -0.88  -0.79  2940    1\nNA log_lik[74]   -3.69    0.01 0.61  -4.96  -4.08  -3.65  -3.26  -2.63  3508    1\nNA log_lik[75]   -1.15    0.00 0.11  -1.37  -1.21  -1.14  -1.07  -0.95  3286    1\nNA log_lik[76]   -1.40    0.00 0.14  -1.72  -1.49  -1.39  -1.30  -1.15  3469    1\nNA log_lik[77]   -0.93    0.00 0.07  -1.08  -0.98  -0.92  -0.88  -0.78  3016    1\nNA log_lik[78]   -0.99    0.00 0.08  -1.15  -1.04  -0.99  -0.93  -0.84  3250    1\nNA log_lik[79]   -1.07    0.00 0.13  -1.36  -1.14  -1.05  -0.97  -0.85  2769    1\nNA log_lik[80]   -0.97    0.00 0.09  -1.15  -1.02  -0.96  -0.91  -0.81  2829    1\nNA log_lik[81]   -1.42    0.00 0.21  -1.91  -1.54  -1.39  -1.27  -1.07  2922    1\nNA log_lik[82]   -1.81    0.00 0.22  -2.28  -1.95  -1.79  -1.66  -1.43  3183    1\nNA log_lik[83]   -0.96    0.00 0.08  -1.12  -1.01  -0.95  -0.90  -0.81  2905    1\nNA log_lik[84]   -1.28    0.00 0.16  -1.63  -1.38  -1.26  -1.17  -1.02  3112    1\nNA log_lik[85]   -0.93    0.00 0.08  -1.08  -0.98  -0.92  -0.87  -0.78  2618    1\nNA log_lik[86]   -0.92    0.00 0.07  -1.07  -0.97  -0.92  -0.87  -0.78  2792    1\nNA log_lik[87]   -1.62    0.00 0.25  -2.18  -1.78  -1.60  -1.45  -1.21  2626    1\nNA log_lik[88]   -0.94    0.00 0.08  -1.12  -0.99  -0.94  -0.89  -0.80  3170    1\nNA log_lik[89]   -1.40    0.00 0.30  -2.12  -1.57  -1.35  -1.18  -0.95  3763    1\nNA log_lik[90]   -1.02    0.00 0.12  -1.32  -1.09  -1.00  -0.94  -0.83  2653    1\nNA log_lik[91]   -1.05    0.00 0.16  -1.44  -1.13  -1.02  -0.94  -0.83  2606    1\nNA log_lik[92]   -0.96    0.00 0.08  -1.13  -1.01  -0.96  -0.90  -0.81  2807    1\nNA log_lik[93]   -0.96    0.00 0.09  -1.17  -1.01  -0.95  -0.89  -0.80  2155    1\nNA log_lik[94]   -1.27    0.00 0.11  -1.50  -1.34  -1.26  -1.19  -1.07  3767    1\nNA log_lik[95]   -1.73    0.01 0.32  -2.45  -1.92  -1.70  -1.51  -1.22  2900    1\nNA log_lik[96]   -3.34    0.01 0.45  -4.27  -3.62  -3.31  -3.03  -2.54  3404    1\nNA log_lik[97]   -1.14    0.00 0.11  -1.37  -1.21  -1.14  -1.07  -0.95  3347    1\nNA log_lik[98]   -1.53    0.00 0.21  -1.98  -1.66  -1.52  -1.39  -1.18  3455    1\nNA log_lik[99]   -1.06    0.00 0.11  -1.30  -1.13  -1.05  -0.99  -0.88  2956    1\nNA log_lik[100]  -1.55    0.00 0.14  -1.86  -1.65  -1.54  -1.45  -1.30  3608    1\nNA lp__         -48.59    0.04 1.57 -52.46 -49.39 -48.30 -47.44 -46.44  1270    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:25:37 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html",
    "title": "Single Factor Anova (JAGS))",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#introduction",
    "title": "Single Factor Anova (JAGS))",
    "section": "Introduction",
    "text": "Introduction\nSingle factor Analysis of Variance (ANOVA), also known as single factor classification, is used to investigate the effect of a single factor comprising two or more groups (treatment levels) from a completely randomised design. Completely randomised refers to the absence of restrictions on the random allocation of experimental or sampling units to factor levels.\nFor example, consider a situation in which three types of treatments (A, B and C) are applied to replicate sampling units across the sampling domain. Importantly, the treatments are applied at the scale of the sampling units and the treatments applied to each sampling unit do not extend to any other neighbouring sampling units. Another possible situation is where the scale of a treatment is far larger than that of a sampling unit. This design features two treatments, each replicated three times. Note that additional sampling units within each Site (the scale at which the treatment occurs) would NOT constitute additional replication. Rather, these would be sub-replicates. That is, they would be replicates of the Sites, not the treatments (since the treatments occur at the level of whole sites). In order to genuinely increase the number of replicates, it is necessary to have more Sites. The random allocation of sampling units within the sampling domain (such as population) is appropriate provided either the underlying response is reasonably homogenous throughout the domain, or else, there is a large number of sampling units. If the conditions are relatively hetrogenous, then the exact location of the sampling units is likely to be highly influential and may mask any detectable effects of treatments."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#fixed-and-random-effects",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#fixed-and-random-effects",
    "title": "Single Factor Anova (JAGS))",
    "section": "Fixed and random effects",
    "text": "Fixed and random effects\nFrom a frequentist perspective, fixed factors are factors whose levels represent the specific populations of interest. For example, a factor that comprises “high”, “medium” and “low” temperature treatments is a fixed factor - we are only interested in comparing those three populations. Conclusions about the effects of a fixed factor are restricted to the specific treatment levels investigated and for any subsequent experiments to be comparable, the same specific treatments of the factor would need to be used. By contrast, random factors are factors whose levels are randomly chosen from all the possible levels of populations and are used as random representatives of the populations. For example, five random temperature treatments could be used to represent a full spectrum of temperature treatments. In this case, conclusions are extrapolated to all the possible treatment (temperature) levels and for subsequent experiments, a new random set of treatments of the factor would be selected.\nOther common examples of random factors include sites and subjects - factors for which we are attempting to generalise over. Furthermore, the nature of random factors means that we have no indication of how a new level of that factor (such as another subject or site) are likely to respond and thus it is not possible to predict new observations from random factors. These differences between fixed and random factors are reflected in the way their respective null hypotheses are formulated and interpreted. Whilst fixed factors contrast the effects of the different levels of the factor, random factors are modelled as the amount of additional variability they introduce. Random factors are modelled with a mean of \\(0\\) and their variance is estimated as the effect coefficient."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#linear-model",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#linear-model",
    "title": "Single Factor Anova (JAGS))",
    "section": "Linear model",
    "text": "Linear model\nThe linear model for single factor classification is similar to that of multiple linear regression. The linear model can thus be represented by either:\n\nMeans parameterisation - in which the regression slopes represent the means of each treatment group and the intercept is removed (to prevent over-parameterisation).\n\n\\[\ny_{ij} = \\beta_1(\\text{level}_1)_{ij} + \\beta_2(\\text{level}_2)_{ij} + \\ldots + \\epsilon_{ij},\n\\]\nwhere \\(\\beta_1\\) and \\(\\beta_2\\) respectively represent the means response of treatment level \\(1\\) and \\(2\\). This is often simplified to \\(y_{ij}=\\alpha_i + \\epsilon_{ij}\\).\n\nEffects parameterisation - the intercept represents a property such as the mean of one of the treatment groups (treatment contrasts) or the overall mean (sum contrasts), and the slope parameters represent effects (differences between each other group and the reference mean for example).\n\n\\[\ny_{ij} = \\mu + \\beta_2(\\text{level}_2)_{ij} + \\beta_3(\\text{level}_3)_{ij} + \\ldots + \\epsilon_{ij},\n\\]\nwhere \\(\\mu\\) is the mean of the first treatment group, \\(\\beta_2\\) and \\(\\beta_3\\) respectively represent the effects (change from level \\(1\\)) of level \\(2\\) and \\(3\\) on the mean response. This is often simplified to: \\(y_{ij}=\\mu + \\alpha_i + \\epsilon_{ij}\\), with \\(\\alpha_1=0\\).\nSince we are traditionally interested in investigating effects (differences) rather than treatment means, effects parameterisation is far more common (particularly when coupled with hypothesis testing). In a Bayesian framework, it does not really matter whether models are fit with means or effects parameterisation since the posterior likelihood can be querried in any way and repeatedly - thus enabling us to explore any specific effects after the model has been fit. Nevertheless, to ease comparisons with frequentist approaches, we will stick with effects paramterisation."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#null-hypothesis-fixed-factor",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#null-hypothesis-fixed-factor",
    "title": "Single Factor Anova (JAGS))",
    "section": "Null hypothesis: fixed factor",
    "text": "Null hypothesis: fixed factor\nWe can associate a null hypothesis test with each estimated parameter. For example, in a cell for each estimated mean in a means model we could test a null hypothesis that the population mean is equal to zero (e.g. \\(H_0\\): \\(\\alpha_1=0\\), \\(H_0\\): \\(\\alpha_2=0\\), \\(\\ldots\\)). However, this rarely would be of much interest. By contrast, individual null hypotheses associated with each parameter of the effects model can be used to investigate the differences between each group and a reference group (for example). In addition to the individual null hypothesis tests, a single fixed factor ANOVA tests the collective \\(H_0\\) that there are no differences between the population group means:\n\n\\(H_0: \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\\(H_0 : \\alpha_2=\\alpha_3=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#null-hypothesis-random-factor",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#null-hypothesis-random-factor",
    "title": "Single Factor Anova (JAGS))",
    "section": "Null hypothesis: random factor",
    "text": "Null hypothesis: random factor\nThe collective \\(H_0\\) for a random factor is that the variance between all possible treatment groups equals zero:\n\n\\(H_0 : \\sigma^2_{\\alpha}=0\\) (added variance due to this factor equals zero).\n\nNote that whilst the null hypotheses for fixed and random factors are different (fixed: population group means all equal, random: variances between populations all equal zero), the linear model fitted for fixed and random factors in single factor ANOVA models is identical. For more complex multi-factor ANOVA models however, the distinction between fixed and random factors has important consequences for building and interpreting statistical models and null hypotheses."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#analysis-of-variance",
    "title": "Single Factor Anova (JAGS))",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nWhen the null hypothesis is true (and the populations are identical), the amount of variation among observations within groups should be similar to the amount of variation in observations between groups. However, when the null hypothesis is false (and some means are different from other means), the amount of variation among observations might be expected to be less than the amount of variation within groups. Analysis of variance, or ANOVA, partitions the total variance in the response (dependent) variable into a component of the variance that is explained by combinations of one or more categorical predictor variables (called factors) and a component of the variance that cannot be explained (residual). The variance ratio (F-ratio) from this partitioning can then be used to test the null hypothesis (\\(H_0\\)) that the population group or treatment means are all equal. Ttotal variation can be decomposed into components explained by the groups (\\(MS_{groups}\\)) and and unexplained (\\(MS_{residual}\\)) by the groups. The gray arrows in b) depict the relative amounts explained by the groups. The proposed groupings generally explain why the first few points are higher on the y-axis than the last three points. The probability of collecting our sample, and thus generating the sample ratio of explained to unexplained variation (or one more extreme), when the null hypothesis is true (and population means are equal) is the area under the F-distribution beyond our sample ratio (\\(\\text{F-ratio}=\\frac{MS_{groups}}{MS_{residual}}\\)).\nWhen the null hypothesis is true (and the test assumptions have not been violated), the ratio (F-ratio) of explained to unexplained variance follows a theoretical probability distribution (F-distribution). When the null hypothesis is true, and there is no effect of the treatment on the response variable, the ratio of explained variability to unexplained variability is expected to be \\(\\leq 1\\). Since the denominator should represent the expected numerator in the absence of an effect. Importantly, the denominator in an F-ratio calculation essentially represents what we would expect the numerator to be in the absence of a treatment effect. For simple analyses, identifying what these expected values are is relatively straightforward (equivalent to the degree of within group variability). However, in more complex designs (particularly involving random factors and hierarchical treatment levels), the logical “groups” can be more difficult (and in some cases impossible) to identify. In such cases, nominating the appropriate F-ratio denominator for estimating an specific effect requires careful consideration. The following table depicts the anatomy of the single factor ANOVA table\n\nanova_table\n\nNA          df       MS       F-ratio          \nNA Factor A \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"\nNA Residual \"(n-1)a\" \"MS res\" \"\"\n\n\nand corresponding R syntax.\n\nanova(lm(DV ~ A, dataset))\n# OR\nanova(aov(DV ~ A, dataset))\n\nAn F-ratio substantially greater than \\(1\\) suggests that the model relating the response variable to the categorical variable explains substantially more variability than is left unexplained. In turn, this implies that the linear model does represent the data well and that differences between observations can be explained largely by differences in treatment levels rather than purely the result of random variation. If the probability of getting the observed (sample) F-ratio or one more extreme is less than some predefined critical value (typically \\(5\\)% or \\(0.05\\)), we conclude that it is highly unlikely that the observed samples could have been collected from populations in which the treatment has no effect and therefore we would reject the null hypothesis."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#assumptions",
    "title": "Single Factor Anova (JAGS))",
    "section": "Assumptions",
    "text": "Assumptions\nAn F-ratio from real data can only reliably relate to a theoretical F-distribution when the data conform to certain assumptions. Hypothesis testing for a single factor ANOVA model assumes that the residuals (and therefore the response variable for each of the treatment levels) are all:\n\nnormally distributed - although ANOVA is robust to non-normality provided sample sizes and variances are equal. Boxplots should be used to explore normality, skewness, bimodality and outliers. In the event of homogeneity of variance issues (see below), a Q-Q normal plot can also be useful for exploring normality (as this might be the cause of non-homogeneity). Scale transformations are often useful.\nequally varied - provided sample sizes are equal and the largest to smallest variance ratio does not exceed 3:1 (9:1 for sd), ANOVA is reasonably robust to this assumption, however, relationships between variance and mean and/or sample size are of particular concern as they elevate the Type I error rate. Boxplots and plots of means against variance should be used to explore the spread of values. Residual plots should reveal no patterns. Since unequal variances are often the result of non-normality, transformations that improve normality will also improve variance homogeneity.\nindependent of one another - this assumption must be addressed at the design and collection stages and cannot be compensated for later (unless a model is used that specifically accounts for particular types of non-independent data, such as that introduced with hierarchical designs or autocorrelation)\n\nViolations of these assumptions reduce the reliability of the analysis."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#exploratory-data-analysis",
    "title": "Single Factor Anova (JAGS))",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nNormality and Homogeneity of variance\n\n\nboxplot(y ~ x, data)\n\n# OR via ggplot2\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = y, x = x)) + geom_boxplot() +\n    theme_classic()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity. Obvious violations could be addressed either by, for example, transforming the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-04-single-factor-anova-jags/index.html#model-matrix-formulation",
    "href": "tutorials/2020-02-04-single-factor-anova-jags/index.html#model-matrix-formulation",
    "title": "Single Factor Anova (JAGS))",
    "section": "Model matrix formulation",
    "text": "Model matrix formulation\nFor very simple models such as this example, we can write the models as:\n\nmodelString2 = \"\n  model {\n  #Likelihood\n  for (i in 1:n) {\n  y[i]~dnorm(mean[i],tau)\n  mean[i] &lt;- inprod(beta[],X[i,])\n  }\n  #Priors\n  for (i in 1:ngroups) {\n  beta[i] ~ dnorm(0, 1.0E-6) \n  }\n  sigma ~ dunif(0, 100)\n  tau &lt;- 1 / (sigma * sigma)\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"anovaModel2.txt\")\n\nDefine the data to pass to R2jags.\n\nX &lt;- model.matrix(~x, data)\ndata.list &lt;- with(data, list(y = y, X = X, n = nrow(data), ngroups = ncol(X)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\ndata.r2jags &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params,\n    model.file = \"anovaModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 50\nNA    Unobserved stochastic nodes: 6\nNA    Total graph size: 370\nNA \nNA Initializing model\n\nprint(data.r2jags)\n\nNA Inference for Bugs model at \"anovaModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   40.226   0.901  38.475  39.624  40.222  40.824  41.999 1.001  4800\nNA beta[2]    5.401   1.272   2.906   4.552   5.397   6.242   7.900 1.001 15000\nNA beta[3]   13.492   1.296  10.969  12.634  13.484  14.355  16.038 1.001  5100\nNA beta[4]    0.734   1.279  -1.793  -0.114   0.740   1.582   3.263 1.001 15000\nNA beta[5]  -10.248   1.283 -12.785 -11.108 -10.242  -9.380  -7.731 1.001  9800\nNA sigma      2.863   0.315   2.321   2.642   2.838   3.053   3.558 1.001  6200\nNA deviance 245.551   3.785 240.353 242.765 244.844 247.603 254.815 1.002  1800\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 7.2 and DIC = 252.7\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html",
    "href": "tutorials/2020-02-05-ancova-jags/index.html",
    "title": "Ancova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#introduction",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#introduction",
    "title": "Ancova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations)."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#null-hypothesis",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#null-hypothesis",
    "title": "Ancova (JAGS)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nFactor A: the main treatment effect\n\n\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\n\nThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\n\\(H_0(B):\\beta_1(pooled)=0\\)\n\nThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#linear-models",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#linear-models",
    "title": "Ancova (JAGS)",
    "section": "Linear models",
    "text": "Linear models\nOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\nSingle categorical and single covariate\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\nSingle categorical and two covariates\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\nFactorial designs\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nNested designs\n\nLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nPartly nested designs\n\nLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#analysis-of-variance",
    "title": "Ancova (JAGS)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\nancova_table\n\nNA           df       MS       F-ratio (A&B fixed) F-ratio (B fixed) \nNA Factor A  \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"   \"(MS A)/(MS res)\" \nNA Factor B  \"1\"      \"MS B\"   \"(MS B)/(MS res)\"   \"(MS B)/(MS res)\" \nNA Factor AB \"a-1\"    \"MS AB\"  \"(MS AB)/(MS res)\"  \"(MS AB)/(MS res)\"\nNA Residual  \"(n-2)a\" \"MS res\" \"\"                  \"\"\n\n\nThe corresponding R syntax is given below.\n\nanova(lm(DV ~ B * A, dataset))\n# OR\nanova(aov(DV ~ B * A, dataset))\n# OR (make sure not using treatment contrasts)\nAnova(lm(DV ~ B * A, dataset), type = \"III\")"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#assumptions",
    "title": "Ancova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\nThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\nThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nThe appropriate residuals are independent of one another.\nThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\nFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\nFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\nHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\n\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\nAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\nUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\nSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis",
    "title": "Ancova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nlibrary(car)\nscatterplot(Y ~ B | A, data = data)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data)\n\n# OR via ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\nanova(lm(Y ~ B * A, data = data))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nNA B          1  989.99  989.99  92.6782 1.027e-09 ***\nNA A          2 2320.05 1160.02 108.5956 9.423e-13 ***\nNA B:A        2   51.36   25.68   2.4041    0.1118    \nNA Residuals 24  256.37   10.68                       \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis-1",
    "title": "Ancova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nscatterplot(Y ~ B | A, data = data1)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data1)\n\n\n\n\n\n\n\n# OR via ggplot\nggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\nanova(lm(Y ~ B * A, data = data1))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nNA B          1  442.02  442.02  41.380 1.187e-06 ***\nNA A          2 2760.60 1380.30 129.217 1.418e-13 ***\nNA B:A        2  285.75  142.87  13.375 0.0001251 ***\nNA Residuals 24  256.37   10.68                      \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is strong evidence to suggest that the assumption of equal slopes is violated."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#fitting-the-model",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#fitting-the-model",
    "title": "Ancova (JAGS)",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nmodelString2 = \"\n  model {\n  #Likelihood\n  for (i in 1:n) {\n  y[i]~dnorm(mean[i],tau)\n  mean[i] &lt;- inprod(beta[],X[i,])\n  }\n  #Priors\n  for (i in 1:ngroups) {\n  beta[i] ~ dnorm(0, 1.0E-6) \n  }\n  sigma ~ dunif(0, 100)\n  tau &lt;- 1 / (sigma * sigma)\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"ancovaModel2.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX &lt;- model.matrix(~A * B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = X, n = nrow(data1), ngroups = ncol(X)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\ndata1.r2jags &lt;- jags(data = data1.list, inits = NULL, parameters.to.save = params,\n    model.file = \"ancovaModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 30\nNA    Unobserved stochastic nodes: 7\nNA    Total graph size: 286\nNA \nNA Initializing model\n\nprint(data1.r2jags)\n\nNA Inference for Bugs model at \"ancovaModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000\nNA beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100\nNA beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\nNA beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000\nNA beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000\nNA beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000\nNA sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800\nNA deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 9.8 and DIC = 167.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#mcmc-diagnostics-1",
    "title": "Ancova (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\ndenplot(data1.r2jags, parms = c(\"beta\"))\n\n\n\n\n\n\n\ntraplot(data1.r2jags, parms = c(\"beta\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\ndata1.mcmc = as.mcmc(data1.r2jags)\n#Raftery diagnostic\nraftery.diag(data1.mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  2        3853  3746         1.030     \nNA  beta[2]  2        3689  3746         0.985     \nNA  beta[3]  2        3895  3746         1.040     \nNA  beta[4]  2        3649  3746         0.974     \nNA  beta[5]  2        3918  3746         1.050     \nNA  beta[6]  2        3770  3746         1.010     \nNA  deviance 2        3938  3746         1.050     \nNA  sigma    4        5018  3746         1.340     \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  2        3853  3746         1.030     \nNA  beta[2]  2        3570  3746         0.953     \nNA  beta[3]  2        3811  3746         1.020     \nNA  beta[4]  2        3770  3746         1.010     \nNA  beta[5]  2        3770  3746         1.010     \nNA  beta[6]  2        3895  3746         1.040     \nNA  deviance 2        3981  3746         1.060     \nNA  sigma    4        5131  3746         1.370\n\n\nThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\n#Autocorrelation diagnostic\nautocorr.diag(data1.mcmc)\n\nNA             beta[1]      beta[2]     beta[3]      beta[4]       beta[5]\nNA Lag 0   1.000000000  1.000000000 1.000000000  1.000000000  1.0000000000\nNA Lag 1  -0.002520665 -0.007698073 0.001992162  0.000509790 -0.0005326877\nNA Lag 5   0.001007950  0.009095032 0.001511518 -0.006890623  0.0025773251\nNA Lag 10 -0.011280919  0.007907450 0.005969613 -0.006999313  0.0040454668\nNA Lag 50 -0.012861369 -0.019813696 0.002604518 -0.008791380 -0.0136623372\nNA             beta[6]     deviance        sigma\nNA Lag 0   1.000000000  1.000000000 1.0000000000\nNA Lag 1   0.004381248  0.332075434 0.4518687724\nNA Lag 5  -0.001182603  0.032092130 0.0351574955\nNA Lag 10 -0.004191097  0.003338842 0.0005457235\nNA Lag 50  0.002636154 -0.005426687 0.0039447210"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#model-validation-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#model-validation-1",
    "title": "Ancova (JAGS)",
    "section": "Model validation",
    "text": "Model validation\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nResiduals against predictors\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = newdata1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nnewdata1 = newdata1 %&gt;% cbind(fit, resid)\nggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\n\n\n\n\n\n\n\n\nAnd now for studentised residuals\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nsresid = resid/sd(resid)\nggplot() + geom_point(data1 = NULL, aes(y = sresid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nXmat = model.matrix(~A * B, data1)\n## get median parameter estimates\ncoefs = mcmc[, 1:6]\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\n    ], mcmc[i, \"sigma\"]))\nnewdata1 = data.frame(A = data1$A, B = data1$B, yRep) %&gt;% gather(key = Sample,\n    value = Value, -A, -B)\nggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \"Model\"),\n    alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\n    fill = \"Obs\"), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\n    x = A), position = position_jitter(width = 0.1, height = 0),\n    color = \"black\") + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \"Model\",\n    group = B, color = A), alpha = 0.5) + geom_point(data = data1,\n    aes(y = Y, x = B, group = B, color = A)) + theme_classic()\n\n\n\n\n\n\n\n\nThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\nmcmc_intervals(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \"beta|sigma\")\n\n\n\n\n\n\n\nmcmc_areas(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \"beta|sigma\")"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#parameter-estimates-1",
    "title": "Ancova (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the additive model.\n\nprint(data1.r2jags)\n\nNA Inference for Bugs model at \"ancovaModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000\nNA beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100\nNA beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\nNA beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000\nNA beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000\nNA beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000\nNA sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800\nNA deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 9.8 and DIC = 167.5\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n# OR\ntidyMCMC(as.mcmc(data1.r2jags), conf.int = TRUE, conf.method = \"HPDinterval\")\n\nNA # A tibble: 7 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   48.2      2.03    44.2      52.2   \nNA 2 beta[2]  -10.6      2.88   -16.3      -4.94  \nNA 3 beta[3]  -26.5      2.57   -31.6     -21.4   \nNA 4 beta[4]   -0.351    0.0816  -0.510    -0.187 \nNA 5 beta[5]   -0.271    0.110   -0.491    -0.0541\nNA 6 beta[6]    0.270    0.117    0.0436    0.503 \nNA 7 sigma      3.45     0.535    2.51      4.50\n\n\nConclusions\n\nThe intercept of the first group (Group A) is \\(48.2\\).\nThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\nThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\nA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\ndifference in slope between Group B and Group A \\(-0.270\\).\ndifference in slope between Group C and Group A \\(0.270\\).\n\nThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\n## since values are less than zero\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[2]\"])  # effect of (B-A = 0)\n\nNA [1] 0.0009333333\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[3]\"])  # effect of (C-A = 0)\n\nNA [1] 0\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[4]\"])  # effect of (slope = 0)\n\nNA [1] 0.0003333333\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[5]\"])  # effect of (slopeB - slopeA = 0)\n\nNA [1] 0.0152\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[6]\"])  # effect of (slopeC - slopeA = 0)\n\nNA [1] 0.0232\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, 2:6])  # effect of (model)\n\nNA [1] 0\n\n\nThere is evidence that the reponse differs between the groups."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#graphical-summaries-1",
    "title": "Ancova (JAGS)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix\n## Calculate the fitted values\nnewdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\n    len = 100))\nXmat = model.matrix(~A * B, newdata1)\ncoefs = mcmc[, c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\")]\nfit = coefs %*% t(Xmat)\nnewdata1 = newdata1 %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()\n\n\n\n\n\n\n\n\nAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\n## Calculate partial residuals fitted values\nfdata1 = rdata1 = data1\nfMat = rMat = model.matrix(~A * B, fdata1)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\nrdata1 = rdata1 %&gt;% mutate(partial.resid = resid + fit)\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\n    aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#introduction",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Introduction",
    "text": "Introduction\nFactorial designs are an extension of single factor ANOVA designs in which additional factors are added such that each level of one factor is applied to all levels of the other factor(s) and these combinations are replicated. For example, we might design an experiment in which the effects of temperature (high vs low) and fertiliser (added vs not added) on the growth rate of seedlings are investigated by growing seedlings under the different temperature and fertilizer combinations. In addition to investigating the impacts of the main factors, factorial designs allow us to investigate whether the effects of one factor are consistent across levels of another factor. For example, is the effect of temperature on growth rate the same for both fertilised and unfertilized seedlings and similarly, does the impact of fertiliser treatment depend on the temperature under which the seedlings are grown?\nArguably, these interactions give more sophisticated insights into the dynamics of the system we are investigating. Hence, we could add additional main effects, such as soil pH, amount of water, etc, along with all the two way (temp:fert, temp:pH, temp:water, etc), three-way (temp:fert:pH, temp:pH:water), four-way (and so on) interactions in order to explore how these various factors interact with one another to effect the response. However, the more interactions, the more complex the model becomes to specify, compute and interpret - not to mention the rate at which the number of required observations increases. Factorial designs can consist:\n\nentirely of crossed fixed factors (Model I ANOVA - most common) in which conclusions are restricted to the specific combinations of levels selected for the experiment.\nentirely of crossed random factors (Model II ANOVA).\na mixture of crossed fixed and random factors (Model III ANOVA).\n\nThe latter are useful for investigating the generality of a main treatment effect (fixed) over broad spatial, temporal or clinical levels of organisation. That is whether the observed effects of temperature and/or fertiliser (for example) are observed across the entire genera or country."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#linear-model",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#linear-model",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Linear model",
    "text": "Linear model\nAs with single factor ANOVA, the linear model could be constructed as either effects or means parameterisation, although only effects parameterisation will be considered here. The linear models for two and three factor design are\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk},\n\\]\nand\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\beta)_{ij} + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijkl},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component. Note that although the linear models for Model I, Model II and Model III designs are identical, the interpretation of terms (and thus null hypothesis) differ. Recall from the tutorial on single factor ANOVA, that categorical variables in linear models are actually re-parameterised dummy codes - and thus the \\(\\alpha\\) term above, actually represents one or more dummy codes. Thus, if we actually had two levels of Factor A (A1 and A2) and three levels of Factor B (B1, B2, B3), then the fully parameterised linear model would be:\n\\[\ny=\\beta_{A1B1}+\\beta_{A2B1−A1B1}+\\beta_{A1B2−A1B1}+\\beta_{A1B3−A1B1}+\\beta_{A2B2−A1B2−A2B1−A1B1}+\\beta_{A2B3−A1B3−A2B1−A1B1}.\n\\]\nThus, such a model would have six parameters to estimate (in addition to the variance)."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#null-hypothesis",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#null-hypothesis",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nThere are separate null hypothesis associated with each of the main effects and the interaction terms.\n\nModel 1 - fixed effects\nFactor A\n\n\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal. The mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is rejected indicating that the treatment has been found to affect the response variable. Note, as with multiple regression models, these “effects” represent partial effects. In the above, the effect of Factor A is actually the effect of Factor A at the first level of the Factor(s).\nFactor B\n\n\\(H_0(B):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal - at the first level of Factor A. Equivalent interpretation to Factor A above.\nFactor AB: interaction\n\n\\(H_0(AB):\\mu_{ij}=\\mu_i+\\mu_j-\\mu\\)\n\nThe population group means are all equal. For any given combination of factor levels, the population group mean will be equal to the difference between the overall population mean and the simple additive effects of the individual factor group means. That is, the effects of the main treatment factors are purely additive and independent of one another. This is equivalent to \\(H_0(AB): \\alpha\\beta_{ij}=0\\), no interaction between Factor A and Factor B.\n\n\nModel 2 - random effects\nFactor A\n\n\\(H_0(A):\\sigma^2_{\\alpha}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of A.\nFactor B\n\n\\(H_0(B):\\sigma^2_{\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\n\n\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\n\nThere is no added variance due to all possible interactions between all possible levels of A and B.\n\n\nModel 3 - mixed effects\nFixed factor - e.g. A\n\n\\(H_0(A):\\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\)\n\nThe population group means are all equal. The mean of population \\(1\\) (pooled over all levels of the random factor) is equal to that of population \\(2\\) and so on, and thus all population means are equal to an overall mean pooling over all possible levels of the random factor. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the overall mean (\\(\\alpha_i=\\mu_i-\\mu\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\)\n\nNo effect of any level of this factor pooled over all possible levels of the random factor.\nRandom factor - e.g. B\n\n\\(H_0(B):\\sigma^2_{\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible levels of B.\nFactor AB: interaction\nThe interaction of a fixed and random factor is always considered a random factor.\n\n\\(H_0(AB):\\sigma^2_{\\alpha\\beta}=0\\)\n\nThe population variance equals zero. There is no added variance due to all possible interactions between all possible levels of A and B."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#analysis-of-variance",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nWhen fixed factorial designs are balanced, the total variance in the response variable can be sequentially partitioned into what is explained by each of the model terms (factors and their interactions) and what is left unexplained. For each of the specific null hypotheses, the overall unexplained variability is used as the denominator in F-ratio calculations, and when a null hypothesis is true, an F-ratio should follow an F distribution with an expected value less than \\(1\\). Random factors are added to provide greater generality of conclusions. That is, to enable us to make conclusions about the effect of one factor (such as whether or not fertiliser is added) over all possible levels (not just those sampled) of a random factor (such as all possible locations, seasons, varieties, etc). In order to expand our conclusions beyond the specific levels used in the design, the hypothesis tests (and thus F-ratios) must reflect this extra generality by being more conservative.\nThe appropriate F-ratios for fixed, random and mixed factorial designs are presented below. Generally, once the terms (factors and interactions) have been ordered into a hierarchy (single factors at the top, highest level interactions at the bottom and terms of same order given equivalent positions in the hierarchy), the denominator for any term is selected as the next appropriate random term (an interaction that includes the term to be tested) encountered lower in the hierarchy. Interaction terms that contain one or more random factors are considered themselves to be random terms, as is the overall residual term (as all observations are assumed to be random representations of the entire population(s)). Note, when designs include a mixture of fixed and random crossed effects, exact denominator degrees of freedoms for certain F-ratios are undefined and traditional approaches adopt rather inexact estimated approximate or “Quasi” F-ratios. Pooling of non-significant F-ratio denominator terms, in which lower random terms are added to the denominator (provided \\(\\alpha &gt; 0.25\\)), may also be useful. For random factors within mixed models, selecting F-ratio denominators that are appropriate for the intended hypothesis tests is a particularly complex and controversial issue. Traditionally, there are two alternative approaches and whilst the statistical resumes of each are complicated, essentially they differ in whether or not the interaction term is constrained for the test of the random factor.\nThe constrained or restricted method (Model I), stipulates that for the calculation of a random factor F-ratio (which investigates the added variance added due to the random factor), the overall effect of the interaction is treated as zero. Consequently, the random factor is tested against the residual term. The unconstrained or unrestrained method (Model II) however, does not set the interaction effect to zero and therefore the interaction term is used as the random factor F-ratio denominator. This method assumes that the interaction terms for each level of the random factor are completely independent (correlations between the fixed factor must be consistent across all levels of the random factor). Some statisticians maintain that the independence of the interaction term is difficult to assess for clinical data and therefore, the restricted approach is more appropriate. However, others have suggested that the restricted method is only appropriate for balanced designs."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#quasi-f-ratios",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#quasi-f-ratios",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Quasi F-ratios",
    "text": "Quasi F-ratios\nAn additional complication for three or more factor models that contain two or more random factors, is that there may not be a single appropriate interaction term to use as the denominator for many of the main effects F-ratios. For example, if Factors A and B are random and C is fixed, then there are two random interaction terms of equivalent level under Factor C (\\(A^\\prime \\times C\\) and \\(B^\\prime \\times C\\)). As a result, the value of the of the Mean Squares (MS) expected when the null hypothesis is true cannot be easily defined. The solutions for dealing with such situations (quasi F-ratios) involve adding (and subtracting) terms together to create approximate estimates of F-ratio denominators. Alternatively, for random factors, variance components with confidence intervals can be used. These solutions are sufficiently unsatisfying as to lead many statisticians to recommend that factorial designs with two or more random factors should avoided if possible. Arguably however, linear mixed effects models offer more appropriate solutions to the above issues as they are more robust for unbalanced designs, accommodate covariates and provide a more comprehensive treatment and overview of all the underlying data structures.\n\nfact_anova_table\n\nNA     df           MS       A,B fixed          A,B random       \nNA A   \"a-1\"        \"MS A\"   \"(MS A)/(MS res)\"  \"(MS A)/(MS AB)\" \nNA B   \"b-1\"        \"MS B\"   \"(MS B)/(MS res)\"  \"(MS B)/(MS AB)\" \nNA AB  \"(b-1)(a-1)\" \"MS AB\"  \"(MS AB)/(MS res)\" \"(MS AB)/(MS AB)\"\nNA Res \"(n-1)ba\"    \"MS res\" \"\"                 \"\"               \nNA     A fixed B random (model I) A fixed B random (model II)\nNA A   \"(MS A)/(MS AB)\"           \"(MS A)/(MS AB)\"           \nNA B   \"(MS B)/(MS res)\"          \"(MS B)/(MS AB)\"           \nNA AB  \"(MS AB)/(MS res)\"         \"(MS AB)/(MS res)\"         \nNA Res \"\"                         \"\"\n\n\nThe corresponding R syntax is given below.\n\n#Type I SS (Balanced)\nanova(lm(y ~ A * B, data))\n\n#Type II SS (Unbalanced)\nAnova(lm(y ~ A * B, data), type = \"II\")\n\n#Type III SS (Unbalanced)\nAnova(lm(y ~ A * B, data), type = \"III\")\n\n#Variance components\nsummary(lmer(y ~ 1 + (1 | A) + (1 | B) + (1 | A:B), data))\n\nNote that for fixed factor models, when null hypotheses of interactions are rejected, the null hypothesis of the individual constituent factors are unlikely to represent the true nature of the effects and thus are of little value. The nature of such interactions are further explored by fitting simpler linear models (containing at least one less factor) separately for each of the levels of the other removed factor(s). Such Main effects tests are based on a subset of the data, and therefore estimates of the overall residual (unexplained) variability are unlikely to be as precise as the estimates based on the global model. Consequently, F-ratios involving MSResid should use the estimate of MSResid from the global model rather than that based on the smaller, theoretically less precise subset of data. For random and mixed models, since the objective is to generalise the effect of one factor over and above any interactions with other factors, the main factor effects can be interpreted even in the presence of significant interactions. Nevertheless, it should be noted that when a significant interaction is present in a mixed model, the power of the main fixed effects will be reduced (since the amount of variability explained by the interaction term will be relatively high, and this term is used as the denominator for the F-ratio calculation)."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#assumptions",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nHypothesis tests assume that the residuals are:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see table above) should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another.\n\nPlanned and unplanned comparisons\nAs with single factor analysis of variance, planned and unplanned multiple comparisons (such as Tukey’s test) can be incorporated into or follow the linear model respectively so as to further investigate any patterns or trends within the main factors and/or the interactions. As with single factor analysis of variance, the contrasts must be defined prior to fitting the linear model, and no more than \\(p−1\\) (where \\(p\\) is the number of levels of the factor) contrasts can be defined for a factor.\nUnbalanced designs\nA factorial design can be thought of as a table made up of rows (representing the levels of one factor), columns (levels of another factor), and cells (the individual combinations of the set of factors). Whilst the middle left table does not have equal sample sizes in each cell, the sample sizes are in proportion and as such, does not present the issues discussed below for unbalanced designs.\nIn addition to impacting on normality and homogeneity of variance, unequal sample sizes in factorial designs have major implications for the partitioning of the total sums of squares into each of the model components. For balanced designs, the total sums of squares (SSTotal) is equal to the additive sums of squares of each of the components (including the residual). For example, in a two factor balanced design, SSTotal=SSA+SSB+SSAB+SSResid. This can be represented diagrammatically by a Venn Diagram in which each of the SS for the term components butt against one another and are surrounded by the SSResid. However, in unbalanced designs, the sums of squares will be non-orthogonal and the sum of the individual components does not add up to the total sums of squares. Diagrammatically, the SS of the terms intersect or are separated.\nIn regular sequential sums of squares (Type I SS), the sum of the individual sums of squares must be equal to the total sums of squares, the sums of squares of the last factor to be estimated will be calculated as the difference between the total sums of squares and what has already been accounted for by other components. Consequently, the order in which factors are specified in the model (and thus estimated) will alter their sums of squares and therefore their F-ratios. To overcome this problem, traditionally there are two other alternative methods of calculating sums of squares.\n\nType II (hierarchical) SS estimate the sums of squares of each term as the improvement it contributes upon addition of that term to a model of greater complexity and lower in the hierarchy (recall that the hierarchical structure descends from the simplest model down to the fully populated model). The SS for the interaction as well as the first factor to be estimated are the same as for Type I SS. Type II SS estimate the contribution of a factor over and above the contributions of other factors of equal or lower complexity but not above the contributions of the interaction terms or terms nested within the factor. However, these sums of squares are weighted by the sample sizes of each level and therefore are biased towards the trends produced by the groups (levels) that have higher sample sizes. As a result of the weightings, Type II SS actually test hypotheses about really quite complex combinations of factor levels. Rather than test a hypothesis that \\(\\mu_{High}=\\mu_{Medium}=\\mu_{Low}\\), Type II SS might be testing that \\(4\\times\\mu_{High}=1\\times\\mu_{Medium}=0.25\\times\\mu_{Low}\\).\nType III (marginal or orthogonal) SS estimate the sums of squares of each term as the improvement based on a comparison of models with and without the term and are unweighted by sample sizes. Type III SS essentially measure just the unique contribution of each factor over and above the contributions of the other factors and interactions. For unbalanced designs,Type III SS essentially test equivalent hypotheses to balanced Type I SS and are therefore arguably more appropriate for unbalanced factorial designs than Type II SS. Importantly, Type III SS are only interpretable if they are based on orthogonal contrasts (such as sum or helmert contrasts and not treatment contrasts).\n\nThe choice between Type II and III SS clearly depends on the nature of the question. For example, if we had measured the growth rate of seedlings subjected to two factors (temperature and fertiliser), Type II SS could address whether there was an effect of temperature across the level of fertiliser treatment, whereas Type III SS could assess whether there was an effect of temperature within each level of the fertiliser treatment.\nWhen an entire combination, or cell, is missing (perhaps due to unforeseen circumstances) it is not possible to test all the main effects and/or interactions. The bottom right table above depicts such as situation. One solution is to fit a large single factor ANOVA with as many levels as there are cells (this is known as a cell means model) and investigate various factor and interaction effects via specific contrasts (see the following tables). Difficulties in establishing appropriate error terms, makes missing cells in random and mixed factor designs substantially more complex."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#assumptions-1",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#assumptions-1",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions are:\n\nAll of the observations are independent - this must be addressed at the design and collection stages. Importantly, to be considered independent replicates, the replicates must be made at the same scale at which the treatment is applied. For example, if the experiment involves subjecting organisms housed in tanks to different water temperatures, then the unit of replication is the individual tanks not the individual organisms in the tanks. The individuals in a tank are strictly not independent with respect to the treatment.\nThe response variable (and thus the residuals) should be normally distributed for each sampled populations (combination of factors). Boxplots of each treatment combination are useful for diagnosing major issues with normality.\nThe response variable should be equally varied (variance should not be related to mean as these are supposed to be estimated separately) for each combination of treatments. Again, boxplots are useful."
  },
  {
    "objectID": "tutorials/2020-02-06-factorial-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-06-factorial-anova-stan/index.html#exploratory-data-analysis",
    "title": "Factorial Analysis of Variance (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y ~ A * B, data)\n\n# OR via ggplot2\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = y, x = A, fill = B)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical. There is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html",
    "title": "Variance Heterogeneity (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html#introduction",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html#introduction",
    "title": "Variance Heterogeneity (Stan)",
    "section": "Introduction",
    "text": "Introduction\nUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\n\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html#model-fitting-1",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html#model-fitting-1",
    "title": "Variance Heterogeneity (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\n\nmodelString2 = \"\n  data {\n  int&lt;lower=1&gt; n;\n  int&lt;lower=1&gt; nX;\n  vector [n] y;\n  matrix [n,nX] X;\n  }\n  parameters {\n  vector[nX] beta;\n  vector&lt;lower=0&gt;[nX] sigma;\n  }\n  transformed parameters {\n  vector[n] mu;\n  vector&lt;lower=0&gt;[n] sig;\n\n  mu = X*beta;\n  sig = X*sigma;\n  }\n  model {\n  // Likelihood\n  y~normal(mu,sig);\n  \n  // Priors\n  beta ~ normal(0,1000);\n  sigma~cauchy(0,5);\n  }\n  generated quantities {\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n  log_lik[i] = normal_lpdf(y[i] | mu[i], sig[i]); \n  }\n  }\n  \n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"heteroskModel2.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat &lt;- model.matrix(~x, data.het1)\ndata.het1.list &lt;- with(data.het1, list(y = y, X = Xmat, n = nrow(data.het1),\n    nX = ncol(Xmat)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"log_lik\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1500\n\n\nNow run the Stan code via the rstan interface. Note that the first time Stan is run after the rstan package is loaded, it is often necessary to run any kind of randomization function just to initiate the .Random.seed variable.\n\ndata.het1.rstan &lt;- stan(data = data.het1.list, file = \"heteroskModel2.stan\",\n    chains = nChains, iter = numSavedSteps, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.4e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nNA Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  501 / 2000 [ 25%]  (Sampling)\nNA Chain 1: Iteration:  700 / 2000 [ 35%]  (Sampling)\nNA Chain 1: Iteration:  900 / 2000 [ 45%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nNA Chain 1: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nNA Chain 1: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nNA Chain 1: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nNA Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.054 seconds (Warm-up)\nNA Chain 1:                0.085 seconds (Sampling)\nNA Chain 1:                0.139 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nNA Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  501 / 2000 [ 25%]  (Sampling)\nNA Chain 2: Iteration:  700 / 2000 [ 35%]  (Sampling)\nNA Chain 2: Iteration:  900 / 2000 [ 45%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 2000 [ 55%]  (Sampling)\nNA Chain 2: Iteration: 1300 / 2000 [ 65%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 2000 [ 75%]  (Sampling)\nNA Chain 2: Iteration: 1700 / 2000 [ 85%]  (Sampling)\nNA Chain 2: Iteration: 1900 / 2000 [ 95%]  (Sampling)\nNA Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.054 seconds (Warm-up)\nNA Chain 2:                0.099 seconds (Sampling)\nNA Chain 2:                0.153 seconds (Total)\nNA Chain 2:\n\nprint(data.het1.rstan, par = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA            mean se_mean   sd   2.5%    25%    50%   75% 97.5% n_eff Rhat\nNA beta[1]   40.27    0.02 0.66  38.93  39.83  40.28 40.71 41.55  1325    1\nNA beta[2]    4.10    0.03 1.17   1.87   3.32   4.06  4.88  6.39  1714    1\nNA beta[3]   14.57    0.03 1.04  12.42  13.90  14.58 15.26 16.61  1537    1\nNA beta[4]   -0.64    0.02 0.97  -2.67  -1.28  -0.63  0.00  1.21  1562    1\nNA beta[5]  -10.34    0.02 1.02 -12.34 -11.01 -10.32 -9.65 -8.35  1854    1\nNA sigma[1]   2.00    0.01 0.24   1.58   1.82   1.98  2.14  2.55  2259    1\nNA sigma[2]   0.88    0.01 0.73   0.04   0.37   0.71  1.20  2.73  3093    1\nNA sigma[3]   0.44    0.01 0.46   0.01   0.13   0.30  0.59  1.67  3321    1\nNA sigma[4]   0.28    0.01 0.32   0.01   0.08   0.18  0.39  1.13  2646    1\nNA sigma[5]   0.35    0.01 0.39   0.01   0.10   0.22  0.48  1.32  3089    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:33:58 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html#mcmc-diagnostics-1",
    "title": "Variance Heterogeneity (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nlibrary(mcmcplots)\nmcmc&lt;-As.mcmc.list(data.het1.rstan)\ndenplot(mcmc, parms = c(\"beta\", \"sigma\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"beta\", \"sigma\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html#parameter-estimates-1",
    "title": "Variance Heterogeneity (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the model.\n\nprint(data.het1.rstan, pars = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=2000; warmup=500; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA            mean se_mean   sd   2.5%    25%    50%   75% 97.5% n_eff Rhat\nNA beta[1]   40.27    0.02 0.66  38.93  39.83  40.28 40.71 41.55  1325    1\nNA beta[2]    4.10    0.03 1.17   1.87   3.32   4.06  4.88  6.39  1714    1\nNA beta[3]   14.57    0.03 1.04  12.42  13.90  14.58 15.26 16.61  1537    1\nNA beta[4]   -0.64    0.02 0.97  -2.67  -1.28  -0.63  0.00  1.21  1562    1\nNA beta[5]  -10.34    0.02 1.02 -12.34 -11.01 -10.32 -9.65 -8.35  1854    1\nNA sigma[1]   2.00    0.01 0.24   1.58   1.82   1.98  2.14  2.55  2259    1\nNA sigma[2]   0.88    0.01 0.73   0.04   0.37   0.71  1.20  2.73  3093    1\nNA sigma[3]   0.44    0.01 0.46   0.01   0.13   0.30  0.59  1.67  3321    1\nNA sigma[4]   0.28    0.01 0.32   0.01   0.08   0.18  0.39  1.13  2646    1\nNA sigma[5]   0.35    0.01 0.39   0.01   0.10   0.22  0.48  1.32  3089    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:33:58 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n# OR\ntidyMCMC(data.het1.rstan, pars = c(\"beta\", \"sigma\"), conf.int = TRUE, conf.method = \"HPDinterval\",\n    rhat = TRUE, ess = TRUE)\n\nNA # A tibble: 10 × 7\nNA    term     estimate std.error    conf.low conf.high  rhat   ess\nNA    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\nNA  1 beta[1]    40.3       0.660  39.0          41.6    1.00  1325\nNA  2 beta[2]     4.10      1.17    1.86          6.36   1.00  1714\nNA  3 beta[3]    14.6       1.04   12.5          16.7    1.00  1537\nNA  4 beta[4]    -0.644     0.970  -2.69          1.15   1.00  1562\nNA  5 beta[5]   -10.3       1.02  -12.3          -8.31   1.00  1854\nNA  6 sigma[1]    2.00      0.243   1.52          2.47   1.00  2259\nNA  7 sigma[2]    0.884     0.729   0.00104       2.32   1.00  3093\nNA  8 sigma[3]    0.437     0.461   0.0000642     1.31   1.00  3321\nNA  9 sigma[4]    0.285     0.321   0.000231      0.855  1.00  2646\nNA 10 sigma[5]    0.349     0.390   0.0000285     1.06   1.00  3089\n\n\nConclusions\n\nthe mean of the first group (A) is \\(40.3\\)\nthe mean of the second group (B) is \\(4.12\\) units greater than (A)\nthe mean of the third group (C) is \\(14.6\\) units greater than (A)\nthe mean of the forth group (D) is \\(-0.637\\) units greater (i.e. less) than (A)\nthe mean of the fifth group (E) is \\(-10.3\\) units greater (i.e. less) than (A)\n\nThe \\(95\\)% confidence interval for the effects of B, C and E do not overlap with \\(0\\) implying a significant difference between group A and groups B, C and E. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\nmcmcpvalue &lt;- function(samp) {\n    ## elementary version that creates an empirical p-value for the\n    ## hypothesis that the columns of samp have mean zero versus a general\n    ## multivariate distribution with elliptical contours.\n\n    ## differences from the mean standardized by the observed\n    ## variance-covariance factor\n\n    ## Note, I put in the bit for single terms\n    if (length(dim(samp)) == 0) {\n        std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - mean(samp),\n            transpose = TRUE)\n        sqdist &lt;- colSums(std * std)\n        sum(sqdist[-1] &gt; sqdist[1])/length(samp)\n    } else {\n        std &lt;- backsolve(chol(var(samp)), cbind(0, t(samp)) - colMeans(samp),\n            transpose = TRUE)\n        sqdist &lt;- colSums(std * std)\n        sum(sqdist[-1] &gt; sqdist[1])/nrow(samp)\n    }\n\n}\n## since values are less than zero\nmcmc = as.matrix(data.het1.rstan)\nfor (i in grep(\"beta\", colnames(mcmc), value = TRUE)) print(paste(i, mcmcpvalue(mcmc[,\n    i])))\n\nNA [1] \"beta[1] 0\"\nNA [1] \"beta[2] 0.00133333333333333\"\nNA [1] \"beta[3] 0\"\nNA [1] \"beta[4] 0.497\"\nNA [1] \"beta[5] 0\"\n\nmcmcpvalue(mcmc[, grep(\"beta\", colnames(mcmc))])\n\nNA [1] 0\n\n\nWith a p-value of essentially \\(0\\), we would conclude that there is almost no evidence that the slope was likely to be equal to zero, suggesting there is a relationship."
  },
  {
    "objectID": "tutorials/2020-02-07-heterogeneity-stan/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-07-heterogeneity-stan/index.html#graphical-summaries-1",
    "title": "Variance Heterogeneity (Stan)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = as.matrix(data.het1.rstan)\n## Calculate the fitted values\nnewdata = data.frame(x = levels(data.het1$x))\nXmat = model.matrix(~x, newdata)\nwch = grep(\"beta\", colnames(mcmc))\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nnewdata = newdata %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\nggplot(newdata, aes(y = estimate, x = x)) + geom_pointrange(aes(ymin = conf.low,\n    ymax = conf.high)) + scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nIf you wanted to represent sample data on the figure in such a simple example (single predictor) we could simply over- (or under-) lay the raw data.\n\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = data.het1,\n    aes(y = y, x = x), color = \"gray\") + geom_pointrange(aes(ymin = conf.low,\n    ymax = conf.high)) + scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nA more general solution would be to add the partial residuals to the figure. Partial residuals are the fitted values plus the residuals. In this simple case, that equates to exactly the same as the raw observations since \\(\\text{resid}=\\text{obs}−\\text{fitted}\\) and the fitted values depend only on the single predictor we are interested in.\n\n## Calculate partial residuals fitted values\nfdata = rdata = data.het1\nfMat = rMat = model.matrix(~x, fdata)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data.het1$y - apply(coefs, 2, median) %*% t(rMat))\nrdata = rdata %&gt;% mutate(partial.resid = resid + fit)\nggplot(newdata, aes(y = estimate, x = x)) + geom_point(data = rdata, aes(y = partial.resid),\n    color = \"gray\") + geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\n    scale_y_continuous(\"Y\") + scale_x_discrete(\"X\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html",
    "href": "tutorials/2020-02-08-acf-stan/index.html",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#introduction",
    "href": "tutorials/2020-02-08-acf-stan/index.html#introduction",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Introduction",
    "text": "Introduction\nUp until now (in the proceeding tutorials), the focus has been on models that adhere to specific assumptions about the underlying populations (and data). Indeed, both before and immediately after fitting these models, I have stressed the importance of evaluating and validating the proposed and fitted models to ensure reliability of the models. It is now worth us revisiting those fundamental assumptions as well as exploring the options that are available when the populations (data) do not conform. Let’s explore a simple linear regression model to see how each of the assumptions relate to the model.\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\;\\;\\; \\text{with} \\;\\;\\; \\epsilon_i \\sim \\text{Normal}(0, \\sigma^2).\n\\]\nThe above simple statistical model models the linear relationship of \\(y_i\\) against \\(x_i\\). The residuals (\\(\\epsilon\\)) are assumed to be normally distributed with a mean of zero and a constant (yet unknown) variance (\\(\\sigma\\), homogeneity of variance). The residuals (and thus observations) are also assumed to all be independent.\nHomogeneity of variance and independence are encapsulated within the single symbol for variance (\\(\\sigma^2\\)). In assuming equal variances and independence, we are actually making an assumption about the variance-covariance structure of the populations (and thus residuals). Specifically, we assume that all populations are equally varied and thus can be represented well by a single variance term (all diagonal values in a \\(N\\times N\\) covariance matrix are the same, \\(\\sigma^2\\)) and the covariances between each population are zero (off diagonals). In simple regression, each observation (data point) represents a single observation drawn (sampled) from an entire population of possible observations. The above covariance structure thus assumes that the covariance between each population (observation) is zero - that is, each observation is completely independent of each other observation. Whilst it is mathematically convenient when data conform to these conditions (normality, homogeneity of variance, independence and linearity), data often violate one or more of these assumptions. In the following, I want to discuss and explore the causes and options for dealing with non-compliance to each of these conditions. By gaining a better understanding of how the various model fitting engines perform their task, we are better equipped to accommodate aspects of the data that don’t otherwise conform to the simple regression assumptions. In this tutorial we specifically focus on the topic of heterogeneity of the variance.\nIn order that the estimated parameters represent the underlying populations in an unbiased manner, the residuals (and thus each each observation) must be independent. However, what if we were sampling a population over time and we were interested in investigating how changes in a response relate to changes in a predictor (such as rainfall). For any response that does not “reset” itself on a regular basis, the state of the population (the value of its response) at a given time is likely to be at least partly dependent on the state of the population at the sampling time before. We can further generalise the above into:\n\\[\ny_i \\sim Dist(\\mu_i),\n\\]\nwhere \\(\\mu_i=\\boldsymbol X \\boldsymbol \\beta + \\boldsymbol Z \\boldsymbol \\gamma\\), with \\(\\boldsymbol X\\) and \\(\\boldsymbol \\beta\\) representing the fixed data structure and fixed effects, respectively, while with \\(\\boldsymbol Z\\) and \\(\\boldsymbol \\gamma\\) represent the varying data structure and varying effects, respectively. In simple regression, there are no “varying” effects, and thus:\n\\[\n\\boldsymbol \\gamma \\sim MVN(\\boldsymbol 0, \\boldsymbol \\Sigma),\n\\]\nwhere \\(\\boldsymbol \\Sigma\\) is a variance-covariance matrix of the form\n\\[\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho^{\\phi_{1,2}} & \\ldots & \\rho^{\\phi_{1,n}} \\\\\n   \\rho^{\\phi_{2,1}} & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\phi_{n,1}} & \\ldots & \\ldots & 1\n   \\end{bmatrix}.\n\\]\nNotice that this introduces a very large number of additional parameters that require estimating: \\(\\sigma^2\\) (error variance), \\(\\rho\\) (base autocorrelation) and each of the individual covariances (\\(\\rho^{\\phi_{n,n}}\\)). Hence, there are always going to be more parameters to estimate than there are date avaiable to use to estimate these paramters. We typically make one of a number of alternative assumptions so as to make this task more manageable.\n\nWhen we assume that all residuals are independent (regular regression), i.e. \\(\\rho=0\\), \\(\\boldsymbol \\Sigma\\) is essentially equal to \\(\\sigma^2 \\boldsymbol I\\) and we simply use:\n\n\\[\n\\boldsymbol \\gamma \\sim N( 0,\\sigma^2).\n\\]\n\nWe could assume there is a reasonably simple pattern of correlation that declines over time. The simplest of these is a first order autoregressive (AR1) structure in which exponent on the correlation declines linearly according to the time lag (\\(\\mid t - s\\mid\\)).\n\n\\[\n\\boldsymbol \\Sigma =  \\frac{\\sigma^2}{1-\\rho^2}\n  \\begin{bmatrix}\n   1 & \\rho & \\ldots & \\rho^{\\mid t-s \\mid} \\\\\n   \\rho & 1 & \\ldots & \\vdots\\\\\n   \\vdots & \\ldots & 1 & \\vdots\\\\\n   \\rho^{\\mid t-s \\mid } & \\ldots & \\ldots & 1\n   \\end{bmatrix}.\n\\]\nNote, in making this assumption, we are also assuming that the degree of correlation is dependent only on the lag and not on when the lag occurs (stationarity). That is all lag 1 residual pairs will have the same degree of correlation, all the lag \\(2\\) pairs will have the same correlation and so on."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#model-fitting",
    "href": "tutorials/2020-02-08-acf-stan/index.html#model-fitting",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\nstanString = \"\n  data {\n  int&lt;lower=1&gt; n;\n  vector [n] y;\n  int&lt;lower=1&gt; nX;\n  matrix[n,nX] X;\n  }\n  transformed data {\n  }\n  parameters {\n  vector[nX] beta;\n  real&lt;lower=0&gt; sigma;\n  real&lt;lower=-1,upper=1&gt; phi;\n  }\n  transformed parameters {\n  vector[n] mu;\n  vector[n] epsilon;\n  mu = X*beta;\n  epsilon[1] = y[1] - mu[1];\n  for (i in 2:n) {\n  epsilon[i] = (y[i] - mu[i]);\n  mu[i] = mu[i] + phi*epsilon[i-1];\n  }\n  }\n  model {\n  phi ~ uniform(-1,1);\n  beta ~ normal(0,100);\n  sigma ~ cauchy(0,5);\n  y ~ normal(mu, sigma);\n  }\n  generated quantities {\n  }\n  \n  \"\n\n## write the model to a text file\nwriteLines(stanString, con = \"tempModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list &lt;- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1500\n\n\nNow compile and run the Stan code via the rstan interface.\n\nlibrary(rstan)\n\nDuring the warmup stage, the No-U-Turn sampler (NUTS) attempts to determine the optimum stepsize - the stepsize that achieves the target acceptance rate (\\(0.8\\) or \\(80\\)% by default) without divergence (occurs when the stepsize is too large relative to the curvature of the log posterior and results in approximations that are likely to diverge and be biased) - and without hitting the maximum treedepth (\\(10\\)). At each iteration of the NUTS algorithm, the number of leapfrog steps doubles (as it increases the treedepth) and only terminates when either the NUTS criterion are satisfied or the tree depth reaches the maximum (\\(10\\) by default).\n\ndata.temporalCor.rstan &lt;- stan(data = data.temporalCor.list, file = \"tempModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.4e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.038 seconds (Warm-up)\nNA Chain 1:                0.038 seconds (Sampling)\nNA Chain 1:                0.076 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 7e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nNA Chain 2:                0.032 seconds (Sampling)\nNA Chain 2:                0.068 seconds (Total)\nNA Chain 2:\n\nprint(data.temporalCor.rstan, par = c(\"beta\", \"sigma\", \"phi\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA          mean se_mean    sd 2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 30.71    0.36 11.95 8.68 22.30 30.40 38.27 55.41  1082    1\nNA beta[2]  0.22    0.00  0.10 0.02  0.16  0.22  0.30  0.43  1407    1\nNA sigma   12.11    0.03  1.26 9.93 11.25 12.01 12.86 14.88  1359    1\nNA phi      0.91    0.00  0.05 0.79  0.88  0.92  0.95  0.99  1117    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:39:47 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-08-acf-stan/index.html#mcmc-diagnostics",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nlibrary(mcmcplots)\nmcmc = As.mcmc.list(data.temporalCor.rstan)\ndenplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\n\n\n#Raftery diagnostic\nraftery.diag(mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\n\n\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n\nNA             beta[1]      beta[2]         sigma          phi         lp__\nNA Lag 0   1.000000000  1.000000000  1.0000000000  1.000000000  1.000000000\nNA Lag 1   0.244876071  0.131202988  0.1861700732  0.155207971  0.469635120\nNA Lag 5   0.030363677  0.006018787 -0.0230078904  0.037502444  0.016158672\nNA Lag 10 -0.003669667 -0.014831709 -0.0006623927 -0.009372815  0.022517151\nNA Lag 50  0.014609742  0.005511481 -0.0400835388  0.018467269 -0.003215701\n\n\n\nstan_ac(data.temporalCor.rstan)\n\n\n\n\n\n\n\nstan_rhat(data.temporalCor.rstan)\n\n\n\n\n\n\n\nstan_ess(data.temporalCor.rstan)\n\n\n\n\n\n\n\n\nAll diagnostics seem fine."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#model-validation",
    "href": "tutorials/2020-02-08-acf-stan/index.html#model-validation",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Model validation",
    "text": "Model validation\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[\nRes_i = Y_i - \\mu_i\n\\]\n\\[\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n\\]\n\\[\nRes_i = \\frac{Res_i}{\\sigma}\n\\]\n\nmcmc = as.matrix(data.temporalCor.rstan)\nwch = grep(\"beta\", colnames(mcmc))\n# generate a model matrix\nnewdata = data.frame(x = data.temporalCor$x)\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nlibrary(ggplot2)\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n\n\n\n\n\n\n\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_classic()\n\n\n\n\n\n\n\nplot(acf(resid, lag = 40))\n\n\n\n\n\n\n\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\n    fit[i, ], mcmc[i, \"sigma\"]))\nggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\n    fill = \"Model\"), alpha = 0.5) + geom_density(data = data.temporalCor,\n    aes(x = y, fill = \"Obs\"), alpha = 0.5) + theme_classic()\n\n\n\n\n\n\n\n\nNo obvious autocorrelation or other issues with residuals remaining."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#parameter-estimates",
    "href": "tutorials/2020-02-08-acf-stan/index.html#parameter-estimates",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nExplore parameter estimates.\n\nlibrary(broom)\nlibrary(broom.mixed)\ntidyMCMC(data.temporalCor.rstan, par = c(\"beta\", \"phi\", \"sigma\"),\n    conf.int = TRUE, conf.method = \"HPDinterval\", rhat = TRUE,\n    ess = TRUE)\n\nNA # A tibble: 4 × 7\nNA   term    estimate std.error   conf.low conf.high  rhat   ess\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\nNA 1 beta[1]   30.7     11.9     7.96         54.4    1.00  1082\nNA 2 beta[2]    0.224    0.104  -0.0000273     0.414  1.00  1407\nNA 3 phi        0.910    0.0541  0.810         0.998  1.00  1117\nNA 4 sigma     12.1      1.26    9.88         14.7    1.00  1359"
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#model-fitting-1",
    "href": "tutorials/2020-02-08-acf-stan/index.html#model-fitting-1",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nWe proceed to code the model into JAGS (remember that in this software normal distribution are parameterised in terms of precisions \\(\\tau\\) rather than variances, where \\(\\tau=\\frac{1}{\\sigma^2}\\)). Define the model.\n\nstanString = \"\nfunctions { \n    matrix cov_matrix_ar1(real ar, real sigma, int nrows) { \n  matrix[nrows, nrows] mat; \n    vector[nrows - 1] gamma; \n    mat = diag_matrix(rep_vector(1, nrows)); \n        for (i in 2:nrows) { \n            gamma[i - 1] = pow(ar, i - 1); \n                for (j in 1:(i - 1)) { \n                        mat[i, j] = gamma[i - j]; \n                        mat[j, i] = gamma[i - j]; \n                    } \n                } \n                return sigma^2 / (1 - ar^2) * mat; \n        }\n} \n      \n        data { \n             int&lt;lower=1&gt; n;  // total number of observations \n                 vector[n] y;  // response variable\n                 int&lt;lower=1&gt; nX;\n                     matrix[n,nX] X;\n           } \n             transformed data {\n                vector[n] se2 = rep_vector(0, n); \n             } \n             parameters { \n                vector[nX] beta;\n                    real&lt;lower=0&gt; sigma;  // residual SD \n                    real &lt;lower=-1,upper=1&gt; phi;  // autoregressive effects \n                } \n                transformed parameters { \n                } \n                model {\n                    matrix[n, n] res_cov_matrix;\n                    matrix[n, n] Sigma; \n                    vector[n] mu = X*beta;\n                    res_cov_matrix = cov_matrix_ar1(phi, sigma, n);\n                    Sigma = res_cov_matrix + diag_matrix(se2);\n                    Sigma = cholesky_decompose(Sigma); \n\n                    // priors including all constants\n                    beta ~ student_t(3,30,30);\n                    sigma ~ cauchy(0,5);\n                    y ~ multi_normal_cholesky(mu,Sigma);\n                } \n                generated quantities { \n                }\n  \n  \"\n\n## write the model to a text file\nwriteLines(stanString, con = \"tempModel2.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor variable, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat = model.matrix(~x, data.temporalCor)\ndata.temporalCor.list &lt;- with(data.temporalCor, list(y = y, X = Xmat,\n    n = nrow(data.temporalCor), nX = ncol(Xmat)))\n\nDefine the nodes (parameters and derivatives) to monitor and chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"phi\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1500\n\n\nNow compile and run the Stan code via the rstan interface.\n\ndata.temporalCor2.rstan &lt;- stan(data = data.temporalCor.list, file = \"tempModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.00024 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.4 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.097 seconds (Warm-up)\nNA Chain 1:                0.665 seconds (Sampling)\nNA Chain 1:                1.762 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.000209 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 1.176 seconds (Warm-up)\nNA Chain 2:                0.788 seconds (Sampling)\nNA Chain 2:                1.964 seconds (Total)\nNA Chain 2:\n\nprint(data.temporalCor2.rstan, par = c(\"beta\", \"sigma\", \"phi\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA          mean se_mean    sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 21.61    0.46 16.08 -8.13 12.64 20.76 29.68 55.24  1230 1.00\nNA beta[2]  0.22    0.00  0.10  0.01  0.15  0.22  0.29  0.42  1392 1.00\nNA sigma   12.02    0.03  1.31  9.85 11.06 11.92 12.82 14.89  1462 1.00\nNA phi      0.89    0.00  0.05  0.78  0.86  0.90  0.93  0.98  1131 1.01\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:40:28 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-08-acf-stan/index.html#mcmc-diagnostics-1",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nmcmc = As.mcmc.list(data.temporalCor2.rstan)\ndenplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"beta\", \"sigma\", \"phi\"))\n\n\n\n\n\n\n\n\n\n#Raftery diagnostic\nraftery.diag(mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\n\n\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n\nNA            beta[1]      beta[2]        sigma          phi         lp__\nNA Lag 0   1.00000000  1.000000000  1.000000000  1.000000000  1.000000000\nNA Lag 1   0.21452456  0.112473437  0.181672833  0.124546428  0.527409040\nNA Lag 5  -0.04090736  0.011068413 -0.001147177 -0.026057078  0.008368655\nNA Lag 10 -0.02972948 -0.061470014 -0.027729823 -0.008138883  0.027081616\nNA Lag 50 -0.01132752 -0.009636609  0.057698527  0.024742960 -0.018426417\n\n\n\nstan_ac(data.temporalCor2.rstan)\n\n\n\n\n\n\n\nstan_rhat(data.temporalCor2.rstan)\n\n\n\n\n\n\n\nstan_ess(data.temporalCor2.rstan)\n\n\n\n\n\n\n\n\nAll diagnostics seem fine."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#model-validation-1",
    "href": "tutorials/2020-02-08-acf-stan/index.html#model-validation-1",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Model validation",
    "text": "Model validation\nWhenever we fit a model that incorporates changes to the variance-covariance structures, we need to explore modified standardized residuals. In this case, the raw residuals should be updated to reflect the autocorrelation (subtract residual from previous time weighted by the autocorrelation parameter) before standardising by sigma.\n\\[\nRes_i = Y_i - \\mu_i\n\\]\n\\[\nRes_{i+1} = Res_{i+1} - \\rho Res_i\n\\]\n\\[\nRes_i = \\frac{Res_i}{\\sigma}\n\\]\n\nmcmc = as.matrix(data.temporalCor2.rstan)\nwch = grep(\"beta\", colnames(mcmc))\n# generate a model matrix\nnewdata = data.frame(x = data.temporalCor$x)\nXmat = model.matrix(~x, newdata)\n## get median parameter estimates\ncoefs = mcmc[, wch]\nfit = coefs %*% t(Xmat)\nresid = -1 * sweep(fit, 2, data.temporalCor$y, \"-\")\nn = ncol(resid)\nresid[, -1] = resid[, -1] - (resid[, -n] * mcmc[, \"phi\"])\nresid = apply(resid, 2, median)/median(mcmc[, \"sigma\"])\nfit = apply(fit, 2, median)\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\nggplot() + geom_point(data = NULL, aes(y = resid, x = data.temporalCor$x)) + theme_classic()\n\n\n\n\n\n\n\nggplot(data = NULL, aes(y = resid, x = data.temporalCor$year)) +\n    geom_point() + geom_line() + geom_hline(yintercept = 0, linetype = \"dashed\") + theme_classic()\n\n\n\n\n\n\n\nplot(acf(resid, lag = 40))\n\n\n\n\n\n\n\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data.temporalCor),\n    fit[i, ], mcmc[i, \"sigma\"]))\nggplot() + geom_density(data = NULL, aes(x = as.vector(yRep),\n    fill = \"Model\"), alpha = 0.5) + geom_density(data = data.temporalCor,\n    aes(x = y, fill = \"Obs\"), alpha = 0.5) + theme_classic()\n\n\n\n\n\n\n\n\nNo obvious autocorrelation or other issues with residuals remaining."
  },
  {
    "objectID": "tutorials/2020-02-08-acf-stan/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-08-acf-stan/index.html#parameter-estimates-1",
    "title": "Temporal Autocorrelation (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nExplore parameter estimates.\n\ntidyMCMC(data.temporalCor2.rstan, par = c(\"beta\", \"phi\", \"sigma\"),\n    conf.int = TRUE, conf.method = \"HPDinterval\", rhat = TRUE,\n    ess = TRUE)\n\nNA # A tibble: 4 × 7\nNA   term    estimate std.error conf.low conf.high  rhat   ess\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\nNA 1 beta[1]   21.6     16.1    -9.11       52.5   1.00   1230\nNA 2 beta[2]    0.220    0.104   0.00806     0.410 0.999  1392\nNA 3 phi        0.894    0.0541  0.795       0.990 1.01   1131\nNA 4 sigma     12.0      1.31    9.63       14.5   1.00   1462"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html",
    "title": "Nested Anova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#introduction",
    "title": "Nested Anova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nWhen single sampling units are selected amongst highly heterogeneous conditions, it is unlikely that these single units will adequately represent the populations and repeated sampling is likely to yield very different outcomes. For example, if we were investigating the impacts of fuel reduction burning across a highly heterogeneous landscape, our ability to replicate adequately might be limited by the number of burn sites available.\nAlternatively, sub-replicates within each of the sampling units (e.g. sites) can be collected (and averaged) so as to provided better representatives for each of the units and ultimately reduce the unexplained variability of the test of treatments. In essence, the sub-replicates are the replicates of an additional nested factor whose levels are nested within the main treatment factor. A nested factor refers to a factor whose levels are unique within each level of the factor it is nested within and each level is only represented once. For example, the fuel reduction burn study design could consist of three burnt sites and three un-burnt (control) sites each containing four quadrats (replicates of site and sub-replicates of the burn treatment). Each site represents a unique level of a random factor (any given site cannot be both burnt and un-burnt) that is nested within the fire treatment (burned or not).\nA nested design can be thought of as a hierarchical arrangement of factors (hence the alternative name hierarchical designs) whereby a treatment is progressively sub-replicated. As an additional example, imagine an experiment designed to comparing the leaf toughness of a number of tree species. Working down the hierarchy, five individual trees were randomly selected within (nested within) each species, three branches were randomly selected within each tree, two leaves were randomly selected within each branch and the force required to shear the leaf material in half (transversely) was measured in four random locations along the leaf. Clearly any given leaf can only be from a single branch, tree and species. Each level of sub-replication is introduced to further reduce the amount of unexplained variation and thereby increasing the power of the test for the main treatment effect. Additionally, it is possible to investigate which scale has the greatest (or least, etc) degree of variability - the level of the species, individual tree, branch, leaf, leaf region etc.\n\nNested factors are typically random factors, of which the levels are randomly selected to represent all possible levels (e.g. sites). When the main treatment effect (often referred to as Factor A) is a fixed factor, such designs are referred to as a mixed model nested ANOVA, whereas when Factor A is random, the design is referred to as a Model II nested ANOVA.\nFixed nested factors are also possible. For example, specific dates (corresponding to particular times during a season) could be nested within season. When all factors are fixed, the design is referred to as a Model I mixed model.\nFully nested designs (the topic of this chapter) differ from other multi-factor designs in that all factors within (below) the main treatment factor are nested and thus interactions are un-replicated and cannot be tested. Indeed, interaction effects (interaction between Factor A and site) are assumed to be zero."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-models-frequentist",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-models-frequentist",
    "title": "Nested Anova (Stan)",
    "section": "Linear models (frequentist)",
    "text": "Linear models (frequentist)\nThe linear models for two and three factor nested design are:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk},\n\\]\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + gamma_{k(j(i))}  + \\epsilon_{ijkl},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the effect of Factor B, \\(\\gamma\\) is the effect of Factor C and \\(\\epsilon\\) is the random unexplained or residual component."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-models-bayesian",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-models-bayesian",
    "title": "Nested Anova (Stan)",
    "section": "Linear models (Bayesian)",
    "text": "Linear models (Bayesian)\nSo called “random effects” are modelled differently from “fixed effects” in that rather than estimate their individual effects, we instead estimate the variability due to these “random effects”. Since technically all variables in a Bayesian framework are random, some prefer to use the terms ‘fixed effects’ and ‘varying effects’. As random factors typically represent “random” selections of levels (such as a set of randomly selected sites), incorporated in order to account for the dependency structure (observations within sites are more likely to be correlated to one another - not strickly independent) to the data, we are not overly interested in the individual differences between levels of the ‘varying’ (random) factor. Instead (in addition to imposing a separate correlation structure within each nest), we want to know how much variability is attributed to this level of the design. The linear models for two and three factor nested design are:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}, \\;\\;\\; \\epsilon_{ijk} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B})\n\\]\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_{j(i)} + \\gamma_{k(j(i))} + \\epsilon_{ijkl}, \\;\\;\\; \\epsilon_{ijkl} \\sim N(0, \\sigma^2), \\;\\;\\; \\beta_{j(i)} \\sim N(0, \\sigma^2_{B}) \\;\\;\\; \\gamma_{k(j(i))} \\sim N(0, \\sigma^2_C)\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\alpha\\) is the effect of Factor A, \\(\\beta\\) is the variability of Factor B (nested within Factor A), \\(\\gamma\\) is the variability of Factor C (nested within Factor B) and \\(\\epsilon\\) is the random unexplained or residual component that is assumed to be normally distributed with a mean of zero and a constant amount of standard deviation (\\(\\sigma^2\\)). The subscripts are iterators. For example, the \\(i\\) represents the number of effects to be estimated for Factor A. Thus the first formula can be read as the \\(k\\)-th observation of \\(y\\) is drawn from a normal distribution (with a specific level of variability) and mean proposed to be determined by a base mean (\\(\\mu\\) - mean of the first treatment across all nests) plus the effect of the \\(i\\)-th treatment effect plus the variabilitythe model proposes that, given a base mean (\\(\\mu\\)) and knowing the effect of the \\(i\\)-th treatment (factor A) and which of the \\(j\\)-th nests within the treatment the \\(k\\)-th observation from Block \\(j\\) (factor B) within treatment effect."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#null-hypotheses",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#null-hypotheses",
    "title": "Nested Anova (Stan)",
    "section": "Null hypotheses",
    "text": "Null hypotheses\nSeparate null hypotheses are associated with each of the factors, however, nested factors are typically only added to absorb some of the unexplained variability and thus, specific hypotheses tests associated with nested factors are of lesser biological importance. Hence, rather than estimate the effects of random effects, we instead estimate how much variability they contribute.\nFactor A: the main treatment effect (fixed)\n\n\\(H_0(A): \\mu_1=\\mu_2=\\ldots=\\mu_i=\\mu\\) (the population group means are all equal). That is, that the mean of population \\(1\\) is equal to that of population \\(2\\) and so on, and thus all population means are equal to one another - no effect of the factor on the response. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group mean and the mean of the first group (\\(\\alpha_i=\\mu_i-\\mu_1\\)) then the \\(H_0\\) can alternatively be written as:\n\\(H_0(A) : \\alpha_1=\\alpha_2=\\ldots=\\alpha_i=0\\) (the effect of each group equals zero). If one or more of the \\(\\alpha_i\\) are different from zero (the response mean for this treatment differs from the overall response mean), there is evidence that the null hypothesis is not true indicating that the factor does affect the response variable.\n\nFactor A: the main treatment effect (random)\n\n\\(H_0(A) : \\sigma^2_{\\alpha}=0\\) (population variance equals zero). There is no added variance due to all possible levels of A.\n\nFactor B: the nested effect (random)\n\n\\(H_0(B) : \\sigma^2_{\\beta}=0\\) (population variance equals zero). There is no added variance due to all possible levels of B within the (set or all possible) levels of A.\n\nFactor B: the nested effect (fixed)\n\n\\(H_0(B): \\mu_{1(1)}=\\mu_{2(1)}=\\ldots=\\mu_{j(i)}=\\mu\\) (the population group means of B (within A) are all equal).\n\\(H_0(B): \\beta_{1(1)}=\\beta_{2(1)}=\\ldots=\\beta_{j(i)}=0\\) (the effect of each chosen B group equals zero)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#analysis-of-variance",
    "title": "Nested Anova (Stan)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nAnalysis of variance sequentially partitions the total variability in the response variable into components explained by each of the factors (starting with the factors lowest down in the hierarchy - the most deeply nested) and the components unexplained by each factor. Explained variability is calculated by subtracting the amount unexplained by the factor from the amount unexplained by a reduced model that does not contain the factor. When the null hypothesis for a factor is true (no effect or added variability), the ratio of explained and unexplained components for that factor (F-ratio) should follow a theoretical F-distribution with an expected value less than 1. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different combinations of fixed and random factors in a nested linear model (see Table below).\n\nfact_anova_table\n\nNA       df        MS         F-ratio (B random)    Var comp (B random)        \nNA A     \"a-1\"     \"MS A\"     \"(MS A)/(MS B'(A))\"   \"((MS A) - (MS B'(A)))/nb\" \nNA B'(A) \"(b-1)a\"  \"MS B'(A)\" \"(MS B'(A))/(MS res)\" \"((MS B'(A)) - (MS res))/n\"\nNA Res   \"(n-1)ba\" \"MS res\"   \"\"                    \"\"                         \nNA       F-ratio (B fixed)     Var comp (B fixed)         \nNA A     \"(MS A)/(MS res)\"     \"((MS A) - (MS res))/nb\"   \nNA B'(A) \"(MS B'(A))/(MS res)\" \"((MS B'(A)) - (MS res))/n\"\nNA Res   \"\"                    \"\"\n\n\nThe corresponding R syntax is given below.\n\n#A fixed/random, B random (balanced)\nsummary(aov(y~A+Error(B), data))\nVarCorr(lme(y~A,random=1|B, data))\n\n#A fixed/random, B random (unbalanced)\nanova(lme(y~A,random=1|B, data), type='marginal')\n\n#A fixed/random, B fixed(balanced)\nsummary(aov(y~A+B, data))\n\n#A fixed/random, B fixed (unbalanced)\ncontrasts(data$B) &lt;- contr.sum\nAnova(aov(y~A/B, data), type='III')"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#variance-components",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#variance-components",
    "title": "Nested Anova (Stan)",
    "section": "Variance components",
    "text": "Variance components\nAs previously alluded to, it can often be useful to determine the relative contribution (to explaining the unexplained variability) of each of the factors as this provides insights into the variability at each different scales. These contributions are known as Variance components and are estimates of the added variances due to each of the factors. For consistency with leading texts on this topic, I have included estimated variance components for various balanced nested ANOVA designs in the above table. However, variance components based on a modified version of the maximum likelihood iterative model fitting procedure (REML) is generally recommended as this accommodates both balanced and unbalanced designs. While there are no numerical differences in the calculations of variance components for fixed and random factors, fixed factors are interpreted very differently and arguably have little clinical meaning (other to infer relative contribution). For fixed factors, variance components estimate the variance between the means of the specific populations that are represented by the selected levels of the factor and therefore represent somewhat arbitrary and artificial populations. For random factors, variance components estimate the variance between means of all possible populations that could have been selected and thus represents the true population variance."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#assumptions",
    "title": "Nested Anova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAn F-distribution represents the relative frequencies of all the possible F-ratio’s when a given null hypothesis is true and certain assumptions about the residuals (denominator in the F-ratio calculation) hold. Consequently, it is also important that diagnostics associated with a particular hypothesis test reflect the denominator for the appropriate F-ratio. For example, when testing the null hypothesis that there is no effect of Factor A (\\(H_0(A):\\alpha_i=0\\)) in a mixed nested ANOVA, the means of each level of Factor B are used as the replicates of Factor A. As with single factor anova, hypothesis testing for nested ANOVA assumes the residuals are:\n\nnormally distributed. Factors higher up in the hierarchy of a nested model are based on means (or means of means) of lower factors and thus the Central Limit Theory would predict that normality will usually be satisfied for the higher level factors. Nevertheless, boxplots using the appropriate scale of replication should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another - this requires special consideration so as to ensure that the scale at which sub-replicates are measured is still great enough to enable observations to be independent."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#unbalanced-nested-designs",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#unbalanced-nested-designs",
    "title": "Nested Anova (Stan)",
    "section": "Unbalanced nested designs",
    "text": "Unbalanced nested designs\nDesigns that incorporate fixed and random factors (either nested or factorial), involve F-ratio calculations in which the denominators are themselves random factors other than the overall residuals. Many statisticians argue that when such denominators are themselves not statistically significant (at the \\(0.25\\) level), there are substantial power benefits from pooling together successive non-significant denominator terms. Thus an F-ratio for a particular factor might be recalculated after pooling together its original denominator with its denominators denominator and so on. The conservative \\(0.25\\) is used instead of the usual 0.05 to reduce further the likelihood of Type II errors (falsely concluding an effect is non-significant - that might result from insufficient power).\nFor a simple completely balanced nested ANOVA, it is possible to pool together (calculate their mean) each of the sub-replicates within each nest (site) and then perform single factor ANOVA on those aggregates. Indeed, for a balanced design, the estimates and hypothesis for Factor A will be identical to that produced via nested ANOVA. However, if there are an unequal number of sub-replicates within each nest, then the single factor ANOVA will be less powerful that a proper nested ANOVA. Unbalanced designs are those designs in which sample (subsample) sizes for each level of one or more factors differ. These situations are relatively common in biological research, however such imbalance has some important implications for nested designs.\nFirstly, hypothesis tests are more robust to the assumptions of normality and equal variance when the design is balanced. Secondly (and arguably, more importantly), the model contrasts are not orthogonal (independent) and the sums of squares component attributed to each of the model terms cannot be calculated by simple additive partitioning of the total sums of squares. In such situations, exact F-ratios cannot be constructed (at least in theory), variance components calculations are more complicated and significance tests cannot be computed. The denominator MS in an F-ratio is determined by examining the expected value of the mean squares of each term in a model. Unequal sample sizes result in expected means squares for which there are no obvious logical comparators that enable the impact of an individual model term to be isolated. The severity of this issue depends on which scale of the sub-sampling hierarchy the unbalance(s) occurs as well whether the unbalance occurs in the replication of a fixed or random factor. For example, whilst unequal levels of the first nesting factor (e.g. unequal number of burn vs un-burnt sites) has no effect on F-ratio construction or hypothesis testing for the top level factor (irrespective of whether either of the factors are fixed or random), unequal sub-sampling (replication) at the level of a random (but not fixed) nesting factor will impact on the ability to construct F-ratios and variance components of all terms above it in the hierarchy. There are a number of alternative ways of dealing with unbalanced nested designs. All alternatives assume that the imbalance is not a direct result of the treatments themselves. Such outcomes are more appropriately analysed by modelling the counts of surviving observations via frequency analysis.\n\nSplit the analysis up into separate smaller simple ANOVA’s each using the means of the nesting factor to reflect the appropriate scale of replication. As the resulting sums of squares components are thereby based on an aggregated dataset the analyses then inherit the procedures and requirements of single ANOVA.\nAdopt mixed-modelling techniques.\n\nWe note that, in a Bayesian framework, issues of design balance essentially evaporate."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-mixed-effects-models",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#linear-mixed-effects-models",
    "title": "Nested Anova (Stan)",
    "section": "Linear mixed effects models",
    "text": "Linear mixed effects models\nAlthough the term “mixed-effects” can be used to refer to any design that incorporates both fixed and random predictors, its use is more commonly restricted to designs in which factors are nested or grouped within other factors. Typical examples include nested, longitudinal (measurements repeated over time) data, repeated measures and blocking designs. Furthermore, rather than basing parameter estimations on observed and expected mean squares or error strata (as outline above), mixed-effects models estimate parameters via maximum likelihood (ML) or residual maximum likelihood (REML). In so doing, mixed-effects models more appropriately handle estimation of parameters, effects and variance components of unbalanced designs (particularly for random effects). Resulting fitted (or expected) values of each level of a factor (for example, the expected population site means) are referred to as Best Linear Unbiased Predictors (BLUP’s). As an acknowledgement that most estimated site means will be more extreme than the underlying true population means they estimate (based on the principle that smaller sample sizes result in greater chances of more extreme observations and that nested sub-replicates are also likely to be highly intercorrelated), BLUP’s are less spread from the overall mean than are simple site means. In addition, mixed-effects models naturally model the “within-block” correlation structure that complicates many longitudinal designs.\nWhilst the basic concepts of mixed-effects models have been around for a long time, recent computing advances and adoptions have greatly boosted the popularity of these procedures. Linear mixed effects models are currently at the forefront of statistical development, and as such, are very much a work in progress - both in theory and in practice. Recent developments have seen a further shift away from the traditional practices associated with degrees of freedom, probability distribution and p-value calculations. The traditional approach to inference testing is to compare the fit of an alternative (full) model to a null (reduced) model (via an F-ratio). When assumptions of normality and homogeneity of variance apply, the degrees of freedom are easily computed and the F-ratio has an exact F-distribution to which it can be compared. However, this approach introduces two additional problematic assumptions when estimating fixed effects in a mixed effects model. Firstly, when estimating the effects of one factor, the parameter estimates associated with other factor(s) are assumed to be the true values of those parameters (not estimates). Whilst this assumption is reasonable when all factors are fixed, as random factors are selected such that they represent one possible set of levels drawn from an entire population of possible levels for the random factor, it is unlikely that the associated parameter estimates accurately reflect the true values. Consequently, there is not necessarily an appropriate F-distribution. Furthermore, determining the appropriate degrees of freedom (nominally, the number of independent observations on which estimates are based) for models that incorporate a hierarchical structure is only possible under very specific circumstances (such as completely balanced designs). Degrees of freedom is a somewhat arbitrary defined concept used primarily to select a theoretical probability distribution on which a statistic can be compared. Arguably, however, it is a concept that is overly simplistic for complex hierarchical designs. Most statistical applications continue to provide the “approximate” solutions (as did earlier versions within R). However, R linear mixed effects development leaders argue strenuously that given the above shortcomings, such approximations are variably inappropriate and are thus omitted.\nMarkov chain Monte Carlo (MCMC) sampling methods provide a Bayesian-like alternative for inference testing. Markov chains use the mixed model parameter estimates to generate posterior probability distributions of each parameter from which Monte Carlo sampling methods draw a large set of parameter samples. These parameter samples can then be used to calculate highest posterior density (HPD) intervals (also known as Bayesian credible intervals). Such intervals indicate the interval in which there is a specified probability (typically \\(95\\)%) that the true population parameter lies. Furthermore, whilst technically against the spirit of the Bayesian philosophy, it is also possible to generate P values on which to base inferences."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#exploratory-data-analysis",
    "title": "Nested Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n#Effects of treatment\nboxplot(y~A, ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Site effects\nboxplot(y~Sites, ddply(data.nest, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n## with ggplot2\nggplot(ddply(data.nest, ~A+Sites,numcolwise(mean, na.rm=T)), aes(y=y, x=A)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#full-means-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#full-means-parameterisation",
    "title": "Nested Anova (Stan)",
    "section": "Full means parameterisation",
    "text": "Full means parameterisation\n\nrstanString=\"\ndata{\n   int n;\n   int nA;\n   int nB;\n   vector [n] y;\n   int A[n];\n   int B[n];\n}\n\nparameters{\n  real alpha[nA];\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha ~ normal( 0 , 100 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha[A[i]] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"fullModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.nest.list &lt;- with(data.nest, list(y=y, A=as.numeric(A), B=as.numeric(Sites),\n  n=nrow(data.nest), nB=length(levels(Sites)),nA=length(levels(A))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\nlibrary(rstan)\n\nNow run the Stan code via the rstan interface.\n\ndata.nest.rstan.c &lt;- stan(data = data.nest.list, file = \"fullModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.682 seconds (Warm-up)\nNA Chain 1:                0.37 seconds (Sampling)\nNA Chain 1:                1.052 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.659 seconds (Warm-up)\nNA Chain 2:                0.377 seconds (Sampling)\nNA Chain 2:                1.036 seconds (Total)\nNA Chain 2:\n\nprint(data.nest.rstan.c, par = c(\"alpha\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha[1] 41.90    0.19 5.62 30.38 38.19 41.93 45.39 53.23   866    1\nNA alpha[2] 69.63    0.18 5.43 59.38 66.03 69.66 73.13 80.67   865    1\nNA alpha[3] 82.69    0.17 5.49 71.08 79.24 82.76 86.26 93.08  1020    1\nNA sigma     5.04    0.01 0.30  4.49  4.82  5.03  5.23  5.66  1693    1\nNA sigma_B  11.73    0.08 2.78  7.76  9.78 11.22 13.12 18.49  1174    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:41:33 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest.rstan.c.df &lt;-as.data.frame(extract(data.nest.rstan.c))\nhead(data.nest.rstan.c.df)\n\nNA    alpha.1  alpha.2  alpha.3    sigma   sigma_B      lp__\nNA 1 34.66394 74.84391 83.33568 4.783857 11.030622 -355.9341\nNA 2 38.10838 72.29916 82.46015 5.186627 10.425868 -355.7169\nNA 3 33.98648 69.14414 95.22563 5.515138 11.765572 -360.1139\nNA 4 35.26812 65.90908 84.96725 4.967946  7.948799 -354.9913\nNA 5 30.38241 77.09534 84.52162 4.453866 12.540387 -358.2463\nNA 6 40.79643 65.72887 86.83717 4.993259  8.487666 -353.6030"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#full-effect-parameterisation",
    "title": "Nested Anova (Stan)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nrstan2String=\"\ndata{\n   int n;\n   int nB;\n   vector [n] y;\n   int A2[n];\n   int A3[n];\n   int B[n];\n}\n\nparameters{\n  real alpha0;\n  real alpha2;\n  real alpha3;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha0 ~ normal( 0 , 100 );\n    alpha2 ~ normal( 0 , 100 );\n    alpha3 ~ normal( 0 , 100 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha0 + alpha2*A2[i] + \n               alpha3*A3[i] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstan2String, con = \"full2Model.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA2 &lt;- ifelse(data.nest$A=='a2',1,0)\nA3 &lt;- ifelse(data.nest$A=='a3',1,0)\ndata.nest.list &lt;- with(data.nest, list(y=y, A2=A2, A3=A3, B=as.numeric(Sites),\n   n=nrow(data.nest), nB=length(levels(Sites))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha0\",\"alpha2\",\"alpha3\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.nest.rstan.c2 &lt;- stan(data = data.nest.list, file = \"full2Model.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.48 seconds (Warm-up)\nNA Chain 1:                0.731 seconds (Sampling)\nNA Chain 1:                2.211 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.4e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 1.632 seconds (Warm-up)\nNA Chain 2:                1.047 seconds (Sampling)\nNA Chain 2:                2.679 seconds (Total)\nNA Chain 2:\n\nprint(data.nest.rstan.c2, par = c(\"alpha0\", \"alpha2\", \"alpha3\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha0  42.46    0.19 5.30 32.37 39.01 42.53 45.80 52.98   792    1\nNA alpha2  27.25    0.28 7.67 11.75 22.68 27.36 32.25 41.95   750    1\nNA alpha3  40.71    0.26 7.55 25.57 35.91 40.46 45.42 55.99   850    1\nNA sigma    5.04    0.01 0.31  4.47  4.81  5.03  5.25  5.70  1239    1\nNA sigma_B 11.54    0.09 2.65  7.69  9.70 11.09 12.92 18.04   779    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:42:12 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest.rstan.c2.df &lt;-as.data.frame(extract(data.nest.rstan.c2))\nhead(data.nest.rstan.c2.df)\n\nNA     alpha0   alpha2   alpha3    sigma  sigma_B      lp__\nNA 1 47.83462 26.07003 29.50999 5.543406 13.06805 -355.4811\nNA 2 45.27770 25.90933 37.47072 5.470879 12.19998 -353.8504\nNA 3 40.49191 35.16179 39.21332 5.767743 11.64687 -358.2453\nNA 4 45.51317 26.64989 36.41803 5.613998 10.87368 -360.0820\nNA 5 47.70665 21.64392 33.10168 4.785280 11.33905 -353.9069\nNA 6 31.05164 43.27888 45.95428 4.722624 12.17590 -356.0434"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#matrix-parameterisation",
    "title": "Nested Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nB;\n   int nA;\n   vector [n] y;\n   matrix [n,nA] X;\n   int B[n];\n   vector [nA] a0;\n   matrix [nA,nA] A0;\n}\n\nparameters{\n  vector [nA] alpha;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    //alpha ~ normal( 0 , 100 );\n    alpha ~ multi_normal(a0,A0);\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25);\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = dot_product(X[i],alpha) + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX &lt;- model.matrix(~A, data.nest)\nnA &lt;- ncol(X)\ndata.nest.list &lt;- with(data.nest, list(y=y, X=X, B=as.numeric(Sites),\n   n=nrow(data.nest), nB=length(levels(Sites)), nA=nA,\n   a0=rep(0,nA), A0=diag(100000,nA)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.nest.rstan.m &lt;- stan(data = data.nest.list, file = \"matrixModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 4.1e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.714 seconds (Warm-up)\nNA Chain 1:                1.076 seconds (Sampling)\nNA Chain 1:                2.79 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.4e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 2.052 seconds (Warm-up)\nNA Chain 2:                1.185 seconds (Sampling)\nNA Chain 2:                3.237 seconds (Total)\nNA Chain 2:\n\nprint(data.nest.rstan.m, par = c(\"alpha\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha[1] 41.97    0.21 5.56 30.65 38.51 41.99 45.45 53.17   700    1\nNA alpha[2] 27.79    0.27 7.79 12.57 22.80 27.76 32.78 43.27   860    1\nNA alpha[3] 41.44    0.28 7.77 25.51 36.54 41.54 46.56 56.74   782    1\nNA sigma     5.04    0.01 0.32  4.47  4.81  5.03  5.24  5.71  1484    1\nNA sigma_B  11.58    0.08 2.63  7.53  9.70 11.16 13.06 17.69  1012    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:42:50 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest.rstan.m.df &lt;-as.data.frame(extract(data.nest.rstan.m))\nhead(data.nest.rstan.m.df)\n\nNA    alpha.1  alpha.2  alpha.3    sigma   sigma_B      lp__\nNA 1 38.51260 28.65160 45.31224 5.234954  9.437163 -352.1193\nNA 2 42.44434 27.88099 30.81416 4.885081 10.432388 -354.4981\nNA 3 45.66044 15.23036 31.42478 4.916171 18.635395 -357.9730\nNA 4 38.10635 29.53828 46.86861 4.890449  9.377387 -363.9728\nNA 5 45.01428 21.53882 31.16592 4.814843  9.749733 -355.6716\nNA 6 37.51673 39.07947 51.74908 4.770156  8.408097 -359.3753"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#hierarchical-parameterisation",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#hierarchical-parameterisation",
    "title": "Nested Anova (Stan)",
    "section": "Hierarchical parameterisation",
    "text": "Hierarchical parameterisation\n\nrstanString3=\"\ndata{\n   int n;\n   int nA;\n   int nSites;\n   vector [n] y;\n   matrix [nSites,nA] X;\n   matrix [n,nSites] Z;\n}\n\nparameters{\n   vector[nA] beta;\n   vector[nSites] gamma;\n   real&lt;lower=0&gt; sigma;\n   real&lt;lower=0&gt; sigma_S;\n   \n}\n \nmodel{\n    vector [n] mu_site;\n    vector [nSites] mu;\n\n    // Priors\n    beta ~ normal( 0 , 1000 );\n    gamma ~ normal( 0 , 1000 );\n    sigma ~ cauchy( 0 , 25 );\n    sigma_S~ cauchy( 0 , 25 );\n\n    mu_site = Z*gamma;\n    y ~ normal( mu_site , sigma );\n    mu = X*beta;\n    gamma ~ normal(mu, sigma_S);\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString3, con = \"hierarchicalModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndt.A &lt;- ddply(data.nest,~Sites,catcolwise(unique))\nX&lt;-model.matrix(~A, dt.A)\nZ&lt;-model.matrix(~Sites-1, data.nest)\ndata.nest.list &lt;- list(y=data.nest$y, X=X, Z=Z, n=nrow(data.nest),\n  nSites=nrow(X),nA=ncol(X))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",\"sigma\",\"sigma_S\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.nest.rstan.h &lt;- stan(data = data.nest.list, file = \"hierarchicalModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.248 seconds (Warm-up)\nNA Chain 1:                0.089 seconds (Sampling)\nNA Chain 1:                0.337 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.242 seconds (Warm-up)\nNA Chain 2:                0.103 seconds (Sampling)\nNA Chain 2:                0.345 seconds (Total)\nNA Chain 2:\n\nprint(data.nest.rstan.h, par = c(\"beta\", \"sigma\", \"sigma_S\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 42.39    0.13 5.42 31.60 38.77 42.40 45.95 52.81  1758    1\nNA beta[2] 27.34    0.17 7.66 12.31 22.48 27.20 32.26 42.77  2089    1\nNA beta[3] 40.91    0.17 7.74 25.75 35.93 40.89 45.73 56.32  2100    1\nNA sigma    5.04    0.01 0.31  4.49  4.83  5.03  5.24  5.69  3058    1\nNA sigma_S 11.51    0.05 2.62  7.59  9.65 11.13 12.86 17.92  2297    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:43:22 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest.rstan.h.df &lt;-as.data.frame(extract(data.nest.rstan.h))\nhead(data.nest.rstan.h.df)\n\nNA     beta.1   beta.2   beta.3    sigma   sigma_S      lp__\nNA 1 37.97267 19.74559 46.25986 4.486942 11.265958 -355.4335\nNA 2 43.04120 22.90106 41.98754 5.236119  9.358648 -350.9895\nNA 3 41.08867 34.12170 42.33801 5.183215 12.442448 -354.5326\nNA 4 50.68459 16.02613 25.41038 5.002917 10.957330 -357.6446\nNA 5 47.11724 15.21674 35.28746 5.426139 11.288743 -357.4966\nNA 6 43.36194 26.12687 34.56285 5.336729  8.833431 -355.4301\n\n\nIf you want to include finite-population standard deviations in the model you can use the following code.\n\nrstanString4=\"\ndata{\n   int n;\n   int nA;\n   int nSites;\n   vector [n] y;\n   matrix [nSites,nA] X;\n   matrix [n,nSites] Z;\n}\n\nparameters{\n   vector[nA] beta;\n   vector[nSites] gamma;\n   real&lt;lower=0&gt; sigma;\n   real&lt;lower=0&gt; sigma_S;\n   \n}\n\nmodel{\n    vector [n] mu_site;\n    vector [nSites] mu;\n\n    // Priors\n    beta ~ normal( 0 , 1000 );\n    gamma ~ normal( 0 , 1000 );\n    sigma ~ cauchy( 0 , 25 );\n    sigma_S~ cauchy( 0 , 25 );\n\n    mu_site = Z*gamma;\n    y ~ normal( mu_site , sigma );\n    mu = X*beta;\n    gamma ~ normal(mu, sigma_S);\n}\n\ngenerated quantities {\n    vector [n] mu_site;\n    vector [nSites] mu;\n    vector [n] y_err;\n    real sd_y;\n    vector [nSites] mu_site_err;\n    real sd_site;\n    real sd_A;\n    \n    mu_site = Z*gamma;\n    y_err = mu_site - y;\n    sd_y = sd(y_err);\n\n    mu = X*beta;\n    mu_site_err = mu - gamma;\n    sd_site = sd(mu_site_err);\n\n    sd_A = sd(beta);\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString4, con = \"SDModel.stan\")\n\n#data list\ndt.A &lt;- ddply(data.nest,~Sites,catcolwise(unique))\nX&lt;-model.matrix(~A, dt.A)\nZ&lt;-model.matrix(~Sites-1, data.nest)\ndata.nest.list &lt;- list(y=data.nest$y, X=X, Z=Z, n=nrow(data.nest),\n   nSites=nrow(X),nA=ncol(X))\n\n#parameters and chain details\nparams &lt;- c('beta','sigma','sigma_S','sd_A','sd_site','sd_y')\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest.rstan.SD &lt;- stan(data = data.nest.list, file = \"SDModel.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.249 seconds (Warm-up)\nNA Chain 1:                0.115 seconds (Sampling)\nNA Chain 1:                0.364 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.1e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.247 seconds (Warm-up)\nNA Chain 2:                0.113 seconds (Sampling)\nNA Chain 2:                0.36 seconds (Total)\nNA Chain 2:\n\nprint(data.nest.rstan.SD, par = c('beta','sigma','sigma_S','sd_A','sd_site','sd_y'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 42.38    0.10 5.21 31.59 39.13 42.35 45.65 52.59  2861    1\nNA beta[2] 27.42    0.13 7.35 12.38 22.67 27.53 32.19 42.08  3205    1\nNA beta[3] 40.80    0.14 7.63 25.26 36.06 40.92 45.51 55.99  3090    1\nNA sigma    5.04    0.01 0.31  4.48  4.83  5.03  5.25  5.70  3330    1\nNA sigma_S 11.53    0.05 2.61  7.70  9.61 11.11 12.96 17.74  2592    1\nNA sd_A    10.20    0.09 4.36  2.89  7.14  9.79 12.63 20.38  2148    1\nNA sd_site 10.67    0.03 1.08  9.27  9.95 10.43 11.12 13.40  1385    1\nNA sd_y     5.00    0.00 0.10  4.85  4.93  4.99  5.06  5.23  1269    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:43:55 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest.rstan.SD.df &lt;-as.data.frame(extract(data.nest.rstan.SD))\nhead(data.nest.rstan.SD.df)\n\nNA     beta.1   beta.2   beta.3    sigma  sigma_S     sd_A   sd_site     sd_y\nNA 1 44.26601 29.36915 37.10326 5.296506 13.05594 7.450252  9.905245 4.890486\nNA 2 42.18142 32.66489 37.72024 5.022314 12.72271 4.761357 10.325405 4.927700\nNA 3 39.63720 28.41220 44.19112 4.902748  8.36598 8.121100  9.584279 4.940438\nNA 4 39.76594 39.67555 47.83435 4.457138 13.66167 4.684609 11.376652 4.976266\nNA 5 35.87241 34.38392 37.95117 5.496118 12.26217 1.791749 10.397791 4.946690\nNA 6 38.79789 31.78442 45.31698 5.063085 10.51038 6.767781 10.070051 4.980664\nNA        lp__\nNA 1 -352.4128\nNA 2 -352.8046\nNA 3 -352.5025\nNA 4 -357.9645\nNA 5 -356.4320\nNA 6 -354.2973"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#exploratory-data-analysis-1",
    "title": "Nested Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n#Effects of treatment\nboxplot(y~A, ddply(data.nest1, ~A+Sites,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Site effects\nboxplot(y~Sites, ddply(data.nest1, ~A+Sites+Quads,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n#Quadrat effects\nboxplot(y~Quads, ddply(data.nest1, ~A+Sites+Quads+Pits,numcolwise(mean, na.rm=T)))\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity.\nit is a little difficult to assess normality/homogeneity of variance of quadrats since there are only two pits per quadrat. Nevertheless, there is no suggestion that variance increases with increasing mean.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed)."
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#frequentist-for-comparison",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#frequentist-for-comparison",
    "title": "Nested Anova (Stan)",
    "section": "Frequentist for comparison",
    "text": "Frequentist for comparison\n\nlibrary(nlme)\nd.lme &lt;- lme(y ~ A, random=~1|Sites/Quads,data=data.nest1)\nsummary(d.lme)\n\nNA Linear mixed-effects model fit by REML\nNA   Data: data.nest1 \nNA        AIC      BIC   logLik\nNA   1137.994 1155.937 -562.997\nNA \nNA Random effects:\nNA  Formula: ~1 | Sites\nNA         (Intercept)\nNA StdDev:    10.38249\nNA \nNA  Formula: ~1 | Quads %in% Sites\nNA         (Intercept) Residual\nNA StdDev:    8.441617 7.161177\nNA \nNA Fixed effects:  y ~ A \nNA                Value Std.Error DF  t-value p-value\nNA (Intercept) 41.38646  5.043341 75 8.206159  0.0000\nNA Aa2         21.36271  7.132361 12 2.995180  0.0112\nNA Aa3         39.14584  7.132361 12 5.488482  0.0001\nNA  Correlation: \nNA     (Intr) Aa2   \nNA Aa2 -0.707       \nNA Aa3 -0.707  0.500\nNA \nNA Standardized Within-Group Residuals:\nNA         Min          Q1         Med          Q3         Max \nNA -2.11852502 -0.54600753 -0.03428581  0.53382436  2.26256392 \nNA \nNA Number of Observations: 150\nNA Number of Groups: \nNA            Sites Quads %in% Sites \nNA               15               75\n\nanova(d.lme)\n\nNA             numDF denDF  F-value p-value\nNA (Intercept)     1    75 446.9150  &lt;.0001\nNA A               2    12  15.1037   5e-04"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#full-effect-parameterisation-1",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#full-effect-parameterisation-1",
    "title": "Nested Anova (Stan)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nrstanString=\"\ndata{\n   int n;\n   int nSite;\n   int nQuad;\n   vector [n] y;\n   int A2[n];\n   int A3[n];\n   int Site[n];\n   int Quad[n];\n}\n\nparameters{\n  real alpha0;\n  real alpha2;\n  real alpha3;\n  real&lt;lower=0&gt; sigma;\n  vector [nSite] beta_Site;\n  real&lt;lower=0&gt; sigma_Site;\n  vector [nQuad] beta_Quad;\n  real&lt;lower=0&gt; sigma_Quad;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha0 ~ normal( 0 , 100 );\n    alpha2 ~ normal( 0 , 100 );\n    alpha3 ~ normal( 0 , 100 );\n    beta_Site~ normal( 0 , sigma_Site );\n    sigma_Site ~ cauchy( 0 , 25 );\n    beta_Quad~ normal( 0 , sigma_Quad );\n    sigma_Quad ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha0 + alpha2*A2[i] + \n               alpha3*A3[i] + beta_Site[Site[i]] + beta_Quad[Quad[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"fullModel2.stan\")\n\nA2 &lt;- ifelse(data.nest1$A=='a2',1,0)\nA3 &lt;- ifelse(data.nest1$A=='a3',1,0)\ndata.nest.list &lt;- with(data.nest1, list(y=y, A2=A2, A3=A3, Site=as.numeric(Sites),\n   n=nrow(data.nest1), nSite=length(levels(Sites)),\n   nQuad=length(levels(Quads)), Quad=as.numeric(Quads)))\n\nparams &lt;- c('alpha0','alpha2','alpha3','sigma','sigma_Site', 'sigma_Quad')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest1.rstan.f &lt;- stan(data = data.nest.list, file = \"fullModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.4e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 3.081 seconds (Warm-up)\nNA Chain 1:                2.472 seconds (Sampling)\nNA Chain 1:                5.553 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.9e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 2.708 seconds (Warm-up)\nNA Chain 2:                2.875 seconds (Sampling)\nNA Chain 2:                5.583 seconds (Total)\nNA Chain 2:\n\nprint(data.nest1.rstan.f, par = c('alpha0','alpha2','alpha3','sigma','sigma_Site', 'sigma_Quad'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA             mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha0     41.57    0.14 5.72 30.27 37.98 41.42 45.26 52.74  1614    1\nNA alpha2     21.23    0.21 8.21  5.01 16.18 21.30 26.33 37.63  1515    1\nNA alpha3     38.58    0.19 7.88 21.98 33.85 38.54 43.57 54.66  1750    1\nNA sigma       7.28    0.01 0.61  6.18  6.86  7.25  7.69  8.56  1892    1\nNA sigma_Site 11.33    0.07 3.00  6.76  9.28 10.96 12.94 18.46  1937    1\nNA sigma_Quad  8.60    0.03 1.16  6.53  7.80  8.54  9.34 11.06  1688    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:44:39 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest1.rstan.f.df &lt;-as.data.frame(extract(data.nest1.rstan.f))\nhead(data.nest1.rstan.f.df)\n\nNA     alpha0   alpha2   alpha3    sigma sigma_Site sigma_Quad      lp__\nNA 1 42.75689 18.78058 40.83975 6.864756   9.238436   8.184025 -599.4890\nNA 2 38.59912 22.03675 41.22345 7.177420   8.068274   7.236541 -601.9102\nNA 3 49.30184 16.27629 25.40227 7.029283  12.095883   9.013823 -605.9202\nNA 4 35.43777 24.99815 41.62737 8.155809  10.740356   9.575282 -610.5900\nNA 5 46.74108 16.50691 35.65499 6.641079  10.477957   8.424534 -592.3729\nNA 6 45.12399 15.84058 38.70849 6.232023  13.835963   7.301801 -610.0283"
  },
  {
    "objectID": "tutorials/2020-02-09-nested-anova-stan/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-09-nested-anova-stan/index.html#matrix-parameterisation-1",
    "title": "Nested Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nSite;\n   int nQuad;\n   int nA;\n   vector [n] y;\n   matrix [n,nA] X;\n   int Site[n];\n   int Quad[n];\n   vector [nA] a0;\n   matrix [nA,nA] A0;\n}\n\nparameters{\n  vector [nA] alpha;\n  real&lt;lower=0&gt; sigma;\n  vector [nSite] beta_Site;\n  real&lt;lower=0&gt; sigma_Site;\n  vector [nQuad] beta_Quad;\n  real&lt;lower=0&gt; sigma_Quad;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    //alpha ~ normal( 0 , 100 );\n    alpha ~ multi_normal(a0,A0);\n    beta_Site ~ normal( 0 , sigma_Site );\n    sigma_Site ~ cauchy( 0 , 25);\n    beta_Quad ~ normal( 0 , sigma_Quad );\n    sigma_Quad ~ cauchy( 0 , 25);\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = dot_product(X[i],alpha) + beta_Site[Site[i]] + beta_Quad[Quad[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel2.stan\")\n\nX &lt;- model.matrix(~A, data.nest)\nnA &lt;- ncol(X)\ndata.nest.list &lt;- with(data.nest1, list(y=y, X=X, Site=as.numeric(Sites),\n   Quad=as.numeric(Quads),\n   n=nrow(data.nest1), nSite=length(levels(Sites)),\n   nQuad=length(levels(Quads)),\n   nA=nA,\n   a0=rep(0,nA), A0=diag(100000,nA)))\n\nparams &lt;- c('alpha','sigma','sigma_Site', 'sigma_Quad')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.nest1.rstan.m2 &lt;- stan(data = data.nest.list, file = \"matrixModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 4.7e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 2.162 seconds (Warm-up)\nNA Chain 1:                1.431 seconds (Sampling)\nNA Chain 1:                3.593 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.4e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 2.231 seconds (Warm-up)\nNA Chain 2:                1.191 seconds (Sampling)\nNA Chain 2:                3.422 seconds (Total)\nNA Chain 2:\n\nprint(data.nest1.rstan.m2, par = c('alpha','sigma','sigma_Site', 'sigma_Quad'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA             mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha[1]   41.39    0.16 5.62 29.81 38.00 41.41 44.96 52.01  1183    1\nNA alpha[2]   21.23    0.21 7.88  5.61 16.10 21.25 26.34 36.70  1399    1\nNA alpha[3]   39.05    0.22 8.01 23.57 34.01 39.05 44.01 54.87  1303    1\nNA sigma       7.29    0.01 0.61  6.23  6.87  7.26  7.68  8.54  1638    1\nNA sigma_Site 11.44    0.08 3.14  6.76  9.34 11.00 12.98 18.57  1509    1\nNA sigma_Quad  8.56    0.03 1.13  6.54  7.76  8.48  9.29 10.93  1704    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:45:19 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.nest1.rstan.m2.df &lt;-as.data.frame(extract(data.nest1.rstan.m2))\nhead(data.nest1.rstan.m2.df)\n\nNA    alpha.1  alpha.2  alpha.3    sigma sigma_Site sigma_Quad      lp__\nNA 1 45.46429 15.67815 32.24059 8.029760  15.517948   8.471629 -611.0464\nNA 2 43.55070 13.67668 34.26886 8.049030  12.398252   6.753086 -605.1953\nNA 3 46.52918 16.82928 35.08262 8.001375  14.637084   6.799795 -613.9694\nNA 4 47.98525 21.02618 28.33026 6.677800   8.867418   9.866074 -604.2998\nNA 5 50.02583 18.74219 29.08627 6.694833  14.597826   6.932240 -593.2225\nNA 6 40.88942 22.87433 39.15047 8.547485   9.473757   8.468906 -599.5212"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html",
    "title": "Partly Nested Anova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#introduction",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nSplit-plot designs (plots refer to agricultural field plots for which these designs were originally devised) extend unreplicated factorial (randomised complete block and simple repeated measures) designs by incorporating an additional factor whose levels are applied to entire blocks. Similarly, complex repeated measures designs are repeated measures designs in which there are different types of subjects. Consider the example of a randomised complete block. Blocks of four treatments (representing leaf packs subject to different aquatic taxa) were secured in numerous locations throughout a potentially heterogeneous stream. If some of those blocks had been placed in riffles, some in runs and some in pool habitats of the stream, the design becomes a split-plot design incorporating a between block factor (stream region: runs, riffles or pools) and a within block factor (leaf pack exposure type: microbial, macro invertebrate or vertebrate). Furthermore, the design would enable us to investigate whether the roles that different organism scales play on the breakdown of leaf material in stream are consistent across each of the major regions of a stream (interaction between region and exposure type). Alternatively (or in addition), shading could be artificially applied to half of the blocks, thereby introducing a between block effect (whether the block is shaded or not). Extending the repeated measures examples from Tutorial 9.3a, there might have been different populations (such as different species or histories) of rats or sharks. Any single subject (such as an individual shark or rat) can only be of one of the populations types and thus this additional factor represents a between subject effect."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#linear-models",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#linear-models",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for three and four factor partly nested designs are:\n\\[\ny_{ijkl} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + (\\alpha\\gamma)_{ij} + (\\beta\\gamma)_{jk} + \\epsilon_{ijkl},\n\\]\n\\[\ny_{ijklm} = \\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij} + \\beta_k + \\delta_l + (\\alpha\\delta)_{il} + (\\gamma\\delta)_{jl} + (\\alpha\\gamma\\delta)_{ijl} + \\epsilon_{ijklm}, \\;\\;\\; \\text{(Model 2 additive - 2 between)}\n\\]\n\\[\ny_{ijklm} = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\delta_l +  (\\gamma\\delta)_{kl} + (\\alpha\\gamma)_{ik} + (\\alpha\\delta)_{il} + (\\alpha\\gamma\\delta)_{ikl} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2 additive - 1 between)}\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B and \\(\\epsilon\\) is the random unexplained or residual component."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#assumptions",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAs partly nested designs share elements in common with each of nested, factorial and unreplicated factorial designs, they also share similar assumptions and implications to these other designs. Specifically, hypothesis tests assume that:\n\nthe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator (see Tables above) be used to explore normality. Scale transformations are often useful.\nthe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nthe appropriate residuals are independent of one another. Critically, experimental units within blocks/subjects should be adequately spaced temporally and spatially to restrict contamination or carryover effects. Non-independence resulting from the hierarchical design should be accounted for.\nthat the variance/covariance matrix displays sphericity (strickly, the variance-covariance matrix must display a very specific pattern of sphericity in which both variances and covariances are equal (compound symmetry), however, an F-ratio will still reliably follow an F distribution provided basic sphericity holds). This assumption is likely to be met only if the treatment levels within each block can be randomly ordered. This assumption can be managed by either adjusting the sensitivity of the affected F-ratios or employing linear mixed effects modelling to the design.\nthere are no block by within block interactions. Such interactions render non-significant within block effects difficult to interpret unless we assume that there are no block by within block interactions, non-significant within block effects could be due to either an absence of a treatment effect, or as a result of opposing effects within different blocks. As these block by within block interactions are unreplicated, they can neither be formally tested nor is it possible to perform main effects tests to diagnose non-significant within block effects."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#data-generation",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#data-generation",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (\\(y\\)) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnA &lt;- 3\nnC &lt;- 3\nnBlock &lt;- 36\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nC\nBlock &lt;- gl(nBlock, k=1)\nC &lt;- gl(nC,k=1)\n\n## Specify the cell means\nAC.means&lt;-(rbind(c(40,70,80),c(35,50,70),c(35,40,45)))\n## Convert these to effects\nX &lt;- model.matrix(~A*C,data=expand.grid(A=gl(3,k=1),C=gl(3,k=1)))\nAC &lt;- as.vector(AC.means)\nAC.effects &lt;- solve(X,AC)\n\nA &lt;- gl(nA,nBlock,n)\ndt &lt;- expand.grid(C=C,Block=Block)\ndt &lt;- data.frame(dt,A)\n\nXmat &lt;- cbind(model.matrix(~-1+Block, data=dt),model.matrix(~A*C, data=dt))\nblock.effects &lt;-  rnorm(n = nBlock, mean =0 , sd = sigma.block)\nall.effects &lt;- c(block.effects, AC.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.splt &lt;- data.frame(y=y, A=A,dt)\nhead(data.splt)  #print out the first six rows of the data set\n\nNA          y A C Block A.1\nNA 1 36.04388 1 1     1   1\nNA 2 62.96473 1 2     1   1\nNA 3 71.74448 1 3     1   1\nNA 4 35.33552 1 1     2   1\nNA 5 63.76434 1 2     2   1\nNA 6 76.19828 1 3     2   1\n\ntapply(data.splt$y,data.splt$A,mean)\n\nNA        1        2        3 \nNA 65.71431 49.43047 41.36212\n\ntapply(data.splt$y,data.splt$C,mean)\n\nNA        1        2        3 \nNA 38.41079 53.56792 64.52819\n\nreplications(y~A*C+Error(Block), data.splt)\n\nNA   A   C A:C \nNA  36  36  12\n\nlibrary(ggplot2)\nggplot(data.splt, aes(y=y, x=C, linetype=A, group=A)) + geom_line(stat='summary', fun.y=mean)\n\n\n\n\n\n\n\nggplot(data.splt, aes(y=y, x=C,color=A)) + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#exploratory-data-analysis",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\n# check between plot effects\nboxplot(y~A, ddply(data.splt,~A+Block, summarise,y=mean(y)))\n\n\n\n\n\n\n\n#OR\nggplot(ddply(data.splt,~A+Block, summarise,y=mean(y)), aes(y=y, x=A)) + geom_boxplot()\n\n\n\n\n\n\n\n# check within plot effects\nboxplot(y~A*C, data.splt)\n\n\n\n\n\n\n\n#OR \nggplot(data.splt, aes(y=y, x=C, fill=A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the y-axis. Hence it there is no evidence of non-homogeneity.\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.splt, interaction.plot(C,Block,y))\n\n\n\n\n\n\n\n#OR with ggplot\nlibrary(ggplot2)\nggplot(data.splt, aes(y=y, x=C, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A*C, data.splt))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA C                                    \nNA Tukey test    1.4518           0.1466\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A*C, data.splt))\n\nNA      Test    Pvalue \nNA 1.4517644 0.1465671\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (C). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#model-fitting",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nWe will only explore the matrix parameterisation (random intercepts) of the model, where\n\\[\n\\text{number of lesions}_i = \\beta \\text{Site}_{j(i)} + \\epsilon_{i},\n\\]\nwhere \\(\\epsilon_i∼ N(0,\\sigma^2)\\) and we treat Distance as a factor.\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau.res)\n      mu[i] &lt;- inprod(beta[],X[i,]) + inprod(gamma[],Z[i,])\n      y.err[i] &lt;- y[i] - mu[1]\n   }\n\n   #Priors and derivatives\n   for (i in 1:nZ) {\n      gamma[i] ~ dnorm(0,tau.plate)\n   }\n   for (i in 1:nX) {\n      beta[i] ~ dnorm(0,1.0E-06)\n   }\n\n   tau.res &lt;- pow(sigma.res,-2)\n   sigma.res &lt;- z/sqrt(chSq)\n   z ~ dnorm(0, .0016)I(0,)\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.plate &lt;- pow(sigma.plate,-2)\n   sigma.plate &lt;- z.plate/sqrt(chSq.plate)\n   z.plate ~ dnorm(0, .0016)I(0,)\n   chSq.plate ~ dgamma(0.5, 0.5)\n\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"matrixModel.txt\")\n\n\n#sort the data set so that the copper treatments are in a more logical order\nlibrary(dplyr)\ncopper$DIST &lt;- factor(copper$DIST)\ncopper$PLATE &lt;- factor(copper$PLATE)\ncopper.sort &lt;- arrange(copper,COPPER,PLATE,DIST)\n\nXmat &lt;- model.matrix(~COPPER*DIST, data=copper.sort)\nZmat &lt;- model.matrix(~-1+PLATE, data=copper.sort)\ncopper.list &lt;- list(y=copper.sort$WORMS,\n               X=Xmat, nX=ncol(Xmat),\n                           Z=Zmat, nZ=ncol(Zmat),\n                           n=nrow(copper.sort)\n                           )\n\nparams &lt;- c(\"beta\",\"gamma\",\"sigma.res\",\"sigma.plate\")\nburnInSteps = 1000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nlibrary(R2jags)\nlibrary(coda)\n\ncopper.r2jags.b &lt;- jags(data = copper.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 60\nNA    Unobserved stochastic nodes: 31\nNA    Total graph size: 1971\nNA \nNA Initializing model\n\nprint(copper.r2jags.b)\n\nNA Inference for Bugs model at \"matrixModel.txt\", fit using jags,\nNA  2 chains, each with 1500 iterations (first 1000 discarded)\nNA  n.sims = 1000 iterations saved\nNA             mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]      10.814   0.685   9.401  10.369  10.795  11.258  12.157 1.001  1000\nNA beta[2]      -3.544   0.984  -5.440  -4.199  -3.525  -2.869  -1.574 1.002   640\nNA beta[3]     -10.560   0.966 -12.615 -11.177 -10.559  -9.923  -8.712 1.003   610\nNA beta[4]       1.172   0.884  -0.556   0.586   1.199   1.778   2.892 1.002  1000\nNA beta[5]       1.582   0.878  -0.167   0.999   1.577   2.158   3.184 1.003  1000\nNA beta[6]       2.743   0.857   1.039   2.151   2.719   3.342   4.443 1.003  1000\nNA beta[7]      -0.073   1.233  -2.504  -0.875  -0.120   0.748   2.508 1.000  1000\nNA beta[8]       0.007   1.271  -2.447  -0.792  -0.068   0.868   2.556 1.003  1000\nNA beta[9]      -0.365   1.257  -2.866  -1.165  -0.397   0.499   1.960 1.000  1000\nNA beta[10]      2.184   1.237  -0.254   1.395   2.183   2.954   4.846 1.007  1000\nNA beta[11]     -0.008   1.204  -2.424  -0.763  -0.013   0.781   2.378 1.008   530\nNA beta[12]      4.830   1.235   2.390   4.051   4.840   5.632   7.290 1.018  1000\nNA gamma[1]      0.182   0.496  -0.719  -0.102   0.117   0.461   1.300 1.023   650\nNA gamma[2]     -0.115   0.515  -1.218  -0.384  -0.074   0.153   0.915 1.020   300\nNA gamma[3]      0.301   0.541  -0.720  -0.032   0.210   0.593   1.540 1.028   130\nNA gamma[4]     -0.450   0.567  -1.733  -0.791  -0.359  -0.043   0.455 1.032    61\nNA gamma[5]     -0.404   0.520  -1.489  -0.705  -0.328  -0.034   0.455 1.028   130\nNA gamma[6]      0.867   0.712  -0.169   0.295   0.793   1.306   2.470 1.084    25\nNA gamma[7]     -0.186   0.549  -1.386  -0.497  -0.120   0.106   0.856 1.011   290\nNA gamma[8]     -0.530   0.589  -1.808  -0.936  -0.432  -0.051   0.326 1.059    35\nNA gamma[9]      0.153   0.523  -0.919  -0.130   0.100   0.444   1.301 1.008  1000\nNA gamma[10]    -0.154   0.512  -1.206  -0.452  -0.101   0.136   0.848 1.026   290\nNA gamma[11]    -0.113   0.517  -1.317  -0.384  -0.078   0.181   0.896 1.004   920\nNA gamma[12]     0.221   0.546  -0.780  -0.087   0.146   0.541   1.373 1.034   200\nNA gamma[13]     0.136   0.520  -0.822  -0.170   0.081   0.400   1.345 1.017  1000\nNA gamma[14]     0.171   0.541  -0.896  -0.123   0.106   0.466   1.345 1.019   470\nNA gamma[15]    -0.085   0.500  -1.090  -0.374  -0.051   0.202   0.886 1.012  1000\nNA sigma.plate   0.633   0.346   0.050   0.381   0.622   0.842   1.352 1.094    23\nNA sigma.res     1.385   0.166   1.085   1.274   1.377   1.488   1.750 1.038    44\nNA deviance    207.885   8.472 192.227 202.168 207.696 213.399 225.038 1.060    31\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 34.7 and DIC = 242.6\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#mcmc-diagnostics",
    "title": "Partly Nested Anova (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nBefore fully exploring the parameters, it is prudent to examine the convergence and mixing diagnostics. Chose either any of the parameterizations (they should yield much the same).\n\nlibrary(mcmcplots)\ndenplot(copper.r2jags.b, parms = c(\"gamma\",\"beta\"))\n\n\n\n\n\n\n\ntraplot(copper.r2jags.b, parms = c(\"gamma\",\"beta\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(copper.r2jags.b))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\nautocorr.diag(as.mcmc(copper.r2jags.b))\n\nNA             beta[1]     beta[2]      beta[3]      beta[4]      beta[5]\nNA Lag 0   1.000000000  1.00000000  1.000000000  1.000000000  1.000000000\nNA Lag 1   0.021272766  0.04648343  0.007883585 -0.031877909  0.005935939\nNA Lag 5  -0.008158584 -0.04203414  0.003333370 -0.025041071 -0.049596493\nNA Lag 10  0.031505586 -0.03660104  0.063264397  0.005126694  0.061062870\nNA Lag 50 -0.027782043 -0.01419507 -0.063446191 -0.025966769  0.000139520\nNA             beta[6]      beta[7]      beta[8]     beta[9]    beta[10]\nNA Lag 0   1.000000000  1.000000000  1.000000000  1.00000000  1.00000000\nNA Lag 1   0.003141284  0.006332145 -0.016090936  0.02230158 -0.01371002\nNA Lag 5  -0.047609108 -0.015586534 -0.004392271 -0.04095146  0.03636817\nNA Lag 10 -0.021534565  0.002483458  0.022938630  0.03931772  0.09976040\nNA Lag 50  0.018649619  0.039287014 -0.026677246  0.02487322 -0.01257863\nNA            beta[11]    beta[12]   deviance    gamma[1]    gamma[2]   gamma[3]\nNA Lag 0   1.000000000  1.00000000 1.00000000  1.00000000  1.00000000 1.00000000\nNA Lag 1   0.003598499  0.04451183 0.42158932  0.09957956  0.03142211 0.07683730\nNA Lag 5  -0.048681325 -0.02569540 0.12353548  0.03983927 -0.00533499 0.02599357\nNA Lag 10 -0.025741832 -0.01822980 0.08655390 -0.02625359  0.05903335 0.05050285\nNA Lag 50  0.008573506 -0.02525275 0.02010397 -0.04670946 -0.04143951 0.01017881\nNA           gamma[4]   gamma[5]   gamma[6]    gamma[7]    gamma[8]     gamma[9]\nNA Lag 0   1.00000000 1.00000000  1.0000000  1.00000000  1.00000000  1.000000000\nNA Lag 1   0.20659505 0.19599762  0.5113019  0.01034209  0.22908668  0.003942025\nNA Lag 5   0.13726039 0.11488655  0.2890791 -0.07543631  0.15468366  0.039009815\nNA Lag 10  0.08819534 0.06826430  0.1643047  0.03128544  0.03212642 -0.007477517\nNA Lag 50 -0.03923514 0.01121642 -0.1002922 -0.01843480 -0.04706169 -0.012306197\nNA           gamma[10]     gamma[11]    gamma[12]   gamma[13]   gamma[14]\nNA Lag 0   1.000000000  1.0000000000  1.000000000  1.00000000 1.000000000\nNA Lag 1   0.010206952  0.0028638893  0.009332531  0.03815594 0.007373479\nNA Lag 5  -0.061360721  0.0008173756 -0.012857899 -0.02174086 0.022461865\nNA Lag 10 -0.013288697 -0.0226321328  0.001324936  0.03479040 0.031318743\nNA Lag 50  0.008887211 -0.0289618811 -0.026443165  0.01353287 0.037485638\nNA           gamma[15] sigma.plate  sigma.res\nNA Lag 0   1.000000000   1.0000000 1.00000000\nNA Lag 1  -0.028327792   0.8371048 0.54229156\nNA Lag 5  -0.010034686   0.5053673 0.03764157\nNA Lag 10 -0.010388153   0.3404067 0.01350504\nNA Lag 50  0.002533215  -0.1081944 0.07948304"
  },
  {
    "objectID": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#parameter-estimates",
    "href": "tutorials/2020-02-11-partly-nested-anova-jags/index.html#parameter-estimates",
    "title": "Partly Nested Anova (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\n\nprint(copper.r2jags.b)\n\nNA Inference for Bugs model at \"matrixModel.txt\", fit using jags,\nNA  2 chains, each with 1500 iterations (first 1000 discarded)\nNA  n.sims = 1000 iterations saved\nNA             mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]      10.814   0.685   9.401  10.369  10.795  11.258  12.157 1.001  1000\nNA beta[2]      -3.544   0.984  -5.440  -4.199  -3.525  -2.869  -1.574 1.002   640\nNA beta[3]     -10.560   0.966 -12.615 -11.177 -10.559  -9.923  -8.712 1.003   610\nNA beta[4]       1.172   0.884  -0.556   0.586   1.199   1.778   2.892 1.002  1000\nNA beta[5]       1.582   0.878  -0.167   0.999   1.577   2.158   3.184 1.003  1000\nNA beta[6]       2.743   0.857   1.039   2.151   2.719   3.342   4.443 1.003  1000\nNA beta[7]      -0.073   1.233  -2.504  -0.875  -0.120   0.748   2.508 1.000  1000\nNA beta[8]       0.007   1.271  -2.447  -0.792  -0.068   0.868   2.556 1.003  1000\nNA beta[9]      -0.365   1.257  -2.866  -1.165  -0.397   0.499   1.960 1.000  1000\nNA beta[10]      2.184   1.237  -0.254   1.395   2.183   2.954   4.846 1.007  1000\nNA beta[11]     -0.008   1.204  -2.424  -0.763  -0.013   0.781   2.378 1.008   530\nNA beta[12]      4.830   1.235   2.390   4.051   4.840   5.632   7.290 1.018  1000\nNA gamma[1]      0.182   0.496  -0.719  -0.102   0.117   0.461   1.300 1.023   650\nNA gamma[2]     -0.115   0.515  -1.218  -0.384  -0.074   0.153   0.915 1.020   300\nNA gamma[3]      0.301   0.541  -0.720  -0.032   0.210   0.593   1.540 1.028   130\nNA gamma[4]     -0.450   0.567  -1.733  -0.791  -0.359  -0.043   0.455 1.032    61\nNA gamma[5]     -0.404   0.520  -1.489  -0.705  -0.328  -0.034   0.455 1.028   130\nNA gamma[6]      0.867   0.712  -0.169   0.295   0.793   1.306   2.470 1.084    25\nNA gamma[7]     -0.186   0.549  -1.386  -0.497  -0.120   0.106   0.856 1.011   290\nNA gamma[8]     -0.530   0.589  -1.808  -0.936  -0.432  -0.051   0.326 1.059    35\nNA gamma[9]      0.153   0.523  -0.919  -0.130   0.100   0.444   1.301 1.008  1000\nNA gamma[10]    -0.154   0.512  -1.206  -0.452  -0.101   0.136   0.848 1.026   290\nNA gamma[11]    -0.113   0.517  -1.317  -0.384  -0.078   0.181   0.896 1.004   920\nNA gamma[12]     0.221   0.546  -0.780  -0.087   0.146   0.541   1.373 1.034   200\nNA gamma[13]     0.136   0.520  -0.822  -0.170   0.081   0.400   1.345 1.017  1000\nNA gamma[14]     0.171   0.541  -0.896  -0.123   0.106   0.466   1.345 1.019   470\nNA gamma[15]    -0.085   0.500  -1.090  -0.374  -0.051   0.202   0.886 1.012  1000\nNA sigma.plate   0.633   0.346   0.050   0.381   0.622   0.842   1.352 1.094    23\nNA sigma.res     1.385   0.166   1.085   1.274   1.377   1.488   1.750 1.038    44\nNA deviance    207.885   8.472 192.227 202.168 207.696 213.399 225.038 1.060    31\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 34.7 and DIC = 242.6\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html",
    "title": "Goodness of fit tests (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#introduction",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#introduction",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nThe analyses described in previous tutorials have all involved response variables that implicitly represent normally distributed and continuous population responses. In this context, continuous indicates that (at least in theory), any value of measurement down to an infinite number of decimal places is possible. Population responses can also be categorical such that the values could be logically or experimentally constrained to a set number of discrete possibilities. For example, individuals in a population can be categorized as either male or female, reaches in a stream could be classified as either riffles, runs or pools and salinity levels of sites might be categorized as either high, medium or low. Typically, categorical response variables are tallied up to generate the frequency of replicates in each of the possible categories. From above, we would tally up the frequency of males and females, the number of riffles, runs and pools and the high, medium and low salinity sites. Hence, rather than model data in which a response was measured from each replicate in the sample (as was the case for previous analyses in this series), frequency analyses model data on the frequency of replicates in each possible category. Furthermore, frequency data follow a Poisson distribution rather than a normal distribution. The Poisson distribution is a symmetrical distribution in which only discrete integer values are possible and whose variance is equal to its mean.\nSince the mean and variance of a Poisson distribution are equal, distributions with higher expected values are shorter and wider than those with smaller means. Note that a Poisson distribution with an expected less than less than \\(5\\) will be obviously asymmetrical as a Poisson distribution is bounded to the left by zero. This has important implications for the reliability of frequency analyses when sample sizes are low. The frequencies expected for each category are determined by the size of the sample and the nature of the (null) hypothesis. For example, if the null hypothesis is that there are three times as many females as males in a population (ratio of \\(3:1\\)), then a sample of \\(110\\) individuals would be expected to yield \\(0.75\\times110=82.5\\) females and \\(0.25\\times110=27.5\\) males."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#the-chi-square-statistic",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#the-chi-square-statistic",
    "title": "Goodness of fit tests (JAGS)",
    "section": "The Chi-square statistic",
    "text": "The Chi-square statistic\nThe degree of difference between the observed (o) and expected (e) sample category frequencies is represented by the chi-square (\\(\\chi^2\\)) statistic.\n\\[\n\\chi^2=\\sum\\frac{(o-e)^2}{e}.\n\\]\nThis is a relative measure that is standardised by the magnitude of the expected frequencies. When the null hypothesis is true (typically this represents the situation when there are no effects or patterns of interest in the population response category frequencies), and we have sampled in an unbiased manner, we might expect the observed category frequencies in the sample to be very similar (if not equal) to the expected frequencies and thus, the chi-square value should be close to zero. Likewise, repeated sampling from such a population is likely to yield chi-square values close to zero and large chi-square values should be relatively rare. As such, the chi-square statistic approximately follows a \\(\\chi^2\\) distribution, a mathematical probability distribution representing the frequency (and thus probability) of all possible ranges of chi-square statistics that could result when the null hypothesis is true.\nThe \\(\\chi^2\\) distribution is defined as:\n\\[\np(x) = \\frac{1}{2^{\\frac{n}{2}}\\gamma(\\frac{n}{2})}x^{\\frac{n}{2-1}}e^{-\\frac{x}{2}}.\n\\]\nNote that the location AND shape are both determined by a single parameter (the sample size, n which is also equal to the degrees of freedom \\(+ 1\\)). The \\(\\chi^2\\) distribution is an asymmetrical distribution bounded by zero and infinity and whose exact shape is determined by the degrees of freedom (calculated as the total number of categories minus \\(1\\)). Note also that the peak of a chi-square distribution is not actually at zero (although it does approach it when the degrees of freedom is equal to zero). Initially, this might seem counter intuitive. We might expect that when a null hypothesis is true, the most common chi-square value will be zero. However, the \\(\\chi^2\\) distribution takes into account the expected natural variability in a population as well as the nature of sampling (in which multiple samples should yield slightly different results). The more categories there are, the more likely that the observed and expected values will differ. It could be argued that when there are a large number of categories, samples in which all the observed frequencies are very close to the expected frequencies are a little suspicious and may represent dishonesty on the part of the researcher (Indeed the extraordinary conformity of Gregor Mendelâ€™s pea experiments have been subjected to such skepticism).\nBy comparing any given sample chi-square statistic to its appropriate \\(\\chi^2\\) distribution, the probability that the observed category frequencies could have be collected from a population with a specific ratio of frequencies (for example \\(3:1\\)) can be estimated. As is the case for most hypothesis tests, probabilities lower than \\(0.05\\) (\\(5\\)%) are considered unlikely and suggest that the sample is unlikely to have come from a population characterized by the null hypothesis. Chi-squared tests are typically one-tailed tests focusing on the right-hand tail as we are primarily interested in the probability of obtaining large chi-square values. Nevertheless, it is also possible to focus on the left-hand tail so as to investigate whether the observed values are “too good to be true”."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#assumptions",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#assumptions",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nA chi-square statistic will follow a \\(\\chi^2\\) distribution approximately provided that:\n\nAll observations are classified independently of one another. The classification of one replicate should not be influenced by or related to the classification of other replicates. Random sampling should address this.\nNo more than \\(20\\)% of the expected frequencies are less than five. \\(\\chi^2\\) distributions do not reliably approximate the distribution of all possible chi-square values under those circumstances (Expected frequencies less than five result in asymmetrical sampling distributions (since they must be truncated at zero) and thus potentially unrepresentative χ2 distributions). Since the expected values are a function of sample sizes, meeting this assumption is a matter of ensuring sufficient replication. When sample sizes or other circumstances beyond control lead to a violation of this assumption, numerous options are available."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#goodness-of-fit-tests",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#goodness-of-fit-tests",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nHomogeneous frequencies tests\nHomogeneous frequencies tests (often referred to as goodness of fit tests) are used to test null hypotheses that the category frequencies observed within a single variable could arise from a population displaying a specific ratio of frequencies. The null hypothesis (\\(H_0\\)) is that the observed frequencies come from a population with a specific ratio of frequencies.\nDistributional conformity - Kolmogorov-Smirnov tests\nStrictly, goodness of fit tests are used to examine whether a frequency/sampling distribution is homogeneous with some declared distribution. For example, we might use a goodness of fit test to formally investigate whether the distribution of a response variable deviates substantially from a normal distribution. In this case, frequencies of responses in a set of pre-defined bin ranges are compared to those frequencies expected according to the mathematical model of a normal distribution. Since calculations of these expected frequencies also involve estimates of population mean and variance (both required to determine the mathematical formula), a two degree of freedom loss is incurred (hence \\(df=n−2\\))."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#contingency-tables",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#contingency-tables",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Contingency tables",
    "text": "Contingency tables\nContingency tables are used to investigate the associations between two or more categorical variables. That is, they test whether the patterns of frequencies in one categorical variable differ between different levels of other categorical variable(s) or ould the variables be independent of another. In this way, they are analogous to interactions in factorial linear models (such as factorial ANOVA). Contingency tables test the null hypothesis (\\(H_0\\)) that the categorical variables are independent of (not associated with) one another. Note that analyses of contingency tables do not empirically distinguish between response and predictor variables (analogous to correlation), yet causality can be implied when logical and justified by interpretation. As an example, contingency tables could be used to investigate whether incidences of hair and eye color in a population are associated with one another (is one hair color type more commonly observed with a certain eye color). In this case, neither hair color nor eye color influence one another, their incidences are both controlled by a separate set of unmeasured factors. By contrast, an association between the presence or absence of a species of frog and the level of salinity (high, medium or low) could imply that salinity effects the distribution of that species of frog - but not vice versa. Sample replicates are cross-classified according to the levels (categories) of multiple categorical variables. The data are conceptualized as a table (hence the name) with the rows representing the levels of one variable and the column the levels of the other variable(s) such that the cells represent the category combinations. The expected frequency of any given cell is calculated as:\n\\[\n\\frac{\\text{(row total)} \\times \\text{(column total)}}{\\text{(grand total)}}.\n\\]\nThereafter, the chi-square calculations are calculated as described above and the chi-square value is compared to a \\(\\chi^2\\) distribution with \\((r−1)(c−1)\\) degrees of freedom. Contingency tables involving more than two variables have multiple interaction levels and thus multiple potential sources of independence. For example, in a three-way contingency table between variables A, B and C, there are four interactions (A:B, A:C, B:C and A:B:C). Such designs are arguably more appropriately analysed using log-linear models.\nOdds ratios\nThe chi-square test provides an indication of whether or not the occurrences in one set of categories are likely to be associated with other sets of categories (an interaction between two or more categorical variables), yet does not provide any indication of how strongly the variables are associated (magnitude of the effect). Furthermore, for variables with more than two categories (e.g. high, medium, low), there is no indication of which category combinations contribute most to the associations. This role is provided by odds ratios which are essentially a measure of effect size. Odds refer the likelihood of a specific event or outcome occurring (such as the odds of a species being present) versus the odds of it not occurring (and thus the occurrence of an alternative outcome) and are calculated as \\(\\frac{\\pi_j}{(1-\\pi_j)}\\) where \\(\\pi_j\\) refers to the probability of the event occurring. For example, we could calculate the odds of frogs being present in highly saline habitats as the probability of frogs being present divided by the probability of them being absent. Similarly, we could calculate the likelihood of frog presence (odds) within low salinity habitats. The ratio of two of these likelihoods (odds ratio) can then be used to compare whether the likelihood of one outcome (frog presence) is the same for both categories (salinity levels). For example, is the likelihood of frogs being present in highly saline habitats the same as the probability of them being present in habitats with low levels of salinity. In so doing, the odds ratio is a measure of effect size that describes the strength of an association between pairs of cross-classification levels. Although odds and thus odds ratios (\\(\\theta\\)) are technically derived from probabilities, they can also be estimated using cell frequencies (\\(n\\)).\n\\[\n\\theta = \\frac{n_{11}n_{22}}{n_{12}n_{21}}\n\\]\nor alternatively\n\\[\n\\theta = \\frac{(n_{11}+0.5)(n_{22}+0.5)}{(n_{12} + 0.5)(n_{21} + 0.5)}\n\\]\nwhere \\(0.5\\) is a small constant added to prevent division by zero. An odds ratio of one indicates that the event or occurrence (presence of frogs) is equally likely in both categories (high and low salinity habitats). Odds ratios greater than one signify that the event or occurrence is more likely in the first than second category and vice verse for odds ratios less than one. For example, when comparing the presence/absence of frogs in low versus high salinity habitats, an odds ratio of \\(5.8\\) would suggest that frogs are \\(5.8\\) times more likely to be present in low salinity habitats than those that highly saline. The distribution of odds ratios (which range from \\(0\\) to \\(\\infty\\)) is not symmetrical around the null position (\\(1\\)) thereby precluding confidence interval and standard error calculations. Instead, these measures are calculated from log transformed (natural log) odds ratios (the distribution of which is a standard normal distribution centered around \\(0\\)) and then converted back into a linear scale by anti-logging. Odds ratios can only be calculated between category pairs from two variables and therefore \\(2 \\times 2\\) contingency tables (tables with only two rows and two columns). However, tables with more rows and columns can be accommodate by splitting the table up into partial tables of specific category pair combinations. Odds ratios (and confidence intervals) are then calculated from each pairing, notwithstanding their lack of independence. For example, if there were three levels of salinity (high, medium and low), the odds ratios from three partial tables (high vs medium, high vs low, medium vs low) could be calculated.\nSince odds ratios only explore pairwise patterns within two-way interactions, odds ratios for multi-way (three or more variables) tables are considerably more complex to calculate and interpret. Partial tables between two of the variables (e.g frog presence/absence and high/low salinity) are constructed for each level of a third (season: summer/winter). This essentially removes the effect of the third variable by holding it constant. Associations in partial tables are therefore referred to as conditional associations - since the outcomes (associated or independent) from each partial table are explicitly conditional on the level of the third variable at which they were tested.\nSpecific contributions to a lack of independence (significant associations) can also be investigated by exploring the residuals. Recall that residuals are the difference between the observed values (frequencies) and those predicted or expected when the null hypothesis is true (no association between variables). Hence the magnitude of each residual indicates how much each of the cross classification combinations differs from what is expected. The residuals are typically standardized (by dividing by the square of the expected frequencies) to enable individual residuals to be compared relative to one another. Large residuals (in magnitude) indicate large deviations from what is expected when the null hypothesis is true and thus also indicate large influences (contributions) to the overall association. The sign (\\(+\\) or \\(-\\)) of the residual indicates whether the frequencies were higher or lower than expected."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#g-tests",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#g-tests",
    "title": "Goodness of fit tests (JAGS)",
    "section": "G tests",
    "text": "G tests\nAn alternative to the chi-square test for goodness of fit and contingency table analyses is the G-test. The G-test is based on a log likelihood-ratio test. A log likelihood ratio is a ratio of maximum likelihoods of the alternative and null hypotheses. More simply, a log likelihood ratio test essentially examines how likely (the probability) the alternative hypothesis (representing an effect) is compared to how likely the null hypothesis (no effect) is given the collected data. The G2 statistic is calculated as:\n\\[\nG^2 = 2 \\sum o \\; ln\\frac{o}{e}\n\\]\nwhere o and e are the observed and expected sample category frequencies respectively and ln denotes the natural logarithm (base e). When the null hypothesis is true, the G2 statistic approximately follows a theoretical \\(\\chi^2\\) distribution with the same degrees of freedom as the corresponding chi-square statistic. The G2 statistic (which is twice the value of the log-likelihood ratio) is arguably more appropriate than the chi-square statistic as it is closely aligned with the theoretical basis of the χ2 distribution (for which the chi-squared statistic is a convenient approximation). For large sample sizes, G2 and \\(\\chi^2\\) statistics are equivalent, however the former is a better approximation of the theoretical chi2 distribution when the difference between the observed and expected is less than the expected frequencies (ie \\(|o−e|&lt;e\\)). Nevertheless, G-tests operate under the same assumptions are the chi-square statistic and thus very small sample sizes (expected values less than \\(5\\)) are still problematic. G-tests have the additional advantage that they can be used additively with more complex designs and a thus more extensible than the chi-squared statistic."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#small-sample-sizes",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#small-sample-sizes",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Small sample sizes",
    "text": "Small sample sizes\nAs discussed previously, both the \\(\\chi^2\\) and G2 statistics are poor approximations of theoretical \\(\\chi^2\\) distributions when sample sizes are very small. Under these circumstances a number of alternative options are available:\n\nIf the issue has arisen due to a large number of category levels in one or more of the variables, some categories could be combined together.\nFishers exact test which essentially calculates the probability of obtaining the cell frequencies given the observed marginal totals in \\(2 \\times 2\\) tables. The calculations involved in such tests are extremely tedious as they involve calculating probabilities from hypergeometric distributions (discrete distributions describing the number of successes from sequences of samples drawn with out replacement) for all combinations of cell values that result in the given marginal totals.\nYates’ continuity correction calculates the test statistic after adding and subtracting \\(0.5\\) from observed values less than and greater than expected values respectively. Yates’ correction can only be applied to designs with a single degree of freedom (goodness-of-fit designs with two categories or \\(2 \\times 2\\) tables) and for goodness-of-fit tests provide p-values that are closer to those of an exact binomial. However, they typically yield over inflated p-values in contingency tables and so have gone out of favour.\nWilliams’ correction is applied by dividing the test statistic by \\(1+(p2−1)6nv\\), where \\(p\\) is the number of categories, \\(n\\) is the total sample size (total of observed frequencies) and \\(v\\) is the number of degrees of freedom \\((p−1)\\). Williams’ corrections can be applied to designs with greater than one degree of freedom, and are considered marginally more appropriate than Yates’ corrections if corrections are insisted.\nRandomisation tests in which the sample test statistic (either \\(\\chi^2\\) or G2) is compared to a probability distribution generated by repeatedly calculating the test statistic from an equivalent number of observations drawn from a population (sampling with replacement) with the specific ratio of category frequencies defined by the null hypothesis. Significance is thereafter determined by the proportion of the randomised test statistic values that are greater than or equal to the value of the statistic that is based on observed data.\nLog-linear modelling (as a form of generalized linear model)"
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#exploratory-data-analysis",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThe data should logically follow a binomial distribution (since the observations are counts of positive events out of a total)."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#model-fitting",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nWe now translate the likelihood model into JAGS code and store the code in an external file.\n\nmodelString=\"\nmodel {\n  #Likelihood\n for (i in 1:nGroups) {\n   obs[i] ~ dbin(p[i],n[i])\n   p[i] ~ dbeta(a[i],b[i])\n   a[i] ~ dgamma(1,0.01)\n   b[i] ~ dgamma(1,0.01)\n }\n }\n\"\n## write the model to a text file \nwriteLines(modelString,con=\"chi2model.txt\")\n\n\nThe likelihood model indicates that the observed counts are modeled by a binomial distribution with a probability of p (fraction) from n trials (items).\nThe prior on each p is defined as a beta distribution with shape parameters a and b\nThe hyperpriors for each a and b are drawn from imprecise (vague, flat) gamma distributions.\n\nDefine the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\n# The observed item frequencies\nobs &lt;- c(15, 9, 23)\ndata.list &lt;- list(obs = obs, n = c(47, 47, 47), nGroups = 3)\ndata.list\n\nNA $obs\nNA [1] 15  9 23\nNA \nNA $n\nNA [1] 47 47 47\nNA \nNA $nGroups\nNA [1] 3\n\n\nDefine the parameters to monitor and the chain details\n\nparams &lt;- c(\"p\")\nnChains = 2\nburnInSteps = 1000\nthinSteps = 1\nnumSavedSteps = 5000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nFit the model in JAGS using the function jags in the package R2jags (which should be loaded first).\n\nlibrary(R2jags)\n# Fit the model for the 1:1:1 ratio\ndata.r2jags &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, \n                    model.file = \"chi2model.txt\",n.chains = nChains, n.iter = nIter, \n                    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 3\nNA    Unobserved stochastic nodes: 9\nNA    Total graph size: 18\nNA \nNA Initializing model\n\nprint(data.r2jags)\n\nNA Inference for Bugs model at \"chi2model.txt\", fit using jags,\nNA  2 chains, each with 2500 iterations (first 1000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA p[1]       0.331   0.066  0.209  0.286  0.329  0.373  0.465 1.002  1500\nNA p[2]       0.204   0.056  0.104  0.164  0.201  0.240  0.323 1.009   220\nNA p[3]       0.481   0.070  0.337  0.436  0.482  0.529  0.614 1.006   390\nNA deviance  15.201   2.387 12.516 13.432 14.568 16.297 21.137 1.010   180\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 2.8 and DIC = 18.0\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nConclusions: Initially, we should focus our attention on the Rhat and n.eff columns. These are the scale reduction and number of effective samples respectively and they provide an indication of the degree of mixing or coverage of the samples. Ideally, the n.eff values should be approximately equal to the number of saved samples (in this case \\(4701\\)), and the Rhat values should be approximately \\(1\\) (complete convergence). Whilst the actual values are likely to differ substantially from run to run (due to the stochastic nature of the way the chains traverse the posterior distribution), on this occasion, the n.eff of the first two probability parameters (p[1] and p[2]) are substantially lower than \\(4700\\). Hence, the samples of these parameters may not accurately reflect the posterior distribution. We might consider altering one or more of the chain behavioural paramters (such as the thinning rate), alter the model definition (or priors) itself."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#model-evaluation",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#model-evaluation",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nlibrary(mcmcplots)\ndenplot(data.r2jags, parms = c(\"p\"))\n\n\n\n\n\n\n\ntraplot(data.r2jags, parms = c(\"p\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(data.r2jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\nautocorr.diag(as.mcmc(data.r2jags))\n\nNA          deviance       p[1]        p[2]       p[3]\nNA Lag 0  1.00000000 1.00000000  1.00000000 1.00000000\nNA Lag 1  0.69694316 0.77542752  0.81100727 0.81682813\nNA Lag 5  0.19279534 0.29798006  0.42995558 0.41561204\nNA Lag 10 0.03297520 0.07747643  0.19693603 0.23855095\nNA Lag 50 0.01818888 0.03336105 -0.05892592 0.01968776\n\n\nConclusions: Minimum required number of MCMC samples to ensure that sufficient samples had been collected to achieve good accuracy is \\(3746\\). We had \\(5000\\) per chain (\\(5000\\times3=15000\\)).\n\nparams &lt;- c(\"p\")\nnChains = 2\nburnInSteps = 1000\nthinSteps = 50\nnumSavedSteps = 5000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\ndata.r2jags &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, \n                    model.file = \"chi2model.txt\", n.chains = nChains, n.iter = nIter,\n                    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 3\nNA    Unobserved stochastic nodes: 9\nNA    Total graph size: 18\nNA \nNA Initializing model\n\nprint(data.r2jags)\n\nNA Inference for Bugs model at \"chi2model.txt\", fit using jags,\nNA  2 chains, each with 125000 iterations (first 1000 discarded), n.thin = 50\nNA  n.sims = 4960 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA p[1]       0.327   0.067  0.205  0.279  0.324  0.369  0.466 1.001  5000\nNA p[2]       0.205   0.057  0.106  0.162  0.201  0.241  0.327 1.001  5000\nNA p[3]       0.491   0.072  0.349  0.442  0.492  0.540  0.631 1.001  5000\nNA deviance  15.309   2.420 12.554 13.530 14.691 16.384 21.687 1.001  5000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 2.9 and DIC = 18.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nConclusions: Rhat and n.eff are now much better for the probability parameters. The estimated fractions for A, B and C are:\n\nA: 0.327 (0.207, 0.466)\nB: 0.200 (0.104, 0.323)\nC: 0.491 (0.355, 0.625)\n\nCollectively, the fractions of 1/3, 1/3 and 1/3 do not fall within these ranges. However, collectively the fractions 1/4, 1/4, 2/4 do fall comfortably within these ranges. This suggests that the population ratio is more likely to be 1:1:2 than 1:1:1."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#exploratory-data-analysis-1",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nSo lets calculate the expected frequencies as a means to evaluate this assumption. The expected values are calculated as:\n\\[\ne=\\text{total counts} \\times \\text{expected fraction}.\n\\]\nIt is clear that in neither case are any of the expected frequencies less than \\(5\\). Therefore, we would conclude that probabilities derived from the \\(\\chi^2\\) distribution are likely to be reliable."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#model-fitting-1",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nWe now translate the likelihood model into JAGS code and store the code in an external file.\n\nmodelString2=\"\ndata {\nfor (i in 1:n){\n     resid[i] &lt;- pow(obs[i]-exp[i],2)/exp[i]\n   }\n   chisq &lt;- sum(resid)\n}\nmodel {\n  #Likelihood\n  chisq  ~ dchisqr(k)\n  #Priors\n  k ~ dunif(0.01,100)\n }\n\"\n## write the model to a text file \nwriteLines(modelString2,con=\"chi2model2.txt\")\n\n\nFirst of all, the standardized residuals and chi-square statistic are calculated according to the formula listed above.\nThe likelihood model indicates that the chi-squared statistic can be modeled by a \\(\\chi^2\\) distribution with a centrality parameter of \\(k\\).\nThe prior on \\(k\\) is defined as a uniform (thus vague) flat prior whose values could range from \\(0.01\\) to \\(100\\) (all with equal probability).\n\nDefine the data list. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\n# The observed item frequencies\nobs &lt;- c(15, 9, 23)\n# The expected item frequencies (for a 1:1:1 ratio)\nexp &lt;- rep(sum(obs) * 1/3, 3)\ndata.list &lt;- list(obs = obs, exp = exp, n = 3)\ndata.list\n\nNA $obs\nNA [1] 15  9 23\nNA \nNA $exp\nNA [1] 15.66667 15.66667 15.66667\nNA \nNA $n\nNA [1] 3\n\n# The expected item frequencies (for a 1:1:2 ratio)\nexp &lt;- sum(obs) * c(1/4, 1/4, 2/4)\ndata.list1 &lt;- list(obs = obs, exp = exp, n = 3)\ndata.list1\n\nNA $obs\nNA [1] 15  9 23\nNA \nNA $exp\nNA [1] 11.75 11.75 23.50\nNA \nNA $n\nNA [1] 3\n\n\nDefine the parameters to monitor and the chain details\n\nparams &lt;- c(\"chisq\", \"resid\", \"k\")\nnChains = 2\nburnInSteps = 1000\nthinSteps = 1\nnumSavedSteps = 5000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nFit the model in JAGS using the function jags in the package R2jags (which should be loaded first).\n\n# Fit the model for the 1:1:1 ratio\ndata.r2jags2 &lt;- jags(data = data.list, inits = NULL, parameters.to.save = params, \n                    model.file = \"chi2model2.txt\",n.chains = nChains, n.iter = nIter, \n                    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling data graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA    Initializing\nNA    Reading data back into data table\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 1\nNA    Unobserved stochastic nodes: 1\nNA    Total graph size: 14\nNA \nNA Initializing model\n\nprint(data.r2jags2)\n\nNA Inference for Bugs model at \"chi2model2.txt\", fit using jags,\nNA  2 chains, each with 2500 iterations (first 1000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect  2.5%   25%   50%    75%  97.5%  Rhat n.eff\nNA chisq      6.298   0.000 6.298 6.298 6.298  6.298  6.298 1.000     1\nNA k          8.265   3.522 2.459 5.662 7.934 10.464 15.923 1.001  3000\nNA resid[1]   0.028   0.000 0.028 0.028 0.028  0.028  0.028 1.000     1\nNA resid[2]   2.837   0.000 2.837 2.837 2.837  2.837  2.837 1.000     1\nNA resid[3]   3.433   0.000 3.433 3.433 3.433  3.433  3.433 1.000     1\nNA deviance   5.285   1.381 4.346 4.447 4.775  5.529  9.166 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 1.0 and DIC = 6.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n# Fit the model for the 1:1:2 ratio\ndata.r2jags2.1 &lt;- jags(data = data.list1, inits = NULL, parameters.to.save = params, \n                    model.file = \"chi2model2.txt\",n.chains = nChains, n.iter = nIter, \n                    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling data graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA    Initializing\nNA    Reading data back into data table\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 1\nNA    Unobserved stochastic nodes: 1\nNA    Total graph size: 14\nNA \nNA Initializing model\n\nprint(data.r2jags2.1)\n\nNA Inference for Bugs model at \"chi2model2.txt\", fit using jags,\nNA  2 chains, each with 2500 iterations (first 1000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff\nNA chisq      1.553   0.000 1.553 1.553 1.553 1.553 1.553 1.000     1\nNA k          3.349   1.868 0.556 1.918 3.119 4.499 7.568 1.002  3000\nNA resid[1]   0.899   0.000 0.899 0.899 0.899 0.899 0.899 1.000     1\nNA resid[2]   0.644   0.000 0.644 0.644 0.644 0.644 0.644 1.000     1\nNA resid[3]   0.011   0.000 0.011 0.011 0.011 0.011 0.011 1.000     1\nNA deviance   3.836   1.345 2.870 2.981 3.329 4.150 7.652 1.002  1700\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 0.9 and DIC = 4.7\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#model-evaluation-1",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#model-evaluation-1",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndenplot(data.r2jags2, parms = c(\"k\"))\n\n\n\n\n\n\n\ntraplot(data.r2jags2, parms = c(\"k\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(data.r2jags2))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\nautocorr.diag(as.mcmc(data.r2jags2))\n\nNA        chisq     deviance           k resid[1] resid[2] resid[3]\nNA Lag 0    NaN  1.000000000 1.000000000      NaN      NaN      NaN\nNA Lag 1    NaN  0.433306337 0.271791566      NaN      NaN      NaN\nNA Lag 5    NaN  0.044189125 0.033931771      NaN      NaN      NaN\nNA Lag 10   NaN -0.042022121 0.002692530      NaN      NaN      NaN\nNA Lag 50   NaN -0.006497684 0.001119042      NaN      NaN      NaN\n\n\nConclusions: The trace plots show what appears to be “random noise” about the parameter value. There is no real suggestion of a step or dramatic change in the trend direction along the length of the sampling chain. The samples seem relatively stable. Thus it would seem that the chains are well mixed and have converged. The density plot (for \\(k\\)) is not symmetrical. This suggests that the mean is not a good point estimate for this parameter - the median would be better."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#exploring-model-parameters",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#exploring-model-parameters",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Exploring model parameters",
    "text": "Exploring model parameters\n\nprint(data.r2jags2)\n\nNA Inference for Bugs model at \"chi2model2.txt\", fit using jags,\nNA  2 chains, each with 2500 iterations (first 1000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect  2.5%   25%   50%    75%  97.5%  Rhat n.eff\nNA chisq      6.298   0.000 6.298 6.298 6.298  6.298  6.298 1.000     1\nNA k          8.265   3.522 2.459 5.662 7.934 10.464 15.923 1.001  3000\nNA resid[1]   0.028   0.000 0.028 0.028 0.028  0.028  0.028 1.000     1\nNA resid[2]   2.837   0.000 2.837 2.837 2.837  2.837  2.837 1.000     1\nNA resid[3]   3.433   0.000 3.433 3.433 3.433  3.433  3.433 1.000     1\nNA deviance   5.285   1.381 4.346 4.447 4.775  5.529  9.166 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 1.0 and DIC = 6.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nConclusions: The median degrees of freedom (\\(k\\)) was \\(8.00\\) with a \\(95\\)% spread of \\(2.31-16.16\\). This interval does not include the value of \\(2\\) (expected value of the chi2 distribution for this hypothesis). Hence there is evidence that the population ratio deviates from a 1:1:1 ratio.\n\nprint(data.r2jags2.1)\n\nNA Inference for Bugs model at \"chi2model2.txt\", fit using jags,\nNA  2 chains, each with 2500 iterations (first 1000 discarded)\nNA  n.sims = 3000 iterations saved\nNA          mu.vect sd.vect  2.5%   25%   50%   75% 97.5%  Rhat n.eff\nNA chisq      1.553   0.000 1.553 1.553 1.553 1.553 1.553 1.000     1\nNA k          3.349   1.868 0.556 1.918 3.119 4.499 7.568 1.002  3000\nNA resid[1]   0.899   0.000 0.899 0.899 0.899 0.899 0.899 1.000     1\nNA resid[2]   0.644   0.000 0.644 0.644 0.644 0.644 0.644 1.000     1\nNA resid[3]   0.011   0.000 0.011 0.011 0.011 0.011 0.011 1.000     1\nNA deviance   3.836   1.345 2.870 2.981 3.329 4.150 7.652 1.002  1700\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 0.9 and DIC = 4.7\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nConclusions: The median degrees of freedom (\\(k\\)) was \\(3.31\\) with a \\(95\\)% spread of \\(0.57-7.61\\). This interval comfortably includes the value of \\(2\\) (expected value of the chi2 distribution for this hypothesis). Hence there is no evidence that the population ratio deviates from a 1:1:2 ratio."
  },
  {
    "objectID": "tutorials/2020-02-12-gof-tests-jags/index.html#exploration-of-the-trends",
    "href": "tutorials/2020-02-12-gof-tests-jags/index.html#exploration-of-the-trends",
    "title": "Goodness of fit tests (JAGS)",
    "section": "Exploration of the trends",
    "text": "Exploration of the trends\nThere are a number of avenues we could take in order to explore the data and models further. One thing we could do is calculate the probability that \\(k\\) is greater than $24 (the expected value) for each hypothesis. This can be done either by modifying the JAGS code to include a derivative that uses the step function, or we can derive it within R from the \\(k\\) samples. Lets explore the latter.\n\nk &lt;- data.r2jags2$BUGSoutput$sims.matrix[, \"k\"]\npr &lt;- sum(k &gt; 2)/length(k)\npr\n\nNA [1] 0.982\n\nk &lt;- data.r2jags2.1$BUGSoutput$sims.matrix[, \"k\"]\npr1 &lt;- sum(k &gt; 2)/length(k)\npr1\n\nNA [1] 0.7323333\n\n\nConclusions: the probability that the expected value exceeds \\(2\\) for the 1:1:1 hypothesis is \\(0.982\\) (\\(98.2\\)%). There is an \\(98.2\\)% likelihood that the population is not 1:1:1. We could also compare the two alternative hypotheses. The 1:1:2 hypothesis has lower DIC and is therefore considered a better fit (\\(4.7\\) vs \\(6.4\\)). This is a difference in DIC of around \\(1.7\\) units. So the data have higher support for a 1:1:2 population ratio than a 1:1:1 ratio."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html",
    "href": "tutorials/2020-02-14-glm2-jags/index.html",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#introduction",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#introduction",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nWhilst in many instances, count data can be approximated reasonably well by a normal distribution (particularly if the counts are all above zero and the mean count is greater than about \\(20\\)), more typically, when count data are modelled via normal distribution certain undesirable characteristics arise that are a consequence of the nature of discrete non-negative data.\n\nExpected (predicted) values and confidence bands less than zero are illogical, yet these are entirely possible from a normal distribution\nThe distribution of count data are often skewed when their mean is low (in part because the distribution is truncated to the left by zero) and variance usually increases with increasing mean (variance is typically proportional to mean in count data). By contrast, the Gaussian (normal) distribution assumes that mean and variance are unrelated and thus estimates (particularly of standard error) might well be reasonable inaccurate.\n\nPoisson regression is a type of generalised linear model (GLM) in which a non-negative integer (natural number) response is modelled against a linear predictor via a specific link function. The linear predictor is typically a linear combination of effects parameters. The role of the link function is to transform the expected values of the response y (which is on the scale of (\\(0;\\infty\\)), as is the Poisson distribution from which expectations are drawn) into the scale of the linear predictor (which is \\(-\\infty;\\infty\\)).\nAs implied in the name of this group of analyses, a Poisson rather than Gaussian (normal) distribution is used to represent the errors (residuals). Like count data (number of individuals, species etc), the Poisson distribution encapsulates positive integers and is bound by zero at one end. Consequently, the degree of variability is directly related the expected value (equivalent to the mean of a Gaussian distribution). Put differently, the variance is a function of the mean. Repeated observations from a Poisson distribution located close to zero will yield a much smaller spread of observations than will samples drawn from a Poisson distribution located a greater distance from zero. In the Poisson distribution, the variance has a 1:1 relationship with the mean. The canonical link function for the Poisson distribution is a log-link function.\nWhilst the expectation that the mean=variance (\\(\\mu=\\sigma\\)) is broadly compatible with actual count data (that variance increases at the same rate as the mean), under certain circumstances, this might not be the case. For example, when there are other unmeasured influences on the response variable, the distribution of counts might be somewhat clumped which can result in higher than expected variability (that is \\(\\sigma &gt; \\mu\\)). The variance increases more rapidly than does the mean. This is referred to as overdispersion. The degree to which the variability is greater than the mean (and thus the expected degree of variability) is called dispersion. Effectively, the Poisson distribution has a dispersion parameter (or scaling factor) of \\(1\\).\nIt turns out that overdispersion is very common for count data and it typically underestimates variability, standard errors and thus deflated p-values. There are a number of ways of overcoming this limitation, the effectiveness of which depend on the causes of overdispersion.\nQuasi-Poisson models - these introduce the dispersion parameter (\\(\\phi\\)) into the model. This approach does not utilize an underlying error distribution to calculate the maximum likelihood (there is no quasi-Poisson distribution). Instead, if the Newton-Ralphson iterative reweighting least squares algorithm is applied using a direct specification of the relationship between mean and variance (\\(\\text{var}(y)=\\phi\\mu\\), the estimates of the regression coefficients are identical to those of the maximum likelihood estimates from the Poisson model. This is analogous to fitting ordinary least squares on symmetrical, yet not normally distributed data - the parameter estimates are the same, however they won’t necessarily be as efficient. The standard errors of the coefficients are then calculated by multiplying the Poisson model coefficient standard errors by \\(\\sqrt{\\phi}\\). Unfortunately, because the quasi-poisson model is not estimated via maximum likelihood, properties such as AIC and log-likelihood cannot be derived. Consequently, quasi-poisson and Poisson model fits cannot be compared via either AIC or likelihood ratio tests (nor can they be compared via deviance as uasi-poisson and Poisson models have the same residual deviance). That said, quasi-likelihood can be obtained by dividing the likelihood from the Poisson model by the dispersion (scale) factor.\nNegative binomial model - technically, the negative binomial distribution is a probability distribution for the number of successes before a specified number of failures. However, the negative binomial can also be defined (parameterised) in terms of a mean (\\(\\mu\\)) and scale factor (\\(\\omega\\)),\n\\[\np(y_i) = \\frac{\\Gamma(y_i+\\omega)}{\\Gamma(\\omega)y!} \\times \\frac{\\mu^{y_i}_i\\omega^\\omega}{(\\mu_i+\\omega)^{\\mu_i+\\omega}},\n\\]\nwhere the expectected value of the values \\(y_i\\) (the means) are (\\(\\mu_i\\)) and the variance is \\(y_i=\\frac{\\mu_i+\\mu^2_i}{\\omega}\\). In this way, the negative binomial is a two-stage hierarchical process in which the response is modeled against a Poisson distribution whose expected count is in turn modeled by a Gamma distribution with a mean of \\(\\mu\\) and constant scale parameter (\\(\\omega\\)). Strictly, the negative binomial is not an exponential family distribution (unless \\(\\omega\\) is fixed as a constant), and thus negative binomial models cannot be fit via the usual GLM iterative reweighting algorithm. Instead estimates of the regression parameters along with the scale factor (\\(\\omega\\)) are obtained via maximum likelihood. The negative binomial model is useful for accommodating overdispersal when it is likely caused by clumping (due to the influence of other unmeasured factors) within the response.\nZero-inflated Poisson model - overdispersion can also be caused by the presence of a greater number of zero’s than would otherwise be expected for a Poisson distribution. There are potentially two sources of zero counts - genuine zeros and false zeros. Firstly, there may genuinely be no individuals present. This would be the number expected by a Poisson distribution. Secondly, individuals may have been present yet not detected or may not even been possible. These are false zero’s and lead to zero inflated data (data with more zeros than expected). For example, the number of joeys accompanying an adult koala could be zero because the koala has no offspring (true zero) or because the koala is male or infertile (both of which would be examples of false zeros). Similarly, zero counts of the number of individual in a transect are due either to the absence of individuals or the inability of the observer to detect them. Whilst in the former example, the latent variable representing false zeros (sex or infertility) can be identified and those individuals removed prior to analysis, this is not the case for the latter example. That is, we cannot easily partition which counts of zero are due to detection issues and which are a true indication of the natural state.\nConsistent with these two sources of zeros, zero-inflated models combine a binary logistic regression model (that models count membership according to a latent variable representing observations that can only be zeros - not detectable or male koalas) with a Poisson regression (that models count membership according to a latent variable representing observations whose values could be 0 or any positive integer - fertile female koalas)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#poisson-regression",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#poisson-regression",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Poisson regression",
    "text": "Poisson regression\nThe following equations are provided since in Bayesian modelling, it is occasionally necessary to directly define the log-likelihood calculations (particularly for zero-inflated models and other mixture models).\n\\[\nf(y \\mid \\lambda) = \\frac{\\lambda^ye^{-\\lambda}}{y!},\n\\]\nwhere \\(E[Y]=Var(Y)=\\lambda\\) and \\(\\lambda\\) is the mean."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#data-generation",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#data-generation",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(8)\n#The number of samples\nn.x &lt;- 20\n#Create x values that at uniformly distributed throughout the rate of 1 to 20\nx &lt;- sort(runif(n = n.x, min = 1, max =20))\nmm &lt;- model.matrix(~x)\nintercept &lt;- 0.6\nslope=0.1\n#The linear predictor\nlinpred &lt;- mm %*% c(intercept,slope)\n#Predicted y values\nlambda &lt;- exp(linpred)\n#Add some noise and make binomial\ny &lt;- rpois(n=n.x, lambda=lambda)\ndat &lt;- data.frame(y,x)\n\nWith these sort of data, we are primarily interested in investigating whether there is a relationship between the binary response variable and the linear predictor (linear combination of one or more continuous or categorical predictors)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are at least five main potential models we could consider fitting to these data:\n\nOrdinary least squares regression (general linear model) - assumes normality of residuals\nPoisson regression - assumes mean=variance (dispersion\\(=1\\))\nQuasi-poisson regression - a general solution to overdispersion. Assumes variance is a function of mean, dispersion estimated, however likelihood based statistics unavailable\nNegative binomial regression - a specific solution to overdispersion caused by clumping (due to an unmeasured latent variable). Scaling factor (\\(\\omega\\)) is estimated along with the regression parameters.\nZero-inflation model - a specific solution to overdispersion caused by excessive zeros (due to an unmeasured latent variable). Mixture of binomial and Poisson models.\n\nWhen counts are all very large (not close to \\(0\\)) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson model is likely to be more appropriate than a standard Gaussian model. The potential for overdispersion can be explored by adding a rug to boxplot. The rug is simply tick marks on the inside of an axis at the position corresponding to an observation. As multiple identical values result in tick marks drawn over one another, it is typically a good idea to apply a slight amount of jitter (random displacement) to the values used by the rug.\n\nhist(dat$x)\n\n\n\n\n\n\n\nboxplot(dat$y, horizontal=TRUE)\nrug(jitter(dat$y), side=1)\n\n\n\n\n\n\n\n\nThere is definitely signs of non-normality that would warrant Poisson models. The rug applied to the boxplots does not indicate a series degree of clumping and there appears to be few zero. Thus overdispersion is unlikely to be an issue. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)) and a scatterplot of the relationship between the response (\\(y\\)) and the predictor (\\(x\\))\n\n#now for the scatterplot\nplot(y~x, dat, log=\"y\")\nwith(dat, lines(lowess(y~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables.\n\nAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\n#proportion of 0's in the data\ndat.tab&lt;-table(dat$y==0)\ndat.tab/sum(dat.tab)\n\nNA \nNA FALSE \nNA     1\n\n#proportion of 0's expected from a Poisson distribution\nmu &lt;- mean(dat$y)\ncnts &lt;- rpois(1000, mu)\ndat.tab &lt;- table(cnts == 0)\ndat.tab/sum(dat.tab)\n\nNA \nNA FALSE  TRUE \nNA 0.997 0.003\n\n\nIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, there are no zeros in the observed data which corresponds closely to the very low proportion expected (\\(0.003\\))."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\\[\ny_i \\sim \\text{Pois}(\\lambda_i),\n\\]\nwhere \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\).\n\ndat.list &lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     Y[i] ~ dpois(lambda[i])\n     log(lambda[i]) &lt;- beta0 + beta1*X[i]\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n} \n\"\nwriteLines(modelString, con='modelpois.txt')\n\nparams &lt;- c('beta0','beta1')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\nlibrary(R2jags)\ndat.P.jags &lt;- jags(data=dat.list,model.file='modelpois.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 105\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nlibrary(mcmcplots)\ndenplot(dat.P.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\ntraplot(dat.P.jags, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.P.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    10       10830 3746         2.89      \nNA  beta1    12       12612 3746         3.37      \nNA  deviance 3        4410  3746         1.18      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    12       14878 3746         3.97      \nNA  beta1    10       11942 3746         3.19      \nNA  deviance 2        3995  3746         1.07\n\nautocorr.diag(as.mcmc(dat.P.jags))\n\nNA              beta0         beta1     deviance\nNA Lag 0   1.00000000  1.0000000000  1.000000000\nNA Lag 1   0.83977964  0.8111423616  0.508803232\nNA Lag 5   0.43884918  0.3859514845  0.118732714\nNA Lag 10  0.22584100  0.1883831873  0.029775648\nNA Lag 50 -0.01164622 -0.0003926876 -0.007507996\n\n\nOne very important model validation procedure is to examine a plot of residuals against predicted or fitted values (the residual plot). Ideally, residual plots should show a random scatter of points without outliers. That is, there should be no patterns in the residuals. Patterns suggest inappropriate linear predictor (or scale) and/or inappropriate residual distribution/link function. The residuals used in such plots should be standardized (particularly if the model incorporated any variance-covariance structures - such as an autoregressive correlation structure) Pearsons’s residuals standardize residuals by division with the square-root of the variance. We can generate Pearson’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS. Pearson’s residuals are calculated according to:\n\\[\n\\epsilon = \\frac{y_i - \\mu}{\\sqrt{\\text{var}(y)}},\n\\]\nwhere \\(\\mu\\) is the expected value of \\(Y\\) (\\(=\\lambda\\) for Poisson) and var(\\(y\\)) is the variance of \\(Y\\) (\\(=\\lambda\\) for Poisson).\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]\nXmat &lt;- model.matrix(~x, data=dat)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n#Expected value and variance are both equal to lambda\nexpY &lt;- varY &lt;- lambda\n#sweep across rows and then divide by lambda\nResid &lt;- -1*sweep(expY,2,dat$y,'-')/sqrt(varY)\n#plot residuals vs expected values\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\n\n\n\n\n\n\n\n\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data.\n\nSSres&lt;-apply(Resid^2,1,sum)\n\n#generate a matrix of draws from a poisson distribution\n# the matrix is the same dimensions as lambda and uses the probabilities of lambda\nYNew &lt;- matrix(rpois(length(lambda),lambda=lambda),nrow=nrow(lambda))\n\nResid1&lt;-(lambda - YNew)/sqrt(lambda)\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm = T)\n\nNA [1] 0.4697"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Goodness of fit",
    "text": "Goodness of fit\n\ndat.list1 &lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n    #likelihood function\n    Y[i] ~ dpois(lambda[i])\n    eta[i] &lt;- beta0+beta1*X[i] #linear predictor\n    log(lambda[i]) &lt;- eta[i]   #link function\n\n    #E(Y) and var(Y)\n    expY[i] &lt;- lambda[i]\n    varY[i] &lt;- lambda[i]\n\n    # Calculate RSS\n    Resid[i] &lt;- (Y[i] - expY[i])/sqrt(varY[i])\n    RSS[i] &lt;- pow(Resid[i],2)\n\n    #Simulate data from a Poisson distribution\n    Y1[i] ~ dpois(lambda[i])\n    #Calculate RSS for simulated data\n    Resid1[i] &lt;- (Y1[i] - expY[i])/sqrt(varY[i])\n    RSS1[i] &lt;-pow(Resid1[i],2) \n  }\n  #Priors\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  #Bayesian P-value\n  Pvalue &lt;- mean(sum(RSS1)&gt;sum(RSS))\n} \n\"\n\nwriteLines(modelString, con='modelpois_gof.txt')\n\nparams &lt;- c('beta0','beta1', 'Pvalue')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.P.jags1 &lt;- jags(data=dat.list,model.file='modelpois_gof.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 22\nNA    Total graph size: 272\nNA \nNA Initializing model\n\nprint(dat.P.jags1)\n\nNA Inference for Bugs model at \"modelpois_gof.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA Pvalue     0.478   0.500  0.000  0.000  0.000  1.000  1.000 1.001 10000\nNA beta0      0.546   0.254  0.015  0.381  0.559  0.719  1.013 1.001 10000\nNA beta1      0.112   0.018  0.077  0.099  0.111  0.124  0.149 1.001  3200\nNA deviance  88.372   3.041 86.373 86.883 87.671 89.075 93.868 1.006  2000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 4.6 and DIC = 93.0\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nConclusions: the Bayesian p-value is approximately \\(0.5\\), suggesting that there is a good fit of the model to the data.\nUnfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function. Now we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Bernoulli distribution matching that estimated by the model. Essentially this is estimating how well the Bernoulli distribution and linear model approximates the observed data.\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.P.jags$BUGSoutput$sims.matrix[,1:2]\nXmat &lt;- model.matrix(~x, data=dat)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n\nsimRes &lt;- function(lambda, data,n=250, plot=T, family='poisson') {\n require(gap)\n N = nrow(data)\n sim = switch(family,\n    'poisson' = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE)\n )\n a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\n resid&lt;-NULL\n for (i in 1:nrow(data)) resid&lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\n if (plot==T) {\n   par(mfrow=c(1,2))\n   gap::qqunif(resid,pch = 2, bty = \"n\",\n   logscale = F, col = \"black\", cex = 0.6, main = \"QQ plot residuals\",\n   cex.main = 1, las=1)\n   plot(resid~apply(lambda,2,mean), xlab='Predicted value', ylab='Standardized residual', las=1)\n }\n resid\n}\n\nsimRes(lambda,dat, family='poisson')\n\n\n\n\n\n\n\n\nNA  [1] 0.220 0.544 0.532 0.344 0.812 0.980 0.048 0.592 0.548 0.728 0.164 0.492\nNA [13] 0.856 0.096 0.240 0.292 0.876 0.880 0.148 0.748\n\n\nThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion).\nRecall that the Poisson regression model assumes that variance=mean (var=μϕ where \\(\\phi=1\\)) and thus dispersion (\\(\\phi=\\frac{\\text{var}}{\\mu}=1)\\)). However, we can also calculate approximately what the dispersion factor would be by using sum square of the residuals as a measure of variance and the model residual degrees of freedom as a measure of the mean (since the expected value of a Poisson distribution is the same as its degrees of freedom).\n\\[\n\\phi = \\frac{RSS}{df},\n\\]\nwhere \\(df=n−k\\) and \\(k\\) is the number of estimated model coefficients.\n\nResid &lt;- -1*sweep(lambda,2,dat$y,'-')/sqrt(lambda)\nRSS&lt;-apply(Resid^2,1,sum)\n(df&lt;-nrow(dat)-ncol(coefs))\n\nNA [1] 18\n\nDisp &lt;- RSS/df\ndata.frame(Median=median(Disp), Mean=mean(Disp), HPDinterval(as.mcmc(Disp)),\n           HPDinterval(as.mcmc(Disp),p=0.5))\n\nNA        Median     Mean     lower    upper   lower.1  upper.1\nNA var1 1.053527 1.110853 0.9299722 1.449502 0.9300381 1.053579\n\n\nWe can incorporate the dispersion statistic directly into JAGS.\n\ndat.list &lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     Y[i] ~ dpois(lambda[i])\n     eta[i] &lt;- beta0 + beta1*X[i]\n     log(lambda[i]) &lt;- eta[i]\n     expY[i] &lt;- lambda[i]\n     varY[i] &lt;- lambda[i]\n     Resid[i] &lt;- (Y[i] - expY[i])/sqrt(varY[i]) \n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  RSS &lt;- sum(pow(Resid,2))\n  df &lt;- N-2\n  phi &lt;- RSS/df\n} \n\"\n\nwriteLines(modelString, con='modelpois_disp.txt')\n\nparams &lt;- c('beta0','beta1','phi')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.P.jags &lt;- jags(data=dat.list,model.file='modelpois_disp.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 171\nNA \nNA Initializing model\n\nprint(dat.P.jags)\n\nNA Inference for Bugs model at \"modelpois_disp.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.552   0.256  0.039  0.382  0.557  0.724  1.042 1.001 10000\nNA beta1      0.111   0.019  0.074  0.099  0.112  0.124  0.147 1.001  2800\nNA phi        1.105   0.246  0.934  0.977  1.048  1.169  1.581 1.001 10000\nNA deviance  88.354   2.633 86.368 86.896 87.709 89.074 93.897 1.002  4300\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.5 and DIC = 91.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nThe dispersion statistic is close to \\(1\\) and thus there is no evidence that the data were overdispersed. The Poisson distribution was therefore appropriate."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploring the model parameters",
    "text": "Exploring the model parameters\nIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics.\n\nlibrary(coda)\nprint(dat.P.jags)\n\nNA Inference for Bugs model at \"modelpois_disp.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.552   0.256  0.039  0.382  0.557  0.724  1.042 1.001 10000\nNA beta1      0.111   0.019  0.074  0.099  0.112  0.124  0.147 1.001  2800\nNA phi        1.105   0.246  0.934  0.977  1.048  1.169  1.581 1.001 10000\nNA deviance  88.354   2.633 86.368 86.896 87.709 89.074 93.897 1.002  4300\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.5 and DIC = 91.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nlibrary(plyr)\nadply(dat.P.jags$BUGSoutput$sims.matrix[,1:2], 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1    Median      Mean      lower     upper    lower.1   upper.1\nNA 1 beta0 0.5570252 0.5517525 0.03871735 1.0423628 0.39092213 0.7317579\nNA 2 beta1 0.1115363 0.1113176 0.07499903 0.1484004 0.09893134 0.1239861\n\n#on original scale\nadply(exp(dat.P.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1   Median     Mean     lower    upper  lower.1  upper.1\nNA 1 beta0 1.745472 1.793464 0.9803783 2.734057 1.423789 2.013575\nNA 2 beta1 1.117994 1.117948 1.0778831 1.159977 1.101510 1.129458\n\n\nConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in x is associated with a significant linear increase (positive slope) in the abundance of y. Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.11\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a ($e^{ 0.11} = 1.12 $) \\(1.12\\) unit increase in the abundance of \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Explorations of the trends",
    "text": "Explorations of the trends\nA measure of the strength of the relationship can be obtained according to:\n\\[\nR^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\n\\]\n\nXmat &lt;- model.matrix(~x, data=dat)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n#calculate the raw SS residuals\nSSres &lt;- apply((-1*(sweep(lambda,2,dat$y,'-')))^2,1,sum)\nSSres.null &lt;- sum((dat$y - mean(dat$y))^2)\n#OR \nSSres.null &lt;- crossprod(dat$y - mean(dat$y))\n#calculate the model r2\n1-mean(SSres)/SSres.null\n\nNA           [,1]\nNA [1,] 0.6569594\n\n\nConclusions: \\(65\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). We can also do it directly into JAGS.\n\ndat.list &lt;- with(dat,list(Y=y, X=x,N=nrow(dat)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     Y[i] ~ dpois(lambda[i])\n     eta[i] &lt;- beta0 + beta1*X[i]\n     log(lambda[i]) &lt;- eta[i]\n     res[i] &lt;- Y[i] - lambda[i]\n     resnull[i] &lt;- Y[i] - meanY\n  }\n  meanY &lt;- mean(Y)\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  RSS &lt;- sum(res^2)\n  RSSnull &lt;- sum(resnull^2)\n  r2 &lt;- 1-RSS/RSSnull\n} \n\"\n\nwriteLines(modelString, con='modelpois_disp_r2.txt')\n\nparams &lt;- c('beta0','beta1','r2')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.P.jags &lt;- jags(data=dat.list,model.file='modelpois_disp_r2.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 150\nNA \nNA Initializing model\n\nprint(dat.P.jags)\n\nNA Inference for Bugs model at \"modelpois_disp_r2.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.547   0.257  0.024  0.379  0.556  0.721  1.032 1.001  3900\nNA beta1      0.112   0.019  0.077  0.100  0.112  0.125  0.150 1.001  7000\nNA r2         0.655   0.057  0.510  0.640  0.672  0.690  0.701 1.001 10000\nNA deviance  88.383   2.776 86.372 86.904 87.733 89.122 93.692 1.003  6200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.9 and DIC = 92.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nFinally, we will create a summary plot.\n\npar(mar = c(4, 5, 0, 0))\nplot(y ~ x, data = dat, type = \"n\", ann = F, axes = F)\npoints(y ~ x, data = dat, pch = 16)\nxs &lt;- seq(min(dat$x,na.rm=TRUE),max(dat$x,na.rm=TRUE), l = 1000)\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nys &lt;- exp(eta)\nlibrary(plyr)\nlibrary(coda)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\npoints(Median ~ x, data=data.tab,col = \"black\", type = \"l\")\nlines(lower ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\nlines(upper ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\n\naxis(1)\nmtext(\"X\", 1, cex = 1.5, line = 3)\naxis(2, las = 2)\nmtext(\"Abundance of Y\", 2, cex = 1.5, line = 3)\nbox(bty = \"l\")"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Full log-likelihood function",
    "text": "Full log-likelihood function\nNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\nXmat &lt;- model.matrix(~x, dat)\nnX &lt;- ncol(Xmat)\ndat.list2 &lt;- with(dat,list(Y=y, X=Xmat,N=nrow(dat), mu=rep(0,nX),\n                  Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     zeros[i] ~ dpois(zeros.lambda[i])\n     zeros.lambda[i] &lt;- -ll[i] + C     \n     ll[i] &lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)\n     eta[i] &lt;- inprod(beta[], X[i,])\n     log(lambda[i]) &lt;- eta[i]\n    llm[i] &lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)\n  }\n  meanlambda &lt;- mean(lambda)\n  beta ~ dmnorm(mu[],Sigma[,])\n  dev &lt;- sum(-2*ll)\n  pD &lt;- mean(dev)-sum(-2*llm)\n  AIC &lt;- min(dev+(2*pD))\n} \n\"\n\nwriteLines(modelString, con='modelpois_ll.txt')\n\nparams &lt;- c('beta','dev','AIC')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.P.jags3 &lt;- jags(data=dat.list2,model.file='modelpois_ll.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 1\nNA    Total graph size: 353\nNA \nNA Initializing model\n\nprint(dat.P.jags3)\n\nNA Inference for Bugs model at \"modelpois_ll.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA             mu.vect sd.vect       2.5%        25%        50%        75%\nNA AIC          13.728   4.725      9.624     10.548     11.952     15.158\nNA beta[1]       0.481   0.259     -0.079      0.319      0.506      0.669\nNA beta[2]       0.116   0.019      0.084      0.103      0.114      0.128\nNA dev          88.382   2.009     86.361     86.883     87.731     89.265\nNA deviance 400088.382   2.009 400086.361 400086.883 400087.731 400089.265\nNA               97.5%  Rhat n.eff\nNA AIC          26.878 1.016   180\nNA beta[1]       0.922 1.037    49\nNA beta[2]       0.155 1.029    60\nNA dev          94.071 1.009   300\nNA deviance 400094.071 1.000     1\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 2.0 and DIC = 400090.4\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(37) #16 #35\n#The number of samples\nn.x &lt;- 20\n#Create x values that at uniformly distributed throughout the rate of 1 to 20\nx &lt;- sort(runif(n = n.x, min = 1, max =20))\nmm &lt;- model.matrix(~x)\nintercept &lt;- 0.6\nslope=0.1\n#The linear predictor\nlinpred &lt;- mm %*% c(intercept,slope)\n#Predicted y values\nlambda &lt;- exp(linpred)\n#Add some noise and make binomial\ny &lt;- rnbinom(n=n.x, mu=lambda, size=1)\ndat.nb &lt;- data.frame(y,x)\n\nWhen counts are all very large (not close to \\(0\\)) and their ranges do not span orders of magnitude, they take on very Gaussian properties (symmetrical distribution and variance independent of the mean). Given that models based on the Gaussian distribution are more optimized and recognized than Generalized Linear Models, it can be prudent to adopt Gaussian models for such data. Hence it is a good idea to first explore whether a Poisson or Negative Binomial model is likely to be more appropriate than a standard Gaussian model. Recall from Poisson regression, there are five main potential models that we could consider fitting to these data.\n\nhist(dat$x)\n\n\n\n\n\n\n\n#now for the scatterplot\nplot(y~x, dat.nb, log=\"y\")\nwith(dat.nb, lines(lowess(y~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the scatterplot does not display major deviations from a straight line and thus linearity is satisfied. Violations of linearity could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables.\n\nAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\n#proportion of 0's in the data\ndat.nb.tab&lt;-table(dat.nb$y==0)\ndat.nb.tab/sum(dat.nb.tab)\n\nNA \nNA FALSE  TRUE \nNA  0.95  0.05\n\n#proportion of 0's expected from a Poisson distribution\nmu &lt;- mean(dat.nb$y)\ncnts &lt;- rpois(1000, mu)\ndat.nb.tabE &lt;- table(cnts == 0)\ndat.nb.tabE/sum(dat.nb.tabE)\n\nNA \nNA FALSE \nNA     1\n\n\nIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed is similar to the proportion expected. Indeed, there was only a single zero observed. Hence it is likely that if there is overdispersion it is unlikely to be due to excessive zeros."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\\[\ny_i \\sim \\text{NegBin}(p_i,r),\n\\]\nwhere \\(p_i=\\frac{r}{r+\\lambda_i}\\), with \\(\\log(\\lambda_i)=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1 \\sim N(0, 10000)\\), \\(r \\sim \\text{Unif}(0.001,1000)\\).\n\ndat.nb.list &lt;- with(dat.nb,list(Y=y, X=x,N=nrow(dat.nb)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     Y[i] ~ dnegbin(p[i],size)\n     p[i] &lt;- size/(size+lambda[i])\n     log(lambda[i]) &lt;- beta0 + beta1*X[i]\n  }\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  size ~ dunif(0.001,1000)\n  theta &lt;- pow(1/mean(p),2)\n  scaleparam &lt;- mean((1-p)/p) \n} \n\"\nwriteLines(modelString, con='modelnbin.txt')\n\nparams &lt;- c('beta0','beta1', 'size','theta','scaleparam')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.NB.jags &lt;- jags(data=dat.nb.list,model.file='modelnbin.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 157\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndenplot(dat.NB.jags, parms = c(\"beta0\",\"beta1\",\"size\"))\n\n\n\n\n\n\n\ntraplot(dat.NB.jags, parms = c(\"beta0\",\"beta1\",\"size\"))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.NB.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                   \nNA             Burn-in  Total Lower bound  Dependence\nNA             (M)      (N)   (Nmin)       factor (I)\nNA  beta0      16       17518 3746         4.68      \nNA  beta1      24       28713 3746         7.66      \nNA  deviance   3        4198  3746         1.12      \nNA  scaleparam 16       16290 3746         4.35      \nNA  size       4        5038  3746         1.34      \nNA  theta      16       16244 3746         4.34      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                   \nNA             Burn-in  Total Lower bound  Dependence\nNA             (M)      (N)   (Nmin)       factor (I)\nNA  beta0      18       20025 3746         5.35      \nNA  beta1      24       21072 3746         5.63      \nNA  deviance   3        4267  3746         1.14      \nNA  scaleparam 18       19920 3746         5.32      \nNA  size       3        4375  3746         1.17      \nNA  theta      20       20682 3746         5.52\n\nautocorr.diag(as.mcmc(dat.NB.jags))\n\nNA               beta0        beta1     deviance  scaleparam        size\nNA Lag 0   1.000000000  1.000000000  1.000000000  1.00000000 1.000000000\nNA Lag 1   0.855250119  0.856542892  0.566377262  0.33360033 0.684520361\nNA Lag 5   0.519024321  0.521535488  0.163024546  0.07618281 0.220180993\nNA Lag 10  0.276801196  0.280283232  0.025179110  0.02814049 0.039259726\nNA Lag 50 -0.008060569 -0.004454124 -0.003876422 -0.01103395 0.006904325\nNA              theta\nNA Lag 0   1.00000000\nNA Lag 1   0.26024619\nNA Lag 5   0.05872969\nNA Lag 10  0.02940084\nNA Lag 50 -0.01349378\n\n\nWe now explore the goodness of fit of the models via the residuals and deviance. We could calculate the Pearsons’s residuals within the JAGS model. Alternatively, we could use the parameters to generate the residuals outside of JAGS.\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]\nsize &lt;- dat.NB.jags$BUGSoutput$sims.matrix[,'size']\nXmat &lt;- model.matrix(~x, data=dat.nb)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\nvarY &lt;- lambda + (lambda^2)/size\n#sweep across rows and then divide by lambda\nResid &lt;- -1*sweep(lambda,2,dat.nb$y,'-')/sqrt(varY)\n#plot residuals vs expected values\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\n\n\n\n\n\n\n\n\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Negative binomial distribution matching that estimated by the model. Essentially this is estimating how well the Negative binomial distribution, the log-link function and the linear model approximates the observed data.\n\nSSres&lt;-apply(Resid^2,1,sum)\n\n#generate a matrix of draws from a negative binomial distribution\n# the matrix is the same dimensions as pi and uses the probabilities of pi\nYNew &lt;- matrix(rnbinom(length(lambda),mu=lambda, size=size),nrow=nrow(lambda))\nResid1&lt;-(lambda - YNew)/sqrt(varY)\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm = T)\n\nNA [1] 0.4163\n\n\nConclusions: the Bayesian p-value is approximately \\(0.5\\), suggesting that there is a good fit of the model to the data.\nUnfortunately, unlike with linear models (Gaussian family), the expected distribution of data (residuals) varies over the range of fitted values for numerous (often competing) ways that make diagnosing (and attributing causes thereof) miss-specified generalized linear models from standard residual plots very difficult. The use of standardized (Pearson) residuals or deviance residuals can partly address this issue, yet they still do not offer completely consistent diagnoses across all issues (miss-specified model, over-dispersion, zero-inflation). An alternative approach is to use simulated data from the model posteriors to calculate an empirical cumulative density function from which residuals are are generated as values corresponding to the observed data along the density function.\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.NB.jags$BUGSoutput$sims.matrix[,1:2]\nsize &lt;- dat.NB.jags$BUGSoutput$sims.matrix[,'size']\nXmat &lt;- model.matrix(~x, data=dat.nb)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n\nsimRes &lt;- function(lambda, data,n=250, plot=T, family='negbin', size=NULL) {\n require(gap)\n N = nrow(data)\n sim = switch(family,\n    'poisson' = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),\n    'negbin' = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE)\n )\n a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\n resid&lt;-NULL\n for (i in 1:nrow(data)) resid&lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\n if (plot==T) {\n   par(mfrow=c(1,2))\n   gap::qqunif(resid,pch = 2, bty = \"n\",\n   logscale = F, col = \"black\", cex = 0.6, main = \"QQ plot residuals\",\n   cex.main = 1, las=1)\n   plot(resid~apply(lambda,2,mean), xlab='Predicted value', ylab='Standardized residual', las=1)\n }\n resid\n}\n\nsimRes(lambda,dat.nb, family='negbin', size=mean(size))\n\n\n\n\n\n\n\n\nNA  [1] 0.368 0.944 0.456 0.788 0.148 0.928 0.136 0.704 0.164 0.800 0.500 0.464\nNA [13] 0.100 0.216 0.680 0.212 0.000 0.676 0.924 0.852\n\n\nThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploring the model parameters",
    "text": "Exploring the model parameters\nIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\nprint(dat.NB.jags)\n\nNA Inference for Bugs model at \"modelnbin.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA            mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta0        0.731   0.395  -0.023   0.470   0.717   0.984   1.534 1.001 10000\nNA beta1        0.097   0.032   0.034   0.077   0.098   0.118   0.158 1.001 10000\nNA scaleparam   2.787   1.756   0.704   1.670   2.412   3.444   7.089 1.001 10000\nNA size         3.255   2.190   1.055   1.941   2.697   3.853   9.050 1.001 10000\nNA theta       12.548  12.474   2.669   5.892   9.157  14.790  43.249 1.001 10000\nNA deviance   113.053   2.691 110.093 111.115 112.352 114.190 120.305 1.002  2000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.6 and DIC = 116.7\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nadply(dat.NB.jags$BUGSoutput$sims.matrix, 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA           X1       Median         Mean        lower       upper     lower.1\nNA 1      beta0   0.71693048   0.73121205  -0.05129743   1.5032427   0.4523583\nNA 2      beta1   0.09800852   0.09730699   0.03509028   0.1591976   0.0789372\nNA 3   deviance 112.35178835 113.05255254 109.86520971 118.4814291 110.0898498\nNA 4 scaleparam   2.41198253   2.78665197   0.33094006   5.9583607   1.2865037\nNA 5       size   2.69653197   3.25545915   0.68960555   7.2146030   1.4202953\nNA 6      theta   9.15704708  12.54776430   1.61632232  32.9116959   3.6489231\nNA       upper.1\nNA 1   0.9610028\nNA 2   0.1201659\nNA 3 112.4668566\nNA 4   2.8677393\nNA 5   2.9988148\nNA 6  10.3646959\n\n#on original scale\nadply(exp(dat.NB.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1   Median     Mean     lower    upper  lower.1  upper.1\nNA 1 beta0 2.048137 2.249960 0.8335273 4.249614 1.340384 2.309801\nNA 2 beta1 1.102972 1.102753 1.0357132 1.172570 1.080463 1.125944\n\n\nConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in x is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.09\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a ($e^{ 0.09} = 1.02 $) \\(1.02\\) unit increase in the abundance of \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Explorations of the trends",
    "text": "Explorations of the trends\nA measure of the strength of the relationship can be obtained according to:\n\\[\nR^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\n\\]\n\nXmat &lt;- model.matrix(~x, data=dat.nb)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n#calculate the raw SS residuals\nSSres &lt;- apply((-1*(sweep(lambda,2,dat.nb$y,'-')))^2,1,sum)\nSSres.null &lt;- sum((dat.nb$y - mean(dat.nb$y))^2)\n#OR \nSSres.null &lt;- crossprod(dat.nb$y - mean(dat.nb$y))\n#calculate the model r2\n1-mean(SSres)/SSres.null\n\nNA          [,1]\nNA [1,] 0.270553\n\n\nConclusions: \\(27\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). We can also do it directly into JAGS.\nFinally, we will create a summary plot.\n\npar(mar = c(4, 5, 0, 0))\nplot(y ~ x, data = dat.nb, type = \"n\", ann = F, axes = F)\npoints(y ~ x, data = dat.nb, pch = 16)\nxs &lt;- seq(min(dat.nb$x,na.rm=TRUE),max(dat.nb$x,na.rm=TRUE), l = 1000)\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nys &lt;- exp(eta)\nlibrary(plyr)\nlibrary(coda)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\npoints(Median ~ x, data=data.tab,col = \"black\", type = \"l\")\nlines(lower ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\nlines(upper ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\n\naxis(1)\nmtext(\"X\", 1, cex = 1.5, line = 3)\naxis(2, las = 2)\nmtext(\"Abundance of Y\", 2, cex = 1.5, line = 3)\nbox(bty = \"l\")"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Full log-likelihood function",
    "text": "Full log-likelihood function\nNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\nXmat &lt;- model.matrix(~x, dat.nb)\nnX &lt;- ncol(Xmat)\ndat.nb.list2 &lt;- with(dat.nb,list(Y=y, X=Xmat,N=nrow(dat.nb), mu=rep(0,nX),\n                  Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat.nb)), C=10000))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     zeros[i] ~ dpois(zeros.lambda[i])\n     zeros.lambda[i] &lt;- -ll[i] + C     \n     ll[i] &lt;- loggam(Y[i]+size) - loggam(Y[i]+1) - loggam(size) + size*(log(p[i]) - log(p[i]+1)) - \n              Y[i]*log(p[i]+1)\n     p[i] &lt;- size/lambda[i]\n     eta[i] &lt;- inprod(beta[], X[i,])\n     log(lambda[i]) &lt;- eta[i]\n  }\n  beta ~ dmnorm(mu[],Sigma[,])\n  size ~ dunif(0.001,1000)\n  dev &lt;- sum(-2*ll)\n} \n\"\n\nwriteLines(modelString, con='modelnbin_ll.txt')\n\nparams &lt;- c('beta','dev')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.NB.jags3 &lt;- jags(data=dat.nb.list2,model.file='modelnbin_ll.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 2\nNA    Total graph size: 453\nNA \nNA Initializing model\n\nprint(dat.NB.jags3)\n\nNA Inference for Bugs model at \"modelnbin_ll.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA             mu.vect sd.vect       2.5%        25%        50%        75%\nNA beta[1]       0.739   0.386      0.039      0.484      0.726      0.968\nNA beta[2]       0.096   0.031      0.034      0.077      0.096      0.116\nNA dev         112.830   2.548    110.074    111.037    112.105    113.842\nNA deviance 400112.830   2.548 400110.074 400111.037 400112.105 400113.842\nNA               97.5%  Rhat n.eff\nNA beta[1]       1.536 1.015   160\nNA beta[2]       0.153 1.010   230\nNA dev         119.701 1.002  1200\nNA deviance 400119.701 1.000     1\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.2 and DIC = 400116.1\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(9) #34.5  #4 #10 #16 #17 #26\n#The number of samples\nn.x &lt;- 20\n#Create x values that at uniformly distributed throughout the rate of 1 to 20\nx &lt;- sort(runif(n = n.x, min = 1, max =20))\nmm &lt;- model.matrix(~x)\nintercept &lt;- 0.6\nslope=0.1\n#The linear predictor\nlinpred &lt;- mm %*% c(intercept,slope)\n#Predicted y values\nlambda &lt;- exp(linpred)\n#Add some noise and make binomial\nlibrary(gamlss.dist)\n#fixed latent binomial\ny&lt;- rZIP(n.x,lambda, 0.4)\n#latent binomial influenced by the linear predictor \n#y&lt;- rZIP(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))\ndat.zip &lt;- data.frame(y,x)\n\nsummary(glm(y~x, dat.zip, family=\"poisson\"))\n\nNA \nNA Call:\nNA glm(formula = y ~ x, family = \"poisson\", data = dat.zip)\nNA \nNA Coefficients:\nNA             Estimate Std. Error z value Pr(&gt;|z|)    \nNA (Intercept)  0.30200    0.25247   1.196    0.232    \nNA x            0.10691    0.01847   5.789 7.09e-09 ***\nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nNA \nNA (Dispersion parameter for poisson family taken to be 1)\nNA \nNA     Null deviance: 111.495  on 19  degrees of freedom\nNA Residual deviance:  79.118  on 18  degrees of freedom\nNA AIC: 126.64\nNA \nNA Number of Fisher Scoring iterations: 5\n\nplot(glm(y~x, dat.zip, family=\"poisson\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(pscl)\nsummary(zeroinfl(y ~ x | 1, dist = \"poisson\", data = dat.zip))\n\nNA \nNA Call:\nNA zeroinfl(formula = y ~ x | 1, data = dat.zip, dist = \"poisson\")\nNA \nNA Pearson residuals:\nNA     Min      1Q  Median      3Q     Max \nNA -1.1625 -0.9549  0.1955  0.8125  1.4438 \nNA \nNA Count model coefficients (poisson with log link):\nNA             Estimate Std. Error z value Pr(&gt;|z|)    \nNA (Intercept)  0.88696    0.28825   3.077  0.00209 ** \nNA x            0.09374    0.02106   4.450 8.58e-06 ***\nNA \nNA Zero-inflation model coefficients (binomial with logit link):\nNA             Estimate Std. Error z value Pr(&gt;|z|)\nNA (Intercept)  -0.4581     0.4725   -0.97    0.332\nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \nNA \nNA Number of iterations in BFGS optimization: 8 \nNA Log-likelihood: -38.58 on 3 Df\n\nplot(resid(zeroinfl(y ~ x | 1, dist = \"poisson\", data = dat.zip))~fitted(zeroinfl(y ~ x | 1, dist = \"poisson\")))\n\n\n\n\n\n\n\nlibrary(gamlss)\nsummary(gamlss(y~x,data=dat.zip, family=ZIP))\n\nNA GAMLSS-RS iteration 1: Global Deviance = 77.8434 \nNA GAMLSS-RS iteration 2: Global Deviance = 77.1603 \nNA GAMLSS-RS iteration 3: Global Deviance = 77.1598 \nNA ******************************************************************\nNA Family:  c(\"ZIP\", \"Poisson Zero Inflated\") \nNA \nNA Call:  gamlss(formula = y ~ x, family = ZIP, data = dat.zip) \nNA \nNA Fitting method: RS() \nNA \nNA ------------------------------------------------------------------\nNA Mu link function:  log\nNA Mu Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)    \nNA (Intercept)  0.88620    0.28819   3.075 0.006862 ** \nNA x            0.09387    0.02105   4.458 0.000345 ***\nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nNA \nNA ------------------------------------------------------------------\nNA Sigma link function:  logit\nNA Sigma Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  -0.4582     0.4725   -0.97    0.346\nNA \nNA ------------------------------------------------------------------\nNA No. of observations in the fit:  20 \nNA Degrees of Freedom for the fit:  3\nNA       Residual Deg. of Freedom:  17 \nNA                       at cycle:  3 \nNA  \nNA Global Deviance:     77.15981 \nNA             AIC:     83.15981 \nNA             SBC:     86.14701 \nNA ******************************************************************\n\npredict(gamlss(y~x,data=dat.zip, family=ZIP), se.fit=TRUE, what=\"mu\")\n\nNA GAMLSS-RS iteration 1: Global Deviance = 77.8434 \nNA GAMLSS-RS iteration 2: Global Deviance = 77.1603 \nNA GAMLSS-RS iteration 3: Global Deviance = 77.1598\n\n\nNA $fit\nNA         1         2         3         4         5         6         7         8 \nNA 0.9952647 1.0233409 1.1897115 1.2189891 1.3490911 1.3644351 1.3748867 1.5164069 \nNA         9        10        11        12        13        14        15        16 \nNA 1.6184170 1.6379917 1.6760055 1.6962694 1.7705249 1.8559090 1.8578379 1.8718850 \nNA        17        18        19        20 \nNA 2.1712345 2.5536059 2.7205304 2.7472964 \nNA \nNA $se.fit\nNA         1         2         3         4         5         6         7         8 \nNA 0.3826655 0.3724115 0.3131865 0.3031078 0.2601025 0.2552658 0.2520053 0.2112310 \nNA         9        10        11        12        13        14        15        16 \nNA 0.1872286 0.1833232 0.1765049 0.1733169 0.1646100 0.1610460 0.1610499 0.1611915 \nNA        17        18        19        20 \nNA 0.2055555 0.3248709 0.3848647 0.3947072"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nCheck the distribution of the \\(y\\) abundances.\n\nhist(dat.zip$y)\n\n\n\n\n\n\n\nboxplot(dat.zip$y, horizontal=TRUE)\nrug(jitter(dat.zip$y))\n\n\n\n\n\n\n\n\nThere is definitely signs of non-normality that would warrant Poisson models. Further to that, there appears to be a large number of zeros that are likely to be the cause of overdispersion A zero-inflated Poisson model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.\n\nhist(dat.zip$x)\n\n\n\n\n\n\n\n#now for the scatterplot\nplot(y~x, dat.zip)\nwith(subset(dat.zip,y&gt;0), lines(lowess(y~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables.\n\nAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\n#proportion of 0's in the data\ndat.zip.tab&lt;-table(dat.zip$y==0)\ndat.zip.tab/sum(dat.zip.tab)\n\nNA \nNA FALSE  TRUE \nNA   0.6   0.4\n\n#proportion of 0's expected from a Poisson distribution\nmu &lt;- mean(dat.zip$y)\ncnts &lt;- rpois(1000, mu)\ndat.zip.tabE &lt;- table(cnts == 0)\ndat.zip.tabE/sum(dat.zip.tabE)\n\nNA \nNA FALSE  TRUE \nNA 0.982 0.018\n\n\nIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (\\(45\\)%) far exceeds that that would have been expected (\\(7.9\\)%). Hence it is highly likely that any models will be zero-inflated."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\\[\ny_i \\sim \\text{ZIP}(\\lambda_i, \\theta),\n\\]\nwhere \\(\\text{logit}(\\theta) = \\gamma_0\\), \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1,\\gamma_0 \\sim N(0, 10000)\\).\n\ndat.zip.list &lt;- with(dat.zip,list(Y=y, X=x,N=nrow(dat.nb), z=ifelse(y==0,0,1)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     z[i] ~ dbern(one.minus.theta)\n     Y[i] ~ dpois(lambda[i])\n     lambda[i] &lt;- z[i]*eta[i]\n     log(eta[i]) &lt;- beta0 + beta1*X[i]\n  }\n  one.minus.theta &lt;- 1-theta\n  logit(theta) &lt;- gamma0\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n  gamma0 ~ dnorm(0,1.0E-06)\n} \n\"\nwriteLines(modelString, con='modelzip.txt')\n\nparams &lt;- c('beta0','beta1', 'gamma0','theta')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.zip.jags &lt;- jags(data=dat.zip.list,model.file='modelzip.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 40\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 149\nNA \nNA Initializing model"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndenplot(dat.zip.jags, parms = c('beta', 'gamma0'))\n\n\n\n\n\n\n\ntraplot(dat.zip.jags, parms = c('beta', 'gamma0'))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.zip.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    20       20276 3746         5.41      \nNA  beta1    22       24038 3746         6.42      \nNA  deviance 4        4636  3746         1.24      \nNA  gamma0   5        5908  3746         1.58      \nNA  theta    5        5908  3746         1.58      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    20       21336 3746         5.70      \nNA  beta1    20       22636 3746         6.04      \nNA  deviance 3        4267  3746         1.14      \nNA  gamma0   5        6078  3746         1.62      \nNA  theta    5        6078  3746         1.62\n\nautocorr.diag(as.mcmc(dat.zip.jags))\n\nNA              beta0       beta1    deviance      gamma0       theta\nNA Lag 0   1.00000000  1.00000000  1.00000000 1.000000000 1.000000000\nNA Lag 1   0.88627108  0.88426590  0.51799594 0.232408997 0.227686735\nNA Lag 5   0.58998005  0.59775827  0.19471855 0.002321179 0.001571686\nNA Lag 10  0.35846288  0.35888205  0.06697926 0.017561785 0.015598223\nNA Lag 50 -0.01753582 -0.01936659 -0.01212528 0.022040872 0.021016755"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit-1",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit-1",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Goodness of fit",
    "text": "Goodness of fit\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]\ntheta &lt;- dat.zip.jags$BUGSoutput$sims.matrix[,'theta']\nXmat &lt;- model.matrix(~x, data=dat.zip)\n#expected values on a log scale\nlambda&lt;-coefs %*% t(Xmat)\n#expected value on response scale\neta &lt;- exp(lambda)\nexpY &lt;- sweep(eta,1,(1-theta),\"*\")\nvarY &lt;- eta+sweep(eta^2,1,theta,\"*\")\nvarY &lt;- sweep(varY,1,(1-theta),'*')\n#sweep across rows and then divide by lambda\nResid &lt;- -1*sweep(expY,2,dat.zip$y,'-')/sqrt(varY)\n#plot residuals vs expected values\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\n\n\n\n\n\n\n\n\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.\n\nSSres&lt;-apply(Resid^2,1,sum, na.rm=T)\n\n#generate a matrix of draws from a zero-inflated poisson (ZIP) distribution\n# the matrix is the same dimensions as lambda\nlibrary(gamlss.dist)\n#YNew &lt;- matrix(rZIP(length(lambda),eta, theta),nrow=nrow(lambda))\nlambda &lt;- sweep(eta,1,ifelse(dat.zip$y==0,0,1),'*')\nYNew &lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))\nResid1&lt;-(expY - YNew)/sqrt(varY)\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm = T)\n\nNA [1] 0.5619\n\n\nSince it is difficult to diagnose many issues from the typical residuals we will now explore simulated residuals.\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.zip.jags$BUGSoutput$sims.matrix[,1:2]\ntheta &lt;- dat.zip.jags$BUGSoutput$sims.matrix[,'theta']\nXmat &lt;- model.matrix(~x, data=dat.zip)\n#expected values on a log scale\neta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\nlambda &lt;- exp(eta)\n\nsimRes &lt;- function(lambda, data,n=250, plot=T, family='negbin', size=NULL,theta=NULL) {\n require(gap)\n N = nrow(data)\n sim = switch(family,\n    'poisson' = matrix(rpois(n*N,apply(lambda,2,mean)),ncol=N, byrow=TRUE),\n    'negbin' = matrix(MASS:::rnegbin(n*N,apply(lambda,2,mean),size),ncol=N, byrow=TRUE),\n        'zip' = matrix(gamlss.dist:::rZIP(n*N,apply(lambda,2,mean),theta),ncol=N, byrow=TRUE)\n )\n a = apply(sim + runif(n,-0.5,0.5),2,ecdf)\n resid&lt;-NULL\n for (i in 1:nrow(data)) resid&lt;-c(resid,a[[i]](data$y[i] + runif(1 ,-0.5,0.5)))\n if (plot==T) {\n   par(mfrow=c(1,2))\n   gap::qqunif(resid,pch = 2, bty = \"n\",\n   logscale = F, col = \"black\", cex = 0.6, main = \"QQ plot residuals\",\n   cex.main = 1, las=1)\n   plot(resid~apply(lambda,2,mean), xlab='Predicted value', ylab='Standardized residual', las=1)\n }\n resid\n}\n\nsimRes(lambda,dat.zip, family='zip',theta=theta)\n\n\n\n\n\n\n\n\nNA  [1] 0.718 0.212 0.106 0.050 0.476 0.778 0.248 0.060 0.878 0.704 0.090 0.890\nNA [13] 0.416 0.764 0.282 0.752 0.602 0.848 0.154 0.656\n\n\nThe trend (black symbols) in the qq-plot does not appear to be overly non-linear (matching the ideal red line well), suggesting that the model is not overdispersed. The spread of standardized (simulated) residuals in the residual plot do not appear overly non-uniform. That is there is not trend in the residuals. Furthermore, there is not a concentration of points close to \\(1\\) or \\(0\\) (which would imply overdispersion). Hence, once zero-inflation is accounted for, the model does not display overdispersion. Although there is a slight hint of non-linearity in that the residuals are high for low and high fitted values and lower in the middle, this might well be an artifact of the small data set size. By change, most of the observed values in the middle range of the predictor were zero."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploring the model parameters",
    "text": "Exploring the model parameters\nIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\nprint(dat.zip.jags)\n\nNA Inference for Bugs model at \"modelzip.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.930   0.282  0.365  0.742  0.933  1.128  1.468 1.003   860\nNA beta1      0.090   0.021  0.049  0.076  0.090  0.104  0.132 1.002  1400\nNA gamma0    -0.420   0.458 -1.349 -0.722 -0.417 -0.110  0.459 1.001 10000\nNA theta      0.401   0.105  0.206  0.327  0.397  0.472  0.613 1.001  7600\nNA deviance  80.674   2.501 77.856 78.867 80.008 81.801 87.064 1.001 10000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 3.1 and DIC = 83.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nadply(dat.zip.jags$BUGSoutput$sims.matrix, 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA         X1      Median        Mean       lower      upper    lower.1\nNA 1    beta0  0.93334635  0.92997411  0.37821529  1.4774189  0.7530788\nNA 2    beta1  0.09005537  0.09005981  0.04864163  0.1313802  0.0749821\nNA 3 deviance 80.00841187 80.67383245 77.63946567 85.5798539 77.8273778\nNA 4   gamma0 -0.41667356 -0.41996110 -1.34903991  0.4586909 -0.7013225\nNA 5    theta  0.39731301  0.40136809  0.19516803  0.6012233  0.3173026\nNA       upper.1\nNA 1  1.13629442\nNA 2  0.10333030\nNA 3 80.10544007\nNA 4 -0.09258569\nNA 5  0.46170971\n\n#on original scale\nadply(exp(dat.zip.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1   Median     Mean    lower    upper  lower.1  upper.1\nNA 1 beta0 2.543005 2.636434 1.280362 4.056990 1.911083 2.853498\nNA 2 beta1 1.094235 1.094483 1.049844 1.140401 1.077865 1.108858\n\n\nConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.09\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.09}=1.1\\)) \\(1.1\\) unit increase in the abundance of \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#explorations-of-the-trends-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Explorations of the trends",
    "text": "Explorations of the trends\nA measure of the strength of the relationship can be obtained according to:\n\\[\nR^2 = 1 - \\frac{\\text{RSS}_{model}}{\\text{RSS}_{null}}\n\\]\nAlternatively, we could use McFadden’s psuedo\n\\[\nR^2 = 1- \\frac{LL(Model_{full})}{LL(Model_{reduced}}\n\\]\n\nXmat &lt;- model.matrix(~x, dat=dat.zip)\n#expected values on a log scale\nneta&lt;-coefs %*% t(Xmat)\n#expected value on response scale\neta &lt;- exp(neta)\nlambda &lt;- sweep(eta,2,ifelse(dat.zip$y==0,0,1),'*')\ntheta &lt;- dat.zip.jags$BUGSoutput$sims.matrix[,'theta']\nexpY &lt;- sweep(lambda,2,1-theta,'*')\n#calculate the raw SS residuals\nSSres &lt;- apply((-1*(sweep(expY,2,dat.zip$y,'-')))^2,1,sum)\nmean(SSres)\n\nNA [1] 168.3814\n\nSSres.null &lt;- sum((dat.zip$y - mean(dat.zip$y))^2)\n#calculate the model r2\n1-mean(SSres)/SSres.null\n\nNA [1] 0.5977029\n\n\nConclusions: \\(50\\)% of the variation in \\(y\\) abundance can be explained by its relationship with \\(x\\). Finally, we will create a summary plot.\n\npar(mar = c(4, 5, 0, 0))\nplot(y ~ x, data = dat.zip, type = \"n\", ann = F, axes = F)\npoints(y ~ x, data = dat.zip, pch = 16)\nxs &lt;- seq(min(dat.zip$x,na.rm=TRUE),max(dat.zip$x,na.rm=TRUE), l = 1000)\nXmat &lt;- model.matrix(~xs)\neta&lt;-coefs %*% t(Xmat)\nys &lt;- exp(eta)\nlibrary(plyr)\nlibrary(coda)\ndata.tab &lt;- adply(ys,2,function(x) {\n  data.frame(Median=median(x), HPDinterval(as.mcmc(x)))\n})\ndata.tab &lt;- cbind(x=xs,data.tab)\npoints(Median ~ x, data=data.tab,col = \"black\", type = \"l\")\nlines(lower ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\nlines(upper ~ x, data=data.tab,col = \"black\", type = \"l\", lty = 2)\n\naxis(1)\nmtext(\"X\", 1, cex = 1.5, line = 3)\naxis(2, las = 2)\nmtext(\"Abundance of Y\", 2, cex = 1.5, line = 3)\nbox(bty = \"l\")"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#full-log-likelihood-function-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Full log-likelihood function",
    "text": "Full log-likelihood function\nNow lets try it by specifying log-likelihood and the zero trick. When applying this trick, we need to manually calculate the deviance as the inbuilt deviance will be based on the log-likelihood of estimating the zeros (as part of the zero trick) rather than the deviance of the intended model. The one advantage of the zero trick is that the Deviance and thus DIC, AIC provided by R2jags will be incorrect. Hence, they too need to be manually defined within JAGS I suspect that the AIC calculation I have used is incorrect.\n\nXmat &lt;- model.matrix(~x, dat.zip)\nnX &lt;- ncol(Xmat)\ndat.zip.list2 &lt;- with(dat.zip,list(Y=y, X=Xmat,N=nrow(dat.zip), mu=rep(0,nX),\n                  Sigma=diag(1.0E-06,nX), zeros=rep(0,nrow(dat)), C=10000))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     zeros[i] ~ dpois(zeros.lambda[i])\n     zeros.lambda[i] &lt;- -ll[i] + C     \n     ll[i] &lt;- Y[i]*log(lambda[i]) - lambda[i] - loggam(Y[i]+1)\n     eta[i] &lt;- inprod(beta[], X[i,])\n     log(lambda[i]) &lt;- eta[i]\n    llm[i] &lt;- Y[i]*log(meanlambda) - meanlambda - loggam(Y[i]+1)\n  }\n  meanlambda &lt;- mean(lambda)\n  beta ~ dmnorm(mu[],Sigma[,])\n  dev &lt;- sum(-2*ll)\n  pD &lt;- mean(dev)-sum(-2*llm)\n  AIC &lt;- min(dev+(2*pD))\n} \n\"\n\nwriteLines(modelString, con='modelzip_ll.txt')\n\nparams &lt;- c('beta','dev','AIC')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.ZIP.jags3  &lt;- jags(data=dat.zip.list2,model.file='modelzip_ll.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 20\nNA    Unobserved stochastic nodes: 1\nNA    Total graph size: 328\nNA \nNA Initializing model\n\nprint(dat.ZIP.jags3 )\n\nNA Inference for Bugs model at \"modelzip_ll.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA             mu.vect sd.vect       2.5%        25%        50%        75%\nNA AIC          61.488   3.844     57.991     58.846     60.144     62.785\nNA beta[1]       0.329   0.225     -0.122      0.176      0.331      0.475\nNA beta[2]       0.104   0.017      0.070      0.093      0.104      0.116\nNA dev         124.472   1.801    122.700    123.170    123.871    125.221\nNA deviance 400124.472   1.801 400122.700 400123.170 400123.871 400125.221\nNA               97.5%  Rhat n.eff\nNA AIC          72.257 1.089    35\nNA beta[1]       0.785 1.071    67\nNA beta[2]       0.137 1.051    69\nNA dev         129.054 1.042    53\nNA deviance 400129.054 1.000     1\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 1.6 and DIC = 400126.1\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-3",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#data-generation-3",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nLets say we wanted to model the abundance of an item (\\(y\\)) against a continuous predictor (\\(x\\)). As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(37) #34.5  #4 #10 #16 #17 #26\n#The number of samples\nn.x &lt;- 20\n#Create x values that at uniformly distributed throughout the rate of 1 to 20\nx &lt;- sort(runif(n = n.x, min = 1, max =20))\nmm &lt;- model.matrix(~x)\nintercept &lt;- 0.6\nslope=0.1\n#The linear predictor\nlinpred &lt;- mm %*% c(intercept,slope)\n#Predicted y values\nlambda &lt;- exp(linpred)\n#Add some noise and make binomial\nlibrary(gamlss.dist)\nlibrary(MASS)\n#fixed latent binomial\ny&lt;- rZINBI(n.x,lambda, 0.4)\n#latent binomial influenced by the linear predictor \n#y&lt;- rZINB(n.x,lambda, 1-exp(linpred)/(1+exp(linpred)))\ndat.zinb &lt;- data.frame(y,x)\n\nsummary(dat.glm.nb&lt;-glm.nb(y~x, dat.zinb))\n\nNA \nNA Call:\nNA glm.nb(formula = y ~ x, data = dat.zinb, init.theta = 0.4646673144, \nNA     link = log)\nNA \nNA Coefficients:\nNA             Estimate Std. Error z value Pr(&gt;|z|)\nNA (Intercept) 0.914191   0.796804   1.147    0.251\nNA x           0.009149   0.067713   0.135    0.893\nNA \nNA (Dispersion parameter for Negative Binomial(0.4647) family taken to be 1)\nNA \nNA     Null deviance: 20.303  on 19  degrees of freedom\nNA Residual deviance: 20.282  on 18  degrees of freedom\nNA AIC: 90.365\nNA \nNA Number of Fisher Scoring iterations: 1\nNA \nNA \nNA               Theta:  0.465 \nNA           Std. Err.:  0.218 \nNA \nNA  2 x log-likelihood:  -84.365\n\nplot(glm.nb(y~x, dat.zinb))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(pscl)\nsummary(dat.zeroinfl&lt;-zeroinfl(y ~ x | 1, dist = \"negbin\", data = dat.zinb))\n\nNA \nNA Call:\nNA zeroinfl(formula = y ~ x | 1, data = dat.zinb, dist = \"negbin\")\nNA \nNA Pearson residuals:\nNA     Min      1Q  Median      3Q     Max \nNA -0.9609 -0.9268 -0.4446  1.0425  1.7556 \nNA \nNA Count model coefficients (negbin with log link):\nNA             Estimate Std. Error z value Pr(&gt;|z|)   \nNA (Intercept)  0.92733    0.32507   2.853  0.00433 **\nNA x            0.06870    0.02755   2.494  0.01263 * \nNA Log(theta)   3.36066    3.59739   0.934  0.35020   \nNA \nNA Zero-inflation model coefficients (binomial with logit link):\nNA             Estimate Std. Error z value Pr(&gt;|z|)\nNA (Intercept)  -0.2250     0.4559  -0.494    0.622\nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \nNA \nNA Theta = 28.8082 \nNA Number of iterations in BFGS optimization: 17 \nNA Log-likelihood: -38.54 on 4 Df\n\nplot(resid(zeroinfl(y ~ x | 1, dist = \"negbin\", data = dat.zinb))~fitted(zeroinfl(y ~ x | 1, dist = \"negbin\")))\n\n\n\n\n\n\n\nvuong(dat.glm.nb, dat.zeroinfl)\n\nNA Vuong Non-Nested Hypothesis Test-Statistic: \nNA (test-statistic is asymptotically distributed N(0,1) under the\nNA  null that the models are indistinguishible)\nNA -------------------------------------------------------------\nNA               Vuong z-statistic             H_A p-value\nNA Raw                  -1.2809521 model2 &gt; model1 0.10011\nNA AIC-corrected        -0.9296587 model2 &gt; model1 0.17627\nNA BIC-corrected        -0.7547616 model2 &gt; model1 0.22520\n\nlibrary(gamlss)\nsummary(gamlss(y~x, data=dat.zinb, family='ZINBI'))\n\nNA GAMLSS-RS iteration 1: Global Deviance = 82.35 \nNA GAMLSS-RS iteration 2: Global Deviance = 82.0176 \nNA GAMLSS-RS iteration 3: Global Deviance = 81.8115 \nNA GAMLSS-RS iteration 4: Global Deviance = 81.6241 \nNA GAMLSS-RS iteration 5: Global Deviance = 81.5175 \nNA GAMLSS-RS iteration 6: Global Deviance = 81.5429 \nNA GAMLSS-RS iteration 7: Global Deviance = 81.6384 \nNA GAMLSS-RS iteration 8: Global Deviance = 81.6956 \nNA GAMLSS-RS iteration 9: Global Deviance = 81.702 \nNA GAMLSS-RS iteration 10: Global Deviance = 81.6899 \nNA GAMLSS-RS iteration 11: Global Deviance = 81.644 \nNA GAMLSS-RS iteration 12: Global Deviance = 81.4995 \nNA GAMLSS-RS iteration 13: Global Deviance = 81.4366 \nNA GAMLSS-RS iteration 14: Global Deviance = 81.4913 \nNA GAMLSS-RS iteration 15: Global Deviance = 81.583 \nNA GAMLSS-RS iteration 16: Global Deviance = 81.6803 \nNA GAMLSS-RS iteration 17: Global Deviance = 81.7197 \nNA GAMLSS-RS iteration 18: Global Deviance = 81.7177 \nNA GAMLSS-RS iteration 19: Global Deviance = 81.6711 \nNA GAMLSS-RS iteration 20: Global Deviance = 81.5165 \nNA ******************************************************************\nNA Family:  c(\"ZINBI\", \"Zero inflated negative binomial type I\") \nNA \nNA Call:  gamlss(formula = y ~ x, family = \"ZINBI\", data = dat.zinb) \nNA \nNA Fitting method: RS() \nNA \nNA ------------------------------------------------------------------\nNA Mu link function:  log\nNA Mu Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  0.94930    0.93174   1.019    0.322\nNA x            0.03794    0.07030   0.540    0.596\nNA \nNA ------------------------------------------------------------------\nNA Sigma link function:  log\nNA Sigma Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  -0.2866     0.7254  -0.395    0.697\nNA \nNA ------------------------------------------------------------------\nNA Nu link function:  logit \nNA Nu Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  -0.8664     0.6832  -1.268     0.22\nNA \nNA ------------------------------------------------------------------\nNA No. of observations in the fit:  20 \nNA Degrees of Freedom for the fit:  4\nNA       Residual Deg. of Freedom:  16 \nNA                       at cycle:  20 \nNA  \nNA Global Deviance:     81.51652 \nNA             AIC:     89.51652 \nNA             SBC:     93.49945 \nNA ******************************************************************\n\nsummary(gamlss(y~x, nu.fo=y~x,data=dat.zinb, family='ZINBI'))\n\nNA GAMLSS-RS iteration 1: Global Deviance = 79.3063 \nNA GAMLSS-RS iteration 2: Global Deviance = 77.6325 \nNA GAMLSS-RS iteration 3: Global Deviance = 77.4263 \nNA GAMLSS-RS iteration 4: Global Deviance = 77.1265 \nNA GAMLSS-RS iteration 5: Global Deviance = 77.0111 \nNA GAMLSS-RS iteration 6: Global Deviance = 76.9765 \nNA GAMLSS-RS iteration 7: Global Deviance = 76.9662 \nNA GAMLSS-RS iteration 8: Global Deviance = 76.9626 \nNA GAMLSS-RS iteration 9: Global Deviance = 76.9583 \nNA GAMLSS-RS iteration 10: Global Deviance = 76.9579 \nNA ******************************************************************\nNA Family:  c(\"ZINBI\", \"Zero inflated negative binomial type I\") \nNA \nNA Call:  gamlss(formula = y ~ x, nu.formula = y ~ x, family = \"ZINBI\",  \nNA     data = dat.zinb) \nNA \nNA Fitting method: RS() \nNA \nNA ------------------------------------------------------------------\nNA Mu link function:  log\nNA Mu Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  0.80319    0.49858   1.611    0.128\nNA x            0.05928    0.05851   1.013    0.327\nNA \nNA ------------------------------------------------------------------\nNA Sigma link function:  log\nNA Sigma Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  -0.6147     1.8864  -0.326    0.749\nNA \nNA ------------------------------------------------------------------\nNA Nu link function:  logit \nNA Nu Coefficients:\nNA             Estimate Std. Error t value Pr(&gt;|t|)\nNA (Intercept)  -3.4494     2.7228  -1.267    0.225\nNA x             0.2298     0.1700   1.352    0.196\nNA \nNA ------------------------------------------------------------------\nNA No. of observations in the fit:  20 \nNA Degrees of Freedom for the fit:  5\nNA       Residual Deg. of Freedom:  15 \nNA                       at cycle:  10 \nNA  \nNA Global Deviance:     76.95789 \nNA             AIC:     86.95789 \nNA             SBC:     91.93655 \nNA ******************************************************************"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploratory-data-analysis-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nCheck the distribution of the \\(y\\) abundances.\n\nhist(dat.zinb$y)\n\n\n\n\n\n\n\nboxplot(dat.zinb$y, horizontal=TRUE)\nrug(jitter(dat.zinb$y))\n\n\n\n\n\n\n\n\nThere is definitely signs of non-normality that would warrant Poisson or negative binomial models. Further to that, there appears to be a large number of zeros and a possible clumpiness that are likely to be the cause of overdispersion A zero-inflated negative binomial model is likely to be one of the most effective for modeling these data. Lets now explore linearity by creating a histogram of the predictor variable (\\(x\\)). Note, it is difficult to directly assess issues of linearity. Indeed, a scatterplot with lowess smoother will be largely influenced by the presence of zeros. One possible way of doing so is to explore the trend in the non-zero data.\n\nhist(dat.zinb$x)\n\n\n\n\n\n\n\n#now for the scatterplot\nplot(y~x, dat.zinb, log=\"y\")\nwith(subset(dat.zinb,y&gt;0), lines(lowess(y~x)))\n\n\n\n\n\n\n\n\nConclusions: the predictor (\\(x\\)) does not display any skewness or other issues that might lead to non-linearity. The lowess smoother on the non-zero data cloud does not display major deviations from a straight line and thus linearity is likely to be satisfied. Violations of linearity (whilst difficult to be certain about due to the unknown influence of the zeros) could be addressed by either:\n\ndefine a non-linear linear predictor (such as a polynomial, spline or other non-linear function).\ntransform the scale of the predictor variables.\n\nAlthough we have already established that there are few zeros in the data (and thus overdispersion is unlikely to be an issue), we can also explore this by comparing the number of zeros in the data to the number of zeros that would be expected from a Poisson distribution with a mean equal to the mean count of the data.\n\n#proportion of 0's in the data\ndat.zinb.tab&lt;-table(dat.zinb$y==0)\ndat.zinb.tab/sum(dat.zinb.tab)\n\nNA \nNA FALSE  TRUE \nNA  0.55  0.45\n\n#proportion of 0's expected from a Poisson distribution\nmu &lt;- mean(dat.zinb$y)\nv &lt;- var(dat.zinb$y)\nsize &lt;- mu + (mu^2)/v\ncnts &lt;- rnbinom(1000, mu=mu, size=size)\ndat.zinb.tabE &lt;- table(cnts == 0)\ndat.zinb.tabE/sum(dat.zinb.tabE)\n\nNA \nNA FALSE  TRUE \nNA 0.861 0.139\n\n\nIn the above, the value under FALSE is the proportion of non-zero values in the data and the value under TRUE is the proportion of zeros in the data. In this example, the proportion of zeros observed (\\(45\\)%) far exceeds that that would have been expected (\\(14\\)%). Hence it is highly likely that any models will be zero-inflated."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-3",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-fitting-3",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\\[\ny_i \\sim \\text{ZINB}(\\lambda_i, \\theta),\n\\]\nwhere \\(\\text{logit}(\\theta) = \\gamma_0\\), \\(\\log(\\lambda_i)=\\eta_i\\), with \\(\\eta_i=\\beta_0+\\beta_1x_{i}\\) and \\(\\beta_0,\\beta_1,\\gamma_0 \\sim N(0, 10000)\\).\n\ndat.zinb.list &lt;- with(dat.zinb,list(Y=y, X=x,N=nrow(dat.zinb),z=ifelse(y==0,0,1)))\nmodelString=\"\nmodel {\n  for (i in 1:N) {\n     z[i] ~ dbern(psi.min)\n     Y[i] ~ dnegbin(p[i],size)\n     p[i] &lt;- size/(size+mu.eff[i])\n     mu.eff[i] &lt;- z[i]*mu[i]\n     eta[i] &lt;- beta0 + beta1*X[i]\n     log(mu[i]) &lt;- eta[i]\n  }\n  gamma ~ dnorm(0,0.001)\n  psi.min &lt;- min(0.9999, max(0.00001, (1-psi)))\n  logit(psi) &lt;- max(-20, min(20, gamma))\n  size ~ dunif(0.001, 5)\n  theta &lt;- pow(1/mean(p),2)\n  beta0 ~ dnorm(0,1.0E-06)\n  beta1 ~ dnorm(0,1.0E-06)\n} \n\"\nwriteLines(modelString, con='modelzinb.txt')\n\nparams &lt;- c('beta0','beta1', 'size', 'theta')\nnChains = 2\nburnInSteps = 5000\nthinSteps = 1\nnumSavedSteps = 20000\nnIter = ceiling((numSavedSteps * thinSteps)/nChains)\n\ndat.zinb.jags &lt;- jags(data=dat.zinb.list,model.file='modelzinb.txt', param=params,\n                   n.chains=nChains, n.iter=nIter, n.burnin=burnInSteps, n.thin=thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 40\nNA    Unobserved stochastic nodes: 4\nNA    Total graph size: 205\nNA \nNA Initializing model\n\nprint(dat.zinb.jags)\n\nNA Inference for Bugs model at \"modelzinb.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.971   0.460  0.055  0.678  0.963  1.273  1.868 1.007   250\nNA beta1      0.067   0.042 -0.016  0.039  0.066  0.094  0.151 1.008   200\nNA size       3.501   1.015  1.389  2.763  3.644  4.351  4.935 1.001 10000\nNA theta      2.200   0.367  1.721  1.937  2.115  2.371  3.145 1.001  3300\nNA deviance  82.769   2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 4.0 and DIC = 86.8\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-3",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#model-evaluation-3",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndenplot(dat.zinb.jags, parms = c('beta0','beta1', 'size', 'theta'))\n\n\n\n\n\n\n\ntraplot(dat.zinb.jags, parms = c('beta0','beta1', 'size', 'theta'))\n\n\n\n\n\n\n\nraftery.diag(as.mcmc(dat.zinb.jags))\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    15       16236 3746         4.33      \nNA  beta1    14       15725 3746         4.20      \nNA  deviance 3        4484  3746         1.20      \nNA  size     5        5771  3746         1.54      \nNA  theta    3        4338  3746         1.16      \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta0    27       27564 3746         7.36      \nNA  beta1    18       21057 3746         5.62      \nNA  deviance 3        4410  3746         1.18      \nNA  size     5        5771  3746         1.54      \nNA  theta    2        3995  3746         1.07\n\nautocorr.diag(as.mcmc(dat.zinb.jags))\n\nNA              beta0       beta1    deviance        size       theta\nNA Lag 0   1.00000000  1.00000000  1.00000000 1.000000000 1.000000000\nNA Lag 1   0.82187294  0.82300377  0.55172222 0.391115803 0.387289605\nNA Lag 5   0.44679310  0.44494849  0.13559995 0.045297725 0.067379632\nNA Lag 10  0.19928140  0.20123773  0.05371302 0.008341721 0.013304093\nNA Lag 50 -0.04037202 -0.04473554 -0.02496182 0.011474420 0.007333003"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit-2",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#goodness-of-fit-2",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Goodness of fit",
    "text": "Goodness of fit\n\n#extract the samples for the two model parameters\ncoefs &lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]\ntheta &lt;- dat.zinb.jags$BUGSoutput$sims.matrix[,'theta']\nXmat &lt;- model.matrix(~x, data=dat.zinb)\n#expected values on a log scale\nlambda&lt;-coefs %*% t(Xmat)\n#expected value on response scale\neta &lt;- exp(lambda)\nexpY &lt;- sweep(eta,1,(1-theta),\"*\")\nvarY &lt;- eta+sweep(eta^2,1,theta,\"*\")\nhead(varY)\n\nNA              1         2        3        4        5        6        7        8\nNA [1,] 10.844323 13.189501 15.47499 15.73287 18.21519 26.87133 28.14742 29.27065\nNA [2,] 71.832694 61.952112 54.97632 54.30484 48.72535 36.70113 35.49495 34.51074\nNA [3,] 24.764991 24.273552 23.88302 23.84316 23.49392 22.60135 22.49799 22.41131\nNA [4,]  6.397443  8.786249 11.40150 11.71375 14.89149 28.26188 30.51610 32.55772\nNA [5,] 27.048585 28.561484 29.85015 29.98628 31.21706 34.70423 35.14294 35.51685\nNA [6,] 32.911549 36.163316 39.03708 39.34619 42.18864 50.70280 51.82155 52.78337\nNA             9       10       11       12        13        14        15\nNA [1,] 32.48606 39.45733 45.43874 50.02645  59.31437  71.66019  73.00520\nNA [2,] 32.02933 27.89750 25.25717 23.61192  20.97369  18.40930  18.17586\nNA [3,] 22.18262 21.76433 21.46712 21.26761  20.92017  20.54281  20.50616\nNA [4,] 38.69312 53.42044 67.53693 79.24793 105.19992 144.10581 148.63604\nNA [5,] 36.53055 38.49254 39.97741 41.01961  42.92731  45.14296  45.36660\nNA [6,] 55.42928 60.70818 64.84042 67.81058  73.39529  80.11924  80.81199\nNA             16        17        18        19        20\nNA [1,]  89.37583  90.29710  93.03697  99.45044 121.73297\nNA [2,]  15.83105  15.72115  15.40547  14.72560  12.85289\nNA [3,]  20.11263  20.09293  20.03565  19.90863  19.52937\nNA [4,] 208.15726 211.74132 222.54395 248.66128 348.12988\nNA [5,]  47.86864  47.99890  48.38049  49.24196  51.94528\nNA [6,]  88.73695  89.15824  90.39740  93.22189 102.32692\n\n#varY &lt;- sweep(varY,1,(1-theta),'*')\n#sweep across rows and then divide by lambda\nResid &lt;- -1*sweep(expY,2,dat.zinb$y,'-')/sqrt(varY)\n#plot residuals vs expected values\nplot(apply(Resid,2,mean)~apply(eta,2,mean))\n\n\n\n\n\n\n\n\nNow we will compare the sum of squared residuals to the sum of squares residuals that would be expected from a Poisson distribution matching that estimated by the model. Essentially this is estimating how well the Poisson distribution, the log-link function and the linear model approximates the observed data. When doing so, we need to consider the expected value and variance of the zero-inflated poisson.\n\nSSres&lt;-apply(Resid^2,1,sum, na.rm=T)\n\n#generate a matrix of draws from a zero-inflated poisson (ZINB) distribution\n# the matrix is the same dimensions as lambda\nlibrary(gamlss.dist)\n#YNew &lt;- matrix(rZINB(length(lambda),eta, theta),nrow=nrow(lambda))\nlambda &lt;- sweep(eta,1,ifelse(dat.zinb$y==0,0,1),'*')\nYNew &lt;- matrix(rpois(length(lambda),lambda),nrow=nrow(lambda))\nResid1&lt;-(expY - YNew)/sqrt(varY)\nSSres.sim&lt;-apply(Resid1^2,1,sum)\nmean(SSres.sim&gt;SSres, na.rm = T)\n\nNA [1] 0.5212"
  },
  {
    "objectID": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-3",
    "href": "tutorials/2020-02-14-glm2-jags/index.html#exploring-the-model-parameters-3",
    "title": "Generalised Linear Models part II (JAGS)",
    "section": "Exploring the model parameters",
    "text": "Exploring the model parameters\nIf there was any evidence that the assumptions had been violated or the model was not an appropriate fit, then we would need to reconsider the model and start the process again. In this case, there is no evidence that the test will be unreliable so we can proceed to explore the test statistics. As with most Bayesian models, it is best to base conclusions on medians rather than means.\n\nprint(dat.zinb.jags)\n\nNA Inference for Bugs model at \"modelzinb.txt\", fit using jags,\nNA  2 chains, each with 10000 iterations (first 5000 discarded)\nNA  n.sims = 10000 iterations saved\nNA          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff\nNA beta0      0.971   0.460  0.055  0.678  0.963  1.273  1.868 1.007   250\nNA beta1      0.067   0.042 -0.016  0.039  0.066  0.094  0.151 1.008   200\nNA size       3.501   1.015  1.389  2.763  3.644  4.351  4.935 1.001 10000\nNA theta      2.200   0.367  1.721  1.937  2.115  2.371  3.145 1.001  3300\nNA deviance  82.769   2.843 79.139 80.663 82.139 84.254 89.891 1.001 10000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 4.0 and DIC = 86.8\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\nadply(dat.zinb.jags$BUGSoutput$sims.matrix, 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA         X1      Median        Mean       lower      upper     lower.1\nNA 1    beta0  0.96339931  0.97060322  0.05196655  1.8646388  0.68771701\nNA 2    beta1  0.06565837  0.06658472 -0.01850221  0.1478933  0.03570031\nNA 3 deviance 82.13938661 82.76912313 78.75619568 88.5891138 79.69050804\nNA 4     size  3.64385931  3.50054311  1.63847682  4.9995959  3.62583688\nNA 5    theta  2.11463918  2.19954948  1.65289052  2.9565781  1.83698278\nNA       upper.1\nNA 1  1.28169341\nNA 2  0.09027889\nNA 3 82.76458253\nNA 4  4.98121591\nNA 5  2.20696839\n\n#on original scale\nadply(exp(dat.zinb.jags$BUGSoutput$sims.matrix[,1:2]), 2, function(x) {\n  data.frame(Median=median(x), Mean=mean(x), HPDinterval(as.mcmc(x)), HPDinterval(as.mcmc(x),p=0.5))\n})\n\nNA      X1   Median     Mean     lower    upper  lower.1  upper.1\nNA 1 beta0 2.620590 2.935935 0.7910564 5.701997 1.628127 3.096623\nNA 2 beta1 1.067862 1.069796 0.9816679 1.159389 1.036345 1.094479\n\n\nConclusions: We would reject the null hypothesis of no effect of \\(x\\) on \\(y\\). An increase in \\(x\\) is associated with a significant linear increase (positive slope) in the abundance of \\(y\\). Every \\(1\\) unit increase in \\(x\\) results in a log \\(0.06\\) unit increase in \\(y\\). We usually express this in terms of abundance rather than log abundance, so every \\(1\\) unit increase in \\(x\\) results in a (\\(e^{0.06}=1.07\\)) \\(1.07\\) unit increase in the abundance of \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#data-check",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#data-check",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Data check",
    "text": "Data check\nHow are the number of goals for each team in a football match distributed? Well, let’s start by assuming that all football matches are roughly equally long, that both teams have many chances at making a goal and that each team have the same probability of making a goal each goal chance. Given these assumptions the distribution of the number of goals for each team should be well captured by a Poisson distribution. A quick and dirty comparison between the actual distribution of the number of scored goals and a Poisson distribution having the same mean number of scored goals support this notion.\n\npar(mfcol = c(2, 1), mar = rep(2.2, 4))\nhist(c(d$AwayGoals, d$HomeGoals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = \"Distribution of the number of goals\\nscored by a team in a match.\")\nmean_goals &lt;- mean(c(d$AwayGoals, d$HomeGoals))\nhist(rpois(9999, mean_goals), xlim = c(-0.5, 8), breaks = -1:9 + 0.5, main = \"Random draw from a Poisson distribution with\\nthe same mean as the distribution above.\")"
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#model-fitting",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nAll teams aren’t equally good and it will be assumed that all teams have a latent skill variable and the skill of a team minus the skill of the opposing team defines the predicted outcome of a game. As the number of goals are assumed to be Poisson distributed it is natural that the skills of the teams are on the log scale of the mean of the distribution. The distribution of the number of goals for team \\(i\\) when facing team \\(j\\) is then\n\\[\n\\text{Goals} \\sim \\text{Pois}(\\lambda)\n\\]\nwhere \\(\\log(\\lambda)=\\text{baseline} + \\text{skill}_i - \\text{skill}_j\\). Baseline is the log average number of goals when both teams are equally good. The goal outcome of a match between home team \\(i\\) and away team \\(j\\) is modeled as:\n\\[\n\\text{HomeGoals}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{home},ij}),\n\\]\n\\[\n\\text{AwayGoals}_{ij} \\sim \\text{Pois}(\\lambda_{\\text{away},ij}),\n\\]\nwhere\n\\[\n\\log(\\lambda_{\\text{home},ij}) = \\text{baseline} + \\text{skill}_i - \\text{skill}_j,\n\\]\n\\[\n\\log(\\lambda_{\\text{away},ij}) = \\text{baseline} + \\text{skill}_j - \\text{skill}_i.\n\\]\nAdd some priors to that and you’ve got a Bayesian model going! I set the prior distributions over the baseline to:\n\\[\n\\text{baseline} \\sim N(0, 4^2),\n\\]\nand the skill of all \\(n\\) teams using a hierarchical approach to :\n\\[\n\\text{skill}_{1,\\ldots,n} \\sim N(\\mu_{\\text{teams}}, \\sigma^2_{\\text{teams}}),\n\\]\nso that teams are assumed to have similar but not identical mean and variance parameters for thier skill parameters. These priors are made vague. For example, the prior on the baseline have a SD of 4 but since this is on the log scale of the mean number of goals it corresponds to one SD from the mean 0 covering the range of [0.02,54.6] goals. Turning this into a JAGS model requires some minor adjustments. The model have to loop over all the match results, which adds some for-loops. JAGS parameterises the normal distribution with precision (the reciprocal of the variance) instead of variance so the hyperpriors have to be converted. Finally I have to “anchor” the skill of one team to a constant otherwise the mean skill can drift away freely (conrner constraint) and the model cannot be identified. Doing these adjustments results in the following model description:\n\nm1_string &lt;- \"model {\nfor(i in 1:n_games) {\n  HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])\n  AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])\n}\n\nfor(home_i in 1:n_teams) {\n  for(away_i in 1:n_teams) {\n    lambda_home[home_i, away_i] &lt;- exp(baseline + skill[home_i] - skill[away_i])\n    lambda_away[home_i, away_i] &lt;- exp(baseline + skill[away_i] - skill[home_i])\n  }\n}\n\nskill[1] &lt;- 0\nfor(j in 2:n_teams) {\n  skill[j] ~ dnorm(group_skill, group_tau)\n}  \n\ngroup_skill ~ dnorm(0, 0.0625)\ngroup_tau &lt;- 1 / pow(group_sigma, 2)\ngroup_sigma ~ dunif(0, 3)\nbaseline ~ dnorm(0, 0.0625)\n}\n\"\n\n## write the model to a text file\nwriteLines(m1_string, con = \"model1.txt\")\n\nNext, we define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"baseline\", \"skill\", \"group_skill\", \"group_sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Run the JAGS code via the R2jags interface and the jags function. Note that the first time jags is run after the R2jags package is loaded, it is often necessary to run any kind of randomisation function just to initiate the .Random.seed variable.\n\nm1.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\n    model.file = \"model1.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 3700\nNA    Unobserved stochastic nodes: 31\nNA    Total graph size: 9151\nNA \nNA Initializing model\n\nprint(m1.r2jags)\n\nNA Inference for Bugs model at \"model1.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA               mu.vect sd.vect      2.5%       25%       50%       75%     97.5%\nNA baseline        0.281   0.014     0.253     0.271     0.281     0.291     0.309\nNA group_sigma     0.225   0.034     0.169     0.201     0.222     0.246     0.302\nNA group_skill     0.016   0.062    -0.104    -0.026     0.016     0.057     0.136\nNA skill[1]        0.000   0.000     0.000     0.000     0.000     0.000     0.000\nNA skill[2]        0.185   0.061     0.064     0.145     0.185     0.226     0.307\nNA skill[3]        0.017   0.069    -0.117    -0.030     0.017     0.064     0.154\nNA skill[4]       -0.013   0.061    -0.132    -0.054    -0.013     0.028     0.109\nNA skill[5]       -0.180   0.098    -0.376    -0.247    -0.179    -0.113     0.008\nNA skill[6]       -0.048   0.063    -0.170    -0.090    -0.049    -0.007     0.081\nNA skill[7]       -0.013   0.058    -0.128    -0.051    -0.013     0.025     0.103\nNA skill[8]        0.199   0.060     0.084     0.159     0.199     0.240     0.317\nNA skill[9]       -0.077   0.063    -0.200    -0.119    -0.076    -0.034     0.046\nNA skill[10]      -0.110   0.062    -0.230    -0.152    -0.110    -0.068     0.011\nNA skill[11]       0.698   0.057     0.588     0.658     0.696     0.736     0.811\nNA skill[12]       0.133   0.060     0.017     0.092     0.133     0.174     0.249\nNA skill[13]       0.017   0.059    -0.096    -0.023     0.016     0.057     0.132\nNA skill[14]       0.038   0.061    -0.078    -0.003     0.037     0.080     0.160\nNA skill[15]      -0.008   0.060    -0.124    -0.048    -0.009     0.033     0.108\nNA skill[16]      -0.117   0.099    -0.305    -0.186    -0.118    -0.051     0.080\nNA skill[17]       0.606   0.058     0.495     0.566     0.606     0.646     0.720\nNA skill[18]      -0.071   0.070    -0.205    -0.118    -0.072    -0.026     0.071\nNA skill[19]      -0.115   0.069    -0.247    -0.162    -0.115    -0.068     0.022\nNA skill[20]       0.075   0.064    -0.045     0.032     0.074     0.117     0.204\nNA skill[21]      -0.104   0.065    -0.231    -0.148    -0.105    -0.060     0.027\nNA skill[22]      -0.212   0.099    -0.403    -0.281    -0.213    -0.146    -0.017\nNA skill[23]      -0.161   0.101    -0.360    -0.230    -0.159    -0.094     0.036\nNA skill[24]      -0.118   0.101    -0.319    -0.186    -0.118    -0.050     0.085\nNA skill[25]       0.009   0.071    -0.131    -0.037     0.010     0.057     0.147\nNA skill[26]       0.058   0.069    -0.079     0.011     0.058     0.104     0.195\nNA skill[27]      -0.061   0.080    -0.218    -0.115    -0.060    -0.005     0.088\nNA skill[28]      -0.118   0.079    -0.272    -0.170    -0.119    -0.065     0.037\nNA skill[29]      -0.059   0.105    -0.260    -0.130    -0.062     0.011     0.155\nNA deviance    10912.856   7.406 10900.319 10907.610 10912.214 10917.514 10928.852\nNA              Rhat n.eff\nNA baseline    1.001 15000\nNA group_sigma 1.002  1500\nNA group_skill 1.002  1600\nNA skill[1]    1.000     1\nNA skill[2]    1.005   410\nNA skill[3]    1.009   180\nNA skill[4]    1.007   670\nNA skill[5]    1.002  2400\nNA skill[6]    1.001  4400\nNA skill[7]    1.002  2400\nNA skill[8]    1.001 11000\nNA skill[9]    1.001 15000\nNA skill[10]   1.009   190\nNA skill[11]   1.001  4700\nNA skill[12]   1.005   340\nNA skill[13]   1.001 14000\nNA skill[14]   1.006   310\nNA skill[15]   1.002  2600\nNA skill[16]   1.003 12000\nNA skill[17]   1.001 15000\nNA skill[18]   1.002  2700\nNA skill[19]   1.003   880\nNA skill[20]   1.002  1800\nNA skill[21]   1.002  1000\nNA skill[22]   1.001  2800\nNA skill[23]   1.001 15000\nNA skill[24]   1.005   380\nNA skill[25]   1.001 15000\nNA skill[26]   1.002  2300\nNA skill[27]   1.001 15000\nNA skill[28]   1.001 15000\nNA skill[29]   1.001 13000\nNA deviance    1.001  3900\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 27.4 and DIC = 10940.3\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nUsing the generated MCMC samples I can now look at the credible skill values of any team. Let’s look at the trace plot and the distribution of the skill parameters for FC Sevilla and FC Valencia.\n\nteam_par&lt;-c(which(teams == c(\"FC Sevilla\")), which(teams == \"FC Valencia\"))\ndenplot(m1.r2jags, parms = team_par, style = \"plain\", main = c(\"Sevilla\",\"Valenica\"))\n\n\n\n\n\n\n\ntraplot(m1.r2jags, parms = team_par, style = \"plain\", main = c(\"Sevilla\",\"Valenica\"))"
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#model-validation",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#model-validation",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Model validation",
    "text": "Model validation\nSeems like Sevilla and Valencia have similar skill with Valencia being slightly better. Using the MCMC samples it is not only possible to look at the distribution of parameter values but it is also straight forward to simulate matches between teams and look at the credible distribution of number of goals scored and the probability of a win for the home team, a win for the away team or a draw. The following functions simulates matches with one team as home team and one team as away team and plots the predicted result together with the actual outcomes of any matches in the laliga data set.\n\n# Plots histograms over home_goals, away_goals, the difference in goals\n# and a barplot over match results.\nplot_goals &lt;- function(home_goals, away_goals) {\n    n_matches &lt;- length(home_goals)\n    goal_diff &lt;- home_goals - away_goals\n    match_result &lt;- ifelse(goal_diff &lt; 0, \"away_win\", ifelse(goal_diff &gt; 0,\n        \"home_win\", \"equal\"))\n    hist(home_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)\n    hist(away_goals, xlim = c(-0.5, 10), breaks = (0:100) - 0.5)\n    hist(goal_diff, xlim = c(-6, 6), breaks = (-100:100) - 0.5)\n    barplot(table(match_result)/n_matches, ylim = c(0, 1))\n}\n\n\nplot_pred_comp1 &lt;- function(home_team, away_team, ms) {\n    # Simulates and plots game goals scores using the MCMC samples from the m1\n    # model.\n    par(mar=c(2,2,2,2))\n    par(mfrow = c(2, 4))\n    baseline &lt;- ms[, \"baseline\"]\n    home_skill &lt;- ms[, which(teams == home_team)]\n    away_skill &lt;- ms[, which(teams == away_team)]\n    home_goals &lt;- rpois(nrow(ms), exp(baseline + home_skill - away_skill))\n    away_goals &lt;- rpois(nrow(ms), exp(baseline + away_skill - home_skill))\n    plot_goals(home_goals, away_goals)\n    # Plots the actual distribution of goals between the two teams\n    home_goals &lt;- d$HomeGoals[d$HomeTeam == home_team & d$AwayTeam == away_team]\n    away_goals &lt;- d$AwayGoals[d$HomeTeam == home_team & d$AwayTeam == away_team]\n    plot_goals(home_goals, away_goals)\n}\n\nLet’s look at Valencia (home team) vs. Sevilla (away team). The graph below shows the simulation on the first row and the historical data on the second row.\n\nms1&lt;-as.matrix(m1.r2jags$BUGSoutput$sims.matrix)\nplot_pred_comp1(\"FC Valencia\", \"FC Sevilla\", ms1)\n\n\n\n\n\n\n\n\nHere we discover a problem with the current model. While the simulated data looks the same, except that the home team and the away team swapped places, the historical data now shows that Sevilla often wins against Valencia when being the home team. Our model doesn’t predict this because it doesn’t considers the advantage of being the home team."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#model-fitting-1",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\n\n# model 2\nm2_string &lt;- \"model {\nfor(i in 1:n_games) {\n  HomeGoals[i] ~ dpois(lambda_home[HomeTeam[i],AwayTeam[i]])\n  AwayGoals[i] ~ dpois(lambda_away[HomeTeam[i],AwayTeam[i]])\n}\n\nfor(home_i in 1:n_teams) {\n  for(away_i in 1:n_teams) {\n    lambda_home[home_i, away_i] &lt;- exp( home_baseline + skill[home_i] - skill[away_i])\n    lambda_away[home_i, away_i] &lt;- exp( away_baseline + skill[away_i] - skill[home_i])\n  }\n}\n\nskill[1] &lt;- 0 \nfor(j in 2:n_teams) {\n  skill[j] ~ dnorm(group_skill, group_tau)\n}\n\ngroup_skill ~ dnorm(0, 0.0625)\ngroup_tau &lt;- 1/pow(group_sigma, 2)\ngroup_sigma ~ dunif(0, 3)\n\nhome_baseline ~ dnorm(0, 0.0625)\naway_baseline ~ dnorm(0, 0.0625)\n}\n\"\n\n## write the model to a text file\nwriteLines(m2_string, con = \"model2.txt\")\n\nAnd now re-fit the model\n\nparams &lt;- c(\"home_baseline\", \"away_baseline\", \"skill\", \"group_sigma\", \"group_skill\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\n\nm2.r2jags &lt;- jags(data = data_list, inits = NULL, parameters.to.save = params,\n    model.file = \"model2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 3700\nNA    Unobserved stochastic nodes: 32\nNA    Total graph size: 10863\nNA \nNA Initializing model\n\nprint(m2.r2jags)\n\nNA Inference for Bugs model at \"model2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA                 mu.vect sd.vect      2.5%       25%       50%       75%\nNA away_baseline     0.081   0.022     0.038     0.067     0.082     0.096\nNA group_sigma       0.226   0.035     0.169     0.201     0.221     0.246\nNA group_skill       0.019   0.061    -0.102    -0.022     0.019     0.060\nNA home_baseline     0.449   0.019     0.413     0.436     0.449     0.462\nNA skill[1]          0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[2]          0.187   0.061     0.066     0.147     0.187     0.228\nNA skill[3]          0.017   0.068    -0.120    -0.030     0.017     0.063\nNA skill[4]         -0.011   0.058    -0.126    -0.050    -0.011     0.028\nNA skill[5]         -0.179   0.100    -0.375    -0.246    -0.178    -0.111\nNA skill[6]         -0.044   0.062    -0.167    -0.087    -0.045    -0.002\nNA skill[7]         -0.009   0.061    -0.127    -0.050    -0.009     0.033\nNA skill[8]          0.199   0.059     0.081     0.159     0.201     0.240\nNA skill[9]         -0.078   0.063    -0.205    -0.121    -0.077    -0.035\nNA skill[10]        -0.108   0.064    -0.234    -0.151    -0.108    -0.066\nNA skill[11]         0.699   0.057     0.583     0.661     0.699     0.737\nNA skill[12]         0.132   0.059     0.019     0.092     0.132     0.173\nNA skill[13]         0.025   0.061    -0.097    -0.016     0.025     0.065\nNA skill[14]         0.041   0.059    -0.074     0.000     0.041     0.081\nNA skill[15]        -0.006   0.060    -0.125    -0.046    -0.005     0.035\nNA skill[16]        -0.115   0.099    -0.311    -0.181    -0.116    -0.049\nNA skill[17]         0.611   0.059     0.494     0.571     0.612     0.653\nNA skill[18]        -0.068   0.070    -0.204    -0.115    -0.069    -0.020\nNA skill[19]        -0.114   0.070    -0.252    -0.162    -0.113    -0.065\nNA skill[20]         0.077   0.062    -0.047     0.035     0.077     0.118\nNA skill[21]        -0.102   0.064    -0.228    -0.145    -0.101    -0.058\nNA skill[22]        -0.202   0.098    -0.399    -0.266    -0.201    -0.138\nNA skill[23]        -0.167   0.098    -0.361    -0.233    -0.167    -0.102\nNA skill[24]        -0.115   0.099    -0.306    -0.183    -0.116    -0.049\nNA skill[25]         0.010   0.071    -0.132    -0.035     0.010     0.057\nNA skill[26]         0.061   0.069    -0.075     0.014     0.062     0.109\nNA skill[27]        -0.059   0.078    -0.211    -0.112    -0.060    -0.007\nNA skill[28]        -0.113   0.082    -0.274    -0.167    -0.113    -0.058\nNA skill[29]        -0.051   0.105    -0.265    -0.121    -0.050     0.021\nNA deviance      10742.730   7.596 10729.548 10737.288 10742.120 10747.534\nNA                   97.5%  Rhat n.eff\nNA away_baseline     0.126 1.002  1600\nNA group_sigma       0.305 1.001 15000\nNA group_skill       0.138 1.006   330\nNA home_baseline     0.486 1.001 15000\nNA skill[1]          0.000 1.000     1\nNA skill[2]          0.306 1.005   380\nNA skill[3]          0.149 1.006   310\nNA skill[4]          0.102 1.002  1900\nNA skill[5]          0.013 1.002  1100\nNA skill[6]          0.076 1.005   400\nNA skill[7]          0.110 1.004   440\nNA skill[8]          0.312 1.008   240\nNA skill[9]          0.045 1.003   620\nNA skill[10]         0.014 1.005   330\nNA skill[11]         0.811 1.004   580\nNA skill[12]         0.245 1.008   200\nNA skill[13]         0.144 1.003   840\nNA skill[14]         0.154 1.003   700\nNA skill[15]         0.111 1.008   210\nNA skill[16]         0.078 1.006   300\nNA skill[17]         0.722 1.005   420\nNA skill[18]         0.070 1.005   360\nNA skill[19]         0.019 1.007   290\nNA skill[20]         0.200 1.006   310\nNA skill[21]         0.025 1.010   170\nNA skill[22]        -0.009 1.002  1600\nNA skill[23]         0.028 1.003   900\nNA skill[24]         0.078 1.006   330\nNA skill[25]         0.151 1.007   240\nNA skill[26]         0.196 1.003   770\nNA skill[27]         0.097 1.001  3900\nNA skill[28]         0.051 1.002  1800\nNA skill[29]         0.153 1.003   620\nNA deviance      10759.025 1.004   460\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 28.8 and DIC = 10771.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics-1",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nLooking at the trace plots and distributions of home_baseline and away_baseline shows that there is a considerable home advantage.\n\nteam_par&lt;-c(\"home_baseline\", \"away_baseline\")\ndenplot(m2.r2jags, parms = team_par, style = \"plain\", main = c(\"home_baseline\",\"away_baseline\"))\n\n\n\n\n\n\n\ntraplot(m2.r2jags, parms = team_par, style = \"plain\", main = c(\"home_baseline\",\"away_baseline\"))"
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#model-validation-1",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#model-validation-1",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Model validation",
    "text": "Model validation\nLooking at the difference between exp(home_baseline) and exp(away_baseline) shows that the home advantage is realised as roughly \\(0.5\\) more goals for the home team.\n\nms2&lt;-as.matrix(m2.r2jags$BUGSoutput$sims.matrix)\nplotPost(exp(ms2[, \"home_baseline\"]) - exp(ms2[, \"away_baseline\"]), compVal = 0,\n    xlab = \"Home advantage in number of goals\")\n\n\n\n\n\n\n\n\nNA                                        mean    median      mode hdiMass\nNA Home advantage in number of goals 0.4822046 0.4822645 0.4831489    0.95\nNA                                      hdiLow   hdiHigh compVal pcGTcompVal\nNA Home advantage in number of goals 0.4096975 0.5549439       0           1\nNA                                   ROPElow ROPEhigh pcInROPE\nNA Home advantage in number of goals      NA       NA       NA\n\n\nComparing the DIC of the of the two models also indicates that the new model is better.\n\ndic_m1&lt;-m1.r2jags$BUGSoutput$DIC\ndic_m2&lt;-m2.r2jags$BUGSoutput$DIC\ndiff_dic&lt;-dic_m1 - dic_m2\ndiff_dic\n\nNA [1] 168.7556\n\n\nFinally we’ll look at the simulated results for Valencia (home team) vs Sevilla (away team) using the estimates from the new model with the first row of the graph showing the predicted outcome and the second row showing the actual data.\n\nplot_pred_comp2 &lt;- function(home_team, away_team, ms) {\n    par(mar=c(2,2,2,2))\n    par(mfrow = c(2, 4))\n    home_baseline &lt;- ms[, \"home_baseline\"]\n    away_baseline &lt;- ms[, \"away_baseline\"]\n    home_skill &lt;- ms[, col_name(\"skill\", which(teams == home_team))]\n    away_skill &lt;- ms[, col_name(\"skill\", which(teams == away_team))]\n    home_goals &lt;- rpois(nrow(ms), exp(home_baseline + home_skill - away_skill))\n    away_goals &lt;- rpois(nrow(ms), exp(away_baseline + away_skill - home_skill))\n    plot_goals(home_goals, away_goals)\n    home_goals &lt;- d$HomeGoals[d$HomeTeam == home_team & d$AwayTeam == away_team]\n    away_goals &lt;- d$AwayGoals[d$HomeTeam == home_team & d$AwayTeam == away_team]\n    plot_goals(home_goals, away_goals)\n}\n\nplot_pred_comp2(\"FC Valencia\", \"FC Sevilla\", ms2)\n\n\n\n\n\n\n\n\nAnd similarly Sevilla (home team) vs Valencia (away team).\n\nplot_pred_comp2(\"FC Sevilla\", \"FC Valencia\", ms2)\n\n\n\n\n\n\n\n\nNow the results are closer to the historical data as both Sevilla and Valencia are more likely to win when playing as the home team. At this point in the modeling process I decided to try to split the skill parameter into two components, offence skill and defense skill, thinking that some teams might be good at scoring goals but at the same time be bad at keeping the opponent from scoring. This didn’t seem to result in any better fit however, perhaps because the offensive and defensive skill of a team tend to be highly related. There is however one more thing I would like to change with the model."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#data-check-1",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#data-check-1",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "Data check",
    "text": "Data check\n\nqplot(Season, HomeTeam, data = d, ylab = \"Team\", xlab = \"Particicipation by Season\") + theme_classic()\n\n\n\n\n\n\n\n\nThe second iteration of the model was therefore modified to include the year-to-year variability in team skill. This was done by allowing each team to have one skill parameter per season but to connect the skill parameters by using a team’s skill parameter for season \\(t\\) in the prior distribution for that team’s skill parameter for season \\(t+1\\) so that\n\\[\n\\text{skill}_{t+1} \\sim N(\\text{skill}_t,\\sigma^2_{\\text{season}})\n\\]\nfor all different \\(t\\), except the first season which is given a vague prior. Here \\(\\sigma^2_{\\text{season}}\\) is a parameter estimated using the whole data set. The home and away baselines are given the same kind of priors and below is the resulting JAGS model.\n\n# model 3\nm3_string &lt;- \"model {\nfor(i in 1:n_games) {\n  HomeGoals[i] ~ dpois(lambda_home[Season[i], HomeTeam[i],AwayTeam[i]])\n  AwayGoals[i] ~ dpois(lambda_away[Season[i], HomeTeam[i],AwayTeam[i]])\n}\n\nfor(season_i in 1:n_seasons) {\n  for(home_i in 1:n_teams) {\n    for(away_i in 1:n_teams) {\n      lambda_home[season_i, home_i, away_i] &lt;- exp( home_baseline[season_i] + skill[season_i, home_i] - skill[season_i, away_i])\n      lambda_away[season_i, home_i, away_i] &lt;- exp( away_baseline[season_i] + skill[season_i, away_i] - skill[season_i, home_i])\n    }\n  }\n}\n\nskill[1, 1] &lt;- 0 \nfor(j in 2:n_teams) {\n  skill[1, j] ~ dnorm(group_skill, group_tau)\n}\n\ngroup_skill ~ dnorm(0, 0.0625)\ngroup_tau &lt;- 1/pow(group_sigma, 2)\ngroup_sigma ~ dunif(0, 3)\n\nhome_baseline[1] ~ dnorm(0, 0.0625)\naway_baseline[1] ~ dnorm(0, 0.0625)\n\nfor(season_i in 2:n_seasons) {\n  skill[season_i, 1] &lt;- 0 \n  for(j in 2:n_teams) {\n    skill[season_i, j] ~ dnorm(skill[season_i - 1, j], season_tau)\n  }\n  home_baseline[season_i] ~ dnorm(home_baseline[season_i - 1], season_tau)\n  away_baseline[season_i] ~ dnorm(away_baseline[season_i - 1], season_tau)\n}\n\nseason_tau &lt;- 1/pow(season_sigma, 2) \nseason_sigma ~ dunif(0, 3) \n}\n\"\n\n## write the model to a text file\nwriteLines(m3_string, con = \"model3.txt\")\n\nAnd now re-fit the model. These changes to the model unfortunately introduce quite a lot of autocorrelation when running the MCMC sampler. Also, I re-define the data list to include information for the season parameters.\n\ndata_list_m3 &lt;- list(HomeGoals = d$HomeGoals, AwayGoals = d$AwayGoals, HomeTeam = as.numeric(factor(d$HomeTeam,\n    levels = teams)), AwayTeam = as.numeric(factor(d$AwayTeam, levels = teams)),\n    Season = as.numeric(factor(d$Season, levels = seasons)), n_teams = length(teams),\n    n_games = nrow(d), n_seasons = length(seasons))\nparams &lt;- c(\"home_baseline\", \"away_baseline\", \"skill\", \"season_sigma\", \"group_sigma\", \"group_skill\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\n\nm3.r2jags &lt;- jags(data = data_list_m3, inits = NULL, parameters.to.save = params,\n    model.file = \"model3.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 3700\nNA    Unobserved stochastic nodes: 153\nNA    Total graph size: 26525\nNA \nNA Initializing model\n\nprint(m3.r2jags)\n\nNA Inference for Bugs model at \"model3.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA                    mu.vect sd.vect      2.5%       25%       50%       75%\nNA away_baseline[1]     0.105   0.038     0.030     0.079     0.103     0.130\nNA away_baseline[2]     0.078   0.030     0.018     0.059     0.079     0.099\nNA away_baseline[3]     0.067   0.031     0.005     0.047     0.069     0.089\nNA away_baseline[4]     0.064   0.032    -0.001     0.043     0.065     0.087\nNA away_baseline[5]     0.079   0.035     0.011     0.056     0.080     0.101\nNA group_sigma          0.217   0.035     0.158     0.193     0.214     0.238\nNA group_skill          0.018   0.060    -0.090    -0.023     0.015     0.057\nNA home_baseline[1]     0.447   0.029     0.392     0.428     0.446     0.466\nNA home_baseline[2]     0.437   0.026     0.383     0.421     0.438     0.454\nNA home_baseline[3]     0.443   0.026     0.389     0.427     0.443     0.459\nNA home_baseline[4]     0.452   0.027     0.400     0.434     0.451     0.469\nNA home_baseline[5]     0.454   0.031     0.394     0.434     0.452     0.474\nNA season_sigma         0.033   0.019     0.001     0.016     0.032     0.048\nNA skill[1,1]           0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[2,1]           0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[3,1]           0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[4,1]           0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[5,1]           0.000   0.000     0.000     0.000     0.000     0.000\nNA skill[1,2]           0.178   0.067     0.047     0.136     0.175     0.218\nNA skill[2,2]           0.169   0.065     0.039     0.129     0.167     0.207\nNA skill[3,2]           0.181   0.064     0.061     0.139     0.177     0.219\nNA skill[4,2]           0.195   0.065     0.076     0.148     0.189     0.235\nNA skill[5,2]           0.216   0.075     0.090     0.161     0.209     0.265\nNA skill[1,3]           0.015   0.074    -0.126    -0.033     0.013     0.060\nNA skill[2,3]           0.017   0.075    -0.127    -0.030     0.014     0.063\nNA skill[3,3]           0.020   0.074    -0.121    -0.027     0.017     0.066\nNA skill[4,3]           0.022   0.071    -0.111    -0.024     0.018     0.066\nNA skill[5,3]           0.027   0.075    -0.111    -0.021     0.023     0.074\nNA skill[1,4]          -0.008   0.066    -0.122    -0.053    -0.012     0.035\nNA skill[2,4]          -0.011   0.063    -0.118    -0.053    -0.015     0.031\nNA skill[3,4]          -0.010   0.063    -0.120    -0.052    -0.015     0.031\nNA skill[4,4]          -0.020   0.065    -0.136    -0.064    -0.024     0.022\nNA skill[5,4]          -0.022   0.069    -0.151    -0.069    -0.025     0.023\nNA skill[1,5]          -0.174   0.098    -0.370    -0.238    -0.173    -0.107\nNA skill[2,5]          -0.174   0.104    -0.384    -0.242    -0.174    -0.104\nNA skill[3,5]          -0.174   0.111    -0.394    -0.247    -0.174    -0.102\nNA skill[4,5]          -0.174   0.117    -0.408    -0.251    -0.174    -0.098\nNA skill[5,5]          -0.174   0.123    -0.420    -0.254    -0.175    -0.095\nNA skill[1,6]          -0.034   0.074    -0.161    -0.085    -0.039     0.017\nNA skill[2,6]          -0.048   0.068    -0.167    -0.096    -0.052    -0.004\nNA skill[3,6]          -0.058   0.067    -0.176    -0.106    -0.060    -0.015\nNA skill[4,6]          -0.065   0.072    -0.193    -0.117    -0.067    -0.019\nNA skill[5,6]          -0.071   0.075    -0.207    -0.125    -0.072    -0.025\nNA skill[1,7]          -0.006   0.069    -0.124    -0.054    -0.013     0.038\nNA skill[2,7]          -0.014   0.065    -0.129    -0.058    -0.021     0.027\nNA skill[3,7]          -0.009   0.064    -0.120    -0.054    -0.016     0.031\nNA skill[4,7]          -0.006   0.066    -0.117    -0.052    -0.013     0.035\nNA skill[5,7]          -0.001   0.071    -0.122    -0.051    -0.009     0.044\nNA skill[1,8]           0.200   0.067     0.069     0.155     0.198     0.242\nNA skill[2,8]           0.206   0.063     0.086     0.162     0.203     0.246\nNA skill[3,8]           0.209   0.063     0.090     0.164     0.206     0.250\nNA skill[4,8]           0.204   0.064     0.082     0.158     0.202     0.244\nNA skill[5,8]           0.195   0.069     0.059     0.151     0.194     0.239\nNA skill[1,9]          -0.055   0.073    -0.193    -0.102    -0.057    -0.005\nNA skill[2,9]          -0.074   0.068    -0.203    -0.119    -0.077    -0.026\nNA skill[3,9]          -0.087   0.068    -0.219    -0.133    -0.088    -0.040\nNA skill[4,9]          -0.105   0.074    -0.254    -0.155    -0.101    -0.057\nNA skill[5,9]          -0.105   0.083    -0.279    -0.159    -0.100    -0.049\nNA skill[1,10]         -0.121   0.072    -0.246    -0.175    -0.125    -0.072\nNA skill[2,10]         -0.109   0.070    -0.224    -0.163    -0.113    -0.061\nNA skill[3,10]         -0.102   0.071    -0.219    -0.156    -0.106    -0.051\nNA skill[4,10]         -0.111   0.073    -0.234    -0.168    -0.116    -0.060\nNA skill[5,10]         -0.111   0.083    -0.259    -0.173    -0.116    -0.055\nNA skill[1,11]          0.676   0.072     0.526     0.629     0.680     0.726\nNA skill[2,11]          0.696   0.063     0.571     0.654     0.699     0.738\nNA skill[3,11]          0.712   0.060     0.600     0.672     0.712     0.750\nNA skill[4,11]          0.728   0.061     0.617     0.688     0.725     0.761\nNA skill[5,11]          0.727   0.064     0.606     0.686     0.725     0.763\nNA skill[1,12]          0.146   0.064     0.025     0.106     0.144     0.186\nNA skill[2,12]          0.143   0.060     0.028     0.105     0.141     0.180\nNA skill[3,12]          0.131   0.058     0.019     0.094     0.130     0.168\nNA skill[4,12]          0.127   0.059     0.010     0.089     0.126     0.164\nNA skill[5,12]          0.127   0.064     0.002     0.086     0.126     0.166\nNA skill[1,13]          0.031   0.065    -0.087    -0.014     0.028     0.071\nNA skill[2,13]          0.035   0.062    -0.077    -0.008     0.032     0.073\nNA skill[3,13]          0.023   0.061    -0.091    -0.019     0.020     0.062\nNA skill[4,13]          0.017   0.062    -0.101    -0.026     0.015     0.057\nNA skill[5,13]          0.015   0.067    -0.117    -0.030     0.014     0.057\nNA skill[1,14]          0.028   0.064    -0.095    -0.015     0.025     0.069\nNA skill[2,14]          0.027   0.061    -0.091    -0.014     0.024     0.065\nNA skill[3,14]          0.030   0.059    -0.085    -0.010     0.027     0.067\nNA skill[4,14]          0.046   0.062    -0.067     0.004     0.040     0.085\nNA skill[5,14]          0.055   0.068    -0.064     0.008     0.049     0.099\nNA skill[1,15]          0.015   0.065    -0.106    -0.029     0.009     0.055\nNA skill[2,15]          0.019   0.063    -0.095    -0.024     0.012     0.058\nNA skill[3,15]         -0.003   0.061    -0.115    -0.043    -0.005     0.034\nNA skill[4,15]         -0.014   0.062    -0.132    -0.055    -0.015     0.024\nNA skill[5,15]         -0.037   0.071    -0.182    -0.082    -0.034     0.009\nNA skill[1,16]         -0.120   0.096    -0.304    -0.187    -0.122    -0.055\nNA skill[2,16]         -0.121   0.103    -0.319    -0.192    -0.122    -0.051\nNA skill[3,16]         -0.121   0.109    -0.329    -0.195    -0.121    -0.047\nNA skill[4,16]         -0.121   0.116    -0.342    -0.197    -0.122    -0.044\nNA skill[5,16]         -0.120   0.121    -0.357    -0.201    -0.121    -0.041\nNA skill[1,17]          0.543   0.083     0.372     0.491     0.544     0.600\nNA skill[2,17]          0.588   0.067     0.470     0.541     0.586     0.631\nNA skill[3,17]          0.621   0.068     0.495     0.575     0.617     0.666\nNA skill[4,17]          0.646   0.075     0.501     0.593     0.644     0.697\nNA skill[5,17]          0.640   0.077     0.497     0.586     0.637     0.694\nNA skill[1,18]         -0.068   0.075    -0.209    -0.122    -0.069    -0.021\nNA skill[2,18]         -0.075   0.074    -0.219    -0.126    -0.075    -0.029\nNA skill[3,18]         -0.068   0.078    -0.218    -0.122    -0.067    -0.019\nNA skill[4,18]         -0.061   0.081    -0.217    -0.117    -0.060    -0.011\nNA skill[5,18]         -0.054   0.081    -0.208    -0.110    -0.054    -0.002\nNA skill[1,19]         -0.099   0.066    -0.220    -0.144    -0.101    -0.057\nNA skill[2,19]         -0.106   0.065    -0.227    -0.151    -0.108    -0.064\nNA skill[3,19]         -0.122   0.070    -0.260    -0.170    -0.121    -0.074\nNA skill[4,19]         -0.122   0.080    -0.290    -0.174    -0.119    -0.069\nNA skill[5,19]         -0.122   0.089    -0.312    -0.176    -0.118    -0.065\nNA skill[1,20]          0.086   0.068    -0.034     0.038     0.083     0.130\nNA skill[2,20]          0.083   0.065    -0.031     0.036     0.079     0.126\nNA skill[3,20]          0.082   0.065    -0.033     0.034     0.078     0.125\nNA skill[4,20]          0.065   0.070    -0.068     0.017     0.063     0.110\nNA skill[5,20]          0.065   0.079    -0.094     0.014     0.064     0.115\nNA skill[1,21]         -0.097   0.081    -0.245    -0.154    -0.103    -0.042\nNA skill[2,21]         -0.101   0.073    -0.234    -0.152    -0.104    -0.048\nNA skill[3,21]         -0.100   0.071    -0.230    -0.149    -0.103    -0.050\nNA skill[4,21]         -0.107   0.070    -0.237    -0.155    -0.110    -0.059\nNA skill[5,21]         -0.108   0.074    -0.245    -0.158    -0.114    -0.058\nNA skill[1,22]         -0.197   0.106    -0.391    -0.269    -0.198    -0.126\nNA skill[2,22]         -0.204   0.100    -0.389    -0.272    -0.205    -0.138\nNA skill[3,22]         -0.204   0.107    -0.401    -0.278    -0.204    -0.134\nNA skill[4,22]         -0.205   0.114    -0.418    -0.282    -0.205    -0.131\nNA skill[5,22]         -0.205   0.120    -0.432    -0.287    -0.206    -0.127\nNA skill[1,23]         -0.158   0.099    -0.343    -0.228    -0.160    -0.092\nNA skill[2,23]         -0.164   0.094    -0.340    -0.232    -0.165    -0.102\nNA skill[3,23]         -0.163   0.102    -0.357    -0.235    -0.165    -0.097\nNA skill[4,23]         -0.164   0.108    -0.373    -0.239    -0.165    -0.094\nNA skill[5,23]         -0.164   0.114    -0.386    -0.241    -0.165    -0.092\nNA skill[1,24]         -0.102   0.105    -0.310    -0.171    -0.105    -0.033\nNA skill[2,24]         -0.107   0.102    -0.306    -0.174    -0.108    -0.041\nNA skill[3,24]         -0.112   0.097    -0.301    -0.177    -0.112    -0.049\nNA skill[4,24]         -0.112   0.104    -0.317    -0.181    -0.113    -0.044\nNA skill[5,24]         -0.112   0.111    -0.329    -0.185    -0.112    -0.041\nNA skill[1,25]          0.009   0.086    -0.149    -0.046     0.007     0.066\nNA skill[2,25]          0.009   0.081    -0.145    -0.044     0.006     0.061\nNA skill[3,25]          0.008   0.074    -0.140    -0.042     0.006     0.055\nNA skill[4,25]          0.013   0.074    -0.139    -0.038     0.011     0.060\nNA skill[5,25]          0.003   0.077    -0.143    -0.047     0.001     0.053\nNA skill[1,26]          0.048   0.088    -0.117    -0.019     0.046     0.108\nNA skill[2,26]          0.049   0.083    -0.104    -0.013     0.047     0.104\nNA skill[3,26]          0.049   0.076    -0.083    -0.009     0.047     0.099\nNA skill[4,26]          0.068   0.075    -0.063     0.014     0.066     0.118\nNA skill[5,26]          0.091   0.082    -0.057     0.034     0.091     0.145\nNA skill[1,27]         -0.051   0.093    -0.236    -0.108    -0.049     0.000\nNA skill[2,27]         -0.054   0.088    -0.231    -0.108    -0.050    -0.003\nNA skill[3,27]         -0.055   0.084    -0.224    -0.108    -0.052    -0.006\nNA skill[4,27]         -0.057   0.077    -0.213    -0.107    -0.054    -0.012\nNA skill[5,27]         -0.053   0.079    -0.211    -0.104    -0.051    -0.007\nNA skill[1,28]         -0.110   0.099    -0.292    -0.188    -0.112    -0.039\nNA skill[2,28]         -0.113   0.094    -0.288    -0.188    -0.115    -0.046\nNA skill[3,28]         -0.117   0.088    -0.282    -0.186    -0.120    -0.052\nNA skill[4,28]         -0.121   0.081    -0.274    -0.186    -0.123    -0.063\nNA skill[5,28]         -0.124   0.082    -0.278    -0.188    -0.127    -0.066\nNA skill[1,29]         -0.057   0.129    -0.272    -0.154    -0.066     0.028\nNA skill[2,29]         -0.059   0.127    -0.270    -0.155    -0.067     0.022\nNA skill[3,29]         -0.061   0.124    -0.268    -0.154    -0.068     0.019\nNA skill[4,29]         -0.062   0.121    -0.268    -0.152    -0.069     0.014\nNA skill[5,29]         -0.063   0.117    -0.267    -0.149    -0.068     0.010\nNA deviance         10731.636  11.981 10708.084 10723.239 10732.002 10740.556\nNA                      97.5%  Rhat n.eff\nNA away_baseline[1]     0.185 1.056    34\nNA away_baseline[2]     0.135 1.004   500\nNA away_baseline[3]     0.123 1.001  7800\nNA away_baseline[4]     0.121 1.001  3300\nNA away_baseline[5]     0.148 1.006   280\nNA group_sigma          0.297 1.008   210\nNA group_skill          0.146 1.015  5000\nNA home_baseline[1]     0.508 1.018   260\nNA home_baseline[2]     0.487 1.005 15000\nNA home_baseline[3]     0.493 1.004 15000\nNA home_baseline[4]     0.506 1.007   390\nNA home_baseline[5]     0.516 1.011   260\nNA season_sigma         0.069 1.323    11\nNA skill[1,1]           0.000 1.000     1\nNA skill[2,1]           0.000 1.000     1\nNA skill[3,1]           0.000 1.000     1\nNA skill[4,1]           0.000 1.000     1\nNA skill[5,1]           0.000 1.000     1\nNA skill[1,2]           0.323 1.003  5900\nNA skill[2,2]           0.315 1.003 15000\nNA skill[3,2]           0.323 1.008   880\nNA skill[4,2]           0.338 1.012   290\nNA skill[5,2]           0.377 1.016   130\nNA skill[1,3]           0.170 1.017   110\nNA skill[2,3]           0.174 1.014   150\nNA skill[3,3]           0.175 1.013   180\nNA skill[4,3]           0.173 1.011   250\nNA skill[5,3]           0.185 1.004   470\nNA skill[1,4]           0.130 1.005   510\nNA skill[2,4]           0.121 1.004   680\nNA skill[3,4]           0.123 1.006   380\nNA skill[4,4]           0.117 1.003   820\nNA skill[5,4]           0.121 1.002  1200\nNA skill[1,5]           0.015 1.002  1400\nNA skill[2,5]           0.033 1.002  1500\nNA skill[3,5]           0.046 1.002  1800\nNA skill[4,5]           0.060 1.002  2000\nNA skill[5,5]           0.074 1.002  1900\nNA skill[1,6]           0.116 1.008   620\nNA skill[2,6]           0.092 1.010 15000\nNA skill[3,6]           0.084 1.012  3500\nNA skill[4,6]           0.085 1.011  1200\nNA skill[5,6]           0.086 1.012   710\nNA skill[1,7]           0.145 1.005 11000\nNA skill[2,7]           0.133 1.011 15000\nNA skill[3,7]           0.134 1.015 15000\nNA skill[4,7]           0.142 1.012 15000\nNA skill[5,7]           0.154 1.007 15000\nNA skill[1,8]           0.337 1.001 15000\nNA skill[2,8]           0.339 1.003 15000\nNA skill[3,8]           0.341 1.001  6700\nNA skill[4,8]           0.340 1.003 15000\nNA skill[5,8]           0.338 1.003  1400\nNA skill[1,9]           0.090 1.019   140\nNA skill[2,9]           0.059 1.010   520\nNA skill[3,9]           0.046 1.007 13000\nNA skill[4,9]           0.035 1.004   790\nNA skill[5,9]           0.047 1.003   790\nNA skill[1,10]          0.027 1.011   150\nNA skill[2,10]          0.032 1.016   110\nNA skill[3,10]          0.040 1.021    98\nNA skill[4,10]          0.037 1.011   170\nNA skill[5,10]          0.057 1.008   200\nNA skill[1,11]          0.815 1.035    59\nNA skill[2,11]          0.825 1.019   110\nNA skill[3,11]          0.839 1.006   320\nNA skill[4,11]          0.860 1.002  1100\nNA skill[5,11]          0.866 1.002  1000\nNA skill[1,12]          0.279 1.002  1900\nNA skill[2,12]          0.267 1.002  1300\nNA skill[3,12]          0.247 1.009   300\nNA skill[4,12]          0.244 1.012   190\nNA skill[5,12]          0.255 1.006   280\nNA skill[1,13]          0.168 1.002  4300\nNA skill[2,13]          0.167 1.004 15000\nNA skill[3,13]          0.153 1.009   350\nNA skill[4,13]          0.148 1.014   170\nNA skill[5,13]          0.152 1.012   210\nNA skill[1,14]          0.159 1.007   240\nNA skill[2,14]          0.153 1.007   240\nNA skill[3,14]          0.156 1.006   410\nNA skill[4,14]          0.176 1.001 15000\nNA skill[5,14]          0.202 1.002  1400\nNA skill[1,15]          0.155 1.001  4000\nNA skill[2,15]          0.154 1.001  4000\nNA skill[3,15]          0.122 1.008   210\nNA skill[4,15]          0.113 1.013   130\nNA skill[5,15]          0.100 1.030    58\nNA skill[1,16]          0.071 1.004   970\nNA skill[2,16]          0.084 1.002  1200\nNA skill[3,16]          0.095 1.002  1000\nNA skill[4,16]          0.108 1.002   970\nNA skill[5,16]          0.121 1.002  1000\nNA skill[1,17]          0.704 1.009   190\nNA skill[2,17]          0.721 1.002 15000\nNA skill[3,17]          0.759 1.023   100\nNA skill[4,17]          0.800 1.049    48\nNA skill[5,17]          0.796 1.039    58\nNA skill[1,18]          0.086 1.011   160\nNA skill[2,18]          0.077 1.008   220\nNA skill[3,18]          0.089 1.010   170\nNA skill[4,18]          0.099 1.011   160\nNA skill[5,18]          0.108 1.014   120\nNA skill[1,19]          0.039 1.013   130\nNA skill[2,19]          0.028 1.016   100\nNA skill[3,19]          0.017 1.028    62\nNA skill[4,19]          0.036 1.020    83\nNA skill[5,19]          0.054 1.017    98\nNA skill[1,20]          0.229 1.006   280\nNA skill[2,20]          0.221 1.006   300\nNA skill[3,20]          0.220 1.004   510\nNA skill[4,20]          0.207 1.001 15000\nNA skill[5,20]          0.223 1.001 15000\nNA skill[1,21]          0.063 1.006   320\nNA skill[2,21]          0.043 1.005   390\nNA skill[3,21]          0.042 1.007   240\nNA skill[4,21]          0.037 1.007   250\nNA skill[5,21]          0.044 1.009   190\nNA skill[1,22]          0.014 1.011   150\nNA skill[2,22]         -0.001 1.012   160\nNA skill[3,22]          0.010 1.010   180\nNA skill[4,22]          0.026 1.008   220\nNA skill[5,22]          0.038 1.006   270\nNA skill[1,23]          0.043 1.003   710\nNA skill[2,23]          0.026 1.002  1200\nNA skill[3,23]          0.042 1.002  1900\nNA skill[4,23]          0.054 1.001  2600\nNA skill[5,23]          0.070 1.002  2300\nNA skill[1,24]          0.109 1.001 15000\nNA skill[2,24]          0.096 1.001  5000\nNA skill[3,24]          0.085 1.001  2700\nNA skill[4,24]          0.098 1.001  3600\nNA skill[5,24]          0.109 1.002  3300\nNA skill[1,25]          0.186 1.021   480\nNA skill[2,25]          0.174 1.030   410\nNA skill[3,25]          0.163 1.041   410\nNA skill[4,25]          0.169 1.041   230\nNA skill[5,25]          0.162 1.023   310\nNA skill[1,26]          0.228 1.010   240\nNA skill[2,26]          0.220 1.015   170\nNA skill[3,26]          0.203 1.026   120\nNA skill[4,26]          0.224 1.052    49\nNA skill[5,26]          0.257 1.078    31\nNA skill[1,27]          0.143 1.001  3900\nNA skill[2,27]          0.129 1.001  9900\nNA skill[3,27]          0.118 1.001 15000\nNA skill[4,27]          0.103 1.002  1600\nNA skill[5,27]          0.112 1.001  3800\nNA skill[1,28]          0.087 1.003   720\nNA skill[2,28]          0.068 1.004   470\nNA skill[3,28]          0.052 1.006   310\nNA skill[4,28]          0.035 1.009   190\nNA skill[5,28]          0.035 1.012   140\nNA skill[1,29]          0.235 1.058   160\nNA skill[2,29]          0.230 1.061   160\nNA skill[3,29]          0.224 1.068   150\nNA skill[4,29]          0.215 1.076   130\nNA skill[5,29]          0.204 1.085   110\nNA deviance         10752.921 1.060    31\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 69.4 and DIC = 10801.0\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics-2",
    "href": "tutorials/2020-02-15-glmmm-jags/index.html#mcmc-diagnostics-2",
    "title": "Generalised Linear Mixed Models (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\nThe following graph shows the trace plot and distribution of the season_sigma parameter.\n\ndenplot(m3.r2jags, parms = \"season_sigma\", style = \"plain\")\n\n\n\n\n\n\n\ntraplot(m3.r2jags, parms = \"season_sigma\", style = \"plain\")\n\n\n\n\n\n\n\n\nCalculating and comparing the DIC of this model with the former model show no substantial difference.\n\ndic_m2&lt;-m2.r2jags$BUGSoutput$DIC\ndic_m3&lt;-m3.r2jags$BUGSoutput$DIC\ndiff_dic&lt;-dic_m2 - dic_m3\ndiff_dic\n\nNA [1] -29.50679\n\n\nHowever, I believe the assumptions of the current model (m3) are more reasonable so I’ll stick with this model."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html",
    "href": "tutorials/2020-03-21irt-models-stan/index.html",
    "title": "Item Response Theory Models (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#load-the-data",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#load-the-data",
    "title": "Item Response Theory Models (Stan)",
    "section": "Load the data",
    "text": "Load the data\nI read in the data from the file wideformat.csv, which contains (simulated) data from \\(n=1000\\) individuals taking a \\(5\\)-item test. Items are coded \\(1\\) for correct and \\(0\\) for incorrect responses. When we get descriptives of the data, we see that the items differ in terms of the proportion of people who answered correctly, so we expect that we have some differences in item difficulty here.\n\ndata_dicho&lt;-read.csv(\"wideformat.csv\", sep = \",\")\nhead(data_dicho)\n\nNA        ID gender age Item.1 Item.2 Item.3 Item.4 Item.5\nNA 1 person1   Male  40      0      0      0      0      0\nNA 2 person2 Female  27      0      0      0      0      0\nNA 3 person3   Male  13      0      0      0      0      0\nNA 4 person4 Female  17      0      0      0      0      1\nNA 5 person5 Female  30      0      0      0      0      1\nNA 6 person6 Female  46      0      0      0      0      1\n\n#check proportion of correct responses by item\napply(data_dicho[,4:8], 2, sum)/nrow(data_dicho)\n\nNA Item.1 Item.2 Item.3 Item.4 Item.5 \nNA  0.924  0.709  0.553  0.763  0.870\n\n#summarise the data\nlibrary(psych)\ndescribe(data_dicho)\n\nNA         vars    n   mean     sd median trimmed    mad min  max range  skew\nNA ID*        1 1000 500.50 288.82  500.5  500.50 370.65   1 1000   999  0.00\nNA gender*    2 1000   1.50   0.50    2.0    1.51   0.00   1    2     1 -0.02\nNA age        3 1000  25.37  14.43   25.0   25.36  17.79   1   50    49  0.01\nNA Item.1     4 1000   0.92   0.27    1.0    1.00   0.00   0    1     1 -3.20\nNA Item.2     5 1000   0.71   0.45    1.0    0.76   0.00   0    1     1 -0.92\nNA Item.3     6 1000   0.55   0.50    1.0    0.57   0.00   0    1     1 -0.21\nNA Item.4     7 1000   0.76   0.43    1.0    0.83   0.00   0    1     1 -1.24\nNA Item.5     8 1000   0.87   0.34    1.0    0.96   0.00   0    1     1 -2.20\nNA         kurtosis   se\nNA ID*        -1.20 9.13\nNA gender*    -2.00 0.02\nNA age        -1.21 0.46\nNA Item.1      8.22 0.01\nNA Item.2     -1.16 0.01\nNA Item.3     -1.96 0.02\nNA Item.4     -0.48 0.01\nNA Item.5      2.83 0.01"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model",
    "title": "Item Response Theory Models (Stan)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 1PLM to the data. First, I rename and preprocess the data to be passed to Stan.\n\nY&lt;-data_dicho[,4:8]\nn&lt;-nrow(Y)\np&lt;-ncol(Y)\ndata_list&lt;-list(Y=Y,n=n,p=p)\n\nThen I specify the model using the following Stan code.\n\nmodel1&lt;-\"\ndata {\nint&lt;lower=0&gt; n;\nint&lt;lower=0&gt; p;\nint&lt;lower=0,upper=1&gt; Y[n,p];\n}\nparameters {\nvector[n] theta;\nvector[p] delta;\nreal mu_delta;\nreal&lt;lower=0&gt; sigma_delta;\n}\ntransformed parameters{\nvector&lt;lower=0,upper=1&gt;[p] prob[n];\n for(i in 1:n){\n  for (j in 1:p){\n   prob[i,j] = inv_logit(theta[i] - delta[j]);\n  }\n }\n}\nmodel {\ntheta ~ normal(0,1);\ndelta ~ normal(mu_delta,sigma_delta);\nmu_delta ~ normal(0,5);\nsigma_delta ~ cauchy(0,5);\n for(i in 1:n){\n  for (j in 1:p){\n   Y[i,j] ~ bernoulli(prob[i,j]);\n  }\n }\n}\ngenerated quantities {\nvector[p] loglik_y[n];\nvector[p] Y_rep[n];\n for (i in 1: n){\n  for (j in 1: p){\n    loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\n    Y_rep[i,j] = bernoulli_rng(prob[i,j]); \n  }\n }\n}\n\"\n## write the model to a text file\nwriteLines(model1, con = \"model1PLM.stan\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"theta\", \"prob\",\"loglik_y\",\"Y_rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Run the Stan code via the rstan interface and the stan function.\n\nlibrary(rstan)\nset.seed(3456)\nmodel1_stan&lt;- stan(data = data_list, file = \"model1PLM.stan\", \n                   chains = nChains, pars = params, iter = nIter, \n                   warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.000686 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.86 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 8.047 seconds (Warm-up)\nNA Chain 1:                11.298 seconds (Sampling)\nNA Chain 1:                19.345 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.000512 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 5.12 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 7.676 seconds (Warm-up)\nNA Chain 2:                11.291 seconds (Sampling)\nNA Chain 2:                18.967 seconds (Total)\nNA Chain 2:"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\n#extract parameters\nmodel1_stan_par&lt;-extract(model1_stan)\n\n#see average value of item difficulty\ndiff&lt;-model1_stan_par$delta\napply(diff,2,mean)\n\nNA [1] -2.8607139 -1.0645087 -0.2625512 -1.3897885 -2.2186421\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(model1_stan_par$theta, 2, mean)\nprob&lt;-apply(model1_stan_par$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nWe see that item \\(3\\) is the most difficult item (it’s curve is farthest to the right), and item \\(1\\) is the easiest (it’s curve is farthest to the left). The same conclusions can be drawn by checking the difficulty estimates above."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation&lt;-prob*neg_prob\nplot(theta,information[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.3))\nlines(theta,information[,1],col=\"red\")\nlines(theta,information[,2],col=\"blue\")\nlines(theta,information[,3],col=\"orange\")\nlines(theta,information[,4],col=\"green\")\nlines(theta,information[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nSimilar to the ICCs, we see that item \\(3\\) provides the most information about high ability levels (the peak of its IIC is farthest to the right) and item \\(1\\) and \\(5\\) provides the most information about lower ability levels (the peak of its IIC is farthest to the left). We have seen that all ICCs and IICs for the items have the same shape in the 1PL model (i.e. all items are equally good at providing information about the latent trait). In the 2PL and 3PL models, we will see that this does not have to be the case.\nNext, we plot the information curve for the whole test. This is simply the sum of the individual IICs above. Ideally, we want a test which provides fairly good covereage of a wide range of latent ability levels. Otherwise, the test is only good at identifying a limited range of ability levels.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\nlines(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.5527  0.5698  0.7664  0.7753  0.9309  1.0789\n\n\nWe see that this test provides the most information about low ability levels (the peak is around ability level \\(-1.5\\)), and less information about very high ability levels."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit",
    "title": "Item Response Theory Models (Stan)",
    "section": "Assess fit",
    "text": "Assess fit\nWe perform posterior predictive checks to test whether individual items fit the 1PLM by comparing quantities computed from the predictions of the model with those from the observed data. If these match reasonably well, then there is indication that the model has a good fit.\n\nlibrary(bayesplot)\nlibrary(ggplot2)\nY.rep&lt;-model1_stan_par$Y_rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nWe can conclude by summarising and plotting the latent ability scores of the participants\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \nNA -2.0462862 -0.4876188  0.0766153 -0.0001134  0.6947979  0.7444596\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model-1",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model-1",
    "title": "Item Response Theory Models (Stan)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 2PLM to the data using the following Stan code.\n\nmodel2&lt;-\"\ndata {\nint&lt;lower=0&gt; n;\nint&lt;lower=0&gt; p;\nint&lt;lower=0,upper=1&gt; Y[n,p];\n}\nparameters {\nvector[n] theta;\nvector&lt;lower=0&gt; [p] alpha;\nvector[p] delta;\nreal mu_delta;\nreal&lt;lower=0&gt; sigma_alpha;\nreal&lt;lower=0&gt; sigma_delta;\n}\ntransformed parameters{\nvector&lt;lower=0,upper=1&gt;[p] prob[n];\n for(i in 1:n){\n  for (j in 1:p){\n   prob[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));\n  }\n }\n}\nmodel {\ntheta ~ normal(0,1);\ndelta ~ normal(mu_delta,sigma_delta);\nmu_delta ~ normal(0,5);\nsigma_delta ~ cauchy(0,5);\nalpha ~ lognormal(0,sigma_alpha);\nsigma_alpha ~ cauchy(0,5);\n for(i in 1:n){\n  for (j in 1:p){\n   Y[i,j] ~ bernoulli(prob[i,j]);\n  }\n }\n}\ngenerated quantities {\nvector[p] loglik_y[n];\nvector[p] Y_rep[n];\n for (i in 1: n){\n  for (j in 1: p){\n    loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\n    Y_rep[i,j] = bernoulli_rng(prob[i,j]); \n  }\n }\n}\n\"\n## write the model to a text file\nwriteLines(model2, con = \"model2PLM.stan\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"alpha\",\"theta\", \"prob\",\"loglik_y\",\"Y_rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Run the Stan code via the rstan interface and the stan function.\n\nset.seed(3456)\nmodel2_stan&lt;- stan(data = data_list, file = \"model2PLM.stan\", \n                   chains = nChains, pars = params, iter = nIter, \n                   warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.000745 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 7.45 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 17.619 seconds (Warm-up)\nNA Chain 1:                49.041 seconds (Sampling)\nNA Chain 1:                66.66 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.001071 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 10.71 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 16.85 seconds (Warm-up)\nNA Chain 2:                25.604 seconds (Sampling)\nNA Chain 2:                42.454 seconds (Total)\nNA Chain 2:"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves-1",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves-1",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\n#extract parameters\nmodel2_stan_par&lt;-extract(model2_stan)\n\ndiscr&lt;-model2_stan_par$alpha\ndiff&lt;-model2_stan_par$delta\n#see average value of item difficulty\napply(diff,2,mean)\n\nNA [1] -3.1999792 -1.3773558 -0.3093908 -1.8061242 -2.7793706\n\n#see average value of item discriminability\napply(discr,2,mean)\n\nNA [1] 0.9180807 0.7605854 0.8462632 0.7522866 0.7916501\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(model2_stan_par$theta, 2, mean)\nprob&lt;-apply(model2_stan_par$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nUnlike the ICCs for the 1PLM, the ICCs for the 2PLM do not all have the same shape. Item curves which are more “spread out” indicate lower discriminability (i.e. that individuals of a range of ability levels have some probability of getting the item correct). Compare this to an item with high discriminability (steep slope): for this item, we have a better estimate of the individual’s latent ability based on whether they got the question right or wrong. Because of the differing slopes, the rank-order of item difficulty changes across different latent ability levels. We can see that item \\(3\\) is still the most difficult item (i.e. lowest probability of getting correct for most latent trait values, up until about \\(\\theta=0.2\\)). Items \\(1\\) and \\(5\\) are the easiest."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves-1",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves-1",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation&lt;-prob*neg_prob\ninformation2&lt;-information*(apply(discr,2,mean))^2\nplot(theta,information2[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.3))\nlines(theta,information2[,1],col=\"red\")\nlines(theta,information2[,2],col=\"blue\")\nlines(theta,information2[,3],col=\"orange\")\nlines(theta,information2[,4],col=\"green\")\nlines(theta,information2[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nThe item IICs demonstrate that some items provide more information about latent ability for different ability levels. The higher the item discriminability estimate, the more information an item provides about ability levels around the point where there is a \\(50\\%\\) chance of getting the item right (i.e. the steepest point in the ICC slope). For example, item \\(3\\) (orange) clearly provides the most information at high ability levels, around \\(\\theta=-0.5\\), but almost no information about low ability levels (\\(&lt; -1\\)) because the item is already too hard for those participants. In contrast, item \\(1\\) (red), which has low discriminability, doesn’t give very much information overall, but covers a wide range of ability levels.\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information2,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\nlines(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.3521  0.4463  0.5206  0.5280  0.5837  0.9238\n\n\nThe IIC for the whole test shows that the test provides the most information for slightly-lower-than average ability levels (about \\(\\theta=-1\\)), but does not provide much information about extremely high ability levels."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit-1",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit-1",
    "title": "Item Response Theory Models (Stan)",
    "section": "Assess fit",
    "text": "Assess fit\nNext, we check how well the 2PLM fits the data.\n\nY.rep&lt;-model2_stan_par$Y_rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")\n\n\n\n\n\n\n\n\nWe can also compare the fit of the 1PLM and 2PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\nlibrary(loo)\n#extract log-likelihood\nloglik_m1&lt;-model1_stan_par$loglik_y\nloglik_m2&lt;-model2_stan_par$loglik_y\n\n#waic\nwaic_m1&lt;-waic(loglik_m1)\nwaic_m2&lt;-waic(loglik_m2)\n\n#looic\nlooic_m1&lt;-loo(loglik_m1)\nlooic_m2&lt;-loo(loglik_m2)\n\n#compare\ntable_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1])\ntable_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1])\nrownames(table_waic)&lt;-rownames(table_looic)&lt;-c(\"1PLM\",\"2PLM\")\nknitr::kable(cbind(table_waic,table_looic), \"pandoc\", align = \"c\")\n\n\n\n\n\np_waic\nwaic\np_loo\nlooic\n\n\n\n\n1PLM\n1.528685\n6.524715\n1.872122\n7.211591\n\n\n2PLM\n1.425271\n6.459973\n1.707341\n7.024112\n\n\n\n\n\nBoth criteria suggest that the 2PLM has a slightly better fit to the data."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores-1",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores-1",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nPlot the density curve of the estimated ability scores\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \nNA -1.9620636 -0.4347622  0.0659988  0.0006122  0.6347629  0.6738680\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model-2",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#fit-the-model-2",
    "title": "Item Response Theory Models (Stan)",
    "section": "Fit the model",
    "text": "Fit the model\nWe fit the 3PLM to the data using the following Stan code.\n\nmodel3&lt;-\"\ndata {\nint&lt;lower=0&gt; n;\nint&lt;lower=0&gt; p;\nint&lt;lower=0,upper=1&gt; Y[n,p];\n}\nparameters {\nvector[n] theta;\nvector&lt;lower=0&gt; [p] alpha;\nvector[p] delta;\nvector&lt;lower=0,upper=1&gt;[p] eta; //item pseudo-guessing\nreal mu_delta;\nreal&lt;lower=0&gt; sigma_alpha;\nreal&lt;lower=0&gt; sigma_delta;\n}\ntransformed parameters{\nvector&lt;lower=0,upper=1&gt;[p] prob_star[n];\nvector&lt;lower=0,upper=1&gt;[p] prob[n];\n for(i in 1:n){\n  for (j in 1:p){\n   prob_star[i,j] = inv_logit(alpha[j]*(theta[i] - delta[j]));\n   prob[i, j] = eta[j] + (1-eta[j])*prob_star[i,j]; \n  }\n }\n}\nmodel {\ntheta ~ normal(0,1);\ndelta ~ normal(mu_delta,sigma_delta);\nmu_delta ~ normal(0,5);\nsigma_delta ~ cauchy(0,5);\nalpha ~ lognormal(0,sigma_alpha);\nsigma_alpha ~ cauchy(0,5);\neta ~ beta(5,23);\n for(i in 1:n){\n  for (j in 1:p){\n   Y[i,j] ~ bernoulli(prob[i,j]);\n  }\n }\n}\ngenerated quantities {\nvector[p] loglik_y[n];\nvector[p] Y_rep[n];\n for (i in 1: n){\n  for (j in 1: p){\n    loglik_y[i,j] = bernoulli_lpmf(Y[i,j] | prob[i,j]);\n    Y_rep[i,j] = bernoulli_rng(prob[i,j]); \n  }\n }\n}\n\"\n## write the model to a text file\nwriteLines(model3, con = \"model3PLM.stan\")\n\nNext, I define the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"delta\", \"alpha\", \"eta\",\"theta\", \"prob\",\"loglik_y\",\"Y_rep\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2500  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1750\n\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Run the Stan code via the rstan interface and the stan function.\n\nset.seed(3456)\nmodel3_stan&lt;- stan(data = data_list, file = \"model3PLM.stan\", \n                   chains = nChains, pars = params, iter = nIter, \n                   warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 0.000959 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 9.59 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 1: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 1: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 1: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 1: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 1: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 1: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 1: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 1: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 1: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 15.626 seconds (Warm-up)\nNA Chain 1:                29.036 seconds (Sampling)\nNA Chain 1:                44.662 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 0.000674 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 6.74 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1750 [  0%]  (Warmup)\nNA Chain 2: Iteration:  175 / 1750 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  350 / 1750 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1750 [ 28%]  (Sampling)\nNA Chain 2: Iteration:  675 / 1750 [ 38%]  (Sampling)\nNA Chain 2: Iteration:  850 / 1750 [ 48%]  (Sampling)\nNA Chain 2: Iteration: 1025 / 1750 [ 58%]  (Sampling)\nNA Chain 2: Iteration: 1200 / 1750 [ 68%]  (Sampling)\nNA Chain 2: Iteration: 1375 / 1750 [ 78%]  (Sampling)\nNA Chain 2: Iteration: 1550 / 1750 [ 88%]  (Sampling)\nNA Chain 2: Iteration: 1725 / 1750 [ 98%]  (Sampling)\nNA Chain 2: Iteration: 1750 / 1750 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 17.019 seconds (Warm-up)\nNA Chain 2:                29.021 seconds (Sampling)\nNA Chain 2:                46.04 seconds (Total)\nNA Chain 2:"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves-2",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-characteristic-curves-2",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item characteristic curves",
    "text": "Plot the item characteristic curves\nItem characteristic curves (ICC) are the logistic curves which result from the fitted models (e.g. estimated item difficulty, plugged into the item response function). Latent trait/ability is plotted on the \\(x\\)-axis (higher values represent hight ability). Probability of a “correct” answer (\\(Y_{ij}=1\\)) to an item is plotted on the \\(y\\)-axis.\n\n#extract parameters\nmodel3_stan_par&lt;-extract(model3_stan)\n\ndiscr&lt;-model3_stan_par$alpha\ndiff&lt;-model3_stan_par$delta\ngues&lt;-model3_stan_par$eta\n#see average value of item difficulty\napply(diff,2,mean)\n\nNA [1] -2.8000531 -0.7982283  0.1709581 -1.2011374 -2.1901744\n\n#see average value of item discriminability\napply(discr,2,mean)\n\nNA [1] 0.9504686 0.8863304 1.0198040 0.8720698 0.8853051\n\n#see average value of item guessing\napply(gues,2,mean)\n\nNA [1] 0.1894722 0.1841677 0.1635017 0.1926284 0.1920851\n\n#plot icc for each individual with respect to each of the 5 items\ntheta&lt;-apply(model3_stan_par$theta, 2, mean)\nprob&lt;-apply(model3_stan_par$prob,c(2,3),mean)\nplot(theta,prob[,1], type = \"n\", ylab = \"probability of correct response\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1))\nlines(theta,prob[,1],col=\"red\")\nlines(theta,prob[,2],col=\"blue\")\nlines(theta,prob[,3],col=\"orange\")\nlines(theta,prob[,4],col=\"green\")\nlines(theta,prob[,5],col=\"black\")\nlegend(\"bottomright\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nThe slopes of the ICCs look very similar to those of the 2PLM. We can see that all items have \\(y\\)-intercepts greater than zero, so that even at very low ability levels, there is some chance of getting these items correct (via guessing)."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves-2",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-the-item-information-curves-2",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot the item information curves",
    "text": "Plot the item information curves\nItem information curves (IIC) show how much “information” about the latent trait ability an item gives. Mathematically, these are the \\(1\\)st derivatives of the ICCs or, equivalently, to the product of the probability of correct and incorrect response. Item information curves peak at the difficulty value (point where the item has the highest discrimination), with less information at ability levels farther from the difficulty estimate. Practially speaking, we can see how a very difficult item will provide very little information about persons with low ability (because the item is already too hard), and very easy items will provide little information about persons with high ability levels.\nHere I plot the IICs using points, rather than lines, to better display the patterns of the individuals, which vary substantially according to whether the item was correctly chosen due to chance or not.\n\n#plot iic for each individual with respect to each of the 5 items\nneg_prob&lt;-1-prob\ninformation.p1&lt;-neg_prob/prob\ninformation.p2&lt;-(prob-apply(gues,2,mean))^2/(1-apply(gues,2,mean))^2\ninformation3&lt;-(apply(discr,2,mean))^2*(information.p2)*(information.p1)\nplot(theta,information3[,1], type = \"n\", ylab = \"information\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,0.7))\npoints(theta,information3[,1],col=\"red\")\npoints(theta,information3[,2],col=\"blue\")\npoints(theta,information3[,3],col=\"orange\")\npoints(theta,information3[,4],col=\"green\")\npoints(theta,information3[,5],col=\"black\")\nlegend(\"bottomleft\",legend = c(\"1\",\"2\",\"3\",\"4\",\"5\"), lty = c(1), col=c(\"red\",\"blue\",\"orange\",\"green\",\"black\"), bty = \"n\", cex = 0.5)\n\n\n\n\n\n\n\n\nNext, we plot the item information curve for the whole test. This is the sum of all the item IICs above.\n\n#plot iic for each individual with respect to whole test\ninformation_test&lt;-apply(information3,1,sum)\nplot(theta,information_test, type = \"n\", ylab = \"information (test)\", xlab=\"ability\",\n     xlim = c(-2.5,1), ylim = c(0,1.5))\npoints(theta,information_test,col=\"black\",lty=2)\n\n\n\n\n\n\n\nsummary(information_test)\n\nNA    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \nNA  0.4050  0.4876  0.5259  0.5430  0.5794  0.7800"
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit-2",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#assess-fit-2",
    "title": "Item Response Theory Models (Stan)",
    "section": "Assess fit",
    "text": "Assess fit\nNext, we check how well the 3PLM fits the data.\n\nY.rep&lt;-model3_stan_par$Y_rep\n\n#Bar plot of y with yrep medians and uncertainty intervals superimposed on the bars\nppc_bars(Y[,1],Y.rep[1:8,,1]) + ggtitle(\"Item 1\")\n\n\n\n\n\n\n\nppc_bars(Y[,2],Y.rep[1:8,,2]) + ggtitle(\"Item 2\")\n\n\n\n\n\n\n\nppc_bars(Y[,3],Y.rep[1:8,,3]) + ggtitle(\"Item 3\")\n\n\n\n\n\n\n\nppc_bars(Y[,4],Y.rep[1:8,,4]) + ggtitle(\"Item 4\")\n\n\n\n\n\n\n\nppc_bars(Y[,5],Y.rep[1:8,,5]) + ggtitle(\"Item 5\")\n\n\n\n\n\n\n\n\nWe can also compare the fit of the 1PLM, 2PLM and 3PLM using relative measures of fit or information criteria. These are computed based on the deviance and a penalty for model complexity called the effective number of parameters \\(p\\). Here we consider two Bayesian measures known as the Widely Applicable (WAIC) and Leave One Out (LOOIC) Information Criterion, which can be easily obtained through the functions waic and loo in the package loo.\n\n#extract log-likelihood\nloglik_m3&lt;-model3_stan_par$loglik_y\n\n#waic\nwaic_m3&lt;-waic(loglik_m3)\n\n#looic\nlooic_m3&lt;-loo(loglik_m3)\n\n#compare\ntable_waic&lt;-rbind(waic_m1$estimates[2:3,1],waic_m2$estimates[2:3,1],waic_m3$estimates[2:3,1])\ntable_looic&lt;-rbind(looic_m1$estimates[2:3,1],looic_m2$estimates[2:3,1],looic_m3$estimates[2:3,1])\nrownames(table_waic)&lt;-rownames(table_looic)&lt;-c(\"1PLM\",\"2PLM\",\"3PLM\")\nknitr::kable(cbind(table_waic,table_looic), \"pandoc\", align = \"c\")\n\n\n\n\n\np_waic\nwaic\np_loo\nlooic\n\n\n\n\n1PLM\n1.528685\n6.524715\n1.872122\n7.211591\n\n\n2PLM\n1.425271\n6.459973\n1.707341\n7.024112\n\n\n3PLM\n1.404317\n6.431826\n1.696826\n7.016845\n\n\n\n\n\nBoth criteria suggest that both 1PLM and 2PLM have a better fit to the data than 3PLM."
  },
  {
    "objectID": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores-2",
    "href": "tutorials/2020-03-21irt-models-stan/index.html#plot-ability-scores-2",
    "title": "Item Response Theory Models (Stan)",
    "section": "Plot ability scores",
    "text": "Plot ability scores\nPlot the density curve of the estimated ability scores\n\n#summary stats for theta across both iterations and individuals\nsummary(theta)\n\nNA       Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \nNA -1.8453432 -0.4371787  0.0632952  0.0001144  0.6473378  0.7082924\n\n#histogram and kernel density plot of theta averaged across iterations\ndens.theta&lt;-density(theta, bw=0.3)\nhist(theta, breaks = 5, prob = T)\nlines(dens.theta, lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nWe see that the mean of ability scores is around \\(0\\), and the standard deviation about \\(1\\) (these are estimated ability scores are standardised)."
  }
]