[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Randomised Complete Block Anova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAncova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAncova (Stan)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThis tutorial will focus on the use of Bayesian estimation to fit simple linear regression models …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRandomised Complete Block Anova (JAGS)\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSuper basic introduction to JAGS\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R …\n\n\n\nAndrea Gabrio\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSuper basic introduction to Stan\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nStatistics\n\n\n\nThe focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using Stan via R …\n\n\n\nAndrea Gabrio\n\n\nJul 1, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html",
    "href": "tutorials/2020-02-05-ancova-jags/index.html",
    "title": "Ancova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#introduction",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#introduction",
    "title": "Ancova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations)."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#null-hypothesis",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#null-hypothesis",
    "title": "Ancova (JAGS)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nFactor A: the main treatment effect\n\n\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\n\nThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\n\\(H_0(B):\\beta_1(pooled)=0\\)\n\nThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#linear-models",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#linear-models",
    "title": "Ancova (JAGS)",
    "section": "Linear models",
    "text": "Linear models\nOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\nSingle categorical and single covariate\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\nSingle categorical and two covariates\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\nFactorial designs\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nNested designs\n\nLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nPartly nested designs\n\nLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#analysis-of-variance",
    "title": "Ancova (JAGS)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\nancova_table\n\nNA           df       MS       F-ratio (A&B fixed) F-ratio (B fixed) \nNA Factor A  \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"   \"(MS A)/(MS res)\" \nNA Factor B  \"1\"      \"MS B\"   \"(MS B)/(MS res)\"   \"(MS B)/(MS res)\" \nNA Factor AB \"a-1\"    \"MS AB\"  \"(MS AB)/(MS res)\"  \"(MS AB)/(MS res)\"\nNA Residual  \"(n-2)a\" \"MS res\" \"\"                  \"\"\n\n\nThe corresponding R syntax is given below.\n\nanova(lm(DV ~ B * A, dataset))\n# OR\nanova(aov(DV ~ B * A, dataset))\n# OR (make sure not using treatment contrasts)\nAnova(lm(DV ~ B * A, dataset), type = \"III\")"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#assumptions",
    "title": "Ancova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\nThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\nThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nThe appropriate residuals are independent of one another.\nThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\nFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\nFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\nHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\n\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\nAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\nUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\nSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis",
    "title": "Ancova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nlibrary(car)\nscatterplot(Y ~ B | A, data = data)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data)\n\n# OR via ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\nanova(lm(Y ~ B * A, data = data))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nNA B          1  989.99  989.99  92.6782 1.027e-09 ***\nNA A          2 2320.05 1160.02 108.5956 9.423e-13 ***\nNA B:A        2   51.36   25.68   2.4041    0.1118    \nNA Residuals 24  256.37   10.68                       \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#exploratory-data-analysis-1",
    "title": "Ancova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nscatterplot(Y ~ B | A, data = data1)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data1)\n\n\n\n\n\n\n\n# OR via ggplot\nggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\nanova(lm(Y ~ B * A, data = data1))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nNA B          1  442.02  442.02  41.380 1.187e-06 ***\nNA A          2 2760.60 1380.30 129.217 1.418e-13 ***\nNA B:A        2  285.75  142.87  13.375 0.0001251 ***\nNA Residuals 24  256.37   10.68                      \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is strong evidence to suggest that the assumption of equal slopes is violated."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#fitting-the-model",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#fitting-the-model",
    "title": "Ancova (JAGS)",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nmodelString2 = \"\n  model {\n  #Likelihood\n  for (i in 1:n) {\n  y[i]~dnorm(mean[i],tau)\n  mean[i] &lt;- inprod(beta[],X[i,])\n  }\n  #Priors\n  for (i in 1:ngroups) {\n  beta[i] ~ dnorm(0, 1.0E-6) \n  }\n  sigma ~ dunif(0, 100)\n  tau &lt;- 1 / (sigma * sigma)\n  }\n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"ancovaModel2.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nX &lt;- model.matrix(~A * B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = X, n = nrow(data1), ngroups = ncol(X)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\")\nnChains = 2\nburnInSteps = 3000\nthinSteps = 1\nnumSavedSteps = 15000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 10500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\ndata1.r2jags &lt;- jags(data = data1.list, inits = NULL, parameters.to.save = params,\n    model.file = \"ancovaModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 30\nNA    Unobserved stochastic nodes: 7\nNA    Total graph size: 286\nNA \nNA Initializing model\n\nprint(data1.r2jags)\n\nNA Inference for Bugs model at \"ancovaModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000\nNA beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100\nNA beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\nNA beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000\nNA beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000\nNA beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000\nNA sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800\nNA deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 9.8 and DIC = 167.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#mcmc-diagnostics-1",
    "title": "Ancova (JAGS)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\ndenplot(data1.r2jags, parms = c(\"beta\"))\n\n\n\n\n\n\n\ntraplot(data1.r2jags, parms = c(\"beta\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\ndata1.mcmc = as.mcmc(data1.r2jags)\n#Raftery diagnostic\nraftery.diag(data1.mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  2        3853  3746         1.030     \nNA  beta[2]  2        3689  3746         0.985     \nNA  beta[3]  2        3895  3746         1.040     \nNA  beta[4]  2        3649  3746         0.974     \nNA  beta[5]  2        3918  3746         1.050     \nNA  beta[6]  2        3770  3746         1.010     \nNA  deviance 2        3938  3746         1.050     \nNA  sigma    4        5018  3746         1.340     \nNA \nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA                                                 \nNA           Burn-in  Total Lower bound  Dependence\nNA           (M)      (N)   (Nmin)       factor (I)\nNA  beta[1]  2        3853  3746         1.030     \nNA  beta[2]  2        3570  3746         0.953     \nNA  beta[3]  2        3811  3746         1.020     \nNA  beta[4]  2        3770  3746         1.010     \nNA  beta[5]  2        3770  3746         1.010     \nNA  beta[6]  2        3895  3746         1.040     \nNA  deviance 2        3981  3746         1.060     \nNA  sigma    4        5131  3746         1.370\n\n\nThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\n#Autocorrelation diagnostic\nautocorr.diag(data1.mcmc)\n\nNA             beta[1]      beta[2]     beta[3]      beta[4]       beta[5]\nNA Lag 0   1.000000000  1.000000000 1.000000000  1.000000000  1.0000000000\nNA Lag 1  -0.002520665 -0.007698073 0.001992162  0.000509790 -0.0005326877\nNA Lag 5   0.001007950  0.009095032 0.001511518 -0.006890623  0.0025773251\nNA Lag 10 -0.011280919  0.007907450 0.005969613 -0.006999313  0.0040454668\nNA Lag 50 -0.012861369 -0.019813696 0.002604518 -0.008791380 -0.0136623372\nNA             beta[6]     deviance        sigma\nNA Lag 0   1.000000000  1.000000000 1.0000000000\nNA Lag 1   0.004381248  0.332075434 0.4518687724\nNA Lag 5  -0.001182603  0.032092130 0.0351574955\nNA Lag 10 -0.004191097  0.003338842 0.0005457235\nNA Lag 50  0.002636154 -0.005426687 0.0039447210"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#model-validation-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#model-validation-1",
    "title": "Ancova (JAGS)",
    "section": "Model validation",
    "text": "Model validation\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nResiduals against predictors\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = newdata1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nnewdata1 = newdata1 %&gt;% cbind(fit, resid)\nggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\n\n\n\n\n\n\n\n\nAnd now for studentised residuals\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nsresid = resid/sd(resid)\nggplot() + geom_point(data1 = NULL, aes(y = sresid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix %&gt;% as.data.frame %&gt;%\n    dplyr:::select(contains(\"beta\"), sigma) %&gt;% as.matrix\n# generate a model matrix\nXmat = model.matrix(~A * B, data1)\n## get median parameter estimates\ncoefs = mcmc[, 1:6]\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\n    ], mcmc[i, \"sigma\"]))\nnewdata1 = data.frame(A = data1$A, B = data1$B, yRep) %&gt;% gather(key = Sample,\n    value = Value, -A, -B)\nggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \"Model\"),\n    alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\n    fill = \"Obs\"), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\n    x = A), position = position_jitter(width = 0.1, height = 0),\n    color = \"black\") + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \"Model\",\n    group = B, color = A), alpha = 0.5) + geom_point(data = data1,\n    aes(y = Y, x = B, group = B, color = A)) + theme_classic()\n\n\n\n\n\n\n\n\nThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\nmcmc_intervals(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \"beta|sigma\")\n\n\n\n\n\n\n\nmcmc_areas(data1.r2jags$BUGSoutput$sims.matrix, regex_pars = \"beta|sigma\")"
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#parameter-estimates-1",
    "title": "Ancova (JAGS)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the additive model.\n\nprint(data1.r2jags)\n\nNA Inference for Bugs model at \"ancovaModel2.txt\", fit using jags,\nNA  2 chains, each with 10500 iterations (first 3000 discarded)\nNA  n.sims = 15000 iterations saved\nNA          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]   48.194   2.035  44.200  46.864  48.200  49.531  52.217 1.001 15000\nNA beta[2]  -10.562   2.884 -16.240 -12.453 -10.586  -8.688  -4.814 1.001  8100\nNA beta[3]  -26.538   2.568 -31.636 -28.207 -26.525 -24.858 -21.431 1.001 15000\nNA beta[4]   -0.351   0.082  -0.512  -0.404  -0.351  -0.297  -0.188 1.001 15000\nNA beta[5]   -0.271   0.110  -0.491  -0.344  -0.270  -0.198  -0.055 1.001 15000\nNA beta[6]    0.270   0.117   0.039   0.194   0.270   0.346   0.500 1.001 15000\nNA sigma      3.454   0.535   2.601   3.074   3.396   3.757   4.689 1.002  1800\nNA deviance 157.761   4.417 151.465 154.544 156.990 160.166 168.119 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 9.8 and DIC = 167.5\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n# OR\ntidyMCMC(as.mcmc(data1.r2jags), conf.int = TRUE, conf.method = \"HPDinterval\")\n\nNA # A tibble: 7 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   48.2      2.03    44.2      52.2   \nNA 2 beta[2]  -10.6      2.88   -16.3      -4.94  \nNA 3 beta[3]  -26.5      2.57   -31.6     -21.4   \nNA 4 beta[4]   -0.351    0.0816  -0.510    -0.187 \nNA 5 beta[5]   -0.271    0.110   -0.491    -0.0541\nNA 6 beta[6]    0.270    0.117    0.0436    0.503 \nNA 7 sigma      3.45     0.535    2.51      4.50\n\n\nConclusions\n\nThe intercept of the first group (Group A) is \\(48.2\\).\nThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\nThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\nA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\ndifference in slope between Group B and Group A \\(-0.270\\).\ndifference in slope between Group C and Group A \\(0.270\\).\n\nThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\n## since values are less than zero\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[2]\"])  # effect of (B-A = 0)\n\nNA [1] 0.0009333333\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[3]\"])  # effect of (C-A = 0)\n\nNA [1] 0\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[4]\"])  # effect of (slope = 0)\n\nNA [1] 0.0003333333\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[5]\"])  # effect of (slopeB - slopeA = 0)\n\nNA [1] 0.0152\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, \"beta[6]\"])  # effect of (slopeC - slopeA = 0)\n\nNA [1] 0.0232\n\nmcmcpvalue(data1.r2jags$BUGSoutput$sims.matrix[, 2:6])  # effect of (model)\n\nNA [1] 0\n\n\nThere is evidence that the reponse differs between the groups."
  },
  {
    "objectID": "tutorials/2020-02-05-ancova-jags/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-05-ancova-jags/index.html#graphical-summaries-1",
    "title": "Ancova (JAGS)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = data1.r2jags$BUGSoutput$sims.matrix\n## Calculate the fitted values\nnewdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\n    len = 100))\nXmat = model.matrix(~A * B, newdata1)\ncoefs = mcmc[, c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\")]\nfit = coefs %*% t(Xmat)\nnewdata1 = newdata1 %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()\n\n\n\n\n\n\n\n\nAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\n## Calculate partial residuals fitted values\nfdata1 = rdata1 = data1\nfMat = rMat = model.matrix(~A * B, fdata1)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\nrdata1 = rdata1 %&gt;% mutate(partial.resid = resid + fit)\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\n    aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html",
    "href": "tutorials/2020-02-01-ancova-stan/index.html",
    "title": "Ancova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages dedicated to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#introduction",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#introduction",
    "title": "Ancova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nPrevious tutorials have concentrated on designs for either continuous (Regression) or categorical (ANOVA) predictor variables. Analysis of covariance (ANCOVA) models are essentially ANOVA models that incorporate one or more continuous and categorical variables (covariates). Although the relationship between a response variable and a covariate may itself be of substantial clinical interest, typically covariate(s) are incorporated to reduce the amount of unexplained variability in the model and thereby increase the power of any treatment effects.\nIn ANCOVA, a reduction in unexplained variability is achieved by adjusting the response (to each treatment) according to slight differences in the covariate means as well as accounting for any underlying trends between the response and covariate(s). To do so, the extent to which the within treatment group small differences in covariate means between groups and treatment groups are essentially compared via differences in their \\(y\\)-intercepts. The total variation is thereafter partitioned into explained (using the deviations between the overall trend and trends approximated for each of the treatment groups) and unexplained components (using the deviations between the observations and the approximated within group trends). In this way, ANCOVA can be visualized as a regular ANOVA in which the group and overall means are replaced by group and overall trendlines. Importantly, it should be apparent that ANCOVA is only appropriate when each of the within group trends have the same slope and are thus parallel to one another and the overall trend. Furthermore, ANCOVA is not appropriate when the resulting adjustments must be extrapolated from a linear relationship outside the measured range of the covariate.\nAs an example, an experiment might be set up to investigate the energetic impacts of sexual vs parthenogenetic (egg development without fertilization) reproduction on leaf insect food consumption. To do so, researchers could measure the daily food intake of individual adult female leaf insects from female only (parthenogenetic) and mixed (sexual) populations. Unfortunately, the available individual leaf insects varied substantially in body size which was expected to increase the variability of daily food intake of treatment groups. Consequently, the researchers also measured the body mass of the individuals as a covariate, thereby providing a means by which daily food consumption could be standardized for body mass. ANCOVA attempts to reduce unexplained variability by standardising the response to the treatment by the effects of the specific covariate condition. Thus ANCOVA provides a means of exercising some statistical control over the variability when it is either not possible or not desirable to exercise experimental control (such as blocking or using otherwise homogeneous observations)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#null-hypothesis",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#null-hypothesis",
    "title": "Ancova (Stan)",
    "section": "Null hypothesis",
    "text": "Null hypothesis\nFactor A: the main treatment effect\n\n\\(H_0(A):\\mu_1(adj)=\\mu_2(adj)=\\ldots=\\mu_i(adj)=\\mu(adj)\\)\n\nThe adjusted population group means are all equal. The mean of population \\(1\\) adjusted for the covariate is equal to that of population \\(2\\) adjusted for the covariate and so on, and thus all population means adjusted for the covariate are equal to an overall adjusted mean. If the effect of the \\(i\\)-th group is the difference between the \\(i\\)-th group adjusted mean and the overall adjusted mean (\\(\\alpha_i(adj)=\\mu_i(adj)−\\mu(adj)\\)) then the \\(H_0\\) can alternatively be written as:\n\n\\(H_0(A):\\alpha_1(adj)=\\alpha_2(adj)=\\ldots=\\alpha_i(adj)=0\\)\n\nThe effect of each group equals zero. If one or more of the \\(\\alpha_i(adj)\\) are different from zero (the response mean for this treatment differs from the overall response mean), the null hypothesis is not true, indicating that the treatment does affect the response variable.\nFactor B: the covariate effect\n\n\\(H_0(B):\\beta_1(pooled)=0\\)\n\nThe pooled population slope equals zero. Note, that this null hypothesis is rarely of much interest. It is precisely because of this nuisance relationship that ANCOVA designs are applied."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#linear-models",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#linear-models",
    "title": "Ancova (Stan)",
    "section": "Linear models",
    "text": "Linear models\nOne or more covariates can be incorporated into single factor, nested, factorial and partly nested designs in order to reduce the unexplained variation. Fundamentally, the covariate(s) are purely used to adjust the response values prior to the regular analysis. The difficulty is in determining the appropriate adjustments. Following is a list of the appropriate linear models and adjusted response calculations for a range of ANCOVA designs. Note that these linear models do not include interactions involving the covariates as these are assumed to be zero. The inclusion of these interaction terms is a useful means of testing the homogeneity of slopes assumption.\n\nSingle categorical and single covariate\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta(x_{ij}-\\bar{x}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b(x_{ij} - \\bar{x})\\)\n\nSingle categorical and two covariates\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\beta_{YX}(x_{ij}-\\bar{x}) + \\beta_{YZ}(z_{ij}-\\bar{z}) + \\epsilon_{ij}\\)\nAdjustments: \\(y_{ij(adj)}=y_{ij} - b_{YX}(x_{ij} - \\bar{x}) - b_{YZ}(z_{ij} - \\bar{z})\\)\n\nFactorial designs\n\nLinear model: \\(y_{ij}=\\mu + \\alpha_i + \\gamma_j + (\\alpha\\gamma)_{ij}+ \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nNested designs\n\nLinear model: \\(y_{ijk}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijk}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijk} - b(x_{ijk} - \\bar{x})\\)\n\nPartly nested designs\n\nLinear model: \\(y_{ijkl}=\\mu + \\alpha_i + \\gamma_{j(i)} + \\delta_k + (\\alpha\\delta)_{ik} + (\\gamma\\delta)_{j(i)k} + \\beta(x_{ijk}-\\bar{x}) + \\epsilon_{ijkl}\\)\nAdjustments: \\(y_{ijk(adj)}=y_{ijkl} - b_{between}(x_{i} - \\bar{x}) - b_{within}(x_{ijk} - \\bar{x}_i)\\)"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#analysis-of-variance",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#analysis-of-variance",
    "title": "Ancova (Stan)",
    "section": "Analysis of variance",
    "text": "Analysis of variance\nIn ANCOVA, the total variability of the response variable is sequentially partitioned into components explained by each of the model terms, starting with the covariate and is therefore equivalent to performing a regular analysis of variance on the response variables that have been adjusted for the covariate. The appropriate unexplained residuals and therefore the appropriate F-ratios for each factor differ according to the different null hypotheses associated with different linear models as well as combinations of fixed and random factors in the model (see the following tables). Note that since the covariate levels measured are typically different for each group, ANCOVA designs are inherently non-orthogonal (unbalanced). Consequently, sequential (Type I sums of squares) should not be used. For very simple Ancova designs that incorporate a single categorical and single covariate, Type I sums of squares can be used provided the covariate appears in the linear model first (and thus is partitioned out last) as we are typically not interested in estimating this effect.\n\nancova_table\n\nNA           df       MS       F-ratio (A&B fixed) F-ratio (B fixed) \nNA Factor A  \"a-1\"    \"MS A\"   \"(MS A)/(MS res)\"   \"(MS A)/(MS res)\" \nNA Factor B  \"1\"      \"MS B\"   \"(MS B)/(MS res)\"   \"(MS B)/(MS res)\" \nNA Factor AB \"a-1\"    \"MS AB\"  \"(MS AB)/(MS res)\"  \"(MS AB)/(MS res)\"\nNA Residual  \"(n-2)a\" \"MS res\" \"\"                  \"\"\n\n\nThe corresponding R syntax is given below.\n\nanova(lm(DV ~ B * A, dataset))\n# OR\nanova(aov(DV ~ B * A, dataset))\n# OR (make sure not using treatment contrasts)\nAnova(lm(DV ~ B * A, dataset), type = \"III\")"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#assumptions",
    "title": "Ancova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAs ANCOVA designs are essentially regular ANOVA designs that are first adjusted (centered) for the covariate(s), ANCOVA designs inherit all of the underlying assumptions of the appropriate ANOVA design. Specifically, hypothesis tests assume that:\n\nThe appropriate residuals are normally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator, see the above tables) should be used to explore normality. Scale transformations are often useful.\nThe appropriate residuals are equally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nThe appropriate residuals are independent of one another.\nThe relationship between the response variable and the covariate should be linear. Linearity can be explored using scatterplots and residual plots should reveal no patterns.\nFor repeated measures and other designs in which treatment levels within blocks can not be be randomly ordered, the variance/covariance matrix is assumed to display sphericity.\nFor designs that utilise blocking, it is assumed that there are no block by within block interactions.\n\nHomogeneity of Slopes\nIn addition to the above assumptions, ANCOVA designs also assume that slopes of relationships between the response variable and the covariate(s) are the same for each treatment level (group). That is, all the trends are parallel. If the individual slopes deviate substantially from each other (and thus the overall slope), then adjustments made to each of the observations are nonsensical. This situation is analogous to an interaction between two or more factors. In ANCOVA, interactions involving the covariate suggest that the nature of the relationship between the response and the covariate differs between the levels of the categorical treatment. More importantly, they also indicate that whether or not there is an effect of the treatment depends on what range of the covariate you are focussed on. Clearly then, it is not possible to make conclusions about the main effects of treatments in the presence of such interactions. The assumption of homogeneity of slopes can be examined via interaction plots or more formally, by testing hypotheses about the interactions between categorical variables and the covariate(s). There are three broad approaches for dealing with ANCOVA designs with heterogeneous slopes and selection depends on the primary focus of the study.\n\nWhen the primary objective of the analysis is to investigate the effects of categorical treatments, it is possible to adopt an approach similar to that taken when exploring interactions in multiple regression. The effect of treatments can be examined at specific values of the covariate (such as the mean and \\(\\pm\\) one standard deviation). This approach is really only useful at revealing broad shifts in patterns over the range of the covariate and if the selected values of the covariate do not have some inherent clinical meaning (selected arbitrarily), then the outcomes can be of only limited clinical interest.\nAlternatively, the Johnson-Neyman technique (or Wilxon modification thereof) procedure indicates the ranges of the covariate over which the individual regression lines of pairs of treatment groups overlap or cross. Although less powerful than the previous approach, the Wilcox(J-N) procedure has the advantage of revealing the important range (ranges for which the groups are different and not different) of the covariate rather than being constrained by specific levels selected.\nUse contrast treatments to split up the interaction term into its constituent contrasts for each level of the treatment. Essentially this compares each of the treatment level slopes to the slope from the “control” group and is useful if the primary focus is on the relationships between the response and the covariate.\n\nSimilar covariate ranges\nAdjustments made to the response means in an attempt to statistically account for differences in the covariate involve predicting mean response values along displaced linear relationships between the overall response and covariate variables. The degree of trend displacement for any given group is essentially calculated by multiplying the overall regression slope by the degree of difference between the overall covariate mean and the mean of the covariate for that group. However, when the ranges of the covariate within each of the groups differ substantially from one another, these adjustments are effectively extrapolations and therefore of unknown reliability. If a simple ANOVA of the covariate modelled against the categorical factor indicates that the covariate means differ significantly between groups, it may be necessary to either remove extreme observations or reconsider the analysis.\nRobust ANCOVA\nANCOVA based on rank transformed data can be useful for accommodating data with numerous problematic outliers. Nevertheless, problems about the difficulties of detecting interactions from rank transformed data, obviously have implications for inferential tests of homogeneity of slopes. Randomisation tests that maintain response0covariate pairs and repeatedly randomise these observations amongst the levels of the treatments can also be useful, particularly when there is doubt over the independence of observations. Both planned and unplanned comparisons follow those of other ANOVA chapters without any real additional complications. Notably, recent implementations of the Tukey’s test (within R) accommodate unbalanced designs and thus negate the need for some of the more complicated and specialised techniques that have been highlighted in past texts."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis",
    "title": "Ancova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nlibrary(car)\nscatterplot(Y ~ B | A, data = data)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data)\n\n# OR via ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions\nThere is no evidence of obvious non-normality. The assumption of linearity seems reasonable. The variability of the three groups seems approximately equal. The slopes (\\(Y\\) vs B trends) appear broadly similar for each treatment group.\nWe can explore inferential evidence of unequal slopes by examining estimated effects of the interaction between the categorical variable and the covariate. Note, pay no attention to the main effects - only the interaction. Even though I intend to illustrate Bayesian analyses here, for such a simple model, it is considerably simpler to use traditional OLS for testing for the presence of an interaction.\n\nanova(lm(Y ~ B * A, data = data))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nNA B          1  989.99  989.99  92.6782 1.027e-09 ***\nNA A          2 2320.05 1160.02 108.5956 9.423e-13 ***\nNA B:A        2   51.36   25.68   2.4041    0.1118    \nNA Residuals 24  256.37   10.68                       \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is very little evidence to suggest that the assumption of equal slopes will be inappropriate."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#exploratory-data-analysis-1",
    "title": "Ancova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nscatterplot(Y ~ B | A, data = data1)\n\n\n\n\n\n\n\nboxplot(Y ~ A, data1)\n\n\n\n\n\n\n\n# OR via ggplot\nggplot(data1, aes(y = Y, x = B, group = A)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nggplot(data1, aes(y = Y, x = A)) + geom_boxplot()\n\n\n\n\n\n\n\n\nThe slopes (\\(Y\\) vs B trends) do appear to differ between treatment groups - in particular, Group C seems to portray a different trend to Groups A and B.\n\nanova(lm(Y ~ B * A, data = data1))\n\nNA Analysis of Variance Table\nNA \nNA Response: Y\nNA           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nNA B          1  442.02  442.02  41.380 1.187e-06 ***\nNA A          2 2760.60 1380.30 129.217 1.418e-13 ***\nNA B:A        2  285.75  142.87  13.375 0.0001251 ***\nNA Residuals 24  256.37   10.68                      \nNA ---\nNA Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is strong evidence to suggest that the assumption of equal slopes is violated."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#fitting-the-model",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#fitting-the-model",
    "title": "Ancova (Stan)",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nmodelString2 = \"\n  data {\n  int&lt;lower=1&gt; n;\n  int&lt;lower=1&gt; nX;\n  vector [n] y;\n  matrix [n,nX] X;\n  }\n  parameters {\n  vector[nX] beta;\n  real&lt;lower=0&gt; sigma;\n  }\n  transformed parameters {\n  vector[n] mu;\n\n  mu = X*beta;\n  }\n  model {\n  // Likelihood\n  y~normal(mu,sigma);\n  \n  // Priors\n  beta ~ normal(0,100);\n  sigma~cauchy(0,5);\n  }\n  generated quantities {\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n  log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); \n  }\n  }\n  \n  \"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"ancovaModel2.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat &lt;- model.matrix(~A * B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = Xmat, nX = ncol(Xmat), n = nrow(data1)))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\", \"sigma\", \"log_lik\")\nnChains = 2\nburnInSteps = 500\nthinSteps = 1\nnumSavedSteps = 2000  #across all chains\nnIter = ceiling(burnInSteps + (numSavedSteps * thinSteps)/nChains)\nnIter\n\nNA [1] 1500\n\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model).\n\ndata1.rstan &lt;- stan(data = data1.list, file = \"ancovaModel2.stan\", chains = nChains, pars = params, iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 2.9e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.065 seconds (Warm-up)\nNA Chain 1:                0.045 seconds (Sampling)\nNA Chain 1:                0.11 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 4e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.069 seconds (Warm-up)\nNA Chain 2:                0.045 seconds (Sampling)\nNA Chain 2:                0.114 seconds (Total)\nNA Chain 2:\n\nprint(data1.rstan, par = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]  48.04    0.10 1.91  44.19  46.84  48.04  49.29  51.73   375    1\nNA beta[2] -10.41    0.12 2.74 -15.56 -12.18 -10.39  -8.72  -4.83   561    1\nNA beta[3] -26.32    0.12 2.43 -31.11 -27.90 -26.35 -24.79 -21.39   430    1\nNA beta[4]  -0.34    0.00 0.08  -0.50  -0.40  -0.35  -0.29  -0.19   416    1\nNA beta[5]  -0.28    0.00 0.11  -0.49  -0.34  -0.27  -0.21  -0.08   483    1\nNA beta[6]   0.26    0.00 0.11   0.04   0.19   0.26   0.33   0.48   511    1\nNA sigma     3.36    0.02 0.50   2.55   3.01   3.29   3.65   4.50  1090    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 11:50:52 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#mcmc-diagnostics-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#mcmc-diagnostics-1",
    "title": "Ancova (Stan)",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\nmcmc &lt;- As.mcmc.list(data1.rstan)\n\ndenplot(mcmc, parms = c(\"beta\"))\n\n\n\n\n\n\n\ntraplot(mcmc, parms = c(\"beta\"))\n\n\n\n\n\n\n\n\nTrace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space. When there are a lot of parameters, this can result in a very large number of traceplots. To focus on just certain parameters (such as \\(\\beta\\)s).\n\n#Raftery diagnostic\nraftery.diag(mcmc)\n\nNA [[1]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\nNA \nNA [[2]]\nNA \nNA Quantile (q) = 0.025\nNA Accuracy (r) = +/- 0.005\nNA Probability (s) = 0.95 \nNA \nNA You need a sample size of at least 3746 with these values of q, r and s\n\n\nThe Raftery diagnostics for each chain estimate that we would require no more than \\(5000\\) samples to reach the specified level of confidence in convergence. As we have \\(10500\\) samples, we can be confidence that convergence has occurred.\n\n#Autocorrelation diagnostic\nautocorr.diag(mcmc)\n\nNA            beta[1]     beta[2]     beta[3]     beta[4]     beta[5]     beta[6]\nNA Lag 0   1.00000000  1.00000000  1.00000000  1.00000000  1.00000000  1.00000000\nNA Lag 1   0.47965891  0.40490097  0.42921394  0.47213590  0.43786125  0.40348216\nNA Lag 5   0.10276601  0.04520477  0.08439589  0.08552624  0.07188554  0.05055728\nNA Lag 10  0.03369686  0.04235285  0.03837580  0.04276107  0.06964443  0.05297790\nNA Lag 50 -0.05533000 -0.03563501 -0.08740181 -0.04617438 -0.01873684 -0.08254539\nNA               sigma  log_lik[1]   log_lik[2]  log_lik[3]   log_lik[4]\nNA Lag 0   1.000000000  1.00000000  1.000000000  1.00000000  1.000000000\nNA Lag 1   0.244539989  0.30295557  0.115863972  0.31646347  0.029392467\nNA Lag 5   0.034914225  0.03692325  0.025476065  0.04472861  0.009289407\nNA Lag 10  0.027104230  0.04375764  0.001071547  0.03530329  0.036276640\nNA Lag 50 -0.005837235 -0.04398093 -0.022416916 -0.01700789 -0.017665756\nNA         log_lik[5]   log_lik[6]   log_lik[7]   log_lik[8]  log_lik[9]\nNA Lag 0   1.00000000  1.000000000  1.000000000  1.000000000  1.00000000\nNA Lag 1   0.05686476  0.279972053  0.333411513  0.393178740  0.26907784\nNA Lag 5   0.01335058  0.009737116  0.056417267 -0.001796254  0.02998479\nNA Lag 10  0.03736092  0.035609098  0.008349122  0.056906159  0.01515109\nNA Lag 50 -0.01570145 -0.012353687 -0.007107282 -0.056604829 -0.03352728\nNA         log_lik[10]  log_lik[11] log_lik[12]  log_lik[13]  log_lik[14]\nNA Lag 0   1.000000000  1.000000000 1.000000000  1.000000000  1.000000000\nNA Lag 1   0.192883022  0.068556254 0.204989891  0.003861059  0.105690603\nNA Lag 5   0.037648652  0.031608422 0.030527908  0.011161059  0.040450947\nNA Lag 10  0.007251782 -0.006369060 0.001890380  0.036208400  0.008267025\nNA Lag 50 -0.028984961 -0.005015408 0.005481482 -0.018976796 -0.004975828\nNA         log_lik[15]  log_lik[16]   log_lik[17] log_lik[18] log_lik[19]\nNA Lag 0   1.000000000  1.000000000  1.0000000000  1.00000000  1.00000000\nNA Lag 1  -0.021918535  0.028936497  0.1274525386  0.03355231 -0.01865606\nNA Lag 5  -0.006083026  0.025989718  0.0299094329  0.02020100  0.01961257\nNA Lag 10  0.008300113  0.014088547 -0.0117960594  0.01141223  0.02524133\nNA Lag 50  0.004556124 -0.009052317 -0.0004925802  0.02661374 -0.01229104\nNA        log_lik[20]  log_lik[21] log_lik[22]  log_lik[23]  log_lik[24]\nNA Lag 0   1.00000000  1.000000000  1.00000000  1.000000000  1.000000000\nNA Lag 1   0.17819054  0.120052667  0.20632108 -0.024461686 -0.052472858\nNA Lag 5   0.03745210 -0.020154436  0.03360932 -0.031301123 -0.009290672\nNA Lag 10  0.03345325  0.004498098  0.02884422 -0.002109614 -0.014376687\nNA Lag 50 -0.01491661 -0.016212205 -0.02709841 -0.017716486  0.004503930\nNA         log_lik[25]  log_lik[26] log_lik[27]  log_lik[28] log_lik[29]\nNA Lag 0   1.000000000  1.000000000  1.00000000  1.000000000  1.00000000\nNA Lag 1  -0.026932812 -0.010753209 -0.03500057 -0.022972322  0.02432922\nNA Lag 5  -0.007038723 -0.018424596 -0.01393167 -0.006548756 -0.03234786\nNA Lag 10  0.001166187  0.002621322  0.01265460  0.014571694 -0.00114465\nNA Lag 50 -0.007274914 -0.014512013  0.01665142 -0.016130612 -0.01750531\nNA         log_lik[30]          lp__\nNA Lag 0   1.000000000  1.000000e+00\nNA Lag 1  -0.015077656  4.530274e-01\nNA Lag 5   0.006166306 -7.071898e-05\nNA Lag 10  0.028726884  8.583829e-03\nNA Lag 50 -0.014572075 -8.625792e-02\n\n\n\nstan_rhat(data1.rstan)\n\n\n\n\n\n\n\nstan_ess(data1.rstan)\n\n\n\n\n\n\n\n\nRhat and effective sample size. In this instance, most of the parameters have reasonably high effective samples and thus there is likely to be a good range of values from which to estimate paramter properties."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#model-validation-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#model-validation-1",
    "title": "Ancova (Stan)",
    "section": "Model validation",
    "text": "Model validation\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nggplot() + geom_point(data = NULL, aes(y = resid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nResiduals against predictors\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = newdata1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nnewdata1 = newdata1 %&gt;% cbind(fit, resid)\nggplot(newdata1) + geom_point(aes(y = resid, x = A)) + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_point(aes(y = resid, x = B)) + theme_classic()\n\n\n\n\n\n\n\n\nAnd now for studentised residuals\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nnewdata1 = data1\nXmat = model.matrix(~A * B, newdata1)\n## get median parameter estimates\ncoefs = apply(mcmc[, 1:6], 2, median)\nfit = as.vector(coefs %*% t(Xmat))\nresid = data1$Y - fit\nsresid = resid/sd(resid)\nggplot() + geom_point(data = NULL, aes(y = sresid, x = fit)) + theme_classic()\n\n\n\n\n\n\n\n\nFor this simple model, the studentised residuals yield the same pattern as the raw residuals (or the Pearson residuals for that matter). Lets see how well data simulated from the model reflects the raw data.\n\nmcmc = as.data.frame(data1.rstan) %&gt;% dplyr:::select(contains(\"beta\"),\n    sigma) %&gt;% as.matrix\n# generate a model matrix\nXmat = model.matrix(~A * B, data1)\n## get median parameter estimates\ncoefs = mcmc[, 1:6]\nfit = coefs %*% t(Xmat)\n## draw samples from this model\nyRep = sapply(1:nrow(mcmc), function(i) rnorm(nrow(data1), fit[i,\n    ], mcmc[i, \"sigma\"]))\nnewdata1 = data.frame(A = data1$A, B = data1$B, yRep) %&gt;% gather(key = Sample,\n    value = Value, -A, -B)\nggplot(newdata1) + geom_violin(aes(y = Value, x = A, fill = \"Model\"),\n    alpha = 0.5) + geom_violin(data = data1, aes(y = Y, x = A,\n    fill = \"Obs\"), alpha = 0.5) + geom_point(data = data1, aes(y = Y,\n    x = A), position = position_jitter(width = 0.1, height = 0),\n    color = \"black\") + theme_classic()\n\n\n\n\n\n\n\nggplot(newdata1) + geom_violin(aes(y = Value, x = B, fill = \"Model\",\n    group = B, color = A), alpha = 0.5) + geom_point(data = data1,\n    aes(y = Y, x = B, group = B, color = A)) + theme_classic()\n\n\n\n\n\n\n\n\nThe predicted trends do encapsulate the actual data, suggesting that the model is a reasonable representation of the underlying processes. Note, these are prediction intervals rather than confidence intervals as we are seeking intervals within which we can predict individual observations rather than means. We can also explore the posteriors of each parameter.\n\nmcmc_intervals(as.matrix(data1.rstan), regex_pars = \"beta|sigma\")\n\n\n\n\n\n\n\nmcmc_areas(as.matrix(data1.rstan), regex_pars = \"beta|sigma\")"
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#parameter-estimates-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#parameter-estimates-1",
    "title": "Ancova (Stan)",
    "section": "Parameter estimates",
    "text": "Parameter estimates\nFirst, we look at the results from the additive model.\n\nprint(data1.rstan, pars = c(\"beta\", \"sigma\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=1500; warmup=500; thin=1; \nNA post-warmup draws per chain=1000, total post-warmup draws=2000.\nNA \nNA           mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1]  48.04    0.10 1.91  44.19  46.84  48.04  49.29  51.73   375    1\nNA beta[2] -10.41    0.12 2.74 -15.56 -12.18 -10.39  -8.72  -4.83   561    1\nNA beta[3] -26.32    0.12 2.43 -31.11 -27.90 -26.35 -24.79 -21.39   430    1\nNA beta[4]  -0.34    0.00 0.08  -0.50  -0.40  -0.35  -0.29  -0.19   416    1\nNA beta[5]  -0.28    0.00 0.11  -0.49  -0.34  -0.27  -0.21  -0.08   483    1\nNA beta[6]   0.26    0.00 0.11   0.04   0.19   0.26   0.33   0.48   511    1\nNA sigma     3.36    0.02 0.50   2.55   3.01   3.29   3.65   4.50  1090    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 11:50:52 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n# OR\ntidyMCMC(data1.rstan, conf.int = TRUE, conf.method = \"HPDinterval\", pars = c(\"beta\", \"sigma\"))\n\nNA # A tibble: 7 × 5\nNA   term    estimate std.error conf.low conf.high\nNA   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\nNA 1 beta[1]   48.0      1.91    44.2      51.8   \nNA 2 beta[2]  -10.4      2.74   -16.0      -5.34  \nNA 3 beta[3]  -26.3      2.43   -30.9     -21.3   \nNA 4 beta[4]   -0.345    0.0767  -0.492    -0.193 \nNA 5 beta[5]   -0.276    0.105   -0.487    -0.0735\nNA 6 beta[6]    0.262    0.110    0.0393    0.479 \nNA 7 sigma      3.36     0.501    2.52      4.46\n\n\nConclusions\n\nThe intercept of the first group (Group A) is \\(48.2\\).\nThe mean of the second group (Group B) is \\(-10.6\\) units greater than (A).\nThe mean of the third group (Group C) is \\(-26.5\\) units greater than (A).\nA one unit increase in B in Group A is associated with a \\(-0.351\\) units increase in \\(Y\\).\ndifference in slope between Group B and Group A \\(-0.270\\).\ndifference in slope between Group C and Group A \\(0.270\\).\n\nThe \\(95\\)% confidence interval for the effects of Group B, Group C and the partial slope associated with B do not overlapp with \\(0\\) implying a significant difference between group A and groups B, C (at the mean level of predictor B) and a significant negative relationship with B (for Group A). The slope associated with Group B was not found to be significantly different from that associated with Group A, however, the slope associated with Group C was found to be significantly less negative than the slope associated with Group A. While workers attempt to become comfortable with a new statistical framework, it is only natural that they like to evaluate and comprehend new structures and output alongside more familiar concepts. One way to facilitate this is via Bayesian p-values that are somewhat analogous to the frequentist p-values for investigating the hypothesis that a parameter is equal to zero.\n\n## since values are less than zero\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[2]\"])  # effect of (B-A = 0)\n\nNA [1] 0.0015\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[3]\"])  # effect of (C-A = 0)\n\nNA [1] 0\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[4]\"])  # effect of (slope = 0)\n\nNA [1] 0\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[5]\"])  # effect of (slopeB - slopeA = 0)\n\nNA [1] 0.0095\n\nmcmcpvalue(as.matrix(data1.rstan)[, \"beta[6]\"])  # effect of (slopeC - slopeA = 0)\n\nNA [1] 0.028\n\nmcmcpvalue(as.matrix(data1.rstan)[, 2:6])  # effect of (model)\n\nNA [1] 0\n\n\nThere is evidence that the response differs between the groups.\n\n(full = loo(extract_log_lik(data1.rstan)))\n\nNA \nNA Computed from 2000 by 30 log-likelihood matrix.\nNA \nNA          Estimate  SE\nNA elpd_loo    -83.0 4.8\nNA p_loo         6.9 2.0\nNA looic       166.0 9.5\nNA ------\nNA MCSE of elpd_loo is NA.\nNA MCSE and ESS estimates assume independent draws (r_eff=1).\nNA \nNA Pareto k diagnostic values:\nNA                          Count Pct.    Min. ESS\nNA (-Inf, 0.7]   (good)     29    96.7%   174     \nNA    (0.7, 1]   (bad)       1     3.3%   &lt;NA&gt;    \nNA    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nNA See help('pareto-k-diagnostic') for details.\n\n# now fit a model without main factor\nmodelString3 = \"\n  data {\n  int&lt;lower=1&gt; n;\n  int&lt;lower=1&gt; nX;\n  vector [n] y;\n  matrix [n,nX] X;\n  }\n  parameters {\n  vector[nX] beta;\n  real&lt;lower=0&gt; sigma;\n  }\n  transformed parameters {\n  vector[n] mu;\n\n  mu = X*beta;\n  }\n  model {\n  // Likelihood\n  y~normal(mu,sigma);\n  \n  // Priors\n  beta ~ normal(0,1000);\n  sigma~cauchy(0,5);\n  }\n  generated quantities {\n  vector[n] log_lik;\n  \n  for (i in 1:n) {\n  log_lik[i] = normal_lpdf(y[i] | mu[i], sigma); \n  }\n  }\n  \n  \"\n\n## write the model to a stan file \nwriteLines(modelString3, con = \"ancovaModel3.stan\")\n\nXmat &lt;- model.matrix(~A + B, data1)\ndata1.list &lt;- with(data1, list(y = Y, X = Xmat, n = nrow(data1), nX = ncol(Xmat)))\ndata1.rstan.red &lt;- stan(data = data1.list, file = \"ancovaModel3.stan\", chains = nChains,\n    iter = nIter, warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 1.8e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.052 seconds (Warm-up)\nNA Chain 1:                0.035 seconds (Sampling)\nNA Chain 1:                0.087 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 5e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nNA Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nNA Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nNA Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nNA Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nNA Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nNA Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nNA Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nNA Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nNA Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.043 seconds (Warm-up)\nNA Chain 2:                0.031 seconds (Sampling)\nNA Chain 2:                0.074 seconds (Total)\nNA Chain 2:\n\n(reduced = loo(extract_log_lik(data1.rstan.red)))\n\nNA \nNA Computed from 2000 by 30 log-likelihood matrix.\nNA \nNA          Estimate  SE\nNA elpd_loo    -92.3 4.8\nNA p_loo         5.7 2.0\nNA looic       184.6 9.7\nNA ------\nNA MCSE of elpd_loo is NA.\nNA MCSE and ESS estimates assume independent draws (r_eff=1).\nNA \nNA Pareto k diagnostic values:\nNA                          Count Pct.    Min. ESS\nNA (-Inf, 0.7]   (good)     29    96.7%   364     \nNA    (0.7, 1]   (bad)       1     3.3%   &lt;NA&gt;    \nNA    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nNA See help('pareto-k-diagnostic') for details.\n\npar(mfrow = 1:2, mar = c(5, 3.8, 1, 0) + 0.1, las = 3)\nplot(full, label_points = TRUE)\nplot(reduced, label_points = TRUE)\n\n\n\n\n\n\n\n\nThe expected out-of-sample predictive accuracy is substantially lower for the model that includes \\(x\\). This might be used to suggest that the inferential evidence for a general effect of \\(x\\) on \\(y\\)."
  },
  {
    "objectID": "tutorials/2020-02-01-ancova-stan/index.html#graphical-summaries-1",
    "href": "tutorials/2020-02-01-ancova-stan/index.html#graphical-summaries-1",
    "title": "Ancova (Stan)",
    "section": "Graphical summaries",
    "text": "Graphical summaries\n\nmcmc = as.matrix(data1.rstan)\n## Calculate the fitted values\nnewdata1 = expand.grid(A = levels(data1$A), B = seq(min(data1$B), max(data1$B),\n    len = 100))\nXmat = model.matrix(~A * B, newdata1)\ncoefs = mcmc[, c(\"beta[1]\", \"beta[2]\", \"beta[3]\", \"beta[4]\", \"beta[5]\",\n    \"beta[6]\")]\nfit = coefs %*% t(Xmat)\nnewdata1 = newdata1 %&gt;% cbind(tidyMCMC(fit, conf.int = TRUE, conf.method = \"HPDinterval\"))\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()\n\n\n\n\n\n\n\n\nAs this is simple single factor ANOVA, we can simple add the raw data to this figure. For more complex designs with additional predictors, it is necessary to plot partial residuals.\n\n## Calculate partial residuals fitted values\nfdata1 = rdata1 = data1\nfMat = rMat = model.matrix(~A * B, fdata1)\nfit = as.vector(apply(coefs, 2, median) %*% t(fMat))\nresid = as.vector(data1$Y - apply(coefs, 2, median) %*% t(rMat))\nrdata1 = rdata1 %&gt;% mutate(partial.resid = resid + fit)\n\nggplot(newdata1, aes(y = estimate, x = B, fill = A)) + geom_point(data = rdata1,\n    aes(y = partial.resid, x = B, color = A)) + geom_ribbon(aes(ymin = conf.low,\n    ymax = conf.high), alpha = 0.2) + geom_line() + scale_y_continuous(\"Y\") +\n    scale_x_continuous(\"B\") + theme_classic()"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html",
    "href": "tutorials/2019-07-01-intro-jags/index.html",
    "title": "Super basic introduction to JAGS",
    "section": "",
    "text": "The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using JAGS via R.\nPrerequisites:"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#what-is-jags",
    "href": "tutorials/2019-07-01-intro-jags/index.html#what-is-jags",
    "title": "Super basic introduction to JAGS",
    "section": "What is JAGS?",
    "text": "What is JAGS?\nJAGS or Just Another Gibbs Sampler is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Plummer (2004)). JAGS is a free software based on the Bayesian inference Using Gibbs Sampling (informally BUGS) language at the base of WinBUGS/OpenBUGS but, unlike these programs, it is written in C++ and is platform independent. The latest version of JAGS can be downloaded from Martyn Plummer’s repository and is available for different OS. There are different R packages which function as frontends for JAGS. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the R2jags package (Su et al. (2015)) and show how to fit JAGS models using this package."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#installing-jags-and-r2jags",
    "href": "tutorials/2019-07-01-intro-jags/index.html#installing-jags-and-r2jags",
    "title": "Super basic introduction to JAGS",
    "section": "Installing JAGS and R2jags",
    "text": "Installing JAGS and R2jags\nInstall the latest version of JAGS for your OS. Next, install the package R2jags from within R or Rstudio, via the package installer or by typing in the command line\n\ninstall.packages(\"R2jags\", dependencies = TRUE)\n\nThe dependencies = TRUE option will automatically install all the packages on which the functions in the R2jags package rely."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#simulate-data",
    "href": "tutorials/2019-07-01-intro-jags/index.html#simulate-data",
    "title": "Super basic introduction to JAGS",
    "section": "Simulate data",
    "text": "Simulate data\nFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon  \n\\]\nThe R commands which I use to simulate the data are the following:\n\nn.sim=100; set.seed(123)\nx=rnorm(n.sim, mean = 5, sd = 2)\nepsilon=rnorm(n.sim, mean = 0, sd = 1)\nbeta0=1.5\nbeta1=1.2\ny=beta0 + beta1 * x + epsilon\n\nThen, I define all the data for JAGS in a list object\n\ndatalist=list(\"y\",\"x\",\"n.sim\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#model-file",
    "href": "tutorials/2019-07-01-intro-jags/index.html#model-file",
    "title": "Super basic introduction to JAGS",
    "section": "Model file",
    "text": "Model file\nNow, I write the model for JAGS and save it as a text file named \"basic.mod.txt\" in the current working directory\n\nbasic.mod= \"\nmodel {\n#model\n for(i in 1:n.sim){\n  y[i] ~ dnorm(mu[i], tau)\n  mu[i] = beta0 + beta1 * x[i]\n }\n#priors\nbeta0 ~ dnorm(0, 0.01)\nbeta1 ~ dnorm(0, 0.01)\ntau ~ dgamma(0.01,0.01)\n}\n\"\n\nThe part of the model inside the for loop denotes the likelihood, which is evaluated for each individual in the sample using a Normal distribution parameterised by some mean mu and precision tau (where, precision = 1/variance). The covariate x is included at the mean level using a linear regression, which is indexed by the intercept beta0 and slope beta1 terms. The second part defines the prior distributions for all parameters of the model, namely the regression coefficients and the precision. Weakly informative priors are used since I assume that I do not have any prior knowledge about these parameters.\nTo write and save the model as the text file “basic.mod.txt” in the current working directory, I use the writeLines function\n\nwriteLines(basic.mod, \"basic.mod.txt\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#pre-processing",
    "href": "tutorials/2019-07-01-intro-jags/index.html#pre-processing",
    "title": "Super basic introduction to JAGS",
    "section": "Pre-processing",
    "text": "Pre-processing\nDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in JAGS\n\nparams=c(\"beta0\",\"beta1\")\ninits=function(){list(\"beta0\"=rnorm(1), \"beta1\"=rnorm(1))}\n\nThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, JAGS can automatically select the initial values for all parameters in an efficient way even for relatively complex models. This can be achieved by setting inits=NULL, which is then passed to the jags function in R2jags.\nBefore using R2jags for the first time, you need to load the package, and you may want to set a random seed number for making your estimates replicable\n\nlibrary(R2jags)\nset.seed(123)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#fit-the-model",
    "href": "tutorials/2019-07-01-intro-jags/index.html#fit-the-model",
    "title": "Super basic introduction to JAGS",
    "section": "Fit the model",
    "text": "Fit the model\nNow, we can fit the model in JAGS using the jags function in the R2jags package and save it in the object basic.mod\n\nbasic.mod=jags(data = datalist, inits = inits,\n  parameters.to.save = params, n.chains = 2, n.iter = 2000, \n  n.burnin = 1000, model.file = \"basic.mod.txt\")\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 100\nNA    Unobserved stochastic nodes: 3\nNA    Total graph size: 406\nNA \nNA Initializing model\n\n\nWhile the model is running, the function prints out some information related to the Bayesian graph (corresponding to the specification used for the model) underneath JAGS, such as number of observed and unobserved nodes and graph size."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#post-processing",
    "href": "tutorials/2019-07-01-intro-jags/index.html#post-processing",
    "title": "Super basic introduction to JAGS",
    "section": "Post-processing",
    "text": "Post-processing\nOnce the model has finished running, a summary of the posteiror estimates and convergence diagnostics for all parameters specified can be seen by typing print(basic.mod) or, alternatively,\n\nprint(basic.mod$BUGSoutput$summary)\n\n\n\nNA           mean    sd   2.5%   25%   50%   75% 97.5% Rhat n.eff\nNA beta0      1.5 0.294   0.95   1.3   1.5   1.7   2.1    1  2000\nNA beta1      1.2 0.054   1.07   1.1   1.2   1.2   1.3    1  2000\nNA deviance 278.8 2.475 276.03 277.1 278.2 279.9 285.1    1  2000\n\n\nThe posterior distribution of each parameter is summarised in terms of:\n\nThe mean, sd and some percentiles\nPotential scale reduction factor Rhat and effective sample size n.eff (Gelman et al. (2013)). The first is a measure to assess issues in convergence of the MCMC algorithm (typically a value below \\(1.05\\) for all parameters is considered ok). The second is a measure which assesses the adequacy of the posterior sample (typically values close to the total number of iterations are desirable for all parameters).\n\nThe deviance is a goodness of fit statistic and is used in the construction of the “Deviance Information Criterion” or DIC (Spiegelhalter et al. (2014)), which is a relative measure of model comparison. The DIC of the model can be accessed by typing\n\nbasic.mod$BUGSoutput$DIC\n\nNA [1] 282"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-jags/index.html#diagnostics",
    "href": "tutorials/2019-07-01-intro-jags/index.html#diagnostics",
    "title": "Super basic introduction to JAGS",
    "section": "Diagnostics",
    "text": "Diagnostics\nMore diagnostics are available when we convert the model output into an MCMC object using the command\n\nbasic.mod.mcmc=as.mcmc(basic.mod)\n\nDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the mcmcplots package (Curtis (2015)) to obtain graphical diagnostics and results.\n\ninstall.packages(\"mcmcplots\")\nlibrary(mcmcplots)\n\nFor example, density and trace plots can be obtained by typing\n\ndenplot(basic.mod.mcmc, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\ntraplot(basic.mod.mcmc, parms = c(\"beta0\",\"beta1\"))\n\n\n\n\n\n\n\n\nBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains)."
  },
  {
    "objectID": "software/missingHE/index.html",
    "href": "software/missingHE/index.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package aimed at providing some useful tools to analysts in order to handle missing outcome data under a Full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS. The package allows to obtain inferences using Markov Chain Monte Carlo (MCMC) methods under a range of modelling approaches and missing data assumptions. The package also contains functions specifically defined to assess model fit and possible issues in model convergence as well as to summarise the main results from the economic analysis.\nMissing data are iteratively imputed using data augmentation methods according to the type of model, distribution and missingness assumptions specified by the user using different arguments in the functions of the package. The posterior distribution of the main quantities of interest (e.g. some suitable measures of costs and clinical benefits) is then summarised to assess the cost-effectiveness of a new intervention (\\(t=2\\)) against a standard intervention (\\(t=1\\)).\nmissingHE produces plots which compares the observed and imputed values for both cost and benefit measures in each treatment intervention considered to detect possible concerns about the plausibility of the imputation methods. In addition, the output of missingHE cab be analysed using different funtions in the R package BCEA which produces a synthesis of the decision process given the current evidence and uncertainty, as well as several indicators that can be used to perform Probabilistic Sensitivity Analysis to parameter and model uncertainty.\n\n\n\nExample of a graphical output from missingHE"
  },
  {
    "objectID": "software/missingHE/index.html#example",
    "href": "software/missingHE/index.html#example",
    "title": "missingHE",
    "section": "Example",
    "text": "Example\n\nlibrary(missingHE)\nmodel.sel &lt;- selection(data = MenSS, model.eff = e ~ u.0, model.cost = c ~ e, model.me = me ~ 1, model.mc = mc ~ 1, type = \"MAR\", n.chains = 2, n.iter = 1000, n.burnin = 100, dist_e = \"norm\", dist_c = \"norm\")\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 410\n   Unobserved stochastic nodes: 242\n   Total graph size: 2399\n\nInitializing model\n\nsummary(model.sel)\n\n\n Cost-effectiveness analysis summary \n \n Comparator intervention: intervention 1 \n Reference intervention: intervention 2 \n \n Parameter estimates under MAR assumption\n \n Comparator intervention \n                        mean     sd      LB     UB\nmean effects (t = 1)   0.874  0.016   0.847  0.901\nmean costs (t = 1)   236.716 49.778 155.903 319.43\n\n Reference intervention \n                        mean     sd      LB      UB\nmean effects (t = 2)   0.917  0.022   0.882   0.952\nmean costs (t = 2)   185.896 40.642 120.007 250.279\n\n Incremental results \n                   mean     sd       LB     UB\ndelta effects     0.043  0.028   -0.002  0.089\ndelta costs      -50.82 63.696 -155.573 55.615\nICER          -1191.646"
  },
  {
    "objectID": "software/missingHE/index.html#news-and-updates-about-missinghe",
    "href": "software/missingHE/index.html#news-and-updates-about-missinghe",
    "title": "missingHE",
    "section": "News and updates about missingHE",
    "text": "News and updates about missingHE\n\nFrom 25/09/2019, the updated version (1.2.1) of missingHE has become available on CRAN, which allows to perform posterior predictive checks for each type of model as a further way to assess the fit of the model to the observed data.\n\nThe checks can be done by first setting the optional argument ppc = TRUE when fitting the model using one of the main function of the package. For example, when using selection to fit selection models you would have something like this:\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age, model.cost = c ~ age + e, model.me = me ~ age, model.mc = mc ~ age, dist_e = \"norm\", dist_c = \"gamma\", type = \"MAR\", n.iter = 1000, ppc = TRUE)\n\nThen you can use the function ppc to perform different types of posterior predictive checks that you can choose among a set of pre-specified types using the type argument. For example, if we want to compare histograms of the empirical and predictive distributions of the effectiveness variable in one arm (e.g. control), then we can type\n\nppc(model.sel, type = \"histogram\", outcome = \"effects_arm1\")\n\nand we get something like this\n\n\n\nExample of posterior predictive checks in missingHE\n\n\n\nFrom 07/01/2020, the updated version (1.3.2) of missingHE has become available on CRAN, which allows to choose among more distributions for the effectiveness measures, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) health outcomes.\n\nFor example, we can choose to specify a selection model assuming a Bernoulli distribution for the effects (if this is a binary outcome) and a LogNormal distribution for the costs\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age, model.cost = c ~ age + e, model.me = me ~ 1, model.mc = mc ~ 1, dist_e = \"bern\", dist_c = \"lnorm\", type = \"MAR\")\n\n\nFrom 30/04/2020, the updated version (1.4.0) of missingHE has become available on CRAN, which allows to perform fit random effects for each type of model implemented. The random terms can be specified using the following notation\n\n\nmodel.sel &lt;- selection(data = data, model.eff = e ~ age + (age | site), model.cost = c ~ age + e + (age + e | site), model.me = me ~ age + (1 | site), model.mc = mc ~ age + (0 + age | site), dist_e = \"norm\", dist_c = \"gamma\", type = \"MAR\", n.iter = 10000, ppc = TRUE)\n\nI borrowed this notation, alongside with a couple of internal functions, from the lme4 package. The terms inside the brackets on the left of the bar are the terms for which the random effects are assumed (these must also be included as fixed effects). The term on the right of the bar is the clustering variable over which the random effects are specified.\nFor example the formula + (age | site) specifies random effects for the intercept and age across the values of the site variable. It aslo possible to specify random slope only models (i.e. remove the random intercept) by adding the term 0 + inside the brackets on the left of the bar.\nAll functions in the package have been updated to take into account the possibility that random effects are specified and to perform diagnostic and posterior predictive checks based on the random effects if these are included. In addition, a new generic function called coef is now available to extract the fixed or random effect terms from the effectiveness and cost models for each type of model in missingHE. For example, we can extract summary statistics for the fixed effects from the fitted selection model by using the command\n\ncoef(model.sel, random = FALSE)\n\nwhich prints something like this\n\n\n$Comparator\n$Comparator$Effects\n             mean    sd  lower upper\n(Intercept) 0.187 0.138 -0.083 0.457\nu.0         0.779 0.150  0.487 1.074\n\n$Comparator$Costs\n                mean      sd     lower    upper\n(Intercept)  236.716  49.778   142.237  335.056\ne           -963.484 418.694 -1772.698 -149.278\n\n\n$Reference\n$Reference$Effects\n             mean    sd lower upper\n(Intercept) 0.665 0.075 0.520 0.811\nu.0         0.285 0.087 0.115 0.458\n\n$Reference$Costs\n                mean      sd    lower   upper\n(Intercept)  185.896  40.642  103.211 262.101\ne           -187.828 349.812 -871.173 485.674\n\n\nIf we set random = TRUE, then summary statistics for the random effects terms are printed.\n\nFrom 10/06/2020 a new version (1.4.1) of missingHE is available to download from my GitHub page, which includes three vignettes providing some tutorials on how to use the functions of the package. Each vignette is specifically designed to help different types of users:\n\nThe first vignette is named Introduction_to_missingHE and is designed to provide some introductory summary about the use of the functions of the package based on the default settings, what the user needs to specify and how to interpret and extract the results. See the vignette here\nThe second vignette is named Fitting_MNAR_models_in_missingHE and is deisgned to help those who would like to explore MNAR assumptions and how this can be done within each main function of the package. See the vignette here\nThe third vignette is named Model_customisation_in_missingHE and is designed for those who are already familiar with the package but who would like to customise the functions in a more flexible way, for example by including random effects, using different priors or modelling assumptions. See the vignette here\n\nFrom 21/03/2023 a new version (4.2.0) of missingHE is available to download from my GitHub page, which includes an additional vignette providing some tutorials on how to extend the already existing functions within the package to fit longitudinal data. Some functions and options still need to be updated but the default configurations for either selection, pattern, or hurdle functions can now be applied to two-arms within-trial longitudinal data economic evaluations. See the vignette here\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "software/missingHE/index.html#installation",
    "href": "software/missingHE/index.html#installation",
    "title": "missingHE",
    "section": "Installation",
    "text": "Installation\nThere are two ways of installing missingHE. A stable version (currently 4.2.0) is packaged and available from CRAN. You can simply type on your R terminal\n\ninstall.packages(\"missingHE\")\n\nThe second way involves using the development version of missingHE, which is available from GitHub - this will usually be updated more frequently and may be continuously tested. On Windows machines, you need to install a few dependencies, including Rtools first, e.g. by running\n\npkgs &lt;- c(\"R2jags\",\"ggplot2\",\"gridExtra\",\"BCEA\",\"ggmcmc\",\"loo\",\"Rtools\",\"devtools\", \"utils\")\nrepos &lt;- c(\"https://cran.rstudio.com\") \ninstall.packages(pkgs,repos=repos,dependencies = \"Depends\")\n\nbefore installing the package using devtools:\n\ndevtools::install_github(\"AnGabrio/missingHE\", build_vignettes = TRUE)\n\nThe optional argument build_vignettes = TRUE allows to install the vignettes of the package locally on your computer. These consist in brief tutorials to guid the user on how to use and customise the models in missingHE using different functions of the package. Once the package is installed, they can be accessed by using the command\n\nutils::browseVignettes(package = \"missingHE\")\n\nAll models implemented in missingHE are written in the BUGS language using the software JAGS, which needs to be installed from its own repository and instructions for installations under different OS can be found online. Once installed, the software is called in missingHE via the R package R2jags. Note that the missingHE package is currently under active development and therefore it is advisable to reinstall the package directly from GitHub before each use to ensure that you are using the most updated version."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html",
    "href": "research/reviewQES/reviewQES.html",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "",
    "text": "We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a Quality Evaluation Scheme (QES), providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#descriptive-review",
    "href": "research/reviewQES/reviewQES.html#descriptive-review",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Descriptive Review",
    "text": "Descriptive Review\n\n\n\n\n\n\nFigure 2: Missingness methods by outcome and period.\n\n\n\nFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects (\\(\\approx 45\\%\\) and \\(\\approx 38\\%\\)) and costs ( \\(\\approx 50\\%\\) and \\(\\approx 62\\%\\)). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#quality-assessment",
    "href": "research/reviewQES/reviewQES.html#quality-assessment",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Quality assessment",
    "text": "Quality assessment\nGenerally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the QES. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than \\(7\\%\\) of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making."
  },
  {
    "objectID": "research/partsurvHTA/partsurvHTA.html",
    "href": "research/partsurvHTA/partsurvHTA.html",
    "title": "A Bayesian Framework for Patient-Level Partitioned Survival Cost-Utility Analysis",
    "section": "",
    "text": "Modelling Framework\nwe extend the current methods for modelling trial-based partitioned survival cost-utility data, taking advantage of the flexibility of the Bayesian approach, and specify a joint probabilistic model for the health economic outcomes. We propose a general framework that is able to account for the multiple types of complexities affecting individual level data (correlation, missingness, skewness and structural values), while also explicitly modelling the dependence relationships between different types of quality of life and cost components.\nConsider a clinical trial in which patient-level information on a set of suitably defined effectiveness and cost variables is collected at \\(J\\) time points on \\(N\\) individuals, who have been allocated to \\(T\\) intervention groups. Assume that the primary endpoint of the trial is OS, while secondary endpoints include PFS, a self-reported health-related quality of life questionnaire (e.g. EQ-5D) and health records on different types of services (e.g. drug frequency and dosage, hospital visits, etc.). Following standard health economic notation, we denote with \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) the two sets of health economic outcomes (effectiveness and costs) collected for the \\(i\\)-th individual in treatment \\(t\\) of the trial. For simplicity, we define \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) based on the variables used in the analysis.\nThe effectiveness outcomes are represented by pre-progression (\\(e^{PFS}\\_{it}=\\text{QAS}^{\\text{PFS}}\\)) and post-progression (\\(e^{PPS}\\_{it}=\\text{QAS}^{\\text{PPS}}\\)) QAS data calculated using survival and utility data collected up to and beyond progression. We denote the full set of effectiveness variables as \\(\\boldsymbol e_{it}=(e^{\\text{PFS}}\\_{it},e^{\\text{PPS}}\\_{it})\\), formed by the pre and post-progression components. The cost outcomes are represented by a set of \\(K\\) variables (\\(c\\_{it}=c^k\\_{it}\\), for \\(k=1,\\ldots,K\\)) calculated based on \\(K\\) different types of health services and associated unit prices. We denote the full set of cost variables as \\(\\boldsymbol c\\_{it}=(c^1\\_{it},\\ldots,c^K\\_{it})\\), formed by the \\(K\\) different cost components.\nThe objective of the economic evaluation is to perform a patient-level partitioned survival cost-utility analysis by specifying a joint model \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) denotes the full set of model parameters. Among these parameters, interest is in the marginal mean effectiveness and costs \\(\\boldsymbol \\mu=(\\mu\\_{et},\\mu\\_{ct})\\) which are used to inform the decision-making process. Different approaches can be used to specify \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\). Here, we express the joint distribution as\n\\[\np(\\boldsymbol e_{it}, \\boldsymbol c_{it} \\mid \\boldsymbol \\theta) = p(\\boldsymbol e_{it} \\mid \\boldsymbol \\theta_e)p(\\boldsymbol c_{it} \\mid \\boldsymbol  e_{it} , \\boldsymbol  \\theta_c),\n\\tag{1}\\]\nwhere \\(p(\\boldsymbol e_{it} \\mid \\boldsymbol  \\theta_e)\\) is the marginal distribution of the effectiveness and \\(p(\\boldsymbol  c_{it} \\mid \\boldsymbol  e_{it} \\boldsymbol  \\theta_c)\\) is the conditional distribution of the costs given the effectiveness, respectively indexed by \\(\\boldsymbol  \\theta_e\\) and \\(\\boldsymbol  \\theta_c\\), with \\(\\boldsymbol  \\theta=(\\boldsymbol  \\theta_e,\\boldsymbol  \\theta_c)\\). We specify the model in Equation 1 in terms of a marginal distribution for the effectiveness and a conditional distribution for the costs. A key advantage of using a conditional factorisation, compared to a multivariate marginal approach, is that univariate models for each variable can be flexibly specified to tackle the idiosyncrasies of the data (e.g. non-normality ans spikes) while also capturing the potential correlation between the variables. We now describe how the two factors on the right-hand side of the Equation can be specified.\nFigure 1 provides a visual representation of the proposed modelling framework.\n\n\n\n\n\n\nFigure 1: Visual representation of the proposed modelling framework\n\n\n\nThe effectiveness and cost distributions are represented in terms of combined “modules”- the red and blue boxes - in which the random quantities are linked through logical relationships. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in each module.\n\n\nConclusions\nAlthough our approach may not be applicable to all cases, the data analysed are very much representative of the “typical” data used in partitioned survival cost-utility analysis alongside clinical trials. Thus, it is highly likely that the same features apply to other real cases. This is a very important, if somewhat overlooked problem, as methods that do not take into account the complexities affecting patient-level data, while being easier to implement and well established among practitioners, may ultimately mislead cost-effectiveness conclusions and bias the decision-making process."
  },
  {
    "objectID": "research/missingHE/missingHE.html",
    "href": "research/missingHE/missingHE.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variables, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitivity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot can produce graphs, such as those shown in Figure 1, which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\n\n\n\n\n\nFigure 1: Plot of observed (black dots) and imputed (red dots and lines) effectiveness and cost data by treatment group.\n\n\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "research/jointHTA/jointHTA.html",
    "href": "research/jointHTA/jointHTA.html",
    "title": "Joint Longitudinal Models for Dealing With Missing at Random Data in Trial-Based Economic Evaluations",
    "section": "",
    "text": "Introduction\nIn trial-based economic evaluation, some individuals are typically associated with missing data at some time point, so that their corresponding aggregated outcomes (e.g. quality-adjusted life-years) cannot be evaluated. Restricting the analysis to the complete cases is inefficient and can result in biased estimates, while imputation methods are often implemented under a missing at random (MAR) assumption. We propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR.\n\n\nStandard approach in trial-based CEA\nAccording to recent reviews, standard practice in trial-based CEAs handles missingness at the level of the aggregated outcomes and baseline variables. Indeed, estimates of interest are obtained by directly modeling the aggregated outcomes rather than the utility and cost data at each time. This requires the analyst to process the data collected on individual \\(i\\) at time \\(j\\) in treatment \\(t\\), to derive the aggregated measures over the study duration.\nFigure 1 shows a typical data set of trial-based CEA, formed by the sets of utility and cost variables collected at baseline \\(j = 0\\) and some follow-ups \\(j = 1,\\ldots,J\\). The graph represents the standard procedure for processing the data and identifying the variables used in the analysis.\n\n\n\n\n\n\nFigure 1: Schematic representation of the standard procedure for processing trial-based CEA data\n\n\n\nA general limitation of any aggregated method is to ignore the longitudinal nature of the data and discard all follow-up values for partially observed individuals. Conversely, methods that handle missingness at each time point account for the longitudinal structure, incorporate all available evidence, and potentially make the missingness assumptions (e.g. missing at random or MAR) more reasonable.\n\n\nMethods\nWe propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR. We compare the results from methods that handle missingness at an aggregated (case deletion, baseline imputation, and joint aggregated models) and disaggregated (joint longitudinal models) level under MAR. The methods are compared using a simulation study and applied to data from 2 real case studies.\n\n\nConclusions\nJoint longitudinal models provide an alternative and potentially less biased approach for handling missing data with respect to current practice under a missing at random assumption. Methods that ignore some of the available information may be associated with biased results and mislead the decision-making process. This is a potentially serious issue for those who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options."
  },
  {
    "objectID": "research/bookHTA/bookHTA.html",
    "href": "research/bookHTA/bookHTA.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Introduction\nThe type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and \\(P\\)-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n\nwhether the treatments under evaluation are cost-effective given the available evidence and\nwhether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).\n\nThis corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\n\n\nBayesian methods in HTA\nThere are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in Figure 1.\n\n\n\n\n\n\nFigure 1: Diagram representation of the process for health economic evaluation.\n\n\n\nThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g. the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the available evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n\nwhether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and\nwhether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.\n\nWhile the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\n\n\nConclusions\nHTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions."
  },
  {
    "objectID": "publication/2023-a-my-publication/index.html",
    "href": "publication/2023-a-my-publication/index.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Abstract\nEconomic evaluations have been increasingly conducted in different countries to aid national decision-making bodies in resource allocation problems based on current and prospective evidence on costs and effects data for a set of competing health care interventions. In 2016, the Dutch National Health Care Institute issued new guidelines that aggregated and updated previous recommendations on key elements for conducting economic evaluation. However, the impact on standard practice after the introduction of the guidelines in terms of design, methodology and reporting choices, is still uncertain. To assess this impact, we examine and compare key analysis components of economic evaluations conducted in the Netherlands before (2010–2015) and after (2016–2020) the introduction of the recent guidelines. We specifically focus on two aspects of the analysis that are crucial in determining the plausibility of the results: statistical methodology and missing data handling. Our review shows how, over the last period, many components of economic evaluations have changed in accordance with the new recommendations towards more transparent and advanced analytic approaches. However, potential limitations are identified in terms of the use of less advanced statistical software together with rarely satisfactory information to support the choice of missing data methods, especially in sensitivity analysis.\n\n\n\n\n\nCitationBibTeX citation:@online{gabrio2023,\n  author = {Gabrio, Andrea},\n  title = {A Review of Heath Economic Evaluation Practice in the\n    {Netherlands:} Are We Moving Forward?},\n  date = {2023-06-03},\n  url = {https://www.cambridge.org/core/journals/health-economics-policy-and-law/article/abs/review-of-heath-economic-evaluation-practice-in-the-netherlands-are-we-moving-forward/8A4D0D3A9E7EBC83A65B4283D170FCC6},\n  doi = {10.1017/S1744133123000087},\n  langid = {en},\n  abstract = {{[}Economic evaluations have been increasingly conducted\n    in different countries to aid national decision-making bodies in\n    resource allocation problems ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2023. “A Review of Heath Economic Evaluation\nPractice in the Netherlands: Are We Moving Forward?” Health\nEconomics, Policy and Law. June 3, 2023. https://doi.org/10.1017/S1744133123000087."
  },
  {
    "objectID": "publication/2022-a-my-publication/index.html",
    "href": "publication/2022-a-my-publication/index.html",
    "title": "A Scoping Review of Item-Level Missing Data in Within-Trial Cost-Effectiveness Analysis",
    "section": "",
    "text": "Abstract\nCost-effectiveness analysis (CEA) alongside randomized controlled trials often relies on self-reported multi-item questionnaires that are invariably prone to missing item-level data. The purpose of this study is to review how missing multi-item questionnaire data are handled in trial-based CEAs. We searched the National Institute for Health Research journals to identify within-trial CEAs published between January 2016 and April 2021 using multi-item instruments to collect costs and quality of life (QOL) data. Information on missing data handling and methods, with a focus on the level and type of imputation, was extracted. A total of 87 trial-based CEAs were included in the review. Complete case analysis or available case analysis and multiple imputation (MI) were the most popular methods, selected by similar numbers of studies, to handle missing costs and QOL in base-case analysis. Nevertheless, complete case analysis or available case analysis dominated sensitivity analysis. Once imputation was chosen, missing costs were widely imputed at item-level via MI, whereas missing QOL was usually imputed at the more aggregated time point level during the follow-up via MI. Missing costs and QOL tend to be imputed at different levels of missingness in current CEAs alongside randomized controlled trials. Given the limited information provided by included studies, the impact of applying different imputation methods at different levels of aggregation on CEA decision making remains unclear.\n\n\n\n\nCitationBibTeX citation:@online{ling2022,\n  author = {Ling, Xiaoxiao and Gabrio, Andrea and Baio, Gianluca},\n  title = {A {Scoping} {Review} of {Item-Level} {Missing} {Data} in\n    {Within-Trial} {Cost-Effectiveness} {Analysis}},\n  volume = {25},\n  number = {9},\n  date = {2022-03-10},\n  url = {https://www.valueinhealthjournal.com/article/S1098-3015(22)00111-5/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301522001115%3Fshowall%3Dtrue},\n  doi = {10.1016/j.jval.2022.02.009},\n  langid = {en},\n  abstract = {{[}Cost-effectiveness analysis (CEA) alongside randomized\n    controlled trials often relies on self-reported multi-item\n    questionnaires ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nLing, Xiaoxiao, Andrea Gabrio, and Gianluca Baio. 2022. “A Scoping\nReview of Item-Level Missing Data in Within-Trial Cost-Effectiveness\nAnalysis.” Value in Health. March 10, 2022. https://doi.org/10.1016/j.jval.2022.02.009."
  },
  {
    "objectID": "publication/2020-a-my-publication/index.html",
    "href": "publication/2020-a-my-publication/index.html",
    "title": "Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations",
    "section": "",
    "text": "Abstract\nHealth economic evaluations based on patient-level data collected alongside clinical trials (e.g. health related quality of life and resource use measures) are an important component of the process which informs resource allocation decisions. Almost inevitably, the analysis is complicated by the fact that some individuals drop out from the study, which causes their data to be unobserved at some time point. Current practice performs the evaluation by handling the missing data at the level of aggregated variables (e.g. QALYs), which are obtained by combining the economic data over the duration of the study, and are often conducted under a missing at random (MAR) assumption. However, this approach may lead to incorrect inferences since it ignores the longitudinal nature of the data and may end up discarding a considerable amount of observations from the analysis. We propose the use of joint longitudinal models to extend standard cost-effectiveness analysis methods by taking into account the longitudinal structure and incorporate all available data to improve the estimation of the targeted quantities under MAR. Our approach is compared to popular missingness approaches in trial-based analyses, motivated by an exploratory simulation study, and applied to data from two real case studies.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2020,\n  author = {Gabrio, Andrea and M Hunter, Rachael and J Mason, Alexina\n    and Baio, Gianluca},\n  title = {Joint Longitudinal Models for Dealing with Missing at Random\n    Data in Trial-Based Economic Evaluations},\n  volume = {24},\n  number = {5},\n  date = {2020-05-11},\n  url = {https://www.valueinhealthjournal.com/article/S1098-3015(21)00042-5/fulltext?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS1098301521000425%3Fshowall%3Dtrue},\n  doi = {10.1016/j.jval.2020.11.018},\n  langid = {en},\n  abstract = {{[}Health economic evaluations based on patient-level data\n    collected alongside clinical trials (e.g. health related quality of\n    life and resource use measures) are an important component\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Rachael M Hunter, Alexina J Mason, and Gianluca Baio.\n2020. “Joint Longitudinal Models for Dealing with Missing at\nRandom Data in Trial-Based Economic Evaluations.” Value in\nHealth. May 11, 2020. https://doi.org/10.1016/j.jval.2020.11.018."
  },
  {
    "objectID": "publication/2019-c-my-publication/index.html",
    "href": "publication/2019-c-my-publication/index.html",
    "title": "A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations",
    "section": "",
    "text": "Abstract\nTrial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study, using methods that ignore the complexities of utility and cost data (e.g. skewness and spikes). We present an alternative and more efficient Bayesian parametric approach to handle missing longitudinal outcomes in economic evaluations, while accounting for the complexities of the data. We specify a flexible parametric model for the observed data and partially identify the distribution of the missing data with partial identifying restrictions and sensitivity parameters. We explore alternative nonignorable scenarios through different priors for the sensitivity parameters, calibrated on the observed data. Our approach is motivated by, and applied to, data from a trial assessing the cost-effectiveness of a new treatment for intellectual disability and challenging behaviour.\n         \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and J Daniels, Michael and Baio, Gianluca},\n  title = {A {Bayesian} {Parametric} {Approach} to {Handle} {Missing}\n    {Longitudinal} {Outcome} {Data} in {Trial-Based} {Health} {Economic}\n    {Evaluations}},\n  volume = {183},\n  number = {2},\n  date = {2019-09-26},\n  url = {https://academic.oup.com/jrsssa/article/183/2/607/7056293},\n  doi = {10.1111/rssa.12522},\n  langid = {en},\n  abstract = {{[}Trial-based economic evaluations are typically\n    performed on cross-sectional variables, derived from the responses\n    for only the completers in the study ...{]}\\{style=“font-size:\n    85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Michael J Daniels, and Gianluca Baio. 2019. “A\nBayesian Parametric Approach to Handle Missing Longitudinal Outcome Data\nin Trial-Based Health Economic Evaluations.” Journal of the Royal\nStatistical Society Series A. September 26, 2019. https://doi.org/10.1111/rssa.12522."
  },
  {
    "objectID": "publication/2019-a-my-publication/index.html",
    "href": "publication/2019-a-my-publication/index.html",
    "title": "A Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data",
    "section": "",
    "text": "Abstract\nEconomic evaluations from individual level data are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. A critical problem in these analyses is that both effectiveness and cost data typically present some complexity (eg, nonnormality, spikes, and missingness) that should be addressed using appropriate methods. However, in routine analyses, standardised approaches are typically used, possibly leading to biassed inferences. We present a general Bayesian framework that can handle the complexity. We show the benefits of using our approach with a motivating example, the MenSS trial, for which there are spikes at one in the effectiveness and missingness in both outcomes. We contrast a set of increasingly complex models and perform sensitivity analysis to assess the robustness of the conclusions to a range of plausible missingness assumptions. We demonstrate the flexibility of our approach with a second example, the PBS trial, and extend the framework to accommodate the characteristics of the data in this study. This paper highlights the importance of adopting a comprehensive modelling approach to economic evaluations and the strategic advantages of building these complex models within a Bayesian framework.\n         \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and J Mason, Alexina and Baio, Gianluca},\n  title = {A {Full} {Bayesian} {Model} to {Handle} {Structural} {Ones}\n    and {Missingness} in {Economic} {Evaluations} from\n    {Individual-Level} {Data}},\n  volume = {28},\n  number = {8},\n  date = {2019-04-01},\n  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8045},\n  doi = {10.1002/sim.8045},\n  langid = {en},\n  abstract = {{[}Economic evaluations from individual level data are an\n    important component of the process of technology appraisal\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Alexina J Mason, and Gianluca Baio. 2019. “A Full\nBayesian Model to Handle Structural Ones and Missingness in Economic\nEvaluations from Individual-Level Data.” Statistics in Medicine.\nApril 1, 2019. https://doi.org/10.1002/sim.8045."
  },
  {
    "objectID": "posts/2019-12-09-my-blog-post/index.html",
    "href": "posts/2019-12-09-my-blog-post/index.html",
    "title": "Not a very good start…",
    "section": "",
    "text": "After some nice holiday break, I came back to work ready for an exciting 2020 … or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.\n\n\n\n\n\nGoing back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my missingHE package, which is available both on my GitHub page and on the CRAN repository. Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.\nAnother good news is that the last paper written with Michael about missing data handling in economic evaluations will soon be published in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.\nFinally, an announcement about the one-day course I am holding together with my mates from the HEART group about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free …). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.\nNow I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let’s try again 2020."
  },
  {
    "objectID": "posts/2019-10-28-my-blog-post/index.html",
    "href": "posts/2019-10-28-my-blog-post/index.html",
    "title": "Copenhagen, I am coming …",
    "section": "",
    "text": "Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some “applied statistics”-related discussions or the attention is more placed on “economics and clinical” matters. From what I heard by other people who routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.\nI know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available here), but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.\nApart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some routine work for some trial analyses at PRIMENT, which by the way is advertising a new health economist job vacancy for those who might be interested. Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervision for a new PhD student at stats and, after I can find some free time, do some research work on my beloved missing data. Am I ready? not sure about that …"
  },
  {
    "objectID": "posts/2019-09-25-my-blog-post/index.html",
    "href": "posts/2019-09-25-my-blog-post/index.html",
    "title": "MissingHE 1.2.1",
    "section": "",
    "text": "I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function “pattern”. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.\nSecond, I added a new accessory function called “ppc”, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model. The function implements a relatively large number of checks, mostly taken from the R package bayesplot, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention). For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.\n\n\n\nDensity plots for the observed and replicated data\n\n\nI feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution functions, statistics of interest and many others. In addition, these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.\n\n\n\n\n\nOf course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this check is only partial since the fit to the unobserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption because this is based on information which is not directly available from the data at hand and thus impossible to check."
  },
  {
    "objectID": "posts/2019-08-03-my-blog-post/index.html",
    "href": "posts/2019-08-03-my-blog-post/index.html",
    "title": "The P value fallacy",
    "section": "",
    "text": "Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a deductive inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a inductive approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the P value proposed by Fisher and the hypothesis testing developed by Neyman and Pearson.\nThe P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.\nHypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive (\\(\\alpha\\)) and type II error or false-negative (\\(\\beta\\)) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this objectivity is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.\nOver time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting \\(\\alpha\\) and power \\(\\beta\\) before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The P value fallacy is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.\n\n\n\n\n\nI personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.\nI feel that, since the two methods are perceived as “objective”, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as “scientific”.\nThis misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as “less reliable” or “more prone to mistakes” compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed."
  },
  {
    "objectID": "missing_data/spm/spm.html",
    "href": "missing_data/spm/spm.html",
    "title": "Shared Parameter Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Shared Parameter Models(SPM)."
  },
  {
    "objectID": "missing_data/spm/spm.html#shared-parameter-models",
    "href": "missing_data/spm/spm.html#shared-parameter-models",
    "title": "Shared Parameter Models",
    "section": "Shared Parameter Models",
    "text": "Shared Parameter Models\nThe shared parameter model approach consists in an explicit multilevel specification, where random effects \\(b\\) are modelled jointly with \\(Y\\) and \\(R\\) (Wu and Carroll (1988)). The general form of the full data modelling using a SPM approach is\n\\[\nf(y,r \\mid \\omega) = \\int f(y, r, b \\mid \\omega)db.\n\\]\nNext, specific SPMs are formulated by making assumptions about the joint distribution under the integral sign. Main advantages of this models is that they are quite easy to specify and that, through the use of random effects, high-dimensional or multilevel data modelling is relatively easy to accomplish. The main drawback is that the underlying missingness mechanism is often difficult to understand and may not have even a closed form.\n\nExample random coefficients selection model\nWu and Carroll (1988) specified a SPM assuming the response follow a linear random effects model\n\\[\nY_i \\mid x_i,b_i \\sim N(x_i\\beta + w_ib_i, \\Sigma_i(\\phi)),\n\\]\nwhere \\(w_i\\) are the random effects covariates with rows \\(w_i=(1,t_{ij})\\), therefore implying that each individual has a random slope and intercept. The random effects \\(b_i=(b_{i1},b_{i2})\\) are assumed to follow a bivariate normal distribution\n\\[\nb_i \\sim N(0,\\Omega),\n\\]\nwhile the hazard of dropout is Bernoulli with\n\\[\nR_{ij} \\mid R_{ij-1}=1,b_i \\sim Bern(\\pi_{ij}),\n\\]\nwhich depends on the random effects via\n\\[\ng(\\pi_{ij}) = \\psi_0 + \\psi_1b_{i1} + \\psi_2b_{i2}.\n\\]\nThe model can be seen as a special case of the general SPM formulation by noticing that the joint distribution under the integral sign can be factored as\n\\[\nf(y,r,b \\mid x, \\omega) = f(r \\mid b,x,\\psi)f(y \\mid b,x,\\beta,\\phi)f(b \\mid \\Omega)\n\\]\nunder the assumption that \\(R\\) is independent of both \\(Y_{obs}\\) and \\(Y_{mis}\\), conditionally on \\(b\\). However, integrating over the random effects, dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\), is induced and therefore the model characterises a Missing Not At Random(MNAR) mechanism.\nThe conditional linear model (Wu and Bailey (1989)) can also be seen as a version of the SPM, which is formulated as\n\\[\nf(y,r,b \\mid x) = f(y \\mid r,b,x)f(b \\mid r,x)f(r \\mid x).\n\\]"
  },
  {
    "objectID": "missing_data/spm/spm.html#conlcusions",
    "href": "missing_data/spm/spm.html#conlcusions",
    "title": "Shared Parameter Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, shared parameter models are very useful for characterizing joint distributions of repeated measures and event times, and can be particularly useful as a method of data reduction when the dimension of \\(Y\\) is high. Nonetheless, their application to the problem of making full data inference from incomplete longitudinal data should be made with caution and with an eye toward justifying the required assumptions. Sensitivity analysis is an open area of research for these models."
  },
  {
    "objectID": "missing_data/pmm/pmm.html",
    "href": "missing_data/pmm/pmm.html",
    "title": "Pattern Mixture Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Pattern Mixture Models(PMM)."
  },
  {
    "objectID": "missing_data/pmm/pmm.html#pattern-mixture-models",
    "href": "missing_data/pmm/pmm.html#pattern-mixture-models",
    "title": "Pattern Mixture Models",
    "section": "Pattern Mixture Models",
    "text": "Pattern Mixture Models\nThe pattern mixture model approach factors the full data distribution as\n\\[\nf(y,r \\mid \\omega) = f(y \\mid r, \\phi) f(r \\mid y,\\chi),\n\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\phi,\\chi)\\). Thus, under the PMM approach, the response model \\(f(y \\mid \\theta)\\) can be retrieved as a mixture of the pattern specific distributions\n\\[\nf(y \\mid \\theta) = \\sum_{r}f(y \\mid r, \\phi)f(r \\mid \\chi),\n\\]\nwith weights given by the corresponding probabilities of the different patterns. The missingness mechanism \\(f(r \\mid y, \\psi)\\) can also be obtained using Bayes’ rule\n\\[\nf(y \\mid r, \\psi) = \\frac{f(y \\mid r, \\phi)f(r\\mid \\chi)}{f(y \\mid \\theta)}.\n\\]\nThe construction of PMMs requires the specification of the full data distribution conditional on different missingness patterns, which may be cumbersome when the number of patterns is large, but with the advantage of making explicit the parameters that cannot be identified by the observed data. In particular, PMMs are well suited to show that the distribution of the response within each pattern can be decomposed as\n\\[\nf(y_{obs},y_{mis} \\mid r, \\phi)= f(y_{mis} \\mid y_{obs},r,\\phi_{E})f(y_{obs}\\mid r,\\phi_{O}),\n\\]\nwhere \\(\\phi_E = \\lambda_1(\\phi)\\) and \\(\\phi_O=\\lambda_2(\\phi)\\) are functions of the mixture component parameter \\(\\phi\\). The former subset of parameters indexes the so called extrapolation distribution and cannot be identified from the data, i.e. the distribution of the missing values given the observed values, while the latter indexes the observed data distribution and is typically identifiable from the data. Assuming there exists a partition such that \\(\\phi_E=(\\phi_{EI},\\phi_{ENI})\\) and the observed data distribution is a function of \\(\\phi_{EI}\\) but not of \\(\\phi_{ENI}\\), then \\(\\phi_{ENI}\\) is a senstivity parameter in that it can only be identified using information from sources other than the observed data and thus makes a suitable basis to formulate sensitivity analysis using informative priors.\n\nExample of PMM for bivariate normal data\nConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A PMM factors the full data distribution as\n\\[\nf(y_1,y_2,r \\mid \\omega) = f(y_1, y_2 \\mid r, \\phi)f(r \\mid ,\\chi),\n\\]\nwhere, for example, we may have \\(Y \\mid R=1 \\sim N(\\mu^1,\\Sigma^1)\\), \\(Y \\mid R=0 \\sim N(\\mu^0,\\Sigma^0)\\) and \\(R \\sim Bern(\\chi)\\). We define \\(\\mu^r=(\\mu^r_1)\\), while \\(\\Sigma^r\\) has elements \\(\\sigma^r=(\\sigma^r_{11},\\sigma^r_{12},\\sigma^r_{22})\\). Similarly, we can define the parameters \\(\\beta^r_0\\), \\(\\beta^r_1\\) and \\(\\sigma^r_{2\\mid 1}\\) as the intercept, slope and residual variance of the regression of \\(Y_2\\) on \\(Y_1\\) for each pattern \\(r\\). Under this reparameterisation, the full data model parameters are\n\\[\n\\phi=\\{\\mu^r_1,\\sigma^r_{11},\\beta^r_0,\\beta^1_1,\\sigma^r_{2\\mid 1}\\}.\n\\]\nThe extrapolation and observed data distributions, with associated parameters, are then\n\\[\nf(y_{mis}\\mid y_{obs},\\phi_{E}) \\rightarrow \\phi_{E}=(\\beta^0_0, \\beta^0_1,\\sigma^0_{2\\mid1})\n\\]\nand\n\\[\nf(y_{obs}\\mid \\phi_{O}) \\rightarrow \\phi_{O}=(\\mu^1,\\beta^1,\\sigma^1_{11},\\mu^0_0,\\sigma^1_{11}).\n\\]\nIt can be shown that, in this specific example, the observed data distribution does not depend on the parameters indexing the extrapolation distribtuon \\(\\phi_{ENI}=(\\beta^0_0,\\beta^0_1,\\sigma^0_{2\\mid 1})\\). It is possible to set \\(\\beta^0=\\beta=1\\) and \\(\\sigma^0_{2\\mid1}=\\sigma^1_{2\\mid1}\\) to yield a Missing At Random(MAR) assumption. Hence, a function that maps identified parameters and sensitivity parameters \\(\\Delta\\) to the space of unidentified parameters can be used to quantify departures from MAR. For example, assume we impose\n\\[\n\\beta^0_0=\\beta^1_0+\\Delta,\n\\]\nthen assigning a point mass prior at \\(\\Delta=0\\) implies MAR, while fixing \\(\\Delta \\neq 0\\) or using any type of inofrmative prior on this parameter implies a Missing Not At Random(MNAR) assumption."
  },
  {
    "objectID": "missing_data/pmm/pmm.html#conlcusions",
    "href": "missing_data/pmm/pmm.html#conlcusions",
    "title": "Pattern Mixture Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, PMMs have the advantage of being able to find full data parameters indexing the distribution of the missing data that are not identified from the observed data, making inference more transparent. A potential downside is the practical implementation of these models which becomes more difficult as the number of patterns and unidentified parameters grows."
  },
  {
    "objectID": "missing_data/likinf_nig/likinf_nig.html",
    "href": "missing_data/likinf_nig/likinf_nig.html",
    "title": "Likelihood Based Inference with Incomplete Data (Nonignorable)",
    "section": "",
    "text": "In many cases, analysis methods for missing data are based on the ignorable likelihood\n\\[\nL_{ign}\\left(\\theta \\mid Y_0, X \\right) \\propto f\\left(Y_0 \\mid X, \\theta \\right),\n\\]\nregarded as a function of the parameters \\(\\theta\\) for fixed observed data \\(Y_0\\) and some fully observed covariates \\(X\\). The density \\(f(Y_0 \\mid X, \\theta)\\) is obtained by integrating out the missing data \\(Y_1\\) from the joint density \\(f(Y \\mid X, \\theta)=f(Y_0,Y_1\\mid X, \\theta)\\). Sufficient conditions for basing inference about \\(\\theta\\) on the ignorbale likelihood are that the missingness mechanism is Missing At Random(MAR) and the parameters of the model of analysis \\(\\theta\\) and those of the missingness mechanism \\(\\psi\\) are distinct. Here we focus our attention on the situations where the missingness mechanism is Missing Not At Random(MNAR) and valid Maximum Likelihood(ML), Bayesian and Multiple Imputation(MI) inferences generally need to be based on the full likelihood\n\\[\nL_{full}\\left(\\theta, \\psi \\mid Y_0, X, M \\right) \\propto f\\left(Y_0, M \\mid X, \\theta, \\psi \\right),\n\\]\nregarded as a function of \\((\\theta,\\psi)\\) for fixed \\((Y_0,M)\\). Here, \\(f(Y_0,M\\mid \\theta, \\psi)\\) is obtained by integrating out \\(Y_1\\) from the joint density \\(f(Y,M \\mid X, \\theta, \\psi)\\). Two main approaches for formulating MNAR models can be distinguished, namely selection models(SM) and pattern mixture models(PMM)."
  },
  {
    "objectID": "missing_data/likinf_nig/likinf_nig.html#selection-and-pattern-mixture-models",
    "href": "missing_data/likinf_nig/likinf_nig.html#selection-and-pattern-mixture-models",
    "title": "Likelihood Based Inference with Incomplete Data (Nonignorable)",
    "section": "Selection and Pattern Mixture Models",
    "text": "Selection and Pattern Mixture Models\nSMs factor the joint distribution of \\(m_i\\) and \\(y_i\\) as\n\\[\nf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, \\theta)f(m_i \\mid x_i,y_i,\\psi),\n\\]\nwhere the first factor is the distribution of \\(y_i\\) in the population while the second factor is the missingness mechanism, with \\(\\theta\\) and \\(\\psi\\) which are assumed to be distinct. Alternatively, PMMs factor the joint distribution as\n\\[\nf(m_i,y_i \\mid x_i, \\theta, \\psi) = f(y_i \\mid x_i, m_i,\\xi)f(m_i \\mid x_i),\n\\]\nwhere the first factor is the distribution of \\(y_i\\) in the strata defined by different patterns of missingness \\(m_i\\) while the second factor models the probabilities of the different patterns, with \\(\\xi\\) which are assumed to be distinct (Little (1993),Little and Rubin (2019)). The distinction between the two factorisations becomes clearer when considering a specific example.\nSuppose thta missing values are confined to a single variable and let \\(y_i=(y_{i,1},y_{i2})\\) be a bivariate response outcome where \\(y_{i1}\\) is fully observed and \\(y_{i2}\\) is observed for \\(i=1,\\ldots,n_{cc}\\) but missing for \\(i=n_{cc}+1,\\ldots,n\\). Let \\(m_{i2}\\) be the missingness indicator for \\(y_{i2}\\), then a PMM factors the denisty of \\(Y_0\\) and \\(M\\) given \\(X\\) as\n\\[\nf(y_0, M \\mid X, \\xi)=\\prod_{i=1}^{n_{cc}}f(y_{i1},y_{i2}\\mid x_i, m_{i2}=0,\\xi)Pr(m_{i2}=0 \\mid x_i, \\omega) \\times \\prod_{i=n_{cc}+1}^{n}f(y_{i1} \\mid x_i, m_{i2}=1,\\xi)Pr(m_{i2}=1 \\mid x_i, \\omega).\n\\]\nThis expression shows that there are no data with which to estimate directly the distribution \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)\\), because all units with \\(m_{i2}=1\\) have \\(y_{i2}\\) missing. Under MAR, this is identified using the distribution of the observed data \\(f(y_{i2} \\mid x_i, m_{i2}=1,\\xi)=f(y_{i2} \\mid x_i, m_{i2}=0,\\xi)\\), while under MNAR it must be identified using other assumptions. The SM formulation is\n\\[\nf(y_i, m_{i2} \\mid \\theta, \\psi) = f(y_{i1} \\mid x_i, \\theta)f(y_{i2} \\mid x_i, y_{i1},\\theta)f(m_{i2}\\mid x_i,y_{i1},y_{i2},\\psi).\n\\]\nTypically, the missingness mechanism \\(f(m_{i2} \\mid x_i,y_{i1},y_{i2},\\psi)\\) is modelled using some additive probit or logit regression of \\(m_{i2}\\) on \\(x_i\\),\\(y_{i1}\\) and \\(y_{i2}\\). However, the coefficient of \\(y_{i2}\\) in this regression is not directly estimable from the data and hence the model cannot be fully estimated without extra assumptions.\n\nNormal Models for MNAR data\nAssume we have a complete sample \\((y_i,x_i)\\) on a continuous variable \\(Y\\) and a set of fully observed covariates \\(X\\), for \\(i=1,\\ldots,n\\). Suppose that \\(i=1,\\ldots,n_{cc}\\) units are observed while the remaining \\(i=n_{cc}+1,\\ldots,n\\) units are missing, with \\(m_i\\) being the corresponding missingness indicator. Heckman (Heckman (1976)) proposed the following selection model to handle missingness:\n\\[\ny_i \\mid x_i, \\theta, \\psi \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2) \\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,y_i,\\theta,\\psi \\sim Bern\\left(\\Phi(\\psi_0 + \\psi_1x_i + \\psi_2y_i) \\right),\n\\]\nwhere \\(\\theta=(\\beta_0,\\beta_1,\\sigma^2)\\) and \\(\\Phi\\) denotes the probit (cumulative normal) distribution function. Note that if \\(\\psi_2=0\\), the missing data are MAR, while if \\(\\psi_2 \\neq 0\\) the missing data are MNAR since missingness in \\(Y\\) depends on the unobserved value of \\(Y\\). This model can be estimated using either a two-step least squares method, ML in combination with an EM algorithm, or a Bayesian approach. The main issue is the lack of information about \\(\\psi_2\\), which can be partly identified through the specific assumptions about the distribution of the observed data of \\(Y\\). This, however, makes the implicit assumption that the assumed distribution can well described the distribution of the complete (observed and missing) data which can never be tested or checked. An alternative approach is to use a PMM factorisation and model:\n\\[\ny_i \\mid m_i=m,x_i,\\xi,\\omega \\sim N(\\beta_0^m + \\beta_1^mx_i, \\sigma^{2m})\\;\\;\\; \\text{and} \\;\\;\\; m_i \\mid x_i,\\xi,\\omega \\sim Bern\\left(\\Phi(\\omega_0 + \\omega_1x_i) \\right),\n\\]\nwhere \\(\\xi=(\\beta_0^m,\\beta_1^m,\\sigma^{2m},\\;\\;\\; m=0,1)\\). This model implies that the distribution of \\(y_i\\) given \\(x_i\\) in the population is a mixture of two normal distributions with mean\n\\[\n\\left[1 - \\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^0 + \\beta_1^0 x_i \\right] + \\left[\\Phi(\\omega_0 + \\omega_1x_i) \\right] \\left[\\beta_0^1 + \\beta_1^1 x_i \\right].\n\\]\nThe parameters \\((\\beta_0^0,\\beta_1^0,\\sigma^{20},\\omega)\\) can be estimated from the data but the parameters \\((\\beta_0^1,\\beta_1^1,\\sigma^{21})\\) are not estimable because \\(y_i\\) is missing when \\(m_i=1\\). Under MAR, the distribution of \\(Y\\) given \\(X\\) is the same for units with \\(Y\\) observed and missing, such that \\(\\beta_0^0=\\beta_0^1=\\beta_0\\) (as well as for \\(\\beta_1\\) and \\(\\sigma^2\\)). Under MNAR, other assumptions are needed to esitmate the parameters indexed by \\(m=1\\).\nSome final considerations:\n\nBoth SM and PMM model the joint distribution of \\(Y\\) and \\(M\\).\nThe SM formulation is more natural when the substantive interest concerns the relationship between \\(Y\\) and \\(X\\) in the population. However, these parameters can also be derived in PMM by averaging the patterns specific parameters over the missingness patterns.\nThe PMM factorisation is more transparent in terms of the underlying assumptions about the unidentified parameters of the model, while SM tends to impose some obscure constraints in order to identify these parameters, which are also difficult to interpret.\nGiven specific assumptions to identify all the parameters in the model, PMMs are often easier to fit than SMs. In addition, imputations of the missing values are based on the predictive distribution of \\(Y\\) given \\(X\\) and \\(M=0\\).\n\nThese considerations seem to favour PMM over SM as MNAR approaches, especially when considering sensitivity analysis. Bayesian approaches can also be used to identify these models, by assigning prior distributions which can be used to identify those parameters which cannot be estimated from the data. Justifications for the choice of these priors are therefore necessary to ensure the plausibility of the assumptions assessed and the impact of these assumptions on the posterior inference."
  },
  {
    "objectID": "missing_data/jmi/jmi.html",
    "href": "missing_data/jmi/jmi.html",
    "title": "Joint Multiple Imputation",
    "section": "",
    "text": "Multiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example."
  },
  {
    "objectID": "missing_data/jmi/jmi.html#rubins-rules",
    "href": "missing_data/jmi/jmi.html#rubins-rules",
    "title": "Joint Multiple Imputation",
    "section": "Rubin’s rules",
    "text": "Rubin’s rules\nLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\n\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\n\\]\nBecause the imputations under MI are conditional draws, under a good imputation model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\n\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\n\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\nT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\n\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\n(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\n\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\n\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\n\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution."
  },
  {
    "objectID": "missing_data/jmi/jmi.html#joint-multiple-imputation",
    "href": "missing_data/jmi/jmi.html#joint-multiple-imputation",
    "title": "Joint Multiple Imputation",
    "section": "Joint Multiple Imputation",
    "text": "Joint Multiple Imputation\nJoint MI starts from the assumption that the data can be described by a multivariate distribution which in many cases, mostly for practical reasons, corresponds to assuming a multivariate Normal distribution. The general idea is that, for a general missing data pattern $ r$, missingness may occur anywhere in the multivariate outcome vector $ y=(y_1,,y_J)$, so that the distribution from which imputations should be drawn varies based on the observed variables in each pattern. For example, given $ r=(0,0,1,1)$, then imputations should be drawn from the bivariate distribution of the missing variables given the observed variables in that pattern, that is from \\(f(y^{mis}_1,y^{mis}_2 \\mid y^{obs}_3, y^{obs}_4, \\phi_{12})\\), where \\(\\phi_{12}\\) is the probability of being in pattern $ r$ where the first two variables are missing.\nConsider the multivariate Normal distribution \\(y \\sim N(\\mu,\\Sigma)\\), where \\(\\theta=(\\mu,\\Sigma)\\) represent the vector of the parameters of interest which need to be identified. Indeed, for non-monotone missing data, $ $ cannot be generally identified based on the observed data directly $ y^{obs}$, and the typical solution is to iterate imputation and parameter estimation using a general algorithm known as data augmentation(Tanner and Wong (1987)). Following Van Buuren (2018), the general procedure of the algorithm can be summarised as follows:\n\nDefine some plausible starting values for all parameters \\(\\theta_0=(\\mu_0,\\Sigma_0)\\)\nAt each iteration \\(t=1,\\ldots,T\\), draw \\(h=1,\\ldots,H\\) imputations for each missing value from the predictive distribution of the missing data given the observed data and the current value of the parameters at \\(t-1\\), that is\n\n\\[\n\\hat{y}^{mis}_{t} \\sim p(y^{mis} \\mid y^{obs},\\theta_{t-1})\n\\]\n\nRe-estimate the parameters \\(\\theta\\) using the observed and imputed data at \\(t\\) based on the multivariate Normal model, that is\n\n\\[\n\\hat{\\theta}_{t} \\sim p(\\theta \\mid y^{obs}, \\hat{y}^{mis}_{t})\n\\]\nAnd reiterate the steps 2 and 3 until convergence, where the stopping rule typically consists in imposing that the change in the parameters between iterations \\(t-1\\) and \\(t\\) should be smaller than a predefined “small” threshold \\(\\epsilon\\). Schafer (1997) showed that imputations generated under the multivariate Normal model can be robust to non-normal data, even though it is generally more efficient to transform the data towards normality, especially when the parameters of interest are difficult to estimate, such as quantiles and variances.\nThe multivariate Normal model is also often applied to categorical data, with different types of specifications that have been proposed in the literature (Schafer (1997),Horton, Lipsitz, and Parzen (2003),Allison (2005),Bernaards, Belin, and Schafer (2007),Yucel, He, and Zaslavsky (2008),Demirtas (2009)). For examples, missing data in contingency tables can be imputed using log-linear models (Schafer (1997)); mixed continuous-categorical data can be imputed under the general location model which combines a log-linear and multivariate Normal model (Olkin, Tate, et al. (1961)); two-way imputation can be applied to missing test item responses by imputing missing categorical data by conditioning on the row and column sum scores of the multivariate data (Van Ginkel et al. (2007))."
  },
  {
    "objectID": "missing_data/ipw/ipw.html",
    "href": "missing_data/ipw/ipw.html",
    "title": "Inverse Probability Weighting",
    "section": "",
    "text": "In certain cases, it is possible to reduce biases from case deletion by the application of weights. After incomplete cases are removed, the remaining complete cases can be weighted so that their distribution more closely resembles that of the full sample with respect to auxiliary variables. Weighting methods can eliminate bias due to differential response related to the variables used to model the response probabilities, but it cannot correct for biases related to variables that are unused or unmeasured (Little and Rubin (2019)). Robins, Rotnitzky, and Zhao (1994) introduced Inverse Probability Weighting (IPW) as a weighted regression approach that require an explicit model for the missingness but relaxes some of the parametric assumptions in the data model. Their method is an extension of Generalized Estimating Equations (GEE), a popular technique for modeling marginal or populationaveraged relationships between a response variable and predictors (Zeger, Liang, and Albert (1988)).\nLet \\(y_i=(y_{i1},\\ldots,y_{iK})\\) denote a vector of variables for unit \\(i\\) subject to missing values with \\(y_i\\) being fully observed for \\(i=1\\ldots,n_r\\) units and partially-observed for \\(i=n_r+1,\\ldots,n\\) units. Define \\(m_i=1\\) if \\(y_i\\) is incomplete and \\(m_i=0\\) if complete. Let \\(x_i=(x_{i1},\\ldots,x_{ip})\\) denote a vector of fully observed covariates and suppose the interest is in estimating the mean of the distribution of \\(y_i\\) given \\(x_i\\), having the form \\(g(x_i,\\beta)\\), where \\(g()\\) is a possibly non-linear regression function indexed by a parameter \\(\\beta\\) of dimension \\(d\\). Let also \\(z_i=(z_{i1},\\ldots,z_{iq})\\) be a vector of fully observed auxiliary variables that potentially predictive of missingness but are not included in the model for \\(y_i \\mid x_i\\). When there are no missing data, a consistent estimate of \\(\\beta\\) is given by the solution to the following GEE, under mild regularity conditions (Liang and Zeger (1986)),\n\\[\n\\sum_{i=1}^n = D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(D_i(x_i,\\beta)\\) is a suitably chosen \\((d\\times k)\\) matrix of known functions of \\(x_i\\). With missing data, the equation is applied only to the complete cases (\\(n_{r}\\)), which yields consistent estimates provided that\n\\[\np(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,\\phi),\n\\]\nthat is, missingness does not depend on \\(y_i\\) or \\(z_i\\) after conditioning on \\(x_i\\). IPW GEE methods (Robins and Rotnitzky (1995)) replace the equation with\n\\[\n\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) being an estimate of the probability of being a complete unit, obtained for example via logistic regressions on \\(m_i\\) on \\(x_i\\) and \\(z_i\\). If the logistic regression is correctly specified, IPW GEE yields a consistent estimator of \\(\\beta\\) provided that\n\\[\np(m_i=1 \\mid x_i,y_i,z_i,\\phi)=p(m_i=1\\mid x_i,z_i\\phi).\n\\]"
  },
  {
    "objectID": "missing_data/ipw/ipw.html#example",
    "href": "missing_data/ipw/ipw.html#example",
    "title": "Inverse Probability Weighting",
    "section": "Example",
    "text": "Example\nSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). If data were fully observed for \\(i=1,\\ldots,n\\) individuals, an obvious estimator of \\(\\mu\\) would be the sample outcome mean\n\\[\n\\bar{y}=\\frac{1}{n}\\sum_{i=1}^ny_i,\n\\]\nwhich is equivalent to the solution to the estimating equation \\(\\sum_{i=1}^n(y_i-\\mu)=0\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\np(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\n\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, the sample mean of the complete cases \\(\\bar{y}_{cc}=\\frac{\\sum_{i=1}^nc_iy_i}{c_i}\\), i.e. the solution to the equation \\(\\sum_{i=1}^nc_i(y_i-\\mu)=0\\), is not a consistent estimator of \\(\\mu\\). To correct for this, the IPW complete case estimating equation\n\\[\n\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i)}(y_i-\\mu)=0,\n\\]\ncan be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i)\\). The solution of the equation corresponds to the IPW estimator\n\\[\n\\mu_{ipw}=\\left(\\sum_{i=1}^n \\frac{c_i}{\\pi(z_i)} \\right)^{-1} \\sum_{i=1}^n \\frac{c_iy_i}{\\pi(z_i)},\n\\]\nwhich is unbiased under MAR and for \\(\\pi(z)&gt;0\\). In case you want to have a look at the proof of this I put here the link. In most situations \\(\\pi(z_i)\\) is not known and must be estimated from the data, typically posing some model for \\(p(c=1 \\mid z, \\hat{\\alpha})\\), indexed by some parameter \\(\\hat{\\alpha}\\), for example a logistic regression\n\\[\n\\text{logit}(\\pi)=\\alpha_0 + \\alpha_1z.\n\\]\nOf course, if the model for \\(\\pi(z)\\) is misspecified, \\(\\mu_{ipw}\\) can be an inconsistent estimator. In addition, IPW methods typically used data only from the completers discarding all the partially observed values, which is clearly inefficient."
  },
  {
    "objectID": "missing_data/ipw/ipw.html#conclusions",
    "href": "missing_data/ipw/ipw.html#conclusions",
    "title": "Inverse Probability Weighting",
    "section": "Conclusions",
    "text": "Conclusions\nThus, IPW estimators can correct for the bias of unweighted estimators due to the dependence of the missingness mechanism on \\(z_i\\) (Schafer and Graham (2002)). The basic intuition of IPW methods is that each subject’s contribution to the weighted Complete Case Analysis (CCA) is replicated \\(w_i\\) times in order to account once for herself and \\((1-w_i)\\) times for those subjects with the same responses and covariates who are missing. These models are called semiparametric because they apart from requiring the regression equation to have a specific form, they do not specify any probability distribution for the response variable (Molenberghs et al. (2014)). Older GEE methods can accommodate missing values only if they are Missing Completely At Random (MCAR), while more recent methods allow them to be MAR or even Missing Not At Random (MNAR), provided that a model for the missingness is correctly specified (Robins, Rotnitzky, and Zhao (1995),Rotnitzky, Robins, and Scharfstein (1998))."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html",
    "href": "missing_data/intro_bayes/intro_bayes.html",
    "title": "Introduction to Bayesian Inference",
    "section": "",
    "text": "Bayesian inference offers a convenient framework to analyse missing data as it draws no distinction between missing values and parameters, both interprted as unobserved quantities who are associated with a joint posterior distribution conditional on the observed data. In this section, I review basic concepts of Bayesian inference based on fully observed data, with notation and structure mostly taken from Gelman et al. (2013)."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#bayesian-inference-for-complete-data",
    "href": "missing_data/intro_bayes/intro_bayes.html#bayesian-inference-for-complete-data",
    "title": "Introduction to Bayesian Inference",
    "section": "Bayesian Inference for Complete Data",
    "text": "Bayesian Inference for Complete Data\nBayesian inference is the process of fitting a probability model to a set of data \\(Y\\) and summarising the results by a probability distribution on the parameters \\(\\theta\\) of the model and on unobserved quantities \\(\\tilde{Y}\\) (e.g. predictions). Indeed, Bayesian statistical conclusions about \\(\\theta\\) (or \\(\\tilde{Y}\\)) are made in terms of probability statements, conditional on the observed data \\(Y\\), typically indicated with the notation \\(p(\\theta \\mid y)\\) or \\(p(\\tilde{y} \\mid y)\\). Conditioning on the observed data is what makes Bayesian inference different from standard statistical approaches which are instead based on the retrospective evaluation of the procedures used to estimate \\(\\theta\\) (or \\(\\tilde{y}\\)) over the distribution of possible \\(y\\) values conditional on the “true” unknown value of \\(\\theta\\).\n\nBayes’ Rule\nIn order to make probability statements about \\(\\theta\\) given \\(y\\), we start with a model providing a joint probability distribution \\(p(\\theta,y)\\). Thus, the joint probability mass or density function can be written as a product of two densities that are often referred to as the prior distribution \\(p(\\theta)\\) and the sampling distribution \\(p(y \\mid \\theta)\\), respectively:\n\\[\np(\\theta,y) = p(\\theta)p(y \\mid \\theta),\n\\]\nand conditioning on the observed values of \\(y\\), using the basic property of conditional probability known as Bayes’ rule, yields the posterior distribution\n\\[\np(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(\\theta)p(y \\mid \\theta)}{p(y)},\n\\]\nwhere \\(p(y)=\\sum_{\\theta \\in \\Theta}p(\\theta)p(y\\mid \\theta)\\) is the sum (or integral in the case of continous \\(\\theta\\)) over all possible values of \\(\\theta\\) in the sample space \\(\\Theta\\). We can approximate the above equation by omitting the factor \\(p(y)\\) which does not depend on \\(\\theta\\) and, given \\(y\\), can be considered as fixed, yielding the unnormalised posterior density\n\\[\np(\\theta \\mid y) \\propto p(\\theta) p(y \\mid \\theta),\n\\]\nwith the purpose of the analysis being to develop the model \\(p(\\theta,y)\\) and adequately summarise \\(p(\\theta \\mid y)\\).\n\n\nUnivariate Normal Example (known variance)\nLet \\(y=(y_1,\\ldots,y_n)\\) denote an independent and identially distributed sample of \\(n\\) units, which are assumed to come from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), whose sampling density function is\n\\[\np(y \\mid \\mu)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nwhere for the moment we assume the variance \\(\\sigma^2\\) to be known (i.e. constant). Consider now a prior probability distribution for the mean parameter \\(p(\\mu)\\), which belongs to the family of conjugate prior densities, for example a Normal distribution, and parameterised in terms of a prior mean \\(\\mu_0\\) and variance \\(\\sigma^2_0\\). Thus, its prior density function is\n\\[\np(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\text{exp}\\left(-\\frac{1}{2}\\frac{(\\mu -\\mu_0)^2}{\\sigma^2} \\right),\n\\]\nunder the assumption tha the hyperparameters \\(\\mu_0\\) and \\(\\sigma^2_0\\) are known. The conjugate prior density implies that the posterior distribution for \\(\\mu\\) (with \\(\\sigma^2\\) assumed constant) belongs to the same family of distributions of the sampling function, that is Normal, but some algebra is required to reveal its specific form. In particular, the posterior density is\n\\[\np(\\mu \\mid y) = \\frac{p(\\mu)p(y\\mid \\mu)}{p(y)} \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}}\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2} \\left[\\frac{(\\mu - \\mu_0)^2}{\\sigma^2_0} + \\sum_{i=1}^n\\frac{(y_i-\\mu)^2}{\\sigma^2} \\right] \\right).\n\\]\nExapanding the components, collecting terms and completing the square in \\(\\mu\\) gives\n\\[\np(\\mu \\mid y) \\propto \\text{exp}\\left(-\\frac{(\\mu - \\mu_1)}{2\\tau^2_1} \\right),\n\\]\nthat is the posterior distribution of \\(\\mu\\) given \\(y\\) is Normal with posterior mean \\(\\mu_1\\) and variance \\(\\tau^2_1\\), where\n\\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2_0}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}} \\;\\;\\; \\text{and} \\;\\;\\; \\frac{1}{\\tau^2_1}=\\frac{1}{\\tau^2_0} + \\frac{n}{\\sigma^2}.\n\\]\nWe can see that the posterior distribution depends on \\(y\\) only through the sample mean \\(\\bar{y}=\\sum_{i=1}^ny_i\\), which is a sufficient statistic in this model. When working with Normal distributions, the inverse of the variance plays a prominent role and is called the precision and, from the above expressions, it can be seen that for normal data and prior, the posterior precision \\(\\frac{1}{\\tau^2_1}\\) equals the sum of the prior precision \\(\\frac{1}{\\tau^2_0}\\) and the sampling precision \\(\\frac{n}{\\sigma^2}\\). Thus, when \\(n\\) is large, the posterior precision is largely dominated by \\(\\sigma^2\\) and the sample mean \\(\\bar{y}\\) compared to the corresponding prior parameters. In the specific case where \\(\\tau^2_0=\\sigma^2\\), the prior has the same weight as one extra observation with the value of \\(\\mu_0\\) and, as \\(n\\rightarrow\\infty\\), we have that \\(p(\\mu\\mid y)\\approx N\\left(\\mu \\mid \\bar{y},\\frac{\\sigma^2}{n}\\right)\\).\n\n\nUnivariate Normal Example (unknown variance)\nFor \\(p(y \\mid \\mu,\\sigma^2)=N(y \\mid \\mu, \\sigma^2)\\) with \\(\\mu\\) known and \\(\\sigma^2\\) unknown, the sampling distribution for a vector \\(y\\) of \\(n\\) units is\n\\[\np(y \\mid \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nwith the corresponding conjugate prior for \\(\\sigma^2\\) being the Inverse-Gamma distribution \\(\\Gamma^{-1}(\\alpha,\\beta)\\) with density function\n\\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha+1)}\\text{exp}\\left(-\\frac{\\beta}{\\sigma^2}\\right),\n\\]\nindexed by the hyperparameters \\(\\alpha\\) and \\(\\beta\\). A convenient parameterisation is as a Scaled Inverse-Chi Squared distribution \\(\\text{Inv-}\\chi^2(\\sigma^2_0,\\nu_0)\\) with scale and degrees of freedom parameters \\(\\sigma^2_0\\) and \\(\\nu_0\\), respectively. This means that the prior on \\(\\sigma^2\\) corresponds to the distribution of \\(\\frac{\\sigma^2_0 \\nu_0}{X}\\), where \\(X\\sim \\chi^2_{\\nu_0}\\) random variable. After some calculations, the resulting posterior for \\(\\sigma^2\\) is\n\\[\np(\\sigma^2 \\mid y) \\propto (\\sigma^2)^\\left(\\frac{n+\\nu_0}{2}+1\\right)\\text{exp}\\left(-\\frac{\\nu_0 \\sigma^2_0 + n \\nu}{2\\sigma^2} \\right)\n\\]\nwhere \\(\\nu=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\mu)^2\\). This corresponds to say that\n\\[\n\\sigma^2 \\mid y \\sim \\text{Inv-}\\chi^2\\left(\\nu_0 +n, \\frac{\\nu_0\\sigma^2_0+n\\nu}{\\nu_0 + n} \\right),\n\\]\nwith scale equal to the degrees of freedom-weighted average of the prior and data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.\n\n\nUnivariate Normal Example (unknown mean and variance)\nSuppose now that both the mean and variance parameters are unknown such that\n\\[\np(y \\mid \\mu, \\sigma^2) \\sim N(\\mu, \\sigma^2),\n\\]\nand that the interest is centred on making inference about \\(\\mu\\), that is we seek the conditional posterior distribution of the parameters of interest given the observed data \\(p(\\mu \\mid y)\\). This can be derived from the joint posterior distribution density \\(p(\\mu, \\sigma^2 \\mid y)\\) by averaging over all possible values of \\(\\sigma^2\\), that is\n\\[\np(\\mu \\mid y)=\\int p(\\mu, \\sigma^2 \\mid y)d\\sigma^2,\n\\]\nor, alternatively, the joint posterior can be factored as the product of the marginal distribution of one parameter and the conditional distribution of the other given the former and then taking the average over the values of the “nuisance” parameter\n\\[\np(\\mu \\mid y)=\\int p(\\mu \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)d\\sigma^2.\n\\]\nThe integral forms are rarely computed in practice but this expression helps us to understand that posterior distributions can be expressed in terms of the product of marginal and conditional densities, first drawing \\(\\sigma^2\\) from its marginal and then \\(\\mu\\) from its conditional given the drawn value of \\(\\sigma^2\\), so that the integration is indirectly performed. For example, consider the Normal model with both unknown mean and variance and assume a vague prior density \\(p(\\mu,\\sigma^2)\\propto (\\sigma^2)^{-1}\\) (corresponding to uniform prior on \\((\\mu, \\log\\sigma)\\)), then the joint posterior distribution is proportional to the sampling distribution multiplied by the factor \\((\\sigma^2)^{-1}\\), that is\n\\[\np(\\mu,\\sigma^2 \\mid y)\\propto \\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right),\n\\]\nwhere \\(s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2\\) is the sample variance. Next, the conditional posterior density \\(p(\\mu \\mid \\sigma^2)\\) can be shown to be equal to\n\\[\np(\\mu \\mid \\sigma^2,y) \\sim N(\\bar{y},\\frac{\\sigma^2}{n}),\n\\]\nwhile the marginal posterior \\(p(\\sigma^2 \\mid y)\\) can be obtained by averaging the joint \\(p(\\mu,\\sigma^2\\mid y)\\) over \\(\\mu\\), that is\n\\[\np(\\sigma^2 \\mid y)\\propto \\int \\left(\\sigma^{-n-2}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}\\left[(n-1)s^2+n(\\bar{y}-\\mu)^2 \\right] \\right)\\right)d\\mu,\n\\]\nwhich leads to\n\\[\np(\\sigma^2 \\mid ,y) \\sim \\text{Inv-}\\chi^2(n-1,s^2).\n\\]\nTypically, \\(\\mu\\) represents the estimand of interest and the obejective of the analysis is therefore to make inference about the marginal distribution \\(p(\\mu \\mid y)\\), which can be obtained by integrating \\(\\sigma^2\\) out of the joint posterior\n\\[\np(\\mu \\mid y)=\\int_{0}^{\\infty}p(\\mu,\\sigma^2\\mid y)d\\sigma^2 \\propto \\left[1+\\frac{n(\\mu-\\bar{y})}{(n-1)s^2} \\right]\n\\]\nwhich corresponds to a Student-\\(t\\) density with \\(n-1\\) degrees of freedom\n\\[\np(\\mu \\mid y)\\sim t_{n-1}\\left(\\bar{y},\\frac{s^2}{n}\\right)\n\\]\n\n\nMultivariate Normal Example\nSimilar considerations to those applied to the univariate case can be extended to the multivariate case when \\(y\\) is formed by \\(J\\) components coming from the Multivariate Normal distribution\n\\[\np(y\\mid \\mu, \\Sigma) \\sim N(\\mu, \\Sigma),\n\\]\nwhere \\(\\mu\\) is a vector of length \\(J\\) and \\(\\Sigma\\) is a \\(J\\times J\\) covariance matrix, which is symmetric and positive definite. The sampling distribution for a sample of \\(n\\) units is\n\\[\np(y\\mid \\mu, \\Sigma) \\propto \\mid \\Sigma \\mid^{-n/2}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n(y_i-\\mu)^{T}\\Sigma^{-1}(y_i-\\mu) \\right),\n\\]\nAs with the univariate normal model, we can derive the posterior distribution for \\(\\mu\\) and \\(\\Sigma\\) according to the factorisation used of the joint posterior and the prior distributions specified. For example, using the conjugate normal prior for the mean \\(p(\\mu)\\sim N(\\mu_0,\\Sigma_0)\\), given \\(\\Sigma\\) known, the posterior can be shown to be\n\\[\np(\\mu \\mid y) \\sim N(\\mu_1,\\Sigma_1),\n\\]\nwhere the posterior mean is a weighted average of the data and prior mean with weights given by the data and prior precision matrices \\(\\mu_1=(\\Sigma^{-1}_0+n\\Sigma^{-1})^{-1} (\\Sigma_0^{-1}\\mu_0 + n\\Sigma^{-1}\\bar{y})\\), and the posterior precision is the sum of the data and prior precisions \\(\\Sigma^{-1}_1=\\Sigma^{-1}_0+n\\Sigma^{-1}\\).\nIn the situation in which both \\(\\mu\\) and \\(\\Sigma\\) are unknown, convenient conjugate prior distributions which generalise those used in the univariate case are the Inverse-Wishart for the covariance matrix \\(\\Sigma\\sim \\text{Inv-Wishart}(\\Lambda_0,\\nu_0)\\) and the Multivariate Normal for the mean \\(\\mu\\sim N(\\mu_0, \\Sigma_0)\\), where \\(\\nu_0\\) and \\(\\Lambda_0\\) represent the degrees of freedom and the scale matrix for the Inverse-Wishart distribution, while \\(\\mu_0\\) and \\(\\Sigma_0=\\frac{\\Sigma}{\\kappa_0}\\) are the prior mean and covariance matrix for the Multivariate Normal. Woking out the form of the posterior, it can be shown that the joint posterior distribution has the same form of the sampling distribution with parameters\n\\[\np(\\mu \\mid \\Sigma, y) \\sim N(\\mu_1,\\Sigma_1) \\;\\;\\; \\text{and} \\;\\;\\; p(\\Sigma \\mid y) \\sim \\text{Inv-Wishart}(\\Lambda_1,\\nu_1),\n\\]\nwhere \\(\\Sigma_1=\\frac{\\Sigma}{\\kappa_1}\\), \\(\\mu_1=\\frac{1}{\\kappa_0+n}\\mu_0+\\frac{n}{\\kappa_0+n}\\bar{y}\\), \\(\\kappa_1=\\kappa_0+n\\), \\(\\nu_1=\\nu_0+n\\), and \\(\\Lambda_1=\\Lambda_0+\\sum_{i=1}^n(y_i-\\bar{y})(y_i-\\bar{y})^T+\\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)(\\bar{y}-\\mu_0)^2\\)."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#regression-models",
    "href": "missing_data/intro_bayes/intro_bayes.html#regression-models",
    "title": "Introduction to Bayesian Inference",
    "section": "Regression Models",
    "text": "Regression Models\nSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(X=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\)\n\\[\np(y \\mid \\beta,\\sigma^2,X) \\sim N(X\\beta,\\sigma^2I),\n\\]\nwhere \\(\\beta=(\\beta_0,\\ldots,\\beta_J)\\) is the set of regression coefficients and \\(I\\) is the \\(n\\times n\\) identity matrix. Within the normal regression model, a convenient vague prior distribution is uniform on \\((\\beta,\\log\\sigma)\\)\n\\[\np(\\beta,\\sigma^2)\\propto\\sigma^{-2}.\n\\]\nAs with normal distributions with unknown mean and variance we can first determine the marginal posterior of \\(\\sigma^2\\) and factor the joint posterior as \\(p(\\beta,\\sigma^2)=p(\\beta \\mid \\sigma^2, y)p(\\sigma^2 \\mid y)\\) (omit X for simplicity). Then, the conditional distribtuion \\(p(\\beta \\mid \\sigma^2,y)\\) is Normal\n\\[\np(\\beta \\mid \\sigma^2, y) \\sim N(\\hat{\\beta},V_{\\beta}\\sigma^2),\n\\]\nwhere \\(\\hat{\\beta}=(X^{T}X)^{-1}(X^{T}y)\\) and \\(V_{\\beta}=(X^{T}X)^{-1}\\). The marginal posterior \\(p(\\sigma^2 \\mid y)\\) has a scaled Inverse-\\(\\chi^2\\) form\n\\[\np(\\sigma^2\\mid y) \\sim \\text{Inv-}\\chi^2(n-J,s^2),\n\\]\nwhere \\(s^2=\\frac{1}{n-J}(y-X\\hat{\\beta})^{T}(y-X\\hat{\\beta})\\). Finally, the marginal posterior \\(p(\\beta \\mid y)\\), averaging over \\(\\sigma^2\\), is multivariate \\(t\\) with \\(n-J\\) degrees of freedom, even though in practice since we can characterise the joint posterior by drawing from \\(p(\\sigma^2)\\) and then from \\(p(\\beta \\mid \\sigma^2)\\). When the analysis is based on improper priors (do not have finite integral), it is important to check that the posterior is proper. In the case of the regression model, the posterior for \\(\\beta \\mid \\sigma^2\\) is proper only if the number of observations is larger than the number of parameters \\(n&gt;J\\), and that the rank of \\(X\\) equals \\(J\\) (i.e. the columns of \\(X\\) are linearly independent) in order for all \\(J\\) coefficients to be uniquely identified from the data."
  },
  {
    "objectID": "missing_data/intro_bayes/intro_bayes.html#generalised-linear-models",
    "href": "missing_data/intro_bayes/intro_bayes.html#generalised-linear-models",
    "title": "Introduction to Bayesian Inference",
    "section": "Generalised Linear Models",
    "text": "Generalised Linear Models\nThe purpose of Generalised Linear Models(GLM) is to extend the idea of linear modelling to cases for which the linear relationship between \\(X\\) and \\(E[y\\mid X]\\) or the Normal distribution is not appropriate. GLMs are specified in three stages\n\nChoose the linear predictor \\(\\eta=X\\beta\\)\nChoose the link fuction \\(g()\\) that relates the linear predictor to the mean of the outcome variable \\(\\mu=g^{-1}(\\eta)\\)\nChoose the random component specifying the distribution of \\(y\\) with mean \\(E[y\\mid X]\\)\n\nThus, the mean of the distribution of \\(y\\) given \\(X\\) is determined as \\(E[y\\mid X]=g^{-1}(X\\beta)\\). The Normal linear model can be thought as a special case of GLMs where the link function is the identity \\(g(\\mu)=\\mu\\) and the random component is normally distributed. Perhaps, the most commonly used GLMs are those based on Poisson and Binomial distributions to analyse count and binary data, respectively.\n\nPoisson\nCounted data are often modelled using Poisson regression models which assume that \\(y\\) is distributed according to a Poisson distribution with mean \\(\\mu\\). The link function is typically chosen to be the logarithm so that \\(\\log \\mu = X\\beta\\) and the distribution of the data has density\n\\[\np(y\\mid \\beta)=\\prod_{i=1}^n \\frac{1}{y_i}\\text{exp}\\left(-\\text{e}^{(\\eta_i)}(\\text{exp}(\\eta_i))^{y_i}\\right),\n\\]\nwhere \\(\\eta_i=(X\\beta)_i\\) is the linear predictor for the \\(i-\\)th unit.\n\n\nBinomial\nSuppose there are some binomial data \\(y_i \\sim \\text{Bin}(n_i,\\mu_i)\\), with \\(n_i\\) known. It is common to specify the model in terms of the mean of the proportions \\(\\frac{y_i}{n_i}\\) rather than the mean of \\(y_i\\). Choosing the logit tranformation of the probability of success \\(g(\\mu_i)=\\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right)\\) as the link function leads to the logistic regression where data have distribution\n\\[\np(y \\mid \\beta)=\\prod_{i=1}^n {n_i \\choose y_i} {e^{\\eta_i} \\choose 1+e^{\\eta_i}}^{y_i} {1 \\choose 1+e^{\\eta_i}}^{n_i-y_i}.\n\\]\nThe link functions used in the previous models are known as the canonical link functions for each family of distributions, which is the function of the mean parameter that appears in the exponent of the exponential family form of the probability density. However, it is also possible to use link functions which are not canonical."
  },
  {
    "objectID": "missing_data/em/em.html",
    "href": "missing_data/em/em.html",
    "title": "Expectation Maximisation Algorithm",
    "section": "",
    "text": "Patterns of incomplete data in practice often do not have the forms that allow explicit Maximum Likelihood(ML) estimates to be calculated. Suppose we have a model for the complete data \\(Y\\), with density \\(f(Y\\mid \\theta)\\), indexed by the set of unknown parameters \\(\\theta\\). Writing \\(Y=(Y_0,Y_1)\\) in terms of the observed \\(Y_0\\) and missing \\(Y_1\\) components, and assuming that the missingness mechanism is Missing At Random(MAR), we want to maximise the likelihood\n\\[\nL\\left(\\theta \\mid Y_0 \\right) = \\int f\\left(Y_0, Y_1 \\mid \\theta  \\right)dY_1\n\\]\nwith respect to \\(\\theta\\). When the likelihood is differentiable and unimodal, ML estimates can be found by solving the likelihood equation\n\\[\nD_l\\left(\\theta \\mid Y_0 \\right) \\equiv \\frac{\\partial ln L\\left(\\theta \\mid Y_0 \\right)}{\\partial \\theta} = 0,\n\\]\nwhile, if a closed-form solution cannot be found, iterative methods can be applied. One of these methods is the popular Expectation Maximisation(EM) algorithm (Dempster, Laird, and Rubin (1977)).\nThe EM algorithm is a general iterative method for ML estimation in incomplete data problems. The basic idea behind it is based on a sequence of steps:\nThe procedure is then iterated until apparent convergence. Each iteration of EM consists of an expectation step (E step) and a maximisation step (M step) which ensure that, under general conditions, each iteration increases the loglikelihood \\(l(\\theta \\mid Y_0)\\). In addition, if the loglikelihood is bounded, the sequence \\(\\{l(\\theta_t \\mid Y_0), t=(0,1,\\ldots)\\}\\) converges to a stationary value of \\(l(\\theta \\mid Y_0)\\)."
  },
  {
    "objectID": "missing_data/em/em.html#the-e-step-and-the-m-step",
    "href": "missing_data/em/em.html#the-e-step-and-the-m-step",
    "title": "Expectation Maximisation Algorithm",
    "section": "The E step and the M step",
    "text": "The E step and the M step\nThe M step simply consists of performing ML estimation of \\(\\theta\\) as if there were no missing data, that is, after they had been filled in. The E step finds the conditional expectation of the missing values given the observed data and current estimated parameters. In practice, EM does not necessarily substitute the missing values themselves but its key idea is that they are generally not \\(Y_0\\) but the functions of \\(Y_0\\) appearing in the complete data loglikelihood \\(l(\\theta \\mid Y)\\). Specifically, let \\(\\theta_t\\) be the current estimate of \\(\\theta\\), then the E step finds the expected complete data loglikelihood if \\(\\theta\\) were \\(\\theta_t\\):\n\\[\nQ\\left(\\theta \\mid \\theta_t \\right) = \\int l\\left(\\theta \\mid Y \\right)f\\left(Y_0  \\mid Y_1 , \\theta = \\theta_t \\right)dY_0.\n\\]\nThe M step determines \\(\\theta_{t+1}\\) by maximising this expected complete data loglikelihood:\n\\[\nQ\\left(\\theta_{t+1} \\mid \\theta_t \\right) \\geq Q\\left(\\theta \\mid \\theta_t \\right),\n\\]\nfor all \\(\\theta\\).\n\nUnivariate Normal Data Example\nSuppose \\(y_i\\) form a an iid sample from a Normal distribution with population mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,\\ldots,n_{cc}\\) observed units and \\(i=n_{cc}+1,\\ldots,n\\) missing units. Under the assumption that the missingness mechanism is ignorable, the expectation of each missing \\(y_i\\) given \\(Y_{obs}\\) and \\(\\theta=(\\mu,\\sigma^2)\\) is \\(\\mu\\). Since the loglikelihood based on all \\(y_i\\) is linear in the sufficient statistics \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1}^n y^2_i\\), the E step of the algorithm calculates\n\\[\nE\\left(\\sum_{i=1}^{n}y_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y_i + (n-n_{cc})\\mu_t\n\\]\nand\n\\[\nE\\left(\\sum_{i=1}^{n}y^2_i \\mid \\theta_t, Y_0 \\right) = \\sum_{i=1}^{n_{cc}}y^2_i + (n-n_{cc})\\left(\\mu^2_t + \\sigma^2_t \\right)\n\\]\nfor current estimates \\(\\theta_t=(\\mu_t,\\sigma_t)\\) of the parameters. Note that simply substituting \\(\\mu_t\\) for the missing values \\(y_{n_{cc}+1},\\ldots,y_n\\) is not correct since the term \\((n-n_{cc})(\\sigma_t^2)\\) is omitted. Without missing data, the ML estimate of \\(\\mu\\) and \\(\\sigma^2\\) are \\(\\frac{\\sum_{i=1}^ny_i}{n}\\) and \\(\\frac{\\sum_{i=1}^ny^2_i}{n}-\\left(\\frac{\\sum_{i=1}^ny_i}{n}\\right)^2\\), respectively. The M step uses the same expressions based on the current expectations of the sufficient statistics calculated in the E step. Thus, the M step calculates\n\\[\n\\mu_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y_i \\mid \\theta_t, Y_0 \\right)}{n}\n\\]\nand\n\\[\n\\sigma^2_{t+1} = \\frac{E\\left(\\sum_{i=1}^n y^2_i \\mid \\theta_t, Y_0 \\right)}{n} - \\mu^2_{t+1}.\n\\]\nSetting \\(\\mu_t=\\mu_{t+1}=\\hat{\\mu}\\) and \\(\\sigma_t=\\sigma_{t+1}=\\hat{\\sigma}\\) in these equations shows that a fixed point of these iterations is \\(\\hat{\\mu}=\\frac{\\sum_{i=1}^{n_{cc}}y_i}{n_{cc}}\\) and \\(\\hat{\\sigma}^2=\\frac{\\sum_{i=1}^{n_{cc}}y^2_i}{n_{cc}} - \\hat{\\mu}^2\\), which are the ML estimates of the parameters from \\(Y_0\\) assuming MAR and distinctness of the parameters."
  },
  {
    "objectID": "missing_data/em/em.html#extensions-of-em",
    "href": "missing_data/em/em.html#extensions-of-em",
    "title": "Expectation Maximisation Algorithm",
    "section": "Extensions of EM",
    "text": "Extensions of EM\nThere are a variety of applications where the M step does not have a simple computational form. In such cases, one way to avoid an iterative M step is to increase the Q function, rather than maximising it at each iteration, which corresponds to a Generalised Expectation Maximisation(GEM) algorithm. GEM inceases the likelihood at each iteration but appropriate convergence is not guaranteed without further specification of the process of increasing the Q function. One specific case of GEM is the Expectation Conditional Maximisation(ECM) algorithm (Meng and Rubin (1993)), which replaces the M step with a sequence of \\(S\\) conditional maximisation (CM) steps, each of which maximises the Q function over \\(\\theta\\) but with some vector function of \\(\\theta\\), say \\(g_s(\\theta)\\), fixed at its previous values for \\(s=1,\\ldots,S\\). Very briefly, assume that we have a parameter \\(\\theta\\) that can be partitioned into subvectors \\(\\theta=(\\theta_1,\\ldots,\\theta_S)\\), then we can take the \\(s\\)-th of the CM steps to be maximisation with respect to \\(\\theta_s\\) with all other parameters held fixed. Alternatively, it may be useful to take the \\(s\\)-th of the CM steps to be simultaneous maximisation over all of the subvectors expect \\(\\theta_s\\), which is fixed. Because the ECM increases Q, it belongs to the class of GEM algorithms and therefore monotonically increases the likelihood of \\(\\theta\\). When the set of functions \\(g\\) is “space filling” in the sense that it allows unconstrained maximisation over \\(\\theta\\) in its parameter space, ECM converges to a stationary point under the same conditions ensuring convergence of EM.\nThe Expectation Conditional Maximisation Either(ECME) algorithm (Liu and Rubin (1994)) is another version of GEM, which replaces some of the CM steps of ECM, maximising the constrained expected complete data loglikelihood function, with steps that maximise the correspondingly constrained actual likelihood function. The algorithm has stable monotone convergence and basic simplicity implementation relative to competing faster converging methods, and can have faster convergence rate than EM or ECM, measured using either the number of iterations or actual computer time. The The Alternative Expectation Conditional Maximisation(AECM) algorithm (Meng and Van Dyk (1997)) builds on the ECME idea by maximising functions other than Q or L in particular CM steps, corresponding to varying definitions of what constitutes missing data. An iteration of AECM consists of cycles, each consisting of an E step with a particular definition of complete and missing data, followed by CM steps, which can result in enhanced computational efficiency."
  },
  {
    "objectID": "missing_data/bis/bis.html",
    "href": "missing_data/bis/bis.html",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "",
    "text": "A useful alternative approach to Maximum Likelihood(ML) methods, particularly when the sample size is small, is to include a reasonable prior distribution for the parameters and compute the posterior distribution of the parameters of interest. The posterior distribution for a model with ignorable missingness is\n\\[\np(\\theta \\mid Y_0, M) \\equiv p(\\theta \\mid Y_0) \\propto p(\\theta)f(Y_0 \\mid \\theta),\n\\]\nwhere \\(p(\\theta)\\) is the prior and \\(f(Y_0 \\mid \\theta)\\) is the density of the observed data \\(Y_0\\). Simulation from the posterior without iteration can be accomplished if the likelihood can be factored into complete data components, while for general patterns of missing data, Bayesian simulation requires iteration."
  },
  {
    "objectID": "missing_data/bis/bis.html#data-augmentation",
    "href": "missing_data/bis/bis.html#data-augmentation",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData Augmentation(Tanner and Wong (1987)), or DA, is an iterative method of simulating the posteiror distribution of \\(\\theta\\) that combines features of the Expecation Maximisation(EM) algorithm and Multiple Imputation(MI). Starting with an initial draw \\(\\theta_0\\) from an approximation to the posterior, then given the value \\(\\theta_t\\) at iteration \\(t\\):\n\nDraw \\(Y_{1,t+1}\\) with density \\(p(Y_1 \\mid Y_0, \\theta_t)\\) (I step).\nDraw \\(\\theta_{t+1}\\) with density \\(p(\\theta \\mid Y_0, Y_{1,t+1})\\) (P step).\n\nThe procedure is motivated by the fact that the distributions in these two steps are often much easier to draw from than either of the posteriors \\(p(Y_1 \\mid Y_0)\\) and \\(p(\\theta \\mid Y_0)\\), or the joint posterior \\(p(\\theta, Y_1 \\mid Y_0)\\). The procedure can be shown to eventually yield a draw from the joint posterior of \\(Y_1\\) and \\(\\theta\\) given \\(Y_0\\), in the sense that as \\(t\\) tends to infinity this sequence converges to a draw from the joint distribution.\n\nBivariate Normal Data Example\nSuppose having a sample \\(y_i=(y_{1i},y_{2i})\\) from a Bivariate Normal distribution for \\(i=1,\\ldots,n\\) units, with mean vector \\(\\mu=(\\mu_1,\\mu_2)\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). Assume that one group of units has \\(Y_1\\) observed and \\(Y_2\\) missing, while a second group of units has both variables observed and a third group of units has \\(Y_1\\) missing and \\(Y_2\\) observed. Under DA methods, each iteration \\(t\\) consists of an I step and a P step. In the first, missing data are replaced with draws from its conditional distribution given the observed data and current values of the parameters (rather then its conditional mean as in the EM algorithm). Because units are conditionally independent given the parameters, each missing \\(y_{2i}\\) is drawn independently as\n\\[\ny_{2i,t+1} \\sim N\\left(\\beta_{20t} + \\beta_{21t}y_{1i}, \\sigma^2_{2t}  \\right),\n\\]\nwhere \\(\\beta_{20t},\\beta_{21t}\\) and \\(\\sigma^2_{2t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_2\\) on \\(Y_1\\). Analogously, each missing \\(y_{1i}\\) is drawn independently as\n\\[\ny_{1i,t+1} \\sim N\\left(\\beta_{10t} + \\beta_{11t}y_{2i}, \\sigma^2_{1t}  \\right),\n\\]\nwhere \\(\\beta_{10t},\\beta_{11t}\\) and \\(\\sigma^2_{1t}\\) are the \\(t\\)-th iterates of the regression parameters of \\(Y_1\\) on \\(Y_2\\). In the second step, these drawn values are treated as if they were the observed values and one draw of the bivariate Normal parameters is made from the complete data posterior. In the limit, the draws are from the joint posterior of the missing values and the parameters. Thus, a run of DA generates both a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\), and the procedure can be run \\(D\\) times to obtain \\(D\\) iid draws from the joint posterior of \\(\\theta\\) and \\(Y_1\\). Unlike the EM, estimates of the sampling covariance matrix from the filled-in data can be computed without any corrections to the estimated variances because draws from the posterior predictive distribution of the missing values are imputed in the I step of DA, rather than the conditional means as in the E step of EM. The loss of efficiency from imputing draws is limited when the posterior mean from DA is computed over many draws from the posterior."
  },
  {
    "objectID": "missing_data/bis/bis.html#the-gibbs-sampler",
    "href": "missing_data/bis/bis.html#the-gibbs-sampler",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "The Gibbs’ Sampler",
    "text": "The Gibbs’ Sampler\nThe Gibbs’s sampler is an iterative simulation method that is designed to yield draws from the joint posterior distribution in the case of a general pattern of missingness and provides a Bayesian analogous to the Expectation Conditonal Maximisation (ECM) algorithm for ML estimation. The Gibbs’ sampler eventually generates a draw from the distribution \\(p(x_1,\\ldots,x_J)\\) of a set of \\(J\\) random variables \\(X_1,\\ldots,X_J\\) in settings where draws from the joint distribution are hard to compute but draws from the conditional distributions \\(p(x_j \\mid x_1,\\ldots,x_{j-1},x_{j+1},\\ldots, x_J)\\) are relatively easy to compute. Initial values \\(x_{10},\\ldots,x_{J0}\\) are chosen in some way and then, given current values of \\(x_{1t},\\ldots,x_{Jt}\\) at iteration \\(t\\), new values are found by drawing from the following sequence of conditional distributions:\n\\[\nx_{1t+1} \\sim p\\left(x_1 \\mid x_{2t},\\ldots,x_{Jt} \\right),\n\\]\n\\[\nx_{2t+1} \\sim p\\left(x_2 \\mid x_{1t+1},\\ldots,x_{Jt} \\right),\n\\]\nup to\n\\[\nx_{Jt+1} \\sim p\\left(x_J \\mid x_{2t+1},\\ldots,x_{J-1t+1} \\right).\n\\]\nIt can be shown that, under general conditions, the sequence of \\(J\\) iterates converges to a draw from the joint posterior of the variables. When \\(J=2\\), the Gibbs’ sampler is the same as DA if \\(x_1=Y_1\\) and \\(x_2=\\theta\\) and the distributions condition on \\(Y_0\\). We can then obtain a draw from the joint posterior of \\(Y_1,\\theta \\mid Y_0\\) by applying the Gibbs’ sampler, where at iteration \\(t\\) for the \\(d\\)-th imputed data set:\n\\[\nY^d_{1t+1} \\sim p\\left(Y_1 \\mid Y_0, \\theta^d_{t}\\right) \\;\\;\\; \\text{and} \\;\\;\\; \\theta^d_{t+1} \\sim p\\left(\\theta \\mid Y^d_{1t+1}, Y_0\\right),\n\\]\nsuch that one run of the sampler converges to a draw from the posterior predictive distribution of \\(Y_1\\) and a draw from the posterior of \\(\\theta\\). The sampler can be run independently \\(D\\) times to generate \\(D\\) iid draws from the approximate joint posterior of \\(\\theta\\) and \\(Y_1\\). The values of \\(Y_1\\) are multiple imputations of the missing values, drawn from their posterior predictive distribution."
  },
  {
    "objectID": "missing_data/bis/bis.html#assessing-convergence",
    "href": "missing_data/bis/bis.html#assessing-convergence",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Assessing Convergence",
    "text": "Assessing Convergence\nAssessing convergence of the sequence of draws to the target distribution is more difficult than assessing convergence of an EM-type algorithm because there is no single target quantity to monitor like the maximum value of the likelihood. Methods have been proposed to assess convergence of a single sequence (Geyer (1992)), but a more reliable approach is to simulate \\(D&gt;1\\) sequences with starting values dispersed throughout the parameter space, and the convergence of all quantities of interest can then be monitored by comparing variation between and within simulated sequences, until the “within” variation roughly equals the “between” variation. The idea is that when the distribution of each simulated sequence is close enough to the distribution of all the sequences mixed together, they can all be approximating the target distribution. Gelman and Rubin (1992) developed an explicit monitoring statistic based on the following idea. For each scalar estimand \\(\\psi\\), label the draws from \\(D\\) parallel sequences as \\(\\psi^d_{t}\\), for \\(t=1,\\ldots,T\\) iterations and \\(d=1,\\ldots,D\\) sequences, and compute the between \\(B\\) and within \\(\\bar{V}\\) sequence variances as:\n\\[\nB=\\frac{T}{D-1}\\sum_{d=1}^D(\\bar{\\psi}_{d.} - \\bar{\\psi}_{..})^2, \\;\\;\\; \\text{and} \\;\\;\\; \\bar{V}=\\frac{1}{D}\\sum_{d=1}^D s^2_{d},\n\\]\nwhere \\(\\bar{\\psi}_{d.}=\\frac{1}{T}\\sum_{t=1}^T \\psi_{dt}\\), \\(\\bar{\\psi}_{..}=\\frac{1}{D}\\sum_{d=1}^D \\bar{\\psi}_{d}\\), and \\(s^2_{d}=\\frac{1}{T-1}\\sum_{t=1}^T(\\psi_{dt} - \\bar{\\psi}_{d.})^2\\). We can then estimate the marginal posterior variance of the estimand as\n\\[\n\\widehat{Var}(\\psi \\mid Y_0) = \\frac{T-1}{T}\\hat{V} + \\frac{1}{T} B,\n\\]\nwhich will overestimate the marginal posterior variance assuming the starting distribution is appropriately over-dispersed but is unbiased under stationarity (starting distribution equals the target distribution). For any finte \\(T\\), the within variance \\(\\hat{V}\\) will underestimate the marginal variance because individual sequences have not had time to range over all the target distribution and should have smaller variance then B. In the limit as \\(T \\rightarrow \\infty\\) the expecation of \\(\\hat{V}\\) approaches the marginal variance. These facts suggest monitoring convergence by estimating the factor by which the scale of the current distribution for \\(\\psi\\) might be reduced if the simulations were continued. This is the potential scale reduction factor and is estimated by\n\\[\n\\sqrt{\\hat{R}} = \\sqrt{\\frac{\\widehat{Var}(\\psi \\mid Y_0)}{\\hat{V}}},\n\\]\nwhich declines to 1 as \\(T \\rightarrow \\infty\\). When this quantity is high, there is evidence to proceed the simulations further to improve our inference about the target distribution."
  },
  {
    "objectID": "missing_data/bis/bis.html#other-simulation-methods",
    "href": "missing_data/bis/bis.html#other-simulation-methods",
    "title": "Bayesian Iterative Simulation Methods",
    "section": "Other Simulation Methods",
    "text": "Other Simulation Methods\nWhen draws from the sequence of conditional distributions forming the Gibbs’ sampler are not easy to obtain, other simulation approaches are needed. Among these there are the Sequential Imputation (Kong, Liu, and Wong (1994)), Sampling Imprtance Resampling (Gelfand and Smith (1990)), Rejection Sampling (Von Neumann et al. (1951)). One of these alternatives are the Metropolis-Hastings (Metropolis et al. (1953)) algorithms, of which the Gibbs’ sampler is a particular case, which constitute the so-called Markov Chain Monte Carlo (MCMC) algorithms as the sequence of iterates forms a Markov Chain (Gelman et al. (2013))."
  },
  {
    "objectID": "missing_data/aca/aca.html",
    "href": "missing_data/aca/aca.html",
    "title": "Available Case Analysis",
    "section": "",
    "text": "Complete case analysis (CCA) can be particularly inefficient for data sets with a large number of variables which are partially observed. An alternative approach that can be used to conduct univariate analyses in known as Available Case Analysis (ACA), which uses all the available cases, separately for each variable under examination, to estimate the quantities of interest.\nThe main drawback of ACA is that the sample used to perform the analysis varies from variable to variable according to the patterns of missing data, which generates problems of comparability across variables if the missingness mechanism is not missing completely at random (MCAR), i.e. the missing data probabilities depend on the variables under study. While estimates of means and variances can be easily computed, measures of covariation need to be adjusted. In particular, for estimating sample covariances, this approach is known as pairwise deletion or pairwise inclusion"
  },
  {
    "objectID": "missing_data/aca/aca.html#pairwise-measures-of-covariation",
    "href": "missing_data/aca/aca.html#pairwise-measures-of-covariation",
    "title": "Available Case Analysis",
    "section": "Pairwise measures of covariation",
    "text": "Pairwise measures of covariation\nOne possible approach to estimate pairwise measures of covariation for \\(y_j\\) and \\(y_k\\) is to use only those units \\(i=1,\\ldots,n_{ac}\\) for which both variables are observed (Little and Rubin (2019)). For example, one can compute pairwise sample covariances as:\n\\[\ns^{ac}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j}^{ac})(y_{ik}-\\bar{y}_{k}^{ac})}{(n_{ac}-1)},\n\\]\nwhere \\(I_{ac}\\) is the set of \\(n_{ac}\\) with both \\(y_j\\) and \\(y_k\\) observed, while the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\) are calculated over this set of units. We can also estimate the sample correlation\n\\[\nr^{\\star}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^2_{j}s^{2}_{k}}},\n\\]\nwhere \\(s^2_{j}\\) and \\(s^2_{k}\\) are the sample variances computed over the sets of observed units \\(I_{j}\\) and \\(I_{k}\\), respectively. A problem of this type of correlation estimate is that it can lie outside the range \\((-1,1)\\), which is typically addressed by computing pairwise correlations (Wilks (1932)), where variances are estimated from the set of units with both variables observed \\(I_{jk}\\), i.e. \n\\[\nr^{ac}_{jk} = \\frac{s^{ac}_{jk}}{\\sqrt{s^{2,ac}_{j}s^{2,ac}_{k}}}.\n\\]\nIn addition, we could also replace the sample means \\(\\bar{y}^{ac}_{j}\\) and \\(\\bar{y}^{ac}_{k}\\), evaluated on the common set of units \\(I_{jk}\\), with \\(\\bar{y}_{j}\\) and \\(\\bar{y}_{k}\\), which are evaluated on the sets of units \\(I_{j}\\) and \\(I_{k}\\), respectively. This leads to the following estimates for the sample covariances (Matthai (1951)):\n\\[\ns^{\\star}_{jk} = \\frac{\\sum_{i \\in I_{ac}}(y_{ij}-\\bar{y}_{j})(y_{ik}-\\bar{y}_{k})}{(n_{ac}-1)},\n\\]\nPairwise AC estimates aim at recovering information from partially-observed units that are lost by CCA. However, when considered together, the estimates suffer from inconsistencies that undermine the validity of these methods. For example, pairwise correlation matrices may be not positive definite. Because parameters are estimated from different sets of units, different approaches can be used to obtain estimate of the measures of uncertainty (Schafer and Graham (2002))."
  },
  {
    "objectID": "missing_data/aca/aca.html#conclusions",
    "href": "missing_data/aca/aca.html#conclusions",
    "title": "Available Case Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nAC estimates allow to make use of all the available evidence in the data and may be more efficient that CCA when the missingness mechanism is MCAR and correlations are modest (Kim and Curry (1977)). However, when correlations are more substantial, ACA may become even less efficient than CCA (Haitovsky (1968), Azen and Van Guilder (1981))."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses.\n\n\n\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses."
  },
  {
    "objectID": "index.html#research-and-work",
    "href": "index.html#research-and-work",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "missing_data/aipw/aipw.html",
    "href": "missing_data/aipw/aipw.html",
    "title": "Augmented Inverse Probability Weighting",
    "section": "",
    "text": "A general problem associated with the implementatio of Inverse Probability Weighting (IPW) methods is that information in some available data is ignored by focussing only on the complete cases (Schafer and Graham (2002)). This has provided room to extend these methods to make a more efficient use of the available information through the incorporation of an “augmentation” term, which lead to the development of the so called Augmented Inverse Probability Weighting (AIPW) methods. These approaches extend IPW methods by creating predictions from a model to recover the information in the incomplete units and applying IPW to the residuals from the model (Little and Rubin (2019)).\nConsidering the IPW Generalised Estimating Equation (GEE)\n\\[\n\\sum_{i=1}^{n_r} = w_i(\\hat{\\alpha})D_i(x_i,\\beta)(y_i-g(x_i,\\beta))=0,\n\\]\nwhere \\(w_i(\\hat{\\alpha})=\\frac{1}{p(x_i,z_i \\mid \\hat{\\alpha})}\\), with \\(p(x_i,z_i \\mid \\hat{\\alpha})\\) an estimate of the probability of being a complete unit estimated for example using logistic regressions of the missingness indicator \\(m_i\\) on the vectors of the covariate and auxiliary variables \\(x_i\\) and \\(z_i\\), respectively. A problem of this IPW estimator is that it has poor small sample properties when the propensity score gets close to zero or one for some observations, which will lead to high variance in the estimator. AIPW methods can provide estimators of \\(\\beta\\) which are more efficient than their nonaugmented IPW versions. In general, AIPW estimating functions provide a method for constructing estimators of \\(\\beta\\) based on two terms:\nThe basis for the first term is a complete data unbiased estimating function for \\(\\beta\\), whereas the basis for the second term is some function of the observed data chosen so it has conditional mean of zero given the complete data (Molenberghs et al. (2014))."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#doubly-robust-estimators",
    "href": "missing_data/aipw/aipw.html#doubly-robust-estimators",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Doubly Robust Estimators",
    "text": "Doubly Robust Estimators\nAn important class of AIPW methods is known as doubly robust estimators, which have desirable robustness properties (Robins, Rotnitzky, and Laan (2000),Robins and Rotnitzky (2001)). The key feature of these estimators is that they relax the assumption that the model of the missingness probabilities is correctly specified, although requiring additional assumptions on the model for \\(y_i \\mid x_i\\). For example, doubly robust estimators for a population mean parameter \\(\\mu\\) could be obtained as follows:\n\nFit a logistic regression model for the probability of observing \\(y_i\\) as a function of \\(x_i\\) and \\(z_i\\) to derive the individual weights \\(w_i(\\hat{\\alpha})\\).\nFit a generalized linear model for the outcome of responders in function of \\(x_i\\) using weights \\(w_i(\\hat{\\alpha})\\) and let \\(g^\\star(x_i,\\beta)\\) denote the fitted values for subject \\(i\\).\nTake the sample average of the fitted values \\(g^\\star(x_i,\\beta)\\) of both respondents and nonrespondents as an estimate of the population mean \\(\\hat{\\mu}\\)\n\nDoubly robust estimators require the specification of two models: one for the missingness probability and another for the distribution of the incomplete data. When the augmentation term \\(g^\\star(x_i,\\beta)\\) is selected and modelled correctly according to the distribution of the complete data, the resulting estimator of \\(\\beta\\) is consistent even if the model of missingness is misspecified. On the other hand, if the model of missingness is correctly specified, the augmentation term no longer needs to be correctly specified to yield consistent estimators of \\(\\beta\\) (Scharfstein, Daniels, and Robins (2003),Bang and Robins (2005)). Doubly robust estimators therefore allow to obtain an unbiased estimating function for \\(\\beta\\) if either the model for the incomplete data or the model for the missingness mechanism has been correctly specified."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#example",
    "href": "missing_data/aipw/aipw.html#example",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Example",
    "text": "Example\nSuppose the full data consists of a single outcome variable \\(y\\) and an additional variable \\(z\\) and that the objective is to estimate the population outcome mean \\(\\mu=\\text{E}[y]\\). When \\(y\\) is partially observed (while \\(Z\\) is always fully observed), individuals may fall into one of two missingness patterns \\(r=(r_{y},r_{z})\\), namely \\(r=(1,1)\\) if both variables are observed or \\(r=(1,0)\\) if \\(y\\) is missing. Let \\(c=1\\) if \\(r=(1,1)\\) and \\(c=0\\) otherwise, so that the observed data can be summarised as \\((c,cy,z)\\). Assuming that missingness only depends on \\(z\\), that is\n\\[\np(c=1 \\mid y,z)=p(c=1 \\mid z)=\\pi(z),\n\\]\nthen the missing data mechanism is Missing At Random (MAR). Under these conditions, consider the consistent IPW complete case estimating equation\n\\[\n\\sum_{i=1}^n\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu)=0,\n\\]\nwhich can be used to weight the contribution of each complete case by the inverse of \\(\\pi(z_i \\mid \\hat{\\alpha})\\), typically estimated via logistic regressions. A general problem of this type of estimators is that they discard all the available data among the non-completers and are therefore inefficient. However, it is possible to augment the simple IPW complete case estimating equation to improve efficiency. The optimal estimator for \\(\\mu\\) within this class is the solution to the estimating equation\n\\[\n\\sum_{i=1}^n \\left(\\frac{c_i}{\\pi(z_i \\mid \\hat{\\alpha})}(y_i-\\mu) - \\frac{c_i-\\pi(z_i \\mid \\hat{\\alpha})}{\\pi(z_i \\\\mid \\hat{\\alpha})}\\text{E}[(y_i-\\mu)\\mid z_i] \\right),\n\\]\nwhich leads to the estimator\n\\[\n\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} -  \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} \\text{E}[y_i \\mid z_i] \\right).\n\\]\nThe conditional expectation \\(\\text{E}[y_i \\mid z_i]\\) is not known and must be estimated from the data. Under a Missing At Random (MAR) assumption we have that \\(\\text{E}[y \\mid z]=\\text{E}[y \\mid z, c=1]\\), that is the conditional expecation of \\(y\\) given \\(z\\) is the same as that among the completers. Thus, we can specify a model \\(m(z,\\xi)\\) for \\(\\text{E}[y \\mid z]\\), indexed by the parameter \\(\\xi\\), that can be estimated from the completers. If \\(y\\) is continuous, a simple choice is to estimate \\(\\hat{\\xi}\\) by OLS from the completers. The AIPW estimator for \\(\\mu\\) then becomes\n\\[\n\\mu_{aipw}=\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{c_iy_i}{\\pi(z_i\\mid \\hat{\\alpha})} -  \\frac{c_i - \\pi(z_i\\mid \\hat{\\alpha})}{\\pi(z_i\\mid \\hat{\\alpha})} m(z_i\\mid \\hat{\\xi}) \\right).\n\\]\nIt can be shown that this estimator is more efficient that the simple IPW complete case estimator for \\(\\mu\\) and that it has a double robustness property. This ensures that \\(\\mu_{aipw}\\) is a consitent estimator of \\(\\mu\\) if either\n\nthe model \\(\\pi(z\\mid\\alpha)\\) is correctly specified, or\nthe model \\(m(z\\mid \\xi)\\) is correctly specified.\n\nTo see a derivation of the double robustness property I put here a link to some nice paper."
  },
  {
    "objectID": "missing_data/aipw/aipw.html#conlcusions",
    "href": "missing_data/aipw/aipw.html#conlcusions",
    "title": "Augmented Inverse Probability Weighting",
    "section": "Conlcusions",
    "text": "Conlcusions\nAs all weighting methods, such as IPW, AIPW methods are semiparametric methods that aim to achieve robustness and good performance over more general classes of population distributions. However, semiparametric estimators can be less efficient and less powerful than Maximum Likelihood or Bayesian estimators under a well specified parametric model. With missing data, Rubin (1976) results show that likelihood-based methods perform uniformly well over any Missing At Random (MAR) missingness distribution, and the user does not need to specify that distribution. However, semiparametric methods that relax assumptions about the data must in turn assume a specific form for the distribution of missingness. It has been argued that, for these semiparametric methods to gain a substantial advantage over well-specified likelihood methods, the parametric model has to be grossly misspecified (Meng (2000))."
  },
  {
    "objectID": "missing_data/cca/cca.html",
    "href": "missing_data/cca/cca.html",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Complete case analysis (CCA), also known as case or listwise deletion (LD), is one of the oldest methods to handle missing data and consists in discarding any unit or case whose information is incomplete. Only the cases with observed values for all the variables under consideration are used in the analysis. For example, suppose we have a data set formed by \\(i=1,\\ldots,n\\) individuals and that we want to fit a linear regression on some outcome variable \\(y_i\\) using some other variables \\(x_{i1},\\ldots,x_{ik}\\) as covariates. CCA uses only the subset of cases with observed values on all the variables included in the analysis (completers).\nCCA has been a quite popular approach to deal with missingness, mainly because it is very easy to implement (used by default in many statistical programs) and it allows the comparison of different univariate statistics in a straightforward way (calculated on a common set of cases). However, there are a number of potential disadvantages which threatens the validity of this method:\n\nBias, when the missing data mechanism is not missing completely at random (MCAR) and the completers are not a random samples of all the cases\nLoss of efficiency, due to the potential loss of information in discarding the incomplete cases.\n\nCCA may be justified when the loss of precision and bias are minimal, which is more likley to occur when the proportion of completers is high, although it is difficult to formulate rules that apply in general circumstances. Indeed, both the degree of loss of precision and bias depend not only on the fraction of completers and missingness patterns, but also on the extent to which complete and incomplete cases differ and the parameters of interest.\nLet \\(\\hat{\\theta}_{cc}\\) be an estimate of a parameter of interest from the completers. One might measure the increase in variance of \\(\\hat{\\theta}_{cc}\\) with respect to the estimate \\(\\hat{\\theta}\\) that would be obtained in the absence of missing values. Using the notation of Little and Rubin (2019):\n\\[\n\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta})(1 + \\Delta^{\\star}_{cc}),\n\\]\nwhere \\(\\Delta^{\\star}_{cc}\\) is the proportional increase in variance from the loss of information. A more practical measure of the loss of inofrmation is \\(\\Delta_{cc}\\), where\n\\[\n\\text{Var}(\\hat{\\theta}_{cc}) = \\text{Var}(\\hat{\\theta}_{eff})(1 + \\Delta_{cc}),\n\\]\nand \\(\\hat{\\theta}_{eff}\\) is an efficient estimate based on all the available data.\n\n\nConsider bivariate normal monotone data \\(\\bf y = (y_1,y_2)\\), where \\(n_{cc}\\) out of \\(n\\) cases are complete and \\(n - n_{cc}\\) cases have observed values only on \\(y_1\\). Assume for simplicity that the missingness mechanism is MCAR and that the mean of \\(y_j\\) is estimated by the empirical mean from the complete cases \\(\\bar{y}^{cc}_j\\). Then, the loss in sample size for estimating the mean of \\(y_1\\) is:\n\\[\n\\Delta_{cc}(\\bar{y}_1) = \\frac{n - n_{cc}}{n_{cc}},\n\\]\nso that if half the cases are missing, the variance is doubled. For the mean of \\(y_2\\), the loss of information alos depends on the squared correlation \\(\\rho^{2}\\) between the variables: (Little and Rubin (2019))\n\\[\n\\Delta_{cc}(\\bar{y}_2) \\approx \\frac{(n - n_{cc})\\rho^{2}}{n_{cc}(1 - \\rho^{2}) + n_{cc}\\rho^{2}}.\n\\]\n\\(\\Delta_{cc}(\\bar{y}_2)\\) varies from zero (when \\(\\rho=0\\)) to \\(\\Delta_{cc}(\\bar{y}_1)\\) as \\(\\rho^{2} \\rightarrow 1\\). However, for the regression coefficients of \\(y_2\\) on \\(y_1\\) we have that \\(\\Delta_{cc}=0\\) since the incomplete observations of \\(y_1\\) provide no information for estimating the parameters of the regression of \\(y_2\\) on \\(y_1\\).\n\n\n\nFor inference about the population mean \\(\\mu\\), the bias of CCA depends on the proportion of the completers \\(\\pi_{cc}\\) and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable \\(y\\) is partially-observed and that we partition the data into the subset of the completers \\(y_{cc}\\) and incompleters \\(y_{ic}\\), with associated population means \\(\\mu_{cc}\\) and \\(\\mu_{ic}\\), respectively. The overall mean can be written as a weighted average of the means of the two subsets\n\\[\n\\mu = \\pi_{cc}\\mu_{cc} + (1 - \\pi_{cc})\\mu_{ic}.\n\\]\nThe bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases\n\\[\n\\mu_{cc} - \\mu = (1 - \\pi_{cc})(\\mu_{cc} - \\mu_{ic}).  \n\\]\nUnder MCAR, we have that \\(\\mu_{cc} = \\mu_{ic}\\) and therefore the bias is zero.\n\n\n\nConsider the estimation of the regression of \\(y\\) on \\(x_1,\\ldots,x_K\\) from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients \\(\\beta_1,\\ldots,\\beta_K\\) associated with the covariates is null if the probbaility of being a completer depends on the \\(x\\)s but not \\(y\\), since the analysis conditions on the values of the covariates (Glynn and Laird (1986), White and Carlin (2010)). This class of missing data mechanisms includes missing not at random (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on \\(y\\) after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor Bartlett using some nice slides\n\n\n\nThe main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (Schafer and Graham (2002)). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable \\(y\\) based on complete cases with the distribution of \\(y\\) based on incomplete cases for which \\(y\\) is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the missing at random (MAR) assumption."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-1",
    "href": "missing_data/cca/cca.html#example-1",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Consider bivariate normal monotone data \\(\\bf y = (y_1,y_2)\\), where \\(n_{cc}\\) out of \\(n\\) cases are complete and \\(n - n_{cc}\\) cases have observed values only on \\(y_1\\). Assume for simplicity that the missingness mechanism is MCAR and that the mean of \\(y_j\\) is estimated by the empirical mean from the complete cases \\(\\bar{y}^{cc}_j\\). Then, the loss in sample size for estimating the mean of \\(y_1\\) is:\n\\[\n\\Delta_{cc}(\\bar{y}_1) = \\frac{n - n_{cc}}{n_{cc}},\n\\]\nso that if half the cases are missing, the variance is doubled. For the mean of \\(y_2\\), the loss of information alos depends on the squared correlation \\(\\rho^{2}\\) between the variables: (Little and Rubin (2019))\n\\[\n\\Delta_{cc}(\\bar{y}_2) \\approx \\frac{(n - n_{cc})\\rho^{2}}{n_{cc}(1 - \\rho^{2}) + n_{cc}\\rho^{2}}.\n\\]\n\\(\\Delta_{cc}(\\bar{y}_2)\\) varies from zero (when \\(\\rho=0\\)) to \\(\\Delta_{cc}(\\bar{y}_1)\\) as \\(\\rho^{2} \\rightarrow 1\\). However, for the regression coefficients of \\(y_2\\) on \\(y_1\\) we have that \\(\\Delta_{cc}=0\\) since the incomplete observations of \\(y_1\\) provide no information for estimating the parameters of the regression of \\(y_2\\) on \\(y_1\\)."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-2",
    "href": "missing_data/cca/cca.html#example-2",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "For inference about the population mean \\(\\mu\\), the bias of CCA depends on the proportion of the completers \\(\\pi_{cc}\\) and the extent to which complete and incomplete cases differ on the variables of interest. Suppose a variable \\(y\\) is partially-observed and that we partition the data into the subset of the completers \\(y_{cc}\\) and incompleters \\(y_{ic}\\), with associated population means \\(\\mu_{cc}\\) and \\(\\mu_{ic}\\), respectively. The overall mean can be written as a weighted average of the means of the two subsets\n\\[\n\\mu = \\pi_{cc}\\mu_{cc} + (1 - \\pi_{cc})\\mu_{ic}.\n\\]\nThe bias of CCA is then equal to the expected fraction of incomplete cases multiplied by the differences in the means for complete and incomplete cases\n\\[\n\\mu_{cc} - \\mu = (1 - \\pi_{cc})(\\mu_{cc} - \\mu_{ic}).  \n\\]\nUnder MCAR, we have that \\(\\mu_{cc} = \\mu_{ic}\\) and therefore the bias is zero."
  },
  {
    "objectID": "missing_data/cca/cca.html#example-3",
    "href": "missing_data/cca/cca.html#example-3",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "Consider the estimation of the regression of \\(y\\) on \\(x_1,\\ldots,x_K\\) from data with potential missing values on all variables and with the regression function correctly specified. The bias of CCA for estimating the regression coefficients \\(\\beta_1,\\ldots,\\beta_K\\) associated with the covariates is null if the probbaility of being a completer depends on the \\(x\\)s but not \\(y\\), since the analysis conditions on the values of the covariates (Glynn and Laird (1986), White and Carlin (2010)). This class of missing data mechanisms includes missing not at random (MNAR), where the probability that a covariate is missing depends on the value of that covariate. However, CCA is biased if the probability of being a completer depends on \\(y\\) after conditioning on the covariates. A nice example of this particular topic and its implications for the analysis has been provided by professor Bartlett using some nice slides"
  },
  {
    "objectID": "missing_data/cca/cca.html#conclusions",
    "href": "missing_data/cca/cca.html#conclusions",
    "title": "Complete Case Analysis",
    "section": "",
    "text": "The main virtue of case deletion is simplicity. If a missing data problem can be resolved by discarding only a small part of the sample, then the method can be quite effective. However, even in that situation, one should explore the data (Schafer and Graham (2002)). The discarded information from incomplete cases can be used to study whether the complete cases are plausibly a random subsample of the original sample, that is, whether MCAR is a reasonable assumption. A simple procedure is to compare the distribution of a particular variable \\(y\\) based on complete cases with the distribution of \\(y\\) based on incomplete cases for which \\(y\\) is recorded. Significant differences indicate that the MCAR assumption is invalid, and the complete-case analysis yields potentially biased estimates. Such tests are useful but have limited power when the sample of incomplete cases is small. Also the tests can offer no direct evidence on the validity of the missing at random (MAR) assumption."
  },
  {
    "objectID": "missing_data/esi/esi.html",
    "href": "missing_data/esi/esi.html",
    "title": "Explicit Single Imputation",
    "section": "",
    "text": "All case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nIn this part, we focus on some of the most popular Explicit Single Imputation methods. These include: Mean Imputation(SI-M), where means from the observed data are used as imputed values; Regression Imputation(SI-R), where missing values are replaced with values predicited from a regression of the missing variable on some other observed variables; and Stochastic Regression Imputation(SI-SR), where unobserved data are substituted with the predicted values from a regression imputation plus a randomly selected residual drawn to reflect uncertainty in the predicted values."
  },
  {
    "objectID": "missing_data/esi/esi.html#mean-imputation",
    "href": "missing_data/esi/esi.html#mean-imputation",
    "title": "Explicit Single Imputation",
    "section": "Mean Imputation",
    "text": "Mean Imputation\nThe simplest type of SI-M consists in replacing the missing values in a variable with the mean of the observed units from the same variable, a method known as Unconditional Mean Imputation (Little and Rubin (2019),Schafer and Graham (2002)). Let \\(y_{ij}\\) be the value of variable \\(j\\) for unit \\(i\\), such that the unconditional mean based on the observed values of \\(y_j\\) is given by \\(\\bar{y}_j\\). The sample mean of the observed and imputed values is then \\(\\bar{y}^{m}_j=\\bar{y}^{ac}_j\\), i.e. the estimate from ACA, while the sample variance is given by\n\\[\ns^{m}_{j}=s^{ac}_{j}\\frac{(n^{ac}-1)}{(n-1)},\n\\]\nwhere \\(s^{ac}_j\\) is the sample variance estimated from the \\(n^{ac}\\) available units. Under a Missing Completely At Random(MCAR) assumption, \\(s^{ac}_j\\) is a consistent estimator of the tru variance so that the sample variance from the imputed data \\(s^m_j\\) systematically underestimates the true variance by a factor of \\(\\frac{(n^{ac}-1)}{(n-1)}\\), which clearly comes from the fact that missing data are imputed using values at the centre of the distribution. The imputation distorts the empirical distribution of the observed values as well as any quantities that are not linear in the data (e.g. variances, percentiles, measures of shape). The sampel covariance of \\(y_j\\) and \\(y_k\\) from the imputed data is\n\\[\ns^{m}_{jk}=s^{ac}_{jk}\\frac{(n^{as}_{jk}-1)}{(n-1)},\n\\]\nwhere \\(n^{ac}_{jk}\\) is the number of units with both variables observed and \\(s^{ac}_{jk}\\) is the corresponding covariance estimate from ACA. Under MCAR \\(s^{ac}_{jk}\\) is a consistent estimator of the true covariance, so that \\(s^{m}_{jk}\\) underestimates the magnitude of the covariance by a factor of \\(\\frac{(n^{ac}_{jk}-1)}{(n-1)}\\). Obvious adjustments for the variance (\\(\\frac{(n-1)}{(n^{ac}_j-1)}\\)) and the covariance (\\(\\frac{(n-1)}{(n^{ac}_{jk}-1)}\\)) yield ACA estimates, which could lead to covariance matrices that are not positive definite."
  },
  {
    "objectID": "missing_data/esi/esi.html#regression-imputation",
    "href": "missing_data/esi/esi.html#regression-imputation",
    "title": "Explicit Single Imputation",
    "section": "Regression Imputation",
    "text": "Regression Imputation\nAn improvement over SI-M is to impute each missing data using the conditional means given the observed values, a method known SI-R or Conditional Mean Imputation. To be precise, it would also be possible to impute conditional means without using a regression approach, for example by grouping individuals into adjustment classes (analogous to weighting methods) based on the observed data and then impute the missing values using the observed means in each adjustment class (Little and Rubin (2019)). However, for the sake of simplicity, here we will assume that SI-R and conditional mean imputation are the same.\nTo generate imputations under SI-R, consider a set of \\(J-1\\) fully observed response variables \\(y_1,\\ldots,y_{J-1}\\) and a partially observed response variable \\(y_J\\) which has the first \\(n_{cc}\\) units observed and the remaiing \\(n-n_{cc}\\) units missing. SI-R computes the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) complete units and then fills in the missing values as predictions from the regression. For example, for unit \\(i\\), the missing value \\(y_{iJ}\\) is imputed using\n\\[\n\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij},\n\\]\nwhere \\(\\hat{\\beta}_{J0}\\) is the intercept and \\(\\hat{\\beta}_{Jj}\\) is the \\(j\\) coefficient of of the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the \\(n_{cc}\\) units.\nAn extension of regression imputation to a general pattern of missing data is known as Buck’s method (Buck (1960)). This approach first estimates the population mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\) from the sample mean and covariance matrix of the complete units and then uses these estimates to calculate the OLS regressions of the missing variables on the observed variables for each missing data pattern. Predictions of the missing data for each observation are obtained by replacing the values of the present variables in the regressions. The average of the observed and imputed values from this method are consistent estimates of the means and MCAR and mild assumptions about the moments of the distribution (Buck (1960)). They are also consistent when the missingness mechanism depends on observed variables, i.e. under a Missing At Random(MAR) assumption, although addtional assumptions are required in this case (e.g. using linear regressions it assumes that the “true” regression of the missing varables on the observed variables is linear).\nThe filled in data from Buck’s method typically yield reasonable estimates of means, while the sample variances and covariances are biased, although the bias is less than the one associated with unconditional mean imputation. Specifically, the sample variance \\(\\sigma^{2,SI-R}_j\\) from the imputed data underestimates the true variance \\(\\sigma^2_j\\) by a factor of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma^{2}_{ji}\\), where \\(\\sigma^{2}_{ji}\\) is the residual variance from regressing \\(y_j\\) on the variables observed in unit \\(i\\) if \\(y_{ij}\\) is missing and zero if \\(y_{ij}\\) is observed. The sample covariance of \\(y_j\\) and \\(y_k\\) has a bias of \\(\\frac{1}{n-1}\\sum_{i=1}^n\\sigma_{jki}\\), where \\(\\sigma_{jki}\\) is the residual covariance from the multivariate regression of \\((y_{ij},y_{ik})\\) on the variables observed in unit \\(i\\) if both variables are missing and zero otherwise. A consistent estimator of \\(\\Sigma\\) can be constructed under MCAR by replacing consistent estimates of \\(\\sigma^{2}_{ji}\\) and \\(\\sigma_{jki}\\) in the expressions for bias and then adding the resulting quantities to the sample covariance matrix of the filled-in data."
  },
  {
    "objectID": "missing_data/esi/esi.html#stochastic-regression-imputation",
    "href": "missing_data/esi/esi.html#stochastic-regression-imputation",
    "title": "Explicit Single Imputation",
    "section": "Stochastic Regression Imputation",
    "text": "Stochastic Regression Imputation\nAny type of mean or regression imputation will lead to bias when the interest is in the tails of the distributions because “best prediction” imputation systematically underestimates variability and standard errors calculated from the imputed data are typically too small. These considerations suggest an alternative imputation strategy, where imputed values are drawn from a predictive distribution of a plausible set of values rather than from the centre of the distribution. This is the idea behind SI-SR, which imputes a conditional draw\n\\[\n\\hat{y}_{iJ}=\\hat{\\beta}_{J0}+\\sum_{j=1}^{J-1}\\hat{\\beta}_{Jj}y_{ij}+z_{iJ},\n\\]\nwhere \\(z_{iJ}\\) is a random normal deviate with mean 0 and variance \\(\\hat{\\sigma}^2_J\\), the residual variance from the regression of \\(y_J\\) on \\(y_1,\\ldots,y_{J-1}\\) based on the complete units. The addition of the random deviate makes the imputation a random draw from the predictive distribution of the missing values, rather than the mean, which is likely to ameliorate the distortion of the predictive distributions (Little and Rubin (2019)).\n\nExample\nConsider a bivariate normal monotone missing data with \\(y_1\\) fully observed and \\(y_2\\) missing for a fraction \\(\\lambda=\\frac{(n-n_{cc})}{n}\\) and a MCAR mechanism. The following table shows the large sample bias of standard OLS estimates obtained from the filled-in data about the mean, the variance of \\(y_2\\), the regression coefficient of \\(y_2\\) on \\(y_1\\), and the regression coefficient of \\(y_1\\) on \\(y_2\\), using four different single imputation methods: uncondtional mean (UM), unconditional draw (UD), conditional mean (CM), and conditional draw (CD).\n\nBivariate normal monotone MCAR data; large sample bias of four imputation methods.\n\n\n\nmu_2\nsigma_2\nbeta_21\nbeta_12\n\n\n\n\nUM\n0\n-lambda * sigma_2\n-lambda * beta_21\n0\n\n\nUD\n0\n0\n-lambda * beta_21\n-lambda * beta_21\n\n\nCM\n0\n-lambda * (1-rho^2) * sigma_2\n0\n((lambda * (1-rho^2)) / (1-lambda * (1-rho^2)) ) * beta_12\n\n\nCD\n0\n0\n0\n0\n\n\n\n\n\nUnder MCAR, all four methods yield consistent estimates of \\(\\mu_2\\) but both UM and CM underestimate the variance \\(\\sigma_2\\), UD leads to attenuation of the regression coefficients, while CD yields consistent estimates of all four parameters. However, CD has some important drawbacks. First, adding random draws to the conditional mean imputations is inefficient as the large sample variance of the CD estimates of \\(\\mu_2\\) can be shown (Little and Rubin (2019)) to be\n\\[\n\\frac{[1-\\lambda\\rho^2+(1-\\rho^2)\\lambda(1-\\lambda)]\\sigma_2}{n_{cc}},\n\\]\nwhich is larger than the large sample sampling variance of the CM estimate of \\(\\mu_2\\), namely \\(\\frac{[1-\\lambda\\rho^2]\\sigma_2}{n_{cc}}\\). Second, the standard errors of the CD estimates from the imputed data are too small because they do not incorporate imputation uncertainty.\nWhen the analysis involves units with some covariates missing and other observed, it is common practice to condition on the observed covariates when generating the imputations for the missing covariates. It is also possible to condition on the outcome \\(y\\) to impute missing covariates, even if the final objective is to regress \\(y\\) on the full set of covariates and conditioning on \\(y\\) will lead to bias when conditional means are imputed. However, if predictive draws are imputed, this approach will yield consistent estimates of the regression coefficients. Imputing missing covariates using the means by conditioning only the observed covariates (and not also on \\(y\\)) also yields consistent estimates of the regression coefficients under certain conditions, although these are typically less efficient then those from CCA, but yields inconsistent estimates of other parameters such as variances and correlations (Little (1992))."
  },
  {
    "objectID": "missing_data/esi/esi.html#conclusions",
    "href": "missing_data/esi/esi.html#conclusions",
    "title": "Explicit Single Imputation",
    "section": "Conclusions",
    "text": "Conclusions\nAccording to Little and Rubin (2019), imputation should generally be\n\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\nMultivariate, to preserve association between missing variables.\nDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\nNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html",
    "href": "missing_data/intro_mle/intro_mle.html",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "",
    "text": "A possible approach to analyse missing data is to use methods based on the likelihood function under specific modelling assumptions. In this section, I review maximum likelihood methods based on fully observed data alone."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#maximum-likelihood-methods-for-complete-data",
    "href": "missing_data/intro_mle/intro_mle.html#maximum-likelihood-methods-for-complete-data",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Maximum Likelihood Methods for Complete Data",
    "text": "Maximum Likelihood Methods for Complete Data\nLet \\(Y\\) denote the set of data, which are assumed to be generated according to a certain probability density function \\(f(Y= y,\\mid \\theta)=f(y \\mid \\theta)\\) indexed by the set of parameters \\(\\theta\\), which lies on the parameter space \\(\\Theta\\) (i.e. set of values of \\(\\theta\\) for which \\(f(y\\mid \\theta)\\) is a proper density function). The Likelihood function, indicated with \\(L(\\theta \\mid y)\\), is defined as any function of \\(\\theta \\in \\Theta\\) proportional that is to \\(f(y \\mid \\theta)\\). Note that, in contrast to the density function which is defined as a function of the data \\(Y\\) given the values of the parameters \\(\\theta\\), instead the likelihood is defined as a function of the parameters \\(\\theta\\) for fixed data \\(y\\). In addition, the loglikelihood function, indicated with \\(l(\\theta\\mid y)\\) is defined as the natural logarithm of \\(L(\\theta \\mid y)\\).\n\nUnivariate Normal Example\nThe joint density function of \\(n\\) independent and identially distributed units \\(y=(y_1,\\ldots,y_n)\\) from a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), is\n\\[\nf(y \\mid \\mu, \\sigma^2)=\\frac{1}{\\sqrt{\\left(2\\pi\\sigma^2\\right)^n}}\\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right),\n\\]\nand therefore the loglikelihood is\n\\[\nl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2},\n\\]\nwhich is considered as a function of \\(\\theta=(\\mu,\\sigma^2)\\) for fixed data \\(y\\).\n\n\nMultivariate Normal Example\nIf the sample considered has dimension \\(J&gt;1\\), e.g. we have a set of idependent and identically distributed variables \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables, which comes from a Multivariate Normal distribution with mean vector \\(\\mu=(\\mu_1,\\ldots\\mu_J)\\) and covariance matrix \\(\\Sigma=(\\sigma_{jk})\\) for $ j=1,,J, k=1,,K$ and \\(J=K\\), then its density function is\n\\[\nf(y \\mid \\mu, \\Sigma)=\\frac{1}{\\sqrt{\\left(2\\pi \\right)^{nK}\\left(\\mid \\Sigma \\mid \\right)^n}} \\text{exp}\\left(-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^{T}  \\right),\n\\]\nwhere \\(|\\Sigma|\\) denotes the determinant of the matrix \\(\\Sigma\\) and the superscript \\(T\\) denotes the transpose of a matrix or vector, while \\(y_i\\) denotes the row vector of observed values for unit \\(i\\). The loglikelihood of \\(\\theta=(\\mu,\\Sigma)\\) is\n\\[\nl(\\mu,\\Sigma \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(|\\Sigma|)-\\frac{1}{2}\\sum_{i=1}^{n}(y_i-\\mu)\\Sigma^{-1}(y_i-\\mu)^T.\n\\]"
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#mle-estimation",
    "href": "missing_data/intro_mle/intro_mle.html#mle-estimation",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "MLE estimation",
    "text": "MLE estimation\nFinding the maximum value of \\(\\theta\\) that is most likely to have generated the data \\(y\\), corresponding to maximising the likelihood or Maximum Likelihood Estimation(MLE), is a standard approach to make inference about \\(\\theta\\). Suppose a specific value for the parameter \\(\\hat{\\theta}\\) such that \\(L(\\hat{\\theta}\\mid y)\\geq L(\\theta \\mid y)\\) for any other value of \\(\\theta\\). This implies that the observed data \\(y\\) is at least as likely under \\(\\hat{\\theta}\\) as under any other value of \\(\\theta\\), i.e. \\(\\hat{\\theta}\\) is the value best supported by the data. More specifically, a maximum likelihood estimate of \\(\\theta\\) is a value of \\(\\theta \\in \\Theta\\) that maximises the likelihood \\(L(\\theta \\mid y)\\) or, equivalently, that maximises the loglikelihood \\(l(\\theta \\mid y)\\). In general, when the likelihood is differentiable and bounded from above, typically the MLE can be found by differentiating \\(L(\\theta \\mid y)\\) or \\(l(\\theta \\mid y)\\) with respect to \\(\\theta\\), setting the result equal to zero, and solving for \\(\\theta\\). The resulting equation, \\(D_l(\\theta)=\\frac{\\partial l(\\theta \\mid y)}{\\partial \\theta}=0\\), is known as the likelihood equation and the derivative of the loglikelihood as the score function. When \\(\\theta\\) consists in a set of \\(j=1,\\ldots,J\\) components, then the likelihood equation corresponds to a set of \\(J\\) simultaneous equations, obtained by differentiating \\(l(\\theta \\mid y)\\) with respect to each component of \\(\\theta\\).\n\nUnivariate Normal Example\nRecall that, for a Normal sample with \\(n\\) units, the loglikelihood is indexed by the set of parameters \\(\\theta=(\\mu,\\sigma^2)\\) and has the form\n\\[\nl(\\mu, \\sigma^2 \\mid y)= -\\frac{n}{2}\\text{ln}(2\\pi)-\\frac{n}{2}\\text{ln}(\\sigma^2)-\\frac{1}{2}\\sum_{i=1}^n \\frac{(y_i-\\mu)^2}{\\sigma^2}.\n\\]\nNext, the MLE can be derived by first differentiating \\(l(\\theta \\mid y)\\) with respect to \\(\\mu\\) and set the result equal to zero, that is\n\\[\n\\frac{\\partial l(\\theta \\mid y)}{\\partial \\mu}= -\\frac{2}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)(-1)=\\frac{\\sum_{i=1}^n y_i - n\\mu}{\\sigma^2}=0,\n\\]\nNext, after simplifying a bit, we can retrieve the solution\n\\[\n\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^n y_i=\\bar{y},\n\\]\nwhich corresponds to the sample mean of the observations. Next, we differentiate \\(l(\\theta \\mid y)\\) with respect to \\(\\sigma^2\\), that is we set\n\\[\n\\frac{\\partial l(\\theta \\mid y)}{\\partial \\sigma^2}= -\\frac{n}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (y_i-\\mu)^2=0.\n\\]\nWe then simplify and move things around to get\n\\[\n\\frac{1}{\\sigma^3}\\sum_{i=1}^n(y_i-\\mu)^2=\\frac{n}{\\sigma} \\;\\;\\; \\rightarrow \\;\\;\\;  \\sigma^2=\\frac{1}{n}\\sum\\_{i=1}^n(y_i-\\mu)^2.\n\\]\nFinally, we replace \\(\\mu\\) in the expression above with the value \\(\\hat{\\mu}=\\bar{y}\\) found before and obtain the solution\n\\[\n\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\bar{y})^2=s^2,\n\\]\nwhich, however, is a biased estimator of \\(\\sigma^2\\) and therefore is often replaced with the unbiased estimator \\(\\frac{s^2}{(n-1)}\\). In particular, given a population parameter \\(\\theta\\), the estimator \\(\\hat{\\theta}\\) for \\(\\theta\\) is said to be unbiased when \\(E[\\hat{\\theta}]=\\theta\\). This is the case, for example, of the sample mean \\(\\hat{\\mu}=\\bar{y}\\) which is an unbiased estimator of the population mean \\(\\mu\\):\n\\[\nE\\left[\\hat{\\mu} \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i \\right]=\\frac{1}{n} (n\\mu)=\\mu.\n\\]\nHowever, this is not true for the sample variance \\(s^2\\). This can be seen by first rewriting the expression of the estimator as\n\\[\n\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i^2 -2y_i\\bar{y}+\\bar{y}^2)=\\frac{1}{n}\\sum_{i=1}^n y_i^2 -2\\bar{y}\\sum_{i=1}^n y_i + \\frac{1}{n}n\\bar{y}^2=\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2,\n\\]\nand then by computing the expectation of this quantity:\n\\[\nE\\left[\\hat{\\sigma}^2 \\right]=E\\left[\\frac{1}{n}\\sum_{i=1}^n y_i^2 - \\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n E\\left[y_i^2 \\right] - E\\left[\\bar{y}^2 \\right]=\\frac{1}{n}\\sum_{i=1}^n (\\sigma^2 + \\mu^2) - (\\frac{\\sigma^2}{n}+\\mu^2)=\\frac{1}{n}\\left(n\\sigma^2+n\\mu^2\\right) - \\frac{\\sigma^2}{n}-\\mu^2=\\frac{(n-1)\\sigma^2}{n}.\n\\]\nThe above result is obtained by pluggin in the expression for the variance of a general variable \\(y\\) and retrieving the expression for \\(E[y^2]\\) as a function of the variance and \\(E[y]^2\\). More specifically, given that\n\\[\nVar(y)=\\sigma^2=E\\left[y^2 \\right]-E\\left[y \\right]^2,\n\\]\nthen we know that for \\(y\\), \\(E\\left[y^2 \\right]=\\sigma^2+E[y]^2\\), and similarly we can derive the same expression for \\(\\bar{y}\\). However, we can see that \\(\\hat{\\sigma}^2\\) is biased by a factor of \\((n-1)/n\\). Thus, an unbiased estimator for \\(\\sigma^2\\) is given by multiplying \\(\\hat{\\sigma}^2\\) by \\(\\frac{n}{(n-1)}\\), which gives the unbiased estimator \\(\\hat{\\sigma}^{2\\star}=\\frac{s^2}{n-1}\\), where \\(E\\left[\\hat{\\sigma}^{2\\star}\\right]=\\sigma^2\\).\n\n\nMultivariate Normal Example\nThe same procedure can be applied to an independent and identically distributed multivariate sample \\(y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) units and \\(j=1,\\ldots,J\\) variables (Anderson (1962),Rao et al. (1973),Gelman et al. (2013)). It can be shown that, maximising the loglikelihood with respect to \\(\\mu\\) and \\(\\Sigma\\) yields the MLEs\n\\[\n\\hat{\\mu}=\\bar{y} \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma=\\frac{(n-1)\\hat{\\sigma}^{2\\star}}{n},\n\\]\nwhere \\(\\bar{y}=(\\bar{y}_1,\\ldots,\\bar{y}_{J})\\) is the row vectors of sample means and \\(\\hat{\\sigma}^{2\\star}=(s^{\\star_{jk}})\\) is the sample covariance matrix with \\(jk\\)-th element \\(s^\\star_{jk}=\\frac{\\Sigma_{i=1}^n(y_{ij} - \\bar{y}_j)}{(n-1)}\\). In addition, in general, given a function \\(g(\\theta)\\) of the parameter \\(\\theta\\), if \\(\\hat{\\theta}\\) is a MLE of \\(\\theta\\), then \\(g(\\hat{\\theta})\\) is a MLE of \\(g(\\theta)\\)."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#conditional-distribution-of-a-bivariate-normal",
    "href": "missing_data/intro_mle/intro_mle.html#conditional-distribution-of-a-bivariate-normal",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Conditional Distribution of a Bivariate Normal",
    "text": "Conditional Distribution of a Bivariate Normal\nConsider an indpendent and identically distributed sample formed by two variables \\(y=(y_1,y_2)\\), each measured on \\(i=1\\ldots,n\\) units, which come from a Bivariate Normal distribution with mean vector and covariance matrix\n\\[\n\\mu=(\\mu_1,\\mu_2) \\;\\;\\; \\text{and} \\;\\;\\; \\Sigma = \\begin{pmatrix} \\sigma^2_1 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_2\\sigma_1 & \\sigma_2^2 \\ \\end{pmatrix},\n\\]\nwhere \\(\\rho\\) is a correlation parameter between the two variables. Thus, intuitive MLEs for these parameters are\n\\[\n\\hat{\\mu}_j=\\bar{y}_j \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}_{jk}=\\frac{(n-1)s_{jk}}{n},\n\\]\nwhere \\(\\sigma^2_j=\\sigma_{jj}\\), \\(\\rho\\sigma_{j}\\sigma_{k}=\\sigma_{jk}\\), for \\(j,k=1,2\\). By properties of the Bivariate Normal distribution (Ord and Stuart (1994)), the marginal distribution of \\(y_1\\) and the conditional distribution of \\(y_2 \\mid y_1\\) are\n\\[\ny_1 \\sim \\text{Normal}\\left(\\mu\\_1,\\sigma^2_1 \\right) \\;\\;\\; \\text{and} \\;\\;\\; y_2 \\mid y_1 \\sim \\text{Normal}\\left(\\mu_2 + \\beta(y_1-\\mu_1 \\right), \\sigma^2_2 - \\sigma^2_1\\beta^2),\n\\]\nwhere \\(\\beta=\\rho\\frac{\\sigma_2}{\\sigma_1}\\) is the parameter that quantifies the linear dependence between the two variables. The MLEs of \\(\\beta\\) and \\(\\sigma^2_2\\) can also be derived from the likelihood based on the conditional distribution of \\(y_2 \\mid y_1\\), which have strong connections with the least squares estimates derived in a multiple linear regression framework."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#multiple-linear-regression",
    "href": "missing_data/intro_mle/intro_mle.html#multiple-linear-regression",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSuppose the data consist in \\(n\\) units measured on an outcome variable \\(y\\) and a set of \\(J\\) covariates \\(x=(x_{1},\\ldots,x_{J})\\) and assume that the distribution of \\(y\\) given \\(x\\) is Normal with mean \\(\\mu_i=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij}\\) and variance \\(\\sigma^2\\). The loglikelihood of \\(\\theta=(\\beta,\\sigma^2)\\) given the observed data \\((y,x)\\) is given by\n\\[\nl(\\theta \\mid y) = -\\frac{n}{2}\\text{ln}(2\\pi) -\\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n \\left(y_i - \\mu_i \\right)^2}{2\\sigma^2}.\n\\]\nMaximising this expression with respect to \\(\\theta\\), the MLEs are found to be equal to the least squares estimates of the intercept and regression coefficients. Using a matrix notation for the \\(n\\)-th vector of the outcome values \\(Y\\) and the \\(n\\times (J+1)\\) matrix of the covariate values (including the constant term), then the MLEs are:\n\\[\n\\hat{\\beta}=(X^{T}X)^{-1}X^{T}Y \\;\\;\\; \\text{and} \\;\\;\\; \\hat{\\sigma}^{2}=\\frac{(Y-X\\hat{\\beta})(Y-X\\hat{\\beta})}{n},\n\\]\nwhere the numerator of the fraction is known as the Residual Sum of Squares(RSS). Because the denominator of is equal to \\(n\\), the MLE of \\(\\sigma^2\\) does not correct for the loss of degrees of freedom when estimating the \\(J+1\\) location parameters. Thus, the MLE should instead divide the RSS by \\(n-(J+1)\\) to obtain an unbiased estimator. An extension of standard multiple linear regression is the so called weighted multiple linear regression, in which the regression variance is assumed to be equal to\\(\\frac{\\sigma^2}{w_i}\\), for \\((w_i) &gt; 0\\). Thus, the variable \\((y_i-\\mu)\\sqrt{w_i}\\) is Normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\), and the loglikelihood is\n\\[\nl(\\theta \\mid y)= - \\frac{n}{2}\\text{ln}(2\\pi) - \\frac{n}{2}\\text{ln}(\\sigma^2) - \\frac{\\sum_{i=1}^n w_i(y_i - \\mu_i)^2}{2\\sigma^2}.\n\\]\nMaximising this function yields MLEs given by the weighted least squares estimates\n\\[\n\\hat{\\beta}=\\left(X^{T}WX\\right)\\^{-1}\\left(X^{T}WY \\right) \\;\\;\\; \\text{and} \\;\\;\\; \\sigma^{2}=\\frac{\\left(Y-X\\hat{\\beta}\\right)^{T}W\\left(Y-X\\hat{\\beta}\\right)}{n},\n\\]\nwhere \\(W=\\text{Diag}(w_1,\\ldots,w_n)\\)."
  },
  {
    "objectID": "missing_data/intro_mle/intro_mle.html#generalised-linear-models",
    "href": "missing_data/intro_mle/intro_mle.html#generalised-linear-models",
    "title": "Introduction to Maximum Likelihood Estimation",
    "section": "Generalised Linear Models",
    "text": "Generalised Linear Models\nConsider the previous example where we had an outcome variable \\(y\\) and a set of \\(J\\) covariates, each measured on \\(n\\) units. A more general class of models, compare with the Normal model, assumes that, given \\(x\\), the values of \\(y\\) are an independent sample from a regular exponential family distribution\n\\[\nf(y \\mid x,\\beta,\\phi)=\\text{exp}\\left(\\frac{\\left(y\\delta\\left(x,\\beta \\right) - b\\left(\\delta\\left(x,\\beta\\right)\\right)\\right)}{\\phi} + c\\left(y,\\phi\\right)\\right),\n\\]\nwhere \\(\\delta()\\) and \\(b()\\) are known functions that determine the distribution of \\(y\\), and \\(c()\\) is a known function indexed by a scale parameter \\(\\phi\\). The mean of \\(y\\) is assumed to linearly relate to the covariates via\n\\[\nE\\left[y \\mid x,\\beta,\\phi \\right]=g^{-1}\\left(\\beta_0 + \\sum_{j=1}^J\\beta_jx_{j} \\right),\n\\]\nwhere \\(E\\left[y \\mid x,\\beta,\\phi \\right]=\\mu_i\\) and \\(g()\\) is a known one to one function which is called link function because it “links” the expectation of \\(y\\) to a linear combination of the covariates. The canonical link function\n\\[\ng_c(\\mu_i)=\\delta(x_{i},\\beta)=\\beta_0+\\sum_{j=1}^J\\beta_jx_{ij},\n\\]\nwhich is obtained by setting \\(g()\\) equal to the inverse of the derivative of \\(b()\\) with respect to its argument. Examples of canonical links include\n\nNormal linear regression: \\(g_c=\\text{identity}\\), \\(b(\\delta)=\\frac{\\delta^2}{2},\\phi=\\sigma^2\\)\nPoisson regression: \\(g_c=\\log\\), \\(b(\\delta)=\\text{exp}(\\delta),\\phi=1\\)\nLogistic regression: \\(g_c=\\text{logit}\\), \\(b(\\delta)=\\log(1+\\text{exp}(\\delta)),\\phi=1\\)\n\nThe loglikelihood of \\(\\theta=(\\beta,\\phi)\\) given the observed data \\((y,x)\\), is\n\\[\nl(\\theta \\mid y,x)=\\sum_{i=1}^n \\left[\\frac{\\left(y_i\\delta\\left(x_i,\\beta\\right)-b\\left(\\delta\\left(x_i,\\beta\\right)\\right) \\right)}{\\phi}+c\\left(y_i,\\phi\\right)\\right],\n\\]\nwhich for non-normal cases does not have explicit maxima and numerical maximisation can be achieved using iterative algorithms."
  },
  {
    "objectID": "missing_data/isi/isi.html",
    "href": "missing_data/isi/isi.html",
    "title": "Implicit Single Imputation",
    "section": "",
    "text": "All case deletion methods, such as Complete Case Analysis(CCA) or Available Case Analysis(ACA) make no use of units with partially observed data, when estimating the marginal distribution of the variables under study or the covariation between variables. Clearly, this is inefficient and a tempting alternative would be to impute or “fill in” the unobserved data with some plausible values. When a single value is used to replace each missing data, we talk about Single Imputation(SI) methods and, according to the precedure used to generate these imputations, different SI methods can be used. In general, the idea of imputing the missing values is really appealing as it allows to recover the full sample on which standard complete data methods can be applied to derive the estimates of interest.\nHowever, it is important to be aware of the potential problems of imputing missing data without a clear understanding about the process underlying the values we want to impute, which is the key factor to determine whether the selected approach would be plausible in the context considered. Indeed, imputation should be conceptualised as draws from a predictive distribution of the missing values and require methods for creating a predictive distribution for the imputation based on the observed data. According to Little and Rubin (2019), these predictive distributions can be created using\nIn this part, we focus on some of the most popular Implicit Single Imputation methods. These include: Hot Deck Imputation(SI-HD), where missing values are imputed using observed values from similar responding units in the sample; Substitution(SI-S), where nonresponding units are replaced with alternative units not yet selected into the sample; Cold Deck Imputation(SI-CD), where missing values are replaced with a constant value from an external source; Composite Methods, which combine procedures from the previous approaches. We will specifically focus on SI-HD methods, which are the most popular among these."
  },
  {
    "objectID": "missing_data/isi/isi.html#hot-deck-imputation",
    "href": "missing_data/isi/isi.html#hot-deck-imputation",
    "title": "Implicit Single Imputation",
    "section": "Hot Deck Imputation",
    "text": "Hot Deck Imputation\nSI-HD procedures refer to the deck of match Hollerith cards for the donors available for a nonrespondent. Suppose that a sample of \\(n\\) out of \\(N\\) units is selected and that \\(n_{cc}\\) out of \\(n\\) are recorded. Given an equal probability sampling scheme, the mean of \\(y\\) can be estimated from the filled-in data as the mean of the responding and the imputed units\n\\[\n\\bar{y}_{HD}=\\frac{(n_{cc}\\bar{y}_{cc}+(n-n_{cc})\\bar{y}^{\\star})}{n},\n\\]\nwhere \\(\\bar{y}_{cc}\\) is the mean of the responding units, and \\(\\bar{y}^\\star=\\sum_{i=1}^{n_{cc}}\\frac{H_iy_i}{n-n_{cc}}\\). \\(H_i\\) is the number of times \\(y_i\\) is used as substitute for a missing value of \\(y\\), with \\(\\sum_{i=1}^{n_{cc}}H_i=n-n_{cc}\\) being the number of missing units. The proprties of \\(bar{y}_{HD}\\) depend on the procedure used to generate the numbers \\(H_i\\) and in general the mean and sampling variance of this estimator can be written as\n\\[\nE[\\bar{y}_{HD}]=E[E[\\bar{y}_{HD}\\mid y_{obs}]] \\;\\;\\; \\text{and} \\;\\;\\; Var(\\bar{y}_{HD})=Var(E[\\bar{y}_{HD} \\mid y_{obs}]) + E[Var(\\bar{y}_{HD} \\mid y_{obs})],\n\\]\nwhere the inner expectations and variances are taken over the distribution of \\(H_i\\) given the observed data \\(y_{obs}\\), and the outer expectations and variances are taken over the model distribution of \\(y\\). The term \\(E[Var(\\bar{y}_{HD} \\mid y_{obs})]\\) represents the additional sampling variance from the stochastic imputation procedure. Examples of these procedures include predictive mean matching or PMM(Little and Rubin (2019)) and last value carried forward or LVCF(Little and Rubin (2019)).\n\nPredictive Mean Matching\nA general approach to hot-deck imputation is to define a metric \\(d(i,j)\\) measuring the distance between units based on observed variables \\(x_{i1},\\ldots,x_{iJ}\\) and then choose the imputed values that come from responding units close to the unit with the missing value, i.e. we choose the imputed value for \\(y_i\\) from a donor pool of units \\(j\\) that are such that \\(y_j,x_1,\\ldots,x_J\\) are observed and \\(d(i,j)\\) is less than some value \\(d_0\\). Varying the value for \\(d_0\\) can control the number of available donors \\(j\\). When the choice of the metric has the form\n\\[\nd(i,j)=(\\hat{y}(x_i)-\\hat{y}(x_j))^2,\n\\]\nwhere \\(\\hat{y}(x_i)\\) is the predicted value of \\(y\\) from the regression of \\(y\\) on \\(x\\) from the complete units, then the procedure is known as PMM. A powerful aspect of this metric is that it weights predictors according to their ability to predict the missing variable, which allows to have some protection against misspecification of the regression of \\(y\\) on \\(x\\), even though better approaches are available when good matches to donor units cannot be found or the sample size is small.\n\n\nLast Value Carried Forward\nLongitudinal data are often subject to attrition when units leave the study prematurely. Let \\(y_i=(y_{i1},\\ldots,y_{iJ})\\) be a \\((J\\times1)\\) vector of partially-observed outcomes for subject \\(i\\), and denote with \\(y_{i,obs}\\) and \\(y_{i,mis}\\) the observed and missing components of \\(y_i\\), i.e. \\(y=(y_{i,obs},y_{i,mis})\\). Define the indicator variable \\(m_i\\) taking value 0 for complete units and \\(j\\) if subject \\(i\\) drops out between \\(j-1\\) and \\(j\\) time points. LVCF, also called last observation carried forward(Pocock (2013)), imputes all missing values for individual \\(i\\) (for whom \\(m_i=j\\)) using the last recorded value for that unit, that is\n\\[\n\\hat{y}_{it}=y_{i,j-1},\n\\]\nwhere \\(t=j,\\ldots,J\\). Although simple, this approach makes the often unrealistic assumption that the value of the outcome remains unchanged after dropout."
  },
  {
    "objectID": "missing_data/isi/isi.html#conclusions",
    "href": "missing_data/isi/isi.html#conclusions",
    "title": "Implicit Single Imputation",
    "section": "Conclusions",
    "text": "Conclusions\nAccording to Little and Rubin (2019), imputation should generally be\n\nConditional on observed variables, to reduce bias, improve precision and preserve association between variables.\nMultivariate, to preserve association between missing variables.\nDraws from the predictive distributions rather than means, to provide valid estimates of a wide range of estimands.\n\nNevertheless, a main problem of SI methods is that inferences based on the imputed data do not account for imputation uncertainty and standard errors are therefore systematically underestimated, p-values of tests are too significant and confidence intervals are too narrow."
  },
  {
    "objectID": "missing_data/likinf/likinf.html",
    "href": "missing_data/likinf/likinf.html",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "",
    "text": "As for the inference under complete data, inference under incomplete data consists in deriving the likelihood for the parameters based on the available data, either using a Maximum Likelihood (ML) approach (solving the likelihood equation) or using the Bayes’ rule incorporating a prior distribution (performing necessary integrations to obtain the posterior distribution). However, asymptotic standard errors obtained from the information matrix, are more questionable when dealing with missing data since the sample will not be typically iid and results that imply the large sample normality of the likelihood function do not immediately apply. More complications arise when dealing with the process that lead to some of the data to be missing. This can be explained with a simple example.\nLet \\(Y=(y_{ij})\\), for \\(i=1,\\ldots,n\\) and \\(j=1,\\ldots,J\\), denote the complete dataset if there were no missing values, with a total of \\(n\\) units and \\(J\\) variables. Let \\(M=(m_{ij})\\) denote the fully observed matrix of binary missing data indicators with \\(m_{ij}=1\\) if \\(y_{ij}\\) is missing and \\(0\\) otherwise. As an example, we can model the density of the joint distribution of \\(Y\\) and \\(M\\) using the selection model factorisation (Little and Rubin (2019))\n\\[\np(Y=y,M=m \\mid \\theta, \\psi) = f(y \\mid \\theta)f(m \\mid y, \\psi),\n\\]\nwhere \\(\\theta\\) is the parameter vector indexing the response model and \\(\\psi\\) is the parameter vector indexing the missingness mechanism. The observed values \\(m\\) effect a partition \\(y=(y_1,y_0)\\), where \\(y_0=[y_{ij} : m_{ij}=0]\\) is the observed component and \\(y_1=[y_{ij} : m_{ij}=1]\\) is the missing component of \\(y\\). The full likelihood based on the observed data and the assumed model is\n\\[\nL_{full}(\\theta, \\psi \\mid y_{0},m) = \\int f\\left(y_{0},y_{1} \\mid \\theta \\right) f\\left(m \\mid y_{0},y_{1}, \\psi \\right)dy_{1}\n\\]\nand is a function of the parameters \\((\\theta,\\psi)\\). Next, we define the likelihood of ignoring the missingness mechanism or ignorable likelihood as\n\\[\nL_{ign}\\left(\\theta \\mid y_{0} \\right) = \\int f(y_{0},y_{1}\\mid \\theta)dy_{1},\n\\]\nwhich does not involve the model for \\(M\\). In practice, modelling the joint distribution of \\(Y\\) and \\(M\\) is often challenging and, in fact, many approaches to missing data do not model \\(M\\) and (explicitly or implicitly) base inference about \\(\\theta\\) on the ignorable likelihood. It is therefore important to assess under which conditions inferences about \\(\\theta\\) based on \\(L_{ign}\\) can be considered appropriate. More specifically, the missingness mechanism is said to be ignorable if inferences about \\(\\theta\\) based on the ignorable likelihood equation evauluated at some realisations of \\(y_0\\) and \\(m\\) are the same as inferences about \\(\\theta\\) based on the full likelihood equation, evaluated at the same realisations of \\(y_0\\) and \\(m\\). The conditions for ignoring the missingness mechanism depend on whether the inferences are direct likelihood, Bayesian or frequentist."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#direct-likelihood-inference",
    "href": "missing_data/likinf/likinf.html#direct-likelihood-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Direct Likelihood Inference",
    "text": "Direct Likelihood Inference\nDirect Likelihood Inference refers to inference based solely on likelihood ratios for pair of values of the parameters, with the data fixed at their observed values. The missingness mechanism can be ignored for direct likelihood if the likelihood ratio based on the ignorable likelihood is the same as the ratio based on the full likelihood. More precisely, the missingness mechanism is said to be ignorable for direct likelihood inference at some realisations of \\((y_0,m)\\) if the likelihood ratio for two values \\(\\theta\\) and \\(\\theta^\\star\\) is the same whether based on the full or ignorable likelihood. That is\n\\[\n\\frac{L_{full}\\left( \\theta, \\psi \\mid y_{0}, m \\right)}{L_{full}\\left( \\theta^{\\star}, \\psi \\mid y_{0}, m \\right)}=\\frac{L_{ign}\\left( \\theta \\mid y_{0} \\right)}{L_{ign}\\left( \\theta^{\\star} \\mid y_{0}\\right)},\n\\]\nfor all \\(\\theta\\), \\(\\theta^\\star\\) and \\(\\psi\\). In general, the missingnes mechanism is ignorable for direct likelihood inference if the following two conditions hold:\n\nParameter distinctness. The parameters \\(\\theta\\) and \\(\\psi\\) are distinct, in the sense that the joint parameter space \\(\\Omega_{\\theta,\\psi}\\) is the product of the two parameter spaces \\(\\Omega_{\\theta}\\) and \\(\\Omega_{\\psi}\\).\nFactorisation of the full likelihood. The full likelihood factors as\n\n\\[\nL_{full}\\left(\\theta, \\psi \\mid y_{0},m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right)  L_{rest}\\left(\\psi \\mid y_{0},m \\right)\n\\]\nfor all values of \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). The distinctness condition ensures that each value of \\(\\psi \\in \\Omega_{\\psi}\\) is compatible with different values of \\(\\theta \\in \\Omega_{\\theta}\\). A sufficient condition for the factorisation of the full likelihood is that the missing data are Missing At Random(MAR) at the specific realisations of \\(y_{0},m\\). This means that the distribution function of \\(M\\), evaluated at the given realisations \\((y_{0},m)\\), does not depend on the missing values \\(y_1\\), that is\n\\[\nf\\left(m \\mid y_{0}, y_{1}, \\psi \\right)=f\\left(m \\mid y_{0}, y^{\\star}_{1} \\psi \\right),\n\\]\nfor all \\(y_{1},y^\\star_{1},\\psi\\). Thus, we have\n\\[\nf\\left(y_{0}, m \\mid \\theta, \\psi \\right) = f\\left(m \\mid y_{0}, \\psi \\right) \\int f\\left(y_{0},y_{1} \\mid \\theta \\right)dy_{1} = f\\left(m \\mid y_{0}, \\psi \\right) f\\left( y_{0} \\mid \\theta \\right).\n\\]\nFrom this it follows that, if the missing data are MAR at the given realisations of \\((y_{0},m)\\) and \\(\\theta\\) and \\(\\psi\\) are distinct, the missingnes mechanism is ignorable for likelihood inference."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#bayesian-inference",
    "href": "missing_data/likinf/likinf.html#bayesian-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nBayesian Inference under the full model for \\(Y\\) and \\(M\\) requires that the full likelihood is combined with a prior distribution \\(p(\\theta,\\psi)\\) for the parameters \\(\\theta\\) and \\(\\psi\\), that is\n\\[\np\\left(\\theta, \\psi \\mid y_{0}, m \\right) \\propto p(\\theta, \\psi) L_{full}\\left(\\theta, \\psi \\mid y_{0}, m \\right).\n\\]\nBayesian inference ignoring the missingness mechanism combines the ignorable likelihood with a prior distribution for \\(\\theta\\) alone, that is\n\\[\np(\\theta \\mid y_{0}) \\propto p(\\theta) L_{ign}\\left(\\theta \\mid y_{0} \\right).\n\\]\nMore formally, the missingness mechanism is said to be ignorable for Bayesian inference at the given realisations of \\((y_{0},m)\\) if the posterior distribution for \\(\\theta\\) based on the posterior distribution for the full likelihood and prior distribution for \\((\\theta,\\psi)\\) is the same as the posterior distribution for the ignorable likelihood and the prior distribution for \\(\\theta\\) alone. This holds when the following conditions are satisfied:\n\nThe parameters \\(\\theta\\) and \\(\\psi\\) are a priori independent, that is the prior distribution has the form\n\n\\[\np(\\theta , \\psi) = p(\\theta) p(\\psi)\n\\]\n\nThe full likelihood evaluated at the realisations of \\((y_{0},m)\\) factors as for direct likelihood inference\n\nUnder these conditions:\n\\[\np(\\theta, \\psi \\mid y_{0}, m) \\propto \\left(p(\\theta)L_{ign}\\left( \\theta \\mid y\\_{0} \\right) \\right) \\left(p(\\psi)L_{rest}\\left(\\psi \\mid y_{0},m \\right)  \\right).\n\\]\nAs for direct likelihood inference, MAR is a sufficient condition for the factorisation of the full likelihood. This means that, if the data are MAR at the given realisations of \\((y_{0},m)\\) and the parameters \\(\\theta\\) and \\(\\psi\\) are a prior independent, then the missingness mechanism is ignorable for Bayesian inference. We note that the a priori condition is more stringent than the distinctness condition because paramerers with distinct parameter spaces might have dependent prior distributions."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#frequentist-asymptotic-inference",
    "href": "missing_data/likinf/likinf.html#frequentist-asymptotic-inference",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Frequentist Asymptotic Inference",
    "text": "Frequentist Asymptotic Inference\nFrequentist Asymptotic Inference requires that, in order to ignore the missingness mechanism, the factorisation of the full likelihood needs to be valid for values of the observed data under repeated sampling. This means that we require\n\\[\nL_{full}\\left(\\theta,\\psi \\mid y_{0}, m \\right) = L_{ign}\\left(\\theta \\mid y_{0} \\right) L_{rest}\\left(\\psi \\mid y_{0}, m \\right)\n\\]\nfor all \\(y_{0},m\\) and \\(\\theta,\\psi \\in \\Omega_{\\theta,\\psi}\\). For this form of inference, a sufficient condition for ignoring the missingness mechanism is given by the following conditions:\n\nParameter distinctness as defined for direct likelihood inference.\nMissing data are Missing Always At Random (MAAR), that is\n\n\\[\nf\\left(m \\mid y_{0},y_{1},\\psi \\right) = f\\left(m \\mid y_{0}, y^{\\star}_{1},\\psi \\right)\n\\]\nfor all \\(m,y_{0},y_{1},y^\\star_{1},\\psi\\). In the following example we discuss conditions for ignoring the missingness mechanism for direct likelihood and Bayesian inference, which can be extended to the case of frequentist asymptotic inference by requiring that they hold for for values of \\(y_{0},m\\) other than those observed that could arise in repeated sampling."
  },
  {
    "objectID": "missing_data/likinf/likinf.html#bivariate-normal-sample-with-one-variable-subject-to-missingness",
    "href": "missing_data/likinf/likinf.html#bivariate-normal-sample-with-one-variable-subject-to-missingness",
    "title": "Likelihood Based Inference with Incomplete Data",
    "section": "Bivariate Normal Sample with One Variable Subject to Missingness",
    "text": "Bivariate Normal Sample with One Variable Subject to Missingness\nConsider a bivariate normal sample \\(y=(y_{i1},y_{i2})\\), for \\(i=1,\\ldots,n\\) units, but with the values of \\(y_{i2}\\) being missing for \\(i=(n_{cc}+1),\\ldots,n\\). This leads to a monotone missing data pattern with two variables. The loglikelihood of ignoring the missingness mechanism is\n\\[\nl_{ign}\\left(\\mu, \\Sigma \\mid y_{0} \\right) = \\log\\left(L_{ign}\\left(\\mu,\\Sigma \\mid y_{0} \\right) \\right) = - \\frac{1}{2}n_{cc}ln \\mid \\Sigma \\mid - \\frac{1}{2}\\sum_{i=1}^{n_{cc}}(y_i - \\mu ) \\Sigma^{-1}(y_i - \\mu)^{T} - \\frac{1}{2}(n-n_{cc})ln\\sigma_{1} - \\frac{1}{2}\\sum_{i=n_{cc}+1}^{n}\\frac{(y_{i1}-\\mu_1)^2}{\\sigma_{1}}.\n\\]\nThis loglikelihood is appropriate for inference provided the conditional distribution of \\(M\\) does not depend on the values of \\(y_{i2}\\), and \\(\\theta=(\\mu,\\Sigma)\\) is distinct from \\(\\psi\\). Under these conditions, ML estimates of \\(\\theta\\) can be found by maximising this loglikelihood. For Bayesian inference, if these conditions hold and the prior distribution for \\((\\theta,\\psi)\\) has the form \\(p(\\theta)p(\\psi)\\), then the joint posterior distribution of \\(\\theta\\) is proportional to the product of \\(p(\\theta)\\) and \\(L_{ign}(\\theta \\mid y_{0})\\)."
  },
  {
    "objectID": "missing_data/mice/mice.html",
    "href": "missing_data/mice/mice.html",
    "title": "Multiple Imputation by Chained Equations",
    "section": "",
    "text": "Multiple Imputation(MI) refers to the procedure of replacing each missing value by a set of \\(H\\geq 2\\) imputed values. These are ordered in the sense that \\(H\\) completed data sets can be created from the sets of imputations, where the first imputed value replaces the missing value in the first completed data set, the second imputed value in the second completed data set, and so on. Next, standard complete data methods are used to analyse each completed data set. When the \\(H\\) sets of imputations are repeated random draws from the predictive distribution of the missing data under a particular model of missingness, the \\(H\\) completed data inferences can be combined to form one inference that properly reflects uncertainty due to missing values under that model. In general, MI procedures can be summarised in three main steps:\nMi was first proposed by Rubin (Rubin (1978)) and has become more popular over time (Rubin (1996), Schafer and Graham (2002), Little and Rubin (2019)), as well as the focus of research for methodological and practical applications in a variety of fields (Herzog and Rubin (1983), Rubin and Schenker (1987), Schafer (1999), Carpenter and Kenward (2012), Molenberghs et al. (2014), Van Buuren (2018)). MI shares both advantages of Single Imputaiton (SI) methods and solves both disadvantages. Indeed, like SI, MI methods allow the analyst to use familiar complete data methods when analysing the completed data sets. The only disadvantage of MI compared with SI methods is that it takes more time to generate the imputations and analyse the completed data sets. However, Rubin (2004) showed that in order to obtain sufficiently precise estimates, a relatively small number of imputations (typically \\(10\\)) is required. For example, considering a situation with \\(\\lambda=50\\%\\) missing information and \\(H=10\\) imputations, the efficiency of MI can be shown to be equal to \\((1+\\frac{\\lambda}{H})^{-1}=95\\%\\). In addition, in today’s computing environments, the work of analysing the completed data sets is quite modest since it involves performing the same task \\(H\\) times. Thus, once a precedure to combine multiple completed data sets is established, the additonal time and effort to handle \\(50\\), \\(20\\), or \\(10\\) imputations if often of little consequence.\nIn the first step of MI, imputations should ideally be created as repeated draws from the posterior predictive distribution of the missing values \\(y_{mis}\\) given the observed values \\(y_{obs}\\), each repetition being an independent drawing of the parameters and missing values. In practice, implicit imputation models can also be used in place of explicit imputation models (Herzog and Rubin (1983)). In the second step, each completed data set is analysed using the same complete data method that would be used in the absence of missingness. Finally, in the last step, standard procedures should be used to combine the compelted data inferences into a single one. The simplest and most popular method for combining the reuslts of \\(H\\) completed data sets is known as Rubin’s rules (Rubin (2004)), which can be explained with a simple example."
  },
  {
    "objectID": "missing_data/mice/mice.html#rubins-rules",
    "href": "missing_data/mice/mice.html#rubins-rules",
    "title": "Multiple Imputation by Chained Equations",
    "section": "Rubin’s rules",
    "text": "Rubin’s rules\nLet \\(\\hat{\\theta}_h\\) and \\(V_h\\), for \\(h=1,\\ldots,H\\), be the completed data estimates and sampling variances for a scalar estimand \\(\\theta\\), calculated from \\(H\\) repeated imputations under a given imputation model. Then, according to Rubin’s rules, the combined estimate is simply the average of the \\(H\\) completed data estimates, that is\n\\[\n\\bar{\\theta}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}\\hat{\\theta}_{h}.\n\\]\nBecause the imputations under MI are conditional draws, under a good imputaton model, they provide valid estimates for a wide range of estimands. In addition, the averaging over \\(H\\) imputed data sets increases the efficiency of estimation over that obtained from a single completed data set. The variability associated with the pooled estimate has two components: the average within-imputation variance \\(\\bar{V}_H\\) and the between-imputation variance \\(B_H\\), defined as\n\\[\n\\bar{V}_{H}=\\frac{1}{H}\\sum_{h=1}^{H}V_{h} \\;\\;\\; \\text{and} \\;\\;\\; B_{H}=\\frac{1}{H-1}\\sum_{h=1}^{H}(\\hat{\\theta}_{h}-\\bar{\\theta}_{H})^2.\n\\]\nThe total variability associated with \\(\\bar{\\theta}_H\\) is the computed as\n\\[\nT_{H}=\\bar{V}_H + \\frac{H+1}{H}B_{H},\n\\]\nwhere \\((1+\\frac{1}{H})\\) is an adjustment factor for finite due to estimating \\(\\theta\\) by \\(\\bar{\\theta}_H\\). Thus, \\(\\hat{\\lambda}_H=(1+\\frac{1}{H})\\frac{B_H}{T_H}\\) is known as the fraction of missing information and is an estimate of the fraction of information about \\(\\theta\\) that is missing due to nonresponse. For large sample sizes and scalar quantities like \\(\\theta\\), the reference distribution for interval estimates and significance tests is a \\(t\\) distribution\n\\[\n(\\theta - \\bar{\\theta}_H)\\frac{1}{\\sqrt{T^2_H}} \\sim t_v,\n\\]\nwhere the degrees of freedom \\(v\\) can be approximated with the quantity \\(v=(H-1)\\left(1+\\frac{1}{H+1}\\frac{\\bar{V}_H}{B_H} \\right)^2\\) (Rubin and Schenker (1987)). In small data sets, an improved version of \\(v\\) can be obtained as \\(v^\\star=(\\frac{1}{v}+\\frac{1}{\\hat{v}_{obs}})^{-1}\\), where\n\\[\n\\hat{v}_{obs}=(1-\\hat{\\lambda}_{H})\\left(\\frac{v_{com}+1}{v_{com}+3}\\right)v_{com},\n\\]\nwith \\(v_{com}\\) being the degrees of freedom for appropriate or exact \\(t\\) inferences about \\(\\theta\\) when there are no missing values (Barnard and Rubin (1999)).\nThe validity of MI rests on how the imputations are created and how that procedure relates to the model used to subsequently analyze the data. Creating MIs often requires special algorithms (Schafer (1997)). In general, they should be drawn from a distribution for the missing data that reflects uncertainty about the parameters of the data model. Recall that with SI methods, it is desirable to impute from the conditional distribution \\(p(y_{mis}\\mid y_{obs},\\hat{\\theta})\\), where \\(\\hat{\\theta}\\) is an estimate derived from the observed data. MI extends this approach by first simulating \\(H\\) independent plausible values for the parameters \\(\\theta_1,\\ldots,\\theta_H\\) and then drawing the missing values \\(y_{mis}^h\\) from \\(p(y_{mis}\\mid y_{obs}, \\theta_h)\\). Treating parameters as random rather than fixed is an essential part of MI. For this reason, it is natural (but not essential) to motivate MI from the Bayesian perspective, in which the state of knowledge about parameters is represented through a posterior distribution."
  },
  {
    "objectID": "missing_data/mice/mice.html#multiple-imputation-by-chained-equations",
    "href": "missing_data/mice/mice.html#multiple-imputation-by-chained-equations",
    "title": "Multiple Imputation by Chained Equations",
    "section": "Multiple Imputation by Chained Equations",
    "text": "Multiple Imputation by Chained Equations\nMI by Chained Equations, also known as Fully Conditional Specification(FCS), imputes multivariate missing data on a variable-by-variable basis, and therefore requires the specification of an imputation model for each incomplete variable to create imputations per variable in an iterative fashion (Van Buuren (2007)). In contrast to Joint MI, MICE specifies the multivariate distribution for the outcome and missingness pattern \\(p(y,r\\mid \\theta, \\phi)\\), indexed by the parameter vectors of the outcome (\\(\\theta\\)) and missingness models (\\(\\phi\\)), through a set of conditional densities \\(p(y_j \\mid y_{-j},r,\\theta_j, \\phi_j)\\), which is used to impute \\(y_j\\) given the other variables. Starting from a random draw from the marginal distribution of \\(y_1\\), imputation is then carried out by iterating over the conditionally specified imputation models for each \\(y_j=(y_2,\\ldots,y_J)\\) separately given the set of all the other variables \\(y_{-j}\\).\nTha main idea of MICE is to directly draw the missing data from the predictive distribution of conditional densities, therefore avoiding the need to specify a joint multivariate model for all the data. Different approaches can be used to implement MICE. For example, a possible strategy is the following:\n\nStart at iteration \\(t=0\\) by drawing randomly from the the distribution of the missing data given the observed data and all other variables, according to some probability model for each variable \\(y_j\\), that is\n\n\\[\n\\hat{y}^{mis}_{j,0} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, y_{-j}, r)\n\\]\n\nAt each iteration \\(t=1,\\ldots,T\\) and for each variable \\(j=\\ldots,J\\), set\n\n\\[\n\\hat{y}^{mis}_{-j,t}=\\left(\\hat{y}_{1,t},\\ldots, \\hat{y}_{j-1,t}, \\hat{y}_{j+1,t}, \\ldots, \\hat{y}_{J,t} \\right)\n\\]\nas the currently completed data except \\(y_j\\)\n\nDraw \\(h=1,\\ldots,H\\) imputations for each variable \\(y_j\\) from the predictive distribution of the missing data given the observed data and the currently imputed data at \\(t\\), that is\n\n\\[\n\\hat{y}^{mis}_{j,t} \\sim p(y^{mis}_{j} \\mid y^{obs}_{j}, \\hat{y}_{-j,t}, r)\n\\]\nand repeat the steps 2 and 3 until convergence. It is important to stress out that MICE is essentially a Markov Chain Monte Carlo(MCMC) algorithm (Brooks et al. (2011)), where the state space is the collection of all imputed values. More specifically, when the conditional distributions of all variables are compatible with a joint multivariate distribution, the algorithm corresponds to a Gibbs sampler, a Bayesian simulation method that samples from the conditional distributions in order to obtain samples from the joint multivariate distribution of all variables via some conditional factorisation of the latter (Casella and George (1992), Gilks, Richardson, and Spiegelhalter (1996)). A potential issue of MICE is that, since the conditional distributions are specified freely by the user, these may not be compatible with a joint distribution and therefore it is not clear from which distribution the algorithm is sampling from. However, a general advatage of MICE is that it gives freedom to the user for the specification of the univariate models for the variables, which can be tailored to handle different types of variabes (e.g. continuous and categorical) and different statistical issues for each variable (e.g. skewness and non-liner associations).\nRegardless of the theoretical implications of MICE, as a MCMC method, the algorithm converges to a stationary distribution when three conditions are satisfied (Roberts (1996),Brooks et al. (2011)):\n\nThe chain is irreducible, i.e. must be able to reach any state from any state in the state space\nThe chain is aperiodic, i.e. must be able to return to each state after some unknown number of steps or transitions\nThe chain is recurrent, i.e. there is probability of one of eventually returning to each state after some number of steps\n\nTypically periodicity and non-recurrence can be a problem in MICE when the imputation models are not compatible, possibly leading to different inferences based on the stopping point of the chain or to non-stationary behaviours of the chain."
  },
  {
    "objectID": "missing_data/sm/sm.html",
    "href": "missing_data/sm/sm.html",
    "title": "Selection Models",
    "section": "",
    "text": "It is possible to summarise the steps involved in drawing inference from incomplete data as (Daniels and Hogan (2008)):\nIdentification of a full data model, particularly the part involving the missing data \\(Y_{mis}\\), requires making unverifiable assumptions about the full data model \\(f(y,r)\\). Under the assumption of the ignorability of the missingness mechanism, the model can be identified using only the information from the observed data. When ignorability is not believed to be a suitable assumption, one can use a more general class of models that allows missing data indicators to depend on missing responses themselves. These models allow to parameterise the conditional dependence between \\(R\\) and \\(Y_{mis}\\), given \\(Y_{obs}\\). Without the benefit of untestable assumptions, this association structure cannot be identified from the observed data and therefore inference depends on some combination of two elements:\nWe show some simple examples about how these nonignorable models can be constructed, identified and applied. In this section, we specifically focus on the class of nonignorable models known as Selection Models(SM)."
  },
  {
    "objectID": "missing_data/sm/sm.html#selection-models",
    "href": "missing_data/sm/sm.html#selection-models",
    "title": "Selection Models",
    "section": "Selection Models",
    "text": "Selection Models\nThe selection model approach factors the full data distribution as\n\\[\nf(y,r \\mid \\omega) = f(y \\mid \\theta) f(r \\mid y,\\psi),\n\\]\nwhere it is typically assumed that the set of full data parameters \\(\\omega\\) can be decomposed as separate parameters for each factor \\((\\theta,\\psi)\\). Thus, under the SM approach, the response model \\(f(y \\mid \\theta)\\) and the missing data mechanism \\(f(r \\mid y, \\psi)\\) must be specified by the analyst. SMs can be attractive for several reasons, including\n\nThe possibility to directly specify the model of interest \\(f(y \\mid \\theta)\\)\nThe SM factorisation appeals to Rubin’s missing data taxonomy, enabling easy characterisation of the missing data mechanism\nWhen the missingness pattern is monotone, the missigness mechanism can be formulated as a hazard function, where the hazard of dropout at some time point \\(j\\) can depend on parts of the full data vector \\(Y\\)\n\n\nExample of SM for bivariate normal data\nConsider a sample of \\(i=1,\\ldots,n\\) units from a bivariate normal distribution \\(Y=(Y_1,Y_2)\\). Assume also that \\(Y_1\\) is always observed while \\(Y_2\\) may be missing, and let \\(R=R_2\\) be the missingness indicator for the partially-observed response \\(Y_2\\). A SM factors the full data distribution as\n\\[\nf(y_1,y_2,r \\mid \\omega) = f(y_1 \\mid \\theta)f(r \\mid y_1,y_2,\\psi),\n\\]\nwhere we assume \\(\\omega=(\\theta,\\psi)\\). Suppose we specify \\(f(y_1,y_2 \\mid \\theta)\\) as a bivariate normal density with mean \\(\\mu\\) and \\(2\\times2\\) covariance matrix \\(\\Sigma\\). The distribution of \\(r\\) is assumed to be distributed as a Bernoulli variable with probability \\(\\pi_i\\), such that\n\\[\ng(\\pi_i) = \\psi_0 + \\psi_1y_{i1} + \\psi_2y_{i2},\n\\]\nwhere \\(g()\\) denotes a given link function which relates the expected value of the response to the linear predictors in the model. When this is taken as the inverse normal cumulative distribution function \\(\\Phi^{-1}()\\) the model corresponds to the Heckman probit selection model (Heckman (1976)). In general, setting \\(\\psi_2=0\\) leads to a Missing At Random(MAR) assumption; if, in addition, we have distinctness of the parameters \\(f(\\mu,\\Sigma,\\psi)=f(\\mu,\\Sigma)f(\\psi)\\), we have ignorability. We note that, even though the parameter \\(\\psi_2\\) characterises the association between \\(R\\) and \\(Y_2\\), the parametric assumptions made in this example will identify \\(\\psi_2\\) even in the absence of informative priors, that is the observed data likelihood is a function of \\(\\psi_2\\). Moreover, the parameter indexes the joint distribution of observables \\(Y_{obs}\\) and \\(R\\) and in general can be identified from the observed data. This property of parametric SMs make them ill-suited to assessing sensitivity to assumptions about the missingness mechanism.\nThe model can also be generalised to longitudinal data assuming a multivariate normal distribution for \\(Y=(Y_1,\\ldots,Y_J)\\) and replacing \\(\\pi_i\\) with a discrete time hazard function for dropout\n\\[\nh\\left(t_j \\mid \\bar{Y}_{j}\\right) = \\text{Prob}\\left(R_j = 0 \\mid R_{j-1} = 1, Y_{1},\\ldots,Y_{j} \\right).\n\\]\nUsing the logit function to model the discrete time hazard in terms of observed response history \\(\\bar{Y}_{j-1}\\) and the current but possibly unobserved \\(Y_j\\) corresponds to the model of Diggle and Kenward (1994)."
  },
  {
    "objectID": "missing_data/sm/sm.html#conlcusions",
    "href": "missing_data/sm/sm.html#conlcusions",
    "title": "Selection Models",
    "section": "Conlcusions",
    "text": "Conlcusions\nTo summarise, SMs allows to generalise ignorable models to handle nonignorable missingness by letting \\(f(r \\mid y_{obs},y_{mis})\\) to depend on \\(y_{mis}\\) and their structure directly appeals to Rubin’s taxonomy. However, identification of the missing data distribution is accomplished through parametric assumptions about the full data response model \\(f(y \\mid \\theta)\\) and the explicit form of the missingness mechanism. This makes it difficult to disentagle the type of information that is used to identify the model, i.e. parametric modelling assumptions or information from the observed data, therefore complicating the task of assessing the robustness of the results to a range of transparent and plausible assumptions."
  },
  {
    "objectID": "missing_data/wa/wa.html",
    "href": "missing_data/wa/wa.html",
    "title": "Weighting Adjustments",
    "section": "",
    "text": "The notion of reducing bias due to missingness through reweighting methods has its root in the survey literature and the basic idea is closely related to weighting in randomisation inference for finite population surveys (Little and Rubin (2019)). In particular, in probability sampling, a unit selected from a target population with probability \\(\\pi_i\\) can be thought as “representing” \\(\\pi^{-1}_i\\) units in the population and hence should be given weight \\(\\pi^{-1}_i\\) when estimating population quantities. For example, in a stratified random sample, a selected unit in stratum \\(j\\) represents \\(\\frac{N_j}{n_j}\\) population units, where \\(n_j\\) indicates the units sampled from the \\(N_j\\) population units in stratum \\(j=1,\\ldots,J\\). The population total \\(T\\) can then be estimated by the weighted sum\n\\[\nT = \\sum_{i=1}^{n}y_i\\pi^{-1}_i,\n\\]\nknown as the Horvitz-Thompson estimate (Horvitz and Thompson (1952)), while the stratified mean can be written as\n\\[\n\\bar{y}_{w} = \\frac{1}{n}\\sum_{i=1}^{n}w_iy_i,\n\\]\nwhere \\(w_i=\\frac{n\\pi^{-1}_i}{\\sum_{k=1}^n\\pi^{-1}_k}\\) is the sampling weight attached to the \\(i\\)-th unit scaled tosum up to the sample size \\(n\\). Weighting class estimators extend this approach to handle missing data such that, if the probabilities of response for unit \\(\\phi_i\\) were known, then the probability of selection and response is \\(\\pi_i\\phi_i\\) and we have\n\\[\n\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\n\\]\nwhere the sum is now over responding units and \\(w_i=\\frac{n_r(\\pi_i\\phi_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\phi_k)^{-1}}\\). In practice, the response probability \\(\\phi_i\\) is not known and is typically estimated based on the information available for respondents and nonrespondents (Schafer and Graham (2002))."
  },
  {
    "objectID": "missing_data/wa/wa.html#weighting-class-estimator-of-the-mean",
    "href": "missing_data/wa/wa.html#weighting-class-estimator-of-the-mean",
    "title": "Weighting Adjustments",
    "section": "Weighting Class Estimator of the Mean",
    "text": "Weighting Class Estimator of the Mean\nA simple reweighting approach is to partition the sample into \\(J\\) “weighting classes” according to the variables observed for respondents and nonrespondents. If \\(n_j\\) is the sample size, \\(n_{rj}\\) the number of respondents in class \\(j\\), with \\(n_r=\\sum_{j=1}^Jr_j\\), then a simple estimator of the response probability for units in class \\(j\\) is given by \\(\\frac{n_{rj}}{n_j}\\). Thus, responding units in class \\(j\\) receive weight \\(w_i=\\frac{n_r(\\pi_i\\hat{\\phi}_i)^{-1}}{\\sum_{k=1}^{n_r}(\\pi_k\\hat{\\phi}_k)^{-1}}\\), where \\(\\hat{\\phi}_i=\\frac{n_{rj}}{n_j}\\) for unit \\(i\\) in class \\(j\\). The weighting class estimate of the mean is then\n\\[\n\\bar{y}_{w} = \\frac{1}{n_r}\\sum_{i=1}^{n_r}w_iy_i,\n\\]\nwhich is unbiased under the quasirandomisation assumption (Oh and Scheuren (1983)), which requires respondents in weighting class \\(j\\) to be a random sample of the sampled units, i.e. data are Missing Completely At Random (MCAR) within adjustment class \\(j\\). Weighting class adjustments are simple because the same weights are obtained regardless of the outcome tp which they are applied, but these are inefficient and generally involves an increase in sampling variance for outcomes that are weakly related to the weighting class variable. Assuming random sampling within weighting classes, a constant variance \\(\\sigma^2\\) for an outcome \\(y\\), and ignoring sampling variation in the weights, the increase in sampling variance of a sample mean is\n\\[\n\\text{Var}\\left(\\frac{1}{n_{r}}\\sum_{i=1}^{n_{r}}w_iy_i \\right) = \\frac{\\sigma^2}{n_{r}^2}\\left(\\sum_{i=1}^{n_{r}}w_{i}^{2} \\right) = \\frac{\\sigma^2}{n_{r}}(1+\\text{cv}^2(w_i)),\n\\]\nwhere \\(\\text{cv}(w_i)\\) is the coefficient of variation of the weights (scaled to average one), which is a rough measure of the proportional increase in sampling variance due to weighting (Kish (1992)). When the weighting class variable is predictive of \\(y\\), weighting methods can lead to a reduction in sampling variance. Little and Rubin (2019) summarise the effect of weighting on the bias and sampling variance of an estimated mean, according to whether the associations between the adjustment cells and the outcome \\(y\\) and missing indicator \\(m\\) are high or low.\n\nEffect of weighting adjustments on bias and sampling variance of a mean.\n\n\n\nLow (y)\nHigh (y)\n\n\n\n\nLow (m)\nbias: /, var: /\nbias: /, var: -\n\n\nHigh (m)\nbias: /, var: +\nbias: -, var: -\n\n\n\n\n\nThus, weighting is only effective when the outcome is associated with the adjustment cell variable because otherwise the sampling variance is increased with no bias reduction."
  },
  {
    "objectID": "missing_data/wa/wa.html#propensity-weighting",
    "href": "missing_data/wa/wa.html#propensity-weighting",
    "title": "Weighting Adjustments",
    "section": "Propensity Weighting",
    "text": "Propensity Weighting\nIn some settings, weighting class estimates cannot be feasibly derived by all recorded variables X because the number of classes become too large and some may include cells with nonrespondents but no respondents for which the nonresponse weight is infinite. The theory of propensity scores (Rosenbaum and Rubin (1983)) provides a prescription for choosing the coarsest reduction of the variables to a weighting class variable \\(c\\). Suppose the data are Missing At Random (MAR) such that\n\\[\np(m\\mid X,y,\\phi)=p(m\\mid X,\\phi),\n\\]\nwhere \\(\\phi\\) are unknown parameters and define the nonresponse propensity for unit \\(i\\) as\n\\[\n\\rho(x_i,\\phi)=p(m_i=1 \\mid \\phi),\n\\]\nassuming that this is strictly positive for all values of \\(x_i\\). Then, it can be shown that\n\\[\np(m\\mid \\rho(X,\\phi),y,\\phi)=p(m\\mid \\rho(X,\\phi),\\phi),\n\\]\nso that respondents are a random subsample within strata defined by the propensity score \\(\\rho(X,\\phi)\\). In practice the parameter \\(\\phi\\) is unknown and must be estimated from sample data, for example via logistic, probit or robit regressions of the missingness indicator \\(m\\) on \\(X\\) based on respondent and nonrespondent data (Liu (2004)). A variant of this procedure is to weight respondents \\(i\\) directly by the inverse of the estimated propensity score \\(\\rho(X,\\hat{\\phi})^{-1}\\) (Cassel, Sarndal, and Wretman (1983)), which allows to remove bias but may cause two problems: 1) estimates may be associated with very high sampling variances due to nonrespondents with low response propensity estimates receiving large nonresponse weights; 2) more reliance on correct model specification of the propensity score regression than response propensity stratification."
  },
  {
    "objectID": "posts/2019-07-03-my-blog-post/index.html",
    "href": "posts/2019-07-03-my-blog-post/index.html",
    "title": "HESG Summer Meeting 2019",
    "section": "",
    "text": "I have just come back form my first Health Economists’ Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.\nI particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.\n\n\n\nThe beautiful Norwich’s cathedral\n\n\nOther nice people and colleagues from HEART and other UCL department came to HESG with me, including Caroline and Ekaterina (aka Katia), you can see them in thumbnail of this post. I was also pleased to meet Baptiste from LSHTM, who shares with me the interest in missing data methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. I also had the opportunity to talk about my work with the discussant of my session, Catrin Plumpton from the Centre for Health Economics and Medicines Evaluation, who gave me some nice feedback which I really appreciated, especially given her mathematical background.\nAn important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for Emma Mcmanus who was amazing.\n\n\n\n\n\nIn summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as WinBUGS, but this was mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am."
  },
  {
    "objectID": "posts/2019-09-15-my-blog-post/index.html",
    "href": "posts/2019-09-15-my-blog-post/index.html",
    "title": "Discussing my thesis",
    "section": "",
    "text": "I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled “Thesis Thursday” on the The Academic Health Economists blog.\nI happily accepted Chris’s invitation as I believe this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.\nHere you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.\n\n\n\n\n\nI shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so. In fact, there are not many blogs around health economics matters (here a non-comprehensive list), among which The Academic Health Economists and Gianluca’s blog are my favourites.\nI hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment."
  },
  {
    "objectID": "posts/2019-10-01-my-blog-post/index.html",
    "href": "posts/2019-10-01-my-blog-post/index.html",
    "title": "More good news…",
    "section": "",
    "text": "I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methods for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of Baptiste and Alexina which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.\n\n\n\n\n\nI am also excited to visit Brighton, since many people keep telling me that I should go and visit this sort of british version of “Rimini”. To be honest, I do not expect to find a nice weather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.\nFinally, I have started a rubric called missing data on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months."
  },
  {
    "objectID": "posts/2019-11-09-my-blog-post/index.html",
    "href": "posts/2019-11-09-my-blog-post/index.html",
    "title": "Too many things, again….",
    "section": "",
    "text": "I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.\nA couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).\nSecond, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).\nThird, I would like to quickly summarise my first experience at ISPOR Europe in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor Andrea Manca and the always very kind Chris Sampson for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under ICON plc. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend Ryan Pulleyblank, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.\nFinally, as a side note, I have found the time to upload on my arXiv page a nice application of Bayesian hierarchical models for the prediction of volleyball matches which I have been working on the past summer, taking inspiration from the work of Gianluca about predicting football macthes. I hope my work can turn out in something cool as well.\n\n\n\n\n\nThis is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Not a very good start…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nDec 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo many things, again….\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nCopenhagen, I am coming …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMore good news…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 1, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMissingHE 1.2.1\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussing my thesis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\ninterview\n\n\n\n\n\n\n\n\n\nSep 15, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe P value fallacy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHESG Summer Meeting 2019\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication/2017-a-my-publication/index.html",
    "href": "publication/2017-a-my-publication/index.html",
    "title": "Handling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations",
    "section": "",
    "text": "Abstract\nCost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed to collect resource use and preference-based health status data for the purpose of healthcare technology assessment. However, because of the way these measures are collected, they are prone to missing data, which can ultimately affect the decision of whether an intervention is good value for money. We examine how missing cost and effect outcome data are handled in RCT-based CEAs, complementing a previous review (covering 2003-2009, 88 articles) with a new systematic review (2009-2015, 81 articles) focussing on two different perspectives. First, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a quality evaluation scheme, providing a structured approach that can be used to guide the collection of information, elicitation of the assumptions, choice of methods and considerations of possible limitations of the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods. Our review shows that missing data in within-RCT CEAs are still often inadequately handled and the overall level of information provided to support the chosen methods is rarely satisfactory\n      \n\n\n\n\nCitationBibTeX citation:@online{gabrio2017,\n  author = {Gabrio, Andrea and J Mason, Alexina and Baio, Gianluca},\n  title = {Handling {Missing} {Data} in {Within-Trial}\n    {Cost-Effectiveness} {Analysis:} {A} {Review} with {Future}\n    {Recommendations}},\n  volume = {1},\n  number = {2},\n  date = {2017-06-01},\n  url = {https://link.springer.com/article/10.1007/s41669-017-0015-6},\n  doi = {10.1007/s41669-017-0015-6},\n  langid = {en},\n  abstract = {{[}Cost-effectiveness analyses (CEAs) alongside randomised\n    controlled trials (RCTs) are increasingly designed\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Alexina J Mason, and Gianluca Baio. 2017.\n“Handling Missing Data in Within-Trial Cost-Effectiveness\nAnalysis: A Review with Future Recommendations.”\nPharmacoeconomics-Open. June 1, 2017. https://doi.org/10.1007/s41669-017-0015-6."
  },
  {
    "objectID": "publication/2019-b-my-publication/index.html",
    "href": "publication/2019-b-my-publication/index.html",
    "title": "Bayesian Statistical Economic Evaluation Methods for Health Technology Assessment",
    "section": "",
    "text": "Abstract\nThe evidence produced by healthcare economic evaluation studies is a key component of any health technology assessment (HTA) process designed to inform resource allocation decisions in a budget limited context. To improve the quality (and harmonize the generation process) of such evidence, many HTA agencies have established methodological guidelines describing the normative framework inspiring their decision-making process. The information requirements that economic evaluation analyses for HTA must satisfy typically involve the use of complex quantitative syntheses of multiple available datasets, handling mixtures of aggregate and patient-level information, and the use of sophisticated statistical models for the analysis of non-Normal data (e.g. time-to-event, quality of life and costs). Much of the recent methodological research in economic evaluation for healthcare has developed in response to these needs, in terms of sound statistical decision-theoretic foundations, and is increasingly being formulated within a Bayesian paradigm. The rationale for this preference lies in the fact that by taking a probabilistic approach, based on decision rules and available information, a Bayesian economic evaluation study can explicitly account for relevant sources of uncertainty in the decision process and produce information to identify an optimal course of actions. Moreover, the Bayesian approach naturally allows the incorporation of an element of judgement or evidence from different sources (e.g.~expert opinion or multiple studies) into the analysis. This is particularly important when, as often occurs in economic evaluation for HTA, the evidence base is sparse and requires some inevitable mathematical modelling to bridge the gaps in the available data. The availability of free and open source software in the last two decades has greatly reduced the computational costs and facilitated the application of Bayesian methods and has the potential to improve the work of modellers and regulators alike, thus advancing the fields of economic evaluation of health care interventions. This chapter provides an overview of the areas where Bayesian methods have contributed to the address the methodological needs that stem from the normative framework adopted by a number of HTA agencies.\n\n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea and Baio, Gianluca and Manca, Andrea},\n  title = {Bayesian {Statistical} {Economic} {Evaluation} {Methods} for\n    {Health} {Technology} {Assessment}},\n  date = {2019-06-01},\n  url = {https://oxfordre.com/economics/view/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-451},\n  doi = {10.1093/acrefore/9780190625979.013.451},\n  langid = {en},\n  abstract = {{[}The evidence produced by healthcare economic evaluation\n    studies is a key component of any health technology assessment (HTA)\n    process ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Gianluca Baio, and Andrea Manca. 2019. “Bayesian\nStatistical Economic Evaluation Methods for Health Technology\nAssessment.” Oxford Research Encyclopedia of Economics and\nFinance, Oxford University Press. June 1, 2019. https://doi.org/10.1093/acrefore/9780190625979.013.451."
  },
  {
    "objectID": "publication/2019-d-my-publication/index.html",
    "href": "publication/2019-d-my-publication/index.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Abstract\nStatistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives: from identifying the key characteristics which lead a team to win or lose to predicting the outcome of a game or the team rankings in national leagues. Although not as popular as football or basketball, volleyball is a team sport with both national and international level competitions in almost every country. However, there is almost no study investigating the prediction of volleyball game outcomes and team rankings in national leagues. We propose a Bayesian hierarchical model for the prediction of the rankings of volleyball national teams, which also allows to estimate the results of each match in the league. We consider two alternative model specifications of different complexity which are validated using data from the women’s volleyball Italian Serie A1 2017-2018 season.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2019,\n  author = {Gabrio, Andrea},\n  title = {Bayesian {Hierarchical} {Models} for the {Prediction} of\n    {Volleyball} {Results}},\n  volume = {48},\n  number = {2},\n  date = {2019-11-22},\n  url = {https://www.tandfonline.com/doi/full/10.1080/02664763.2020.1723506?casa_token=biq9li_YO4gAAAAA%3AusWgpHzm1x-FNL0tOjGOCyYYwUMpeXXmoAqQBVlWKqpgzHYO1wIb8uVfkfl4JwSvHJIk5_P1kOY},\n  doi = {10.1080/02664763.2020.1723506},\n  langid = {en},\n  abstract = {{[}Statistical modelling of sports data has become more\n    and more popular in the recent years and different types of models\n    have been proposed to achieve a variety of objectives\n    ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2019. “Bayesian Hierarchical Models for the\nPrediction of Volleyball Results.” Journal of Applied Statistics.\nNovember 22, 2019. https://doi.org/10.1080/02664763.2020.1723506."
  },
  {
    "objectID": "publication/2020-b-my-publication/index.html",
    "href": "publication/2020-b-my-publication/index.html",
    "title": "A Bayesian framework for patient-level partitioned survival cost-utility analysis",
    "section": "",
    "text": "Abstract\nPatient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. For end of life treatments, such as cancer treatments, modelling of cost-effectiveness/utility data may involve some form of partitioned survival analysis, where measures of health-related quality of life and survival time for both pre- and post-progression periods are combined to generate some aggregate measure of clinical benefits (e.g. quality-adjusted survival). In addition, resource use data are often collected from health records on different services from which different cost components are obtained (e.g. treatment, hospital or adverse events costs). A critical problem in these analyses is that both effectiveness and cost data present some complexities, including non-normality, spikes, and missingness, that should be addressed using appropriate methods. Bayesian modelling provides a powerful tool which has become more and more popular in the recent health economics and statistical literature to jointly handle these issues in a relatively easy way. This paper presents a general Bayesian framework that takes into account the complex relationships of trial-based partitioned survival cost-utility data, potentially providing a more adequate evidence for policymakers to inform the decision-making process. Our approach is motivated by, and applied to, a working example based on data from a trial assessing the cost-effectiveness of a new treatment for patients with advanced non-small-cell lung cancer.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2020,\n  author = {Gabrio, Andrea},\n  title = {A {Bayesian} Framework for Patient-Level Partitioned Survival\n    Cost-Utility Analysis},\n  volume = {41},\n  number = {8},\n  date = {2020-11-17},\n  url = {https://journals.sagepub.com/doi/full/10.1177/0272989X211012348},\n  doi = {10.1177/0272989X211012348},\n  langid = {en},\n  abstract = {{[}Patient-level health economic data collected alongside\n    clinical trials are an important component of the process of\n    technology appraisal ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea. 2020. “A Bayesian Framework for Patient-Level\nPartitioned Survival Cost-Utility Analysis.” Medical Decision\nMaking. November 17, 2020. https://doi.org/10.1177/0272989X211012348."
  },
  {
    "objectID": "publication/2022-b-my-publication/index.html",
    "href": "publication/2022-b-my-publication/index.html",
    "title": "Linear mixed models to handle missing at random data in trial‐based economic evaluations",
    "section": "",
    "text": "Abstract\nTrial-based cost-effectiveness analyses (CEAs) are an important source of evidence in the assessment of health interventions. In these studies, cost and effectiveness outcomes are commonly measured at multiple time points, but some observations may be missing. Restricting the analysis to the participants with complete data can lead to biased and inefficient estimates. Methods, such as multiple imputation, have been recommended as they make better use of the data available and are valid under less restrictive Missing At Random (MAR) assumption. Linear mixed effects models (LMMs) offer a simple alternative to handle missing data under MAR without requiring imputations, and have not been very well explored in the CEA context. In this manuscript, we aim to familiarise readers with LMMs and demonstrate their implementation in CEA. We illustrate the approach on a randomised trial of antidepressant, and provide the implementation code in R and Stata. We hope that the more familiar statistical framework associated with LMMs, compared to other missing data approaches, will encourage their implementation and move practitioners away from inadequate methods.\n   \n\n\n\n\nCitationBibTeX citation:@online{gabrio2022,\n  author = {Gabrio, Andrea and Plumpton, Catrin and Banerjee, Sube and\n    Leurent, Baptiste},\n  title = {Linear Mixed Models to Handle Missing at Random Data in\n    Trial‐based Economic Evaluations},\n  volume = {31},\n  number = {6},\n  date = {2022-04-10},\n  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/hec.4510},\n  doi = {10.1002/hec.4510},\n  langid = {en},\n  abstract = {{[}Trial-based cost-effectiveness analyses (CEAs) are an\n    important source of evidence in the assessment of health\n    interventions ...{]}\\{style=“font-size: 85\\%”\\}}\n}\nFor attribution, please cite this work as:\nGabrio, Andrea, Catrin Plumpton, Sube Banerjee, and Baptiste Leurent.\n2022. “Linear Mixed Models to Handle Missing at Random Data in\nTrial‐based Economic Evaluations.” Health Economics. April 10,\n2022. https://doi.org/10.1002/hec.4510."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nEconomic evaluations have been increasingly conducted in different countries to aid national decision-making bodies in resource allocation problems … \n\n\n\n\n\nJun 3, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLinear mixed models to handle missing at random data in trial‐based economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nTrial-based cost-effectiveness analyses (CEAs) are an important source of evidence in the assessment of health interventions … \n\n\n\n\n\nApr 10, 2022\n\n\nAndrea Gabrio, Catrin Plumpton, Sube Banerjee, Baptiste Leurent\n\n\n\n\n\n\n\n\n\n\n\n\nA Scoping Review of Item-Level Missing Data in Within-Trial Cost-Effectiveness Analysis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nCost-effectiveness analysis (CEA) alongside randomized controlled trials often relies on self-reported multi-item questionnaires … \n\n\n\n\n\nMar 10, 2022\n\n\nXiaoxiao Ling, Andrea Gabrio, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nA Bayesian framework for patient-level partitioned survival cost-utility analysis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nPatient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal … \n\n\n\n\n\nNov 17, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nJoint longitudinal models for dealing with missing at random data in trial-based economic evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nHealth economic evaluations based on patient-level data collected alongside clinical trials (e.g. health related quality of life and resource use measures) are an important component … \n\n\n\n\n\nMay 11, 2020\n\n\nAndrea Gabrio, Rachael M Hunter, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Hierarchical Models for the Prediction of Volleyball Results\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nstatistics\n\n\n\nStatistical modelling of sports data has become more and more popular in the recent years and different types of models have been proposed to achieve a variety of objectives … \n\n\n\n\n\nNov 22, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA Bayesian Parametric Approach to Handle Missing Longitudinal Outcome Data in Trial-Based Health Economic Evaluations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nTrial-based economic evaluations are typically performed on cross-sectional variables, derived from the responses for only the completers in the study … \n\n\n\n\n\nSep 26, 2019\n\n\nAndrea Gabrio, Michael J Daniels, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistical Economic Evaluation Methods for Health Technology Assessment\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nThe evidence produced by healthcare economic evaluation studies is a key component of any health technology assessment (HTA) process … \n\n\n\n\n\nJun 1, 2019\n\n\nAndrea Gabrio, Gianluca Baio, Andrea Manca\n\n\n\n\n\n\n\n\n\n\n\n\nA Full Bayesian Model to Handle Structural Ones and Missingness in Economic Evaluations from Individual-Level Data\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\nstatistics\n\n\n\nEconomic evaluations from individual level data are an important component of the process of technology appraisal … \n\n\n\n\n\nApr 1, 2019\n\n\nAndrea Gabrio, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\n\n\n\n\n\n\nHandling Missing Data in Within-Trial Cost-Effectiveness Analysis: A Review with Future Recommendations\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nhealth economics\n\n\n\nCost-effectiveness analyses (CEAs) alongside randomised controlled trials (RCTs) are increasingly designed … \n\n\n\n\n\nJun 1, 2017\n\n\nAndrea Gabrio, Alexina J Mason, Gianluca Baio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/hurdleHTA/hurdleHTA.html",
    "href": "research/hurdleHTA/hurdleHTA.html",
    "title": "Bayesian Modelling for Health Economic Evaluations",
    "section": "",
    "text": "Modelling Framework\nWe propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables \\((e_{it}, c_{it})\\) calculated for the \\(i-\\)th person in group \\(t\\) of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator \\(t\\). Equation 1 specifies the joint distribution \\(p(e_i,c_i)\\) as\n\\[\np(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i)\n\\tag{1}\\]\nwhere, for example, \\(p(e_i)\\) is the marginal distribution of the QALYs and \\(p(c_i\\mid e_i)\\) is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. \\(p(e_i)\\) or \\(p(e_i\\mid c_i)\\), which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either \\(e_i\\) determines \\(c_i\\) or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution \\(p(e_i,c_i)\\) in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution \\(p(e_i \\mid \\boldsymbol \\theta_e)\\) indexed by a set of parameters \\(\\boldsymbol \\theta_e\\) comprising a location \\(\\boldsymbol \\phi_{ie}\\) and a set of ancillary parameters \\(\\boldsymbol\\psi_e\\) typically including some measure of marginal variance \\(\\sigma^2_e\\). Equation 2 models the location parameter using a generalised linear structure\n\\[\ng_e(\\phi_{ie})= \\alpha_0 \\,\\,[+ \\ldots]\n\\tag{2}\\]\nwhere \\(\\alpha\\_0\\) is the intercept and the notation \\([+\\ldots]\\) indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version \\(x_i^{\\star} = (x_i - \\bar{x})\\) is used, the parameter \\(\\mu_e = g_e^{-1}(\\alpha_0)\\) represents the population average QALYs. For the costs, we consider a conditional model \\(p(c_i\\mid e_i,\\boldsymbol\\theta_c)\\), which explicitly depends on the QALYs, as well as on a set of quantities \\(\\boldsymbol\\theta_c\\), again comprising a location \\(\\phi_{ic}\\) and ancillary parameters \\(\\boldsymbol \\psi_{c}\\). For example, when normal distributions are assumed for both \\(p(e_i \\mid \\boldsymbol \\theta\\_e)\\) and \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\), i.e. bivariate normal on both outcomes, the ancillary parameters \\(\\boldsymbol\\psi_c\\) include a conditional variance \\(\\tau^2_c\\), which can be expressed as a function of the marginal variance \\(\\sigma^2_c\\). More specifically, the conditional variance of \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\) is a function of the marginal effectiveness and cost variances and has the closed form \\(\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2\\), where \\(\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}\\) and \\(\\rho\\) is the parameter capturing the correlation between the variables.\nEquation 3 models the location as a function of the QALYs as\n\\[\ng\\_c(\\phi\\_{ic}) = \\beta\\_{0} + \\beta\\_{1}(e\\_{i}-\\mu\\_{e})\\,\\,[+\\ldots]\n\\tag{3}\\]\nHere, \\((e_i-\\mu_e)\\) is the centered version of the QALYs, while \\(\\beta_{1}\\) quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, \\(\\mu_c = g_c^{-1}(\\beta_{0})\\) is the estimated population average cost. The Figure 1 shows a graphical representation of the general modelling framework.\n\n\n\n\n\n\nFigure 1: Modelling framework.\n\n\n\nThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\n\n\nExample\nThree model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. Figure 2 shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and \\(90\\%\\) HPD intervals.\n\n\n\n\n\n\nFigure 2: Imputed QALYs under alternative model specifications.\n\n\n\nThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\n\n\nConclusions\nWe have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n\nJointly model costs and QALYs;\nAccount for skewness and structural values;\nAssess the robustness of the results under a set of differing missingness assumptions.\n\nThe original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data."
  },
  {
    "objectID": "research/lmmHTA/lmmHTA.html",
    "href": "research/lmmHTA/lmmHTA.html",
    "title": "Linear mixed models to handle missing at random data in trial based economic evaluations",
    "section": "",
    "text": "Introduction\nCost‐effectiveness analyses (CEAs) conducted alongside randomised controlled trials are an important source of information for decision-makers in the process of technology appraisal (Ramsey et al., 2015). The analysis is based on healthcare outcome data and health service use, typically collected at multiple time points and then combined into overall measures of effectiveness and cost. A popular approach to handle missingness is to discard the participants with incomplete observations (complete case analysis or CCA), allowing for derivation of the overall measures based on the completers alone. We note that slightly different definitions of CCA are possible, depending on the form of the model of interest, the type of missingness and the inclusion of observed covariates. This approach, although appealing by its simplicity, has well-recognised limitations including loss of efficiency and an increased risk of bias. We propose the use of linear mixed effects models (LMMs) as an alternative approach under MAR. LMMs are commonly used for the modelling of dependent data (e.g. repeated-measures) and belong to the general class of likelihood-based methods. LMMs appear surprisingly uncommon for the analysis of repeated measures in trial-based CEA, perhaps because of a lack of awareness or familiarity with fitting LMMs.\n\n\nMethods\nLinear mixed model extends the usual linear model framework by the addition of “random effect” terms, which can take into account the dependence between observations.\n\\[\nY_{ij}=\\beta_1+\\beta_2 X_{i1}+\\ldots+\\beta_(P+1) X_{iP}+\\omega_i+\\epsilon_{ij},\n\\tag{1}\\]\nwhere \\(Y_{ij}\\) denotes the outcome repeatedly collected for each individual \\(i=1,\\ldots,N\\) at multiple times \\(j=1,\\ldots,J\\). The model parameters commonly referred to as fixed effects include an intercept \\(\\beta_1\\) and the coefficients \\((\\beta_2,\\ldots,\\beta_{(P+1)})\\) associated with the predictors \\(X_{i1},\\ldots,X_{iP}\\), while \\(\\omega_i\\) and \\(\\epsilon_{ij}\\) are two random terms: \\(\\epsilon_{ij}\\) is the usual error term and \\(\\omega_i\\) is a random intercept which captures variation in outcomes between individuals. The models can be extended to deal with more complex structures, for example by allowing the effect of the covariates to vary across individuals (random slope) or a different covariance structure of the errors. LMMs can be fitted even if some outcome data are missing and provide correct inferences under MAR.\nA particular type of LMMs commonly used in the analysis of repeated measures in clinical trials is referred to as Mixed Model for Repeated Measurement (MMRM). The model includes a categorical effect for time, an interaction between time and treatment arm, and allows errors to have different variance and correlation over time (i.e. unstructured covariance structure). Figure 1 shows some examples of possible covariance structures that may be explored for LMMs.\n\n\n\n\n\n\nFigure 1: Some examples of covariance structures in LMM\n\n\n\nIncremental (between-group) or marginal (within-group) estimates for aggregated outcomes over the trial period, such as quality-adjusted life years (QALYs) or total costs can be retrieved as linear combinations of the parameter estimates from Equation 1. For example, the mean difference in total cost is obtained by summing up the estimated differences at each follow-up point, while differences on a QALY scale can be obtained as weighted linear combinations of the coefficient estimates of the utility model.\n\n\nConclusions\nWe believe LMMs represent an alternative approach which can overcome some of these limitations.\n\nFirst, practitioners may be more comfortable with the standard regression framework.\nSecond, LMMs can be tailored to address other data features (e.g. cluster-randomised trials or non-normal distribution) while also easily combined with bootstrapping.\nThird, LMMs do not rely on imputation, and results are therefore deterministic and easily reproducible, whereas the Monte Carlo error associated with multiple imputation may cause results to vary from one imputation to another, unless the number of imputations is sufficiently large.\n\nAlthough the methodology illustrated is already known, particularly in the area of statistical analyses, to our knowledge LMMs have rarely been applied to health economic data collected alongside randomised trials. We believe the proposed methods is preferable to a complete-case analysis when CEA data are incomplete, and that it can offer an interesting alternative to imputation methods."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html",
    "href": "research/mnarHTA/mnarHTA.html",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "",
    "text": "Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "href": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "Modelling framework",
    "text": "Modelling framework\nThe distribution of the observed responses \\(\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})\\) is specified in terms of a model for the utility and cost variables at time \\(j=\\{0,1,2\\}\\), which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for \\(\\boldsymbol y_{ijt}\\) is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on \\([0,1]\\) through the transformation \\(u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}\\), and fit the model to these transformed variables. To account for the structural values \\(u_{ij} = 1\\) and \\(c_{ij} = 0\\) we use a hurdle approach by including in the model the indicator variables \\(d^u_{ij}:=\\mathbb{I}(u_{ij}=1)\\) and \\(d^c_{ij}:=\\mathbb{I}(c_{ij}=0)\\), which take value \\(1\\) if subject \\(i\\) is associated with a structural value at time \\(j\\) and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time \\(j=1,2\\), the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at \\(j=0\\)) and the utilities at the previous times (only at \\(j=\\{1,2\\}\\)). The model is summarised by Figure 1:\n\n\n\n\n\n\nFigure 1: Longitudinal model for missingness.\n\n\n\nWe use partial identifying restrictions to link the observed data distribution \\(p(\\boldsymbol y_{obs},\\boldsymbol r)\\) to the extrapolation distribution \\(p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)\\) and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern \\(\\boldsymbol y^{\\boldsymbol r}_{mis}\\) by averaging across the corresponding components that are observed and add the sensitivity parameters \\(\\boldsymbol \\Delta_j\\).\nWe define \\(\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})\\) to be time-specific location shifts at the marginal mean in each pattern and set \\(\\boldsymbol \\Delta_j = \\boldsymbol 0\\) as the benchmark scenario. We then explore departures from this benchmark using alternative priors on \\(\\boldsymbol \\Delta_j\\), which are calibrated using the observed standard deviations for costs and utilities at each time \\(j\\) to define the amplitude of the departures from \\(\\boldsymbol \\Delta_j=\\boldsymbol 0\\)."
  },
  {
    "objectID": "research/reviewNL/reviewNL.html",
    "href": "research/reviewNL/reviewNL.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Introduction\nIn the Netherlands, the Dutch National Health Care Institute (Zorginstituut Nederland or ZIN) is the body in charge of issuing recommendations and guidance on good practice in health economic evaluations, not just for pharmaceutical products, but also in relation to other fields of application such as medical devices, long-term care and forensics. In 2016, ZIN issued an update on the guidance for health economic evaluations, which aggregated into a single document and revised three separately published guidelines for pharmacoeconomics evaluation, outcomes research and costing manual. The novel aspects and future policy direction introduced by these guidelines have already been object of discussion, particularly with respect to the potential impact and concerns associated with their implementation in standard health economics practice in the Netherlands. Given the importance covered by these guidelines, an assessment of their impact on economic evaluation practice is desirable.\nThe objective of this paper was to review the evolution of health economic evaluation practice in the Netherlands before and after the introduction of the ZIN’s 2016 guidelines. Based on some key components within the health economics framework addressed by the new guidelines, we specifically focus on reviewing the statistical methods, missing data methods and software implemented by health economists. Given the intrinsic complexity of analysing health economics data, the choice of the analytical approaches to deal with these problems as well as transparent information on their implementation is crucial in determining the degree of confidence that decision-makers should have towards cost-effectiveness results obtained from these studies\n\n\nThe ZIN 2016 guidelines\nThe main objective of the guidelines is to ensure the comparability and quality of health economic evaluations in the Netherlands, therefore facilitating the task of the decision-maker regarding the reimbursement of new health care interventions. Following the example of guidelines issued by decision-making bodies in other countries, including the National Institute for Health and Care Excellence (NICE) in the UK, the recommended features for economic evaluations are summarised in a typical scenario referred to as ‘reference case’, although deviations from it are allowed when properly justified.\nBased on the structure of the reference case, four essential components of a health economic evaluation are identified: framework, analytic approach, input data and reporting. For the purpose of the review, we only focus on these components in the reference case as the main elements upon which evaluating health economics practice.\n\n\nMethods\nWe performed a bibliographic search in June 2021 using search engines of two online full-text journal repositories: (1) PubMed and (2) Zorginstituut. These sources were chosen to maximise the number of studies that could be accessed given the scoping nature of the review and the lack of a search strategy based on a pre-defined and rigid approach typical of systematic reviews. Articles were considered eligible for the review only if they were cost-effectiveness or cost-utility analyses targeting a Dutch population. To allow the inclusion of a reasonable amount of studies, the key words used in the search strategy were (cost-effectiveness OR cost-utility OR economic evaluation), and we targeted studies published between January 2016 and April 2021.\n\n\nAnalytic approaches\nAlmost all reviewed empirical analyses used bootstrapping (95%), although the number of replications varied largely across the studies, with the most popular choices being 5000 (55%) followed by 2000 (29%). Studies showed even more variability in the choice of the methods used in combination with bootstrapping. Seven general classes of statistical approaches were identified, among which unadjusted methods were the most popular choice across both time periods. A clear change in the type of statistical methods used between the two periods is denoted by a strong decrease (from 64 to 39) in the number of unadjusted analyses in 2016–2020 compared to the earlier period, which is compensated by a rise in the number of adjusted analyses using either SUR or linear mixed effects model (LMM) methods.\nFrom Figure 1 we can look at the different type and combination of software programs used as an indication of the implementation preferences of analysts for health economic evaluations. Although in principle the choice of software should have no impact on the quality of the statistical methods implemented, it has been highlighted how use of simpler software (e.g. spreadsheet calculators such as Excel) may become increasingly cumbersome for matching more realistic and therefore complex modelling requirements.\n\n\n\n\n\n\nFigure 1: Heatmap of the combination of software programs used\n\n\n\nThe most popular software was SPSS, chosen by 87 (52%) of the studies, either in the base-case (33%) or secondary (19%) analyses, often used in combination with Excel or by itself. When either STATA (26%) or R (13%) was used in the base-case analysis, SPSS was still the most popular choice in secondary analyses. Other combinations of software were less frequently chosen, even though 38 (23%) of the studies were unclear about the software implemented.\n\n\nMissing data methods\nAcross both periods limited changes are observed in terms of order of choice for missing data methods, with MI being the most popular base-case analysis, followed by complete case analysis (CCA), as the most popular SA choice. However, two noticeable variations in the frequency of these methods are observed between the two periods. First, the proportion of studies using MI in the base-case analysis has considerably increased over time (from 28 to 39%), which is compensated by a decrease in the proportion of less advanced methods such as CCA (from 14 to 5%) and single imputation (SI) (from 21 to 16%). Second, the number of studies not clearly reporting the methods has also considerably decreased (from 12 to 5%). The observed trend between the two periods may be the result of the specific recommendations from the 2016 guidelines in regards to the ‘optimal’ missing data strategy, resulting in a more frequent adoption of MI techniques and, at the same time, a less frequent use of CCA in the base-case analysis. However, in contrast to these guidelines, a large number of studies still does not perform any SA to missing data assumptions (about 65% in 2010–2015 and 63% in 2016–2020).\nMost of the studies lie in the middle and lower parts of the plot, and are associated with a limited or sufficient quality of information. However, only a few of these studies rely on very strong and unjustified missing data assumptions, while the majority provides either adequate justifications or uses methods associated with weak assumptions. Only 11 (14%) studies are associated with both high-quality scores and less restrictive missingness assumptions. No study was associated with either full information or adequate justifications for the assumptions explored in base-case and sensitivity analysis.\n\n\nDiscussion\nDescriptive information extracted from the reviewed studies provides some first insights about changes in practice in the years following the publication of the guidelines. First, a clear trend is observed towards an increase in the adoption of a societal and health care perspective and of CUA as the reference base-case analysis approach. Second, a similar increment is observed in the use of recommended instruments for the collection and valuation of health economic outcomes, such as EQ-5D-5L for QALYs and friction method for costs. Most of these changes are in accordance with the 2016 guidelines, which are likely to have played a role in influencing analysts and practitioners towards a clearer and more standardised way to report health economic results.\nWhen looking at the type of statistical methods used to perform the analysis, an important shift occurs between the two periods towards the use of methods that allow for regression adjustment, with a considerable uptake in the use of SURs and LMMs in the context of empirical analyses. These techniques are strongly supported by the 2016 guidelines in that they allow us to correct for potential bias due to confounding effects, deal with clustered data and formally take into account the correlation between costs and effects. Bootstrapping remains the most popular methods to quantify uncertainty around parameter estimates across both periods. However, the health economic analysis framework requires that the level of complexity of the analysis model is reflected in the way uncertainty surrounding the estimates is generated.\nThe transition between the two time periods reveals an increase in the use of MI techniques in the base-case analysis together with a decrease in the overall use of CCA. This change is in line with the 2016 guidelines which warns about the inherent limitations and potential bias of simple methods (e.g. CCA) when compared to MI as the potential reference method to handle missing values. Nevertheless, improvements are still needed given that many studies (more than 6%) performed the analysis under a single missing data assumption. This is not ideal since by definition missing data assumptions can never be checked, making the results obtained under a specific method (i.e. assumption) potentially biased.\n\n\nConclusions\nGiven the complexity of the health economics framework, the implementation of simple but likely inadequate analytic approaches may lead to imprecise cost-effectiveness results. This is a potentially serious issue for bodies such as ZIN in the Netherlands that use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new health care interventions. Our review shows, over time, a change in common practice with respect to different analysis components in accordance with the recent ZIN’s 2016 guidelines. This is an encouraging movement towards the standardised use of more suitable and robust analytic methods in terms of both statistical, uncertainty and missing data analysis. Improvements are however still needed, particularly in the choice of adequate statistical techniques to deal with the complexity of the data analysed and in the assessment of the impact of alternative missing data assumptions on the results in SA."
  },
  {
    "objectID": "research/volley/volley.html",
    "href": "research/volley/volley.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or “modules” form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match (\\(y_h\\) and \\(y_a\\)); (2) the module of the binary indicator for the number of sets played (\\(d^s\\)); (3) the module of the binary indicator for the winner of the match (\\(d^m\\)). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\n\n\nIn the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters.\n\n\n\nIn the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]\n\n\n\nThe last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\).\n\n\n\nAlthough the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "href": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters."
  },
  {
    "objectID": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "href": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]"
  },
  {
    "objectID": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "href": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "The last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\)."
  },
  {
    "objectID": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "href": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Although the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nSoftware\n\n\nMissing Data\n\n\nHealth Economics\n\n\n\nmissingHE is a R package aimed at providing some useful tools to analysts in order to handle missing outcome data under a Full Bayesian framework in economic evaluations … \n\n\n\n\n\nMar 21, 2023\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html",
    "href": "tutorials/2019-07-01-intro-stan/index.html",
    "title": "Super basic introduction to Stan",
    "section": "",
    "text": "The focus of this simple tutorial is to provide a brief introduction and overview about how to fit Bayesian models using Stan via R.\nPrerequisites:"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#what-is-stan",
    "href": "tutorials/2019-07-01-intro-stan/index.html#what-is-stan",
    "title": "Super basic introduction to Stan",
    "section": "What is Stan?",
    "text": "What is Stan?\nStan provides full Bayesian inference for continuous-variable models through Markov Chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling\nStan is a program for analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) methods (Gelman, Lee, and Guo (2015)). Stan is a free software and a probabilistic programming language for specifying statistical models using a specific class of MCMC algorithms known as Hamiltonian Monte Carlo methods (HMC). The latest version of Stan can be dowloaded from the web repository and is available for different OS. There are different R packages which function as frontends for Stan. These packages make it easy to process the output of Bayesian models and present it in publication-ready form. In this brief introduction, I will specifically focus on the rstan package (Stan Development Team (2018)) and show how to fit Stan models using this package."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#installing-stan-and-rstan",
    "href": "tutorials/2019-07-01-intro-stan/index.html#installing-stan-and-rstan",
    "title": "Super basic introduction to Stan",
    "section": "Installing Stan and rstan",
    "text": "Installing Stan and rstan\nUnlike other Bayesian software, such as JAGS or OpenBUGS, it is not required to separately install the program and the corresponding frontend R package. Indeed, installing the R package rstan will automatically install Stan on your machine. However, you will also need to make sure to having installed on your pc a C++ compiler which is used by rstan to fit the models. Under a Windows OS, for example, this can be done by installing Rtools, a collection of resources for building packages for R, which is freely available from the web repository.\nNext, install the package rstan from within R or Rstudio, via the package installer or by typing in the command line\n\ninstall.packages(\"rstan\", dependencies = TRUE)\n\nThe dependencies = TRUE option will automatically install all the packages on which the functions in the rstan package rely."
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#simulate-data",
    "href": "tutorials/2019-07-01-intro-stan/index.html#simulate-data",
    "title": "Super basic introduction to Stan",
    "section": "Simulate data",
    "text": "Simulate data\nFor an example dataset, I simulate my own data in R. I create a continuous outcome variable \\(y\\) as a function of one predictor \\(x\\) and a disturbance term \\(\\epsilon\\). I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept \\(\\beta_0\\) and slope \\(\\beta_1\\) coefficients, i.e.\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon  \n\\]\nThe R commands which I use to simulate the data are the following:\n\nn_sim=100; set.seed(123)\nx=rnorm(n_sim, mean = 5, sd = 2)\nepsilon=rnorm(n_sim, mean = 0, sd = 1)\nbeta0=1.5\nbeta1=1.2\ny=beta0 + beta1 * x + epsilon\nn_sim=as.integer(n_sim)\n\nThen, I define all the data for Stan in a list object\n\ndatalist=list(\"y\"=y,\"x\"=x,\"n_sim\"=n_sim)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#model-file",
    "href": "tutorials/2019-07-01-intro-stan/index.html#model-file",
    "title": "Super basic introduction to Stan",
    "section": "Model file",
    "text": "Model file\nNow, I write the model for Stan and save it as a stan file named \"basic.mod.stan\" in the current working directory\n\nbasic.mod= \"\ndata {\nint&lt;lower=0&gt; n_sim;\nvector[n_sim] y;\nvector[n_sim] x;\n}\nparameters {\nreal beta0;\nreal beta1;\nreal&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\nvector[n_sim] mu;\nmu=beta0 + beta1*x;\n} \nmodel {\nsigma~uniform(0,100);\nbeta0~normal(0,1000);\nbeta1~normal(0,1000);\ny~normal(mu,sigma);\n}\n\n\"\n\nStan models are written using an imperative programming language, which means that the order in which you write the elements in your model file matters, i.e. you first need to define your variables (e.g. integers, vectors, matrices, etc.), the constraints which define the range of values your variable can take (e.g. only positive values for standard deviations), and finally define the relationship among the variables (e.g. one is a liner function of another).\nA Stan model is defined by six program blocks:\n\nData (required). The data block reads external information – e.g. data vectors, matrices, integers, etc.\nTransformed data (optional). The transformed data block allows for preprocessing of the data – e.g. transformation or rescaling of the data.\nParameters (required). The parameters block defines the sampling space – e.g. parameters to which prior distributions must be assigned.\nTransformed parameters (optional). The transformed parameters block allows for parameter processing before the posterior is computed – e.g. tranformation or rescaling of the parameters.\nModel (required). In the model block we define our posterior distributions – e.g. choice of distributions for all variables.\nGenerated quantities (optional). The generated quantities block allows for postprocessing – e.g. backtranformation of the parameters using the posterior samples.\n\nFor this introduction I consider a very simple model which only requires the specification of four blocks in the Stan model. In the data block, I first define the size of the sample n_sim as a positive integer number using the expression int&lt;lower=0&gt; n_sim; then I declare the two variables y and x as reals (or vectors) with length equal to N. In the parameters block, I define the coefficients for the linear regression beta0 and beta1 (as two real numbers) and the standard deviation parameter sigma (as a positive real number). In the transformed parameters block, I define the conditional mean mu (a real vector of length N) as a linear function of the intercept beta0, the slope beta1, and the covariate x. Finally, in the model block, I assign weakly informative priors to the regression coefficients and the standard deviation parameters, and I model the outcome data y using a normal distribution indexed by the conditional mean mu and the standard deviation sigma parameters. In many cases, Stan uses sampling statements which can be vectorised, i.e. you do not need to use for loop statements.\nTo write and save the model as the text file “basic.mod.stan” in the current working directory, I use the writeLines function\n\nwriteLines(basic.mod, \"basic.mod.stan\")"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#pre-processing",
    "href": "tutorials/2019-07-01-intro-stan/index.html#pre-processing",
    "title": "Super basic introduction to Stan",
    "section": "Pre-processing",
    "text": "Pre-processing\nDefine the parameters whose posterior distribtuions we are interested in summarising later and set up the initial values for the MCMC sampler in Stan\n\nparams=c(\"beta0\",\"beta1\")\ninits=function(){list(\"beta0\"=rnorm(1), \"beta1\"=rnorm(1))}\n\nThe function creates a list that contains one element for each parameter, which gets assigned a random draw from a normal distribution as a strating value for each chain in the model. For simple models like this, it is generally easy to define the intial values for all parameters in the object inits which is then passed to the stan function in rstan. However, for more complex models, this may not be immediate and a lot of trial and error may be required. However, Stan can automatically select the initial values for all parameters randomly. This can be achieved by setting inits=\"random\", which is then passed to the stan function in rstan.\nBefore using rstan for the first time, you need to load the package, and you may want to set a random seed number for making your estimates reproducible\n\nlibrary(rstan)\nset.seed(123)"
  },
  {
    "objectID": "tutorials/2019-07-01-intro-stan/index.html#fit-the-model",
    "href": "tutorials/2019-07-01-intro-stan/index.html#fit-the-model",
    "title": "Super basic introduction to Stan",
    "section": "Fit the model",
    "text": "Fit the model\nNow, we can fit the model in Stan using the stan function in the rstan package and save it in the object basic.mod\n\nbasic.mod&lt;-stan(data = datalist, pars = params, iter = 9000, \n  warmup = 1000, init = inits, chains = 2, file = \"basic.mod.stan\")\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 2.2e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 9000 [  0%]  (Warmup)\nNA Chain 1: Iteration:  900 / 9000 [ 10%]  (Warmup)\nNA Chain 1: Iteration: 1001 / 9000 [ 11%]  (Sampling)\nNA Chain 1: Iteration: 1900 / 9000 [ 21%]  (Sampling)\nNA Chain 1: Iteration: 2800 / 9000 [ 31%]  (Sampling)\nNA Chain 1: Iteration: 3700 / 9000 [ 41%]  (Sampling)\nNA Chain 1: Iteration: 4600 / 9000 [ 51%]  (Sampling)\nNA Chain 1: Iteration: 5500 / 9000 [ 61%]  (Sampling)\nNA Chain 1: Iteration: 6400 / 9000 [ 71%]  (Sampling)\nNA Chain 1: Iteration: 7300 / 9000 [ 81%]  (Sampling)\nNA Chain 1: Iteration: 8200 / 9000 [ 91%]  (Sampling)\nNA Chain 1: Iteration: 9000 / 9000 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.04 seconds (Warm-up)\nNA Chain 1:                0.277 seconds (Sampling)\nNA Chain 1:                0.317 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 6e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 9000 [  0%]  (Warmup)\nNA Chain 2: Iteration:  900 / 9000 [ 10%]  (Warmup)\nNA Chain 2: Iteration: 1001 / 9000 [ 11%]  (Sampling)\nNA Chain 2: Iteration: 1900 / 9000 [ 21%]  (Sampling)\nNA Chain 2: Iteration: 2800 / 9000 [ 31%]  (Sampling)\nNA Chain 2: Iteration: 3700 / 9000 [ 41%]  (Sampling)\nNA Chain 2: Iteration: 4600 / 9000 [ 51%]  (Sampling)\nNA Chain 2: Iteration: 5500 / 9000 [ 61%]  (Sampling)\nNA Chain 2: Iteration: 6400 / 9000 [ 71%]  (Sampling)\nNA Chain 2: Iteration: 7300 / 9000 [ 81%]  (Sampling)\nNA Chain 2: Iteration: 8200 / 9000 [ 91%]  (Sampling)\nNA Chain 2: Iteration: 9000 / 9000 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.041 seconds (Warm-up)\nNA Chain 2:                0.327 seconds (Sampling)\nNA Chain 2:                0.368 seconds (Total)\nNA Chain 2:\n\n\nDifferent packages are available to perform diagnostic checks for Bayesian models. Here, I install and load the bayesplot package (Gabry and Mahr (2017)) to obtain graphical diagnostics and results.\n\ninstall.packages(\"bayesplot\")\nlibrary(bayesplot)\n\nFor example, density and trace plots can be obtained by typing\n\nmcmc_combo(as.array(basic.mod),regex_pars=\"beta0|beta1\")\n\n\n\n\n\n\n\n\nBoth types of graphs suggest that there are not issues in the convergence of the algorithm (smooth normal densities and hairy caterpillar graphs for both MCMC chains)."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in JAGS (Plummer (2004)) using the package R2jags (Su et al. (2015)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#introduction",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#introduction",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#linear-models",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#linear-models",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for two and three factor nested design are:\n\\[\ny_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#assumptions",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#assumptions",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Assumptions",
    "text": "Assumptions\nAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnTreat &lt;- 3\nnBlock &lt;- 35\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nTreat\nBlock &lt;- gl(nBlock, k=1)\nA &lt;- gl(nTreat,k=1)\ndt &lt;- expand.grid(A=A,Block=Block)\n#Xmat &lt;- model.matrix(~Block + A + Block:A, data=dt)\nXmat &lt;- model.matrix(~-1+Block + A, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\nA.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\n## Sum to zero block effects\nblock.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\nA.effects &lt;- c(40,70,80)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.rcb &lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\nhead(data.rcb)  #print out the first six rows of the data set\n\nNA          y A Block\nNA 1 45.80853 1     1\nNA 2 66.71784 2     1\nNA 3 93.29238 3     1\nNA 4 43.10101 1     2\nNA 5 73.20697 2     2\nNA 6 91.77487 3     2"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~A, data.rcb)\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.rcb, interaction.plot(A,Block,y))\n\n#OR with ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A, data.rcb))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA Tukey test   -1.4163           0.1567\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\n\nNA       Test     Pvalue \nNA -1.4163343  0.1566776\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nlibrary(asbio)\nwith(data.rcb,tukey.add.test(y,A,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 2.0060029   Denom df = 67    p-value = 0.1613102\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting\nFull parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[\n\\boldsymbol \\mu =\n  \\begin{bmatrix} 0  \\\\ 0  \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim   \n  \\begin{bmatrix}\n   1000000 & 0 & 0 \\\\\n   0 & 1000000 & 0 \\\\\n   0 & 0 & 1000000\n   \\end{bmatrix}.\n\\]\nHierarchical parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- beta0 + beta[A[i]] + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   }\n   \n   #Priors\n   beta0 ~ dnorm(0, 1.0E-6)\n   beta[1] &lt;- 0\n   for (i in 2:nA) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=as.numeric(Block),\n         A=as.numeric(A),\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n                 nA = length(levels(A))\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta0\",\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the JAGS model (check the model, load data into the model, specify the number of chains and compile the model). Load the R2jags package.\n\nlibrary(R2jags)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.rcb.r2jags.f &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 42\nNA    Total graph size: 582\nNA \nNA Initializing model\n\nprint(data.rcb.r2jags.f)\n\nNA Inference for Bugs model at \"fullModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     0.000   0.000   0.000   0.000   0.000   0.000   0.000 1.000     1\nNA beta[2]    27.923   1.236  25.587  27.086  27.918  28.736  30.363 1.001  3000\nNA beta[3]    40.263   1.229  37.821  39.463  40.271  41.070  42.706 1.001  3000\nNA beta0      41.834   2.154  37.519  40.406  41.833  43.338  46.002 1.001  3000\nNA gamma[1]    3.731   3.394  -2.865   1.418   3.748   6.043  10.452 1.001  3000\nNA gamma[2]    4.534   3.439  -2.033   2.182   4.508   6.865  11.317 1.003   690\nNA gamma[3]   -3.951   3.464 -10.690  -6.324  -3.884  -1.600   2.829 1.001  3000\nNA gamma[4]   -4.129   3.454 -10.758  -6.477  -4.200  -1.784   2.630 1.003   780\nNA gamma[5]   -5.314   3.480 -12.143  -7.633  -5.325  -2.947   1.594 1.001  3000\nNA gamma[6]   -6.050   3.377 -12.486  -8.331  -6.071  -3.700   0.524 1.001  3000\nNA gamma[7]   -0.709   3.373  -7.083  -3.017  -0.728   1.585   6.032 1.001  3000\nNA gamma[8]  -15.033   3.446 -21.689 -17.322 -15.065 -12.741  -7.939 1.001  3000\nNA gamma[9]   27.856   3.444  20.996  25.498  27.927  30.226  34.525 1.001  3000\nNA gamma[10]  12.830   3.591   5.798  10.453  12.809  15.249  20.065 1.001  3000\nNA gamma[11] -14.936   3.427 -21.825 -17.228 -14.945 -12.635  -8.165 1.001  3000\nNA gamma[12]  -7.878   3.427 -14.571 -10.161  -7.929  -5.551  -1.275 1.001  3000\nNA gamma[13] -10.865   3.430 -17.569 -13.203 -10.852  -8.555  -4.076 1.001  2100\nNA gamma[14]   9.153   3.466   2.557   6.679   9.161  11.473  15.941 1.002  1100\nNA gamma[15]  -3.897   3.495 -10.811  -6.227  -3.882  -1.565   2.866 1.001  3000\nNA gamma[16]   1.321   3.437  -5.486  -0.927   1.328   3.539   8.066 1.001  3000\nNA gamma[17]  -4.137   3.445 -11.102  -6.451  -4.046  -1.802   2.384 1.001  2300\nNA gamma[18]  -4.257   3.449 -10.970  -6.607  -4.280  -1.978   2.695 1.001  2900\nNA gamma[19]  16.435   3.468   9.749  14.031  16.449  18.830  23.059 1.002  1600\nNA gamma[20]  -5.108   3.439 -11.784  -7.392  -5.100  -2.851   1.708 1.001  3000\nNA gamma[21]  18.935   3.517  12.023  16.632  18.857  21.210  25.944 1.001  3000\nNA gamma[22] -20.654   3.459 -27.290 -23.041 -20.593 -18.341 -14.063 1.001  3000\nNA gamma[23]   7.325   3.570   0.265   4.959   7.346   9.652  14.226 1.001  2500\nNA gamma[24]  -1.300   3.500  -8.248  -3.680  -1.256   1.043   5.590 1.001  3000\nNA gamma[25]  -6.114   3.419 -12.672  -8.442  -6.078  -3.725   0.554 1.001  2500\nNA gamma[26]   1.038   3.451  -5.502  -1.351   1.018   3.415   7.820 1.001  3000\nNA gamma[27]  -4.346   3.400 -10.942  -6.604  -4.351  -1.980   2.254 1.001  3000\nNA gamma[28]  -4.721   3.368 -11.281  -6.939  -4.724  -2.572   2.057 1.001  3000\nNA gamma[29] -12.328   3.513 -19.166 -14.660 -12.295 -10.096  -5.361 1.001  3000\nNA gamma[30] -12.858   3.534 -19.927 -15.216 -12.782 -10.455  -5.999 1.001  3000\nNA gamma[31]   0.272   3.457  -6.677  -2.020   0.279   2.562   7.061 1.002  1700\nNA gamma[32]   8.682   3.389   1.905   6.358   8.769  10.986  15.304 1.002  1100\nNA gamma[33]   0.315   3.433  -6.393  -2.013   0.292   2.624   7.101 1.001  3000\nNA gamma[34]   9.586   3.491   2.775   7.232   9.621  11.954  16.438 1.002  1600\nNA gamma[35]  23.906   3.429  17.230  21.620  23.976  26.208  30.709 1.004   660\nNA res[1]      0.243   2.906  -5.276  -1.686   0.233   2.121   5.940 1.001  3000\nNA res[2]     -6.771   2.869 -12.345  -8.647  -6.778  -4.904  -1.088 1.001  3000\nNA res[3]      7.465   2.940   1.587   5.472   7.482   9.382  13.252 1.001  3000\nNA res[4]     -3.267   2.927  -8.857  -5.249  -3.282  -1.245   2.325 1.003   710\nNA res[5]     -1.085   2.946  -6.792  -3.130  -1.110   0.894   4.685 1.003   580\nNA res[6]      5.144   2.913  -0.568   3.244   5.128   7.112  11.094 1.003   600\nNA res[7]     -0.049   2.992  -5.806  -2.023  -0.081   1.917   5.915 1.002  1400\nNA res[8]     -2.652   3.020  -8.617  -4.629  -2.690  -0.658   3.254 1.001  2100\nNA res[9]      2.018   2.992  -3.965  -0.008   1.989   4.022   7.816 1.001  1900\nNA res[10]    -2.071   2.894  -7.884  -4.016  -2.075  -0.143   3.626 1.003   790\nNA res[11]     0.729   2.960  -5.122  -1.274   0.754   2.666   6.590 1.003   660\nNA res[12]     0.287   2.974  -5.811  -1.700   0.396   2.232   5.977 1.003   700\nNA res[13]    -2.939   2.956  -8.894  -4.938  -2.989  -0.959   2.831 1.001  3000\nNA res[14]     4.213   2.943  -1.578   2.254   4.220   6.162  10.013 1.002  3000\nNA res[15]    -2.451   2.949  -8.246  -4.449  -2.478  -0.503   3.535 1.001  3000\nNA res[16]    -2.462   2.911  -8.169  -4.429  -2.422  -0.560   3.167 1.001  3000\nNA res[17]     3.440   2.912  -2.164   1.559   3.394   5.390   9.051 1.001  3000\nNA res[18]    -2.208   2.908  -7.897  -4.117  -2.183  -0.279   3.505 1.001  3000\nNA res[19]    -5.249   2.902 -10.958  -7.149  -5.263  -3.289   0.456 1.001  2800\nNA res[20]     4.201   2.886  -1.484   2.238   4.217   6.100   9.930 1.001  3000\nNA res[21]     1.085   2.889  -4.579  -0.852   1.104   2.988   6.981 1.001  3000\nNA res[22]     0.756   2.958  -5.385  -1.108   0.712   2.747   6.541 1.001  3000\nNA res[23]     1.284   2.979  -4.882  -0.642   1.295   3.319   6.941 1.001  3000\nNA res[24]    -5.388   2.941 -11.391  -7.320  -5.410  -3.425   0.336 1.001  3000\nNA res[25]     3.141   2.979  -2.786   1.153   3.135   5.090   9.051 1.001  3000\nNA res[26]    -4.587   2.978 -10.372  -6.597  -4.605  -2.589   1.268 1.001  3000\nNA res[27]     7.011   2.963   1.359   4.983   6.994   8.965  12.803 1.001  3000\nNA res[28]     7.495   3.018   1.549   5.443   7.508   9.538  13.331 1.002  1600\nNA res[29]     0.730   3.013  -5.133  -1.311   0.723   2.759   6.603 1.001  2200\nNA res[30]    -5.563   3.054 -11.597  -7.683  -5.577  -3.452   0.255 1.001  2100\nNA res[31]    -3.927   2.962  -9.656  -5.903  -3.900  -1.965   1.925 1.001  3000\nNA res[32]     2.986   2.928  -2.794   1.134   2.985   4.946   8.735 1.001  3000\nNA res[33]    -1.871   2.951  -7.734  -3.824  -1.864   0.076   4.020 1.001  3000\nNA res[34]    -0.528   2.951  -6.322  -2.505  -0.516   1.395   5.205 1.001  3000\nNA res[35]    -1.472   2.949  -7.285  -3.350  -1.439   0.455   4.220 1.001  3000\nNA res[36]     0.722   2.938  -4.922  -1.223   0.716   2.663   6.487 1.001  3000\nNA res[37]    -0.493   2.928  -6.329  -2.503  -0.515   1.546   5.111 1.002   940\nNA res[38]    -2.832   2.938  -8.610  -4.822  -2.827  -0.812   2.778 1.002  1300\nNA res[39]     1.268   2.931  -4.339  -0.752   1.281   3.287   6.896 1.002  1200\nNA res[40]     2.968   2.956  -2.952   0.972   2.987   4.925   8.768 1.003   540\nNA res[41]    -2.427   2.942  -8.293  -4.383  -2.363  -0.534   3.629 1.003   660\nNA res[42]     1.150   2.922  -4.453  -0.813   1.176   3.054   7.197 1.003   620\nNA res[43]    -7.026   2.953 -12.859  -8.930  -7.097  -5.036  -1.170 1.001  3000\nNA res[44]     2.862   2.915  -2.600   0.922   2.711   4.854   8.438 1.001  3000\nNA res[45]     3.397   2.955  -2.273   1.404   3.364   5.361   9.127 1.001  3000\nNA res[46]     1.390   2.972  -4.597  -0.607   1.321   3.366   7.167 1.001  2000\nNA res[47]     2.490   2.991  -3.362   0.502   2.495   4.531   8.375 1.001  3000\nNA res[48]    -3.582   2.963  -9.351  -5.573  -3.608  -1.606   2.286 1.001  2700\nNA res[49]    -2.288   2.916  -7.729  -4.298  -2.366  -0.291   3.588 1.003  1000\nNA res[50]    -1.083   2.906  -6.569  -3.067  -1.105   0.863   4.716 1.002  1300\nNA res[51]     2.286   2.912  -3.216   0.331   2.244   4.222   8.075 1.002  1200\nNA res[52]    -2.829   2.903  -8.540  -4.796  -2.804  -0.891   2.706 1.001  3000\nNA res[53]     1.533   2.928  -4.178  -0.380   1.544   3.452   7.214 1.001  2700\nNA res[54]     0.366   2.907  -5.326  -1.576   0.391   2.274   6.016 1.001  3000\nNA res[55]     7.373   2.957   1.529   5.353   7.414   9.358  13.176 1.002  1300\nNA res[56]    -3.029   2.984  -9.041  -4.985  -2.963  -1.047   2.888 1.002  1000\nNA res[57]    -0.932   2.961  -6.889  -2.936  -0.824   0.995   4.741 1.002  1100\nNA res[58]     0.955   2.941  -4.752  -1.089   0.981   2.907   6.650 1.001  3000\nNA res[59]    -2.168   2.938  -8.052  -4.106  -2.164  -0.262   3.668 1.001  3000\nNA res[60]    -0.054   2.957  -5.874  -2.011   0.024   1.879   5.755 1.001  3000\nNA res[61]     4.652   3.013  -1.277   2.681   4.595   6.649  10.447 1.001  3000\nNA res[62]     1.763   3.015  -4.198  -0.283   1.800   3.811   7.690 1.001  3000\nNA res[63]    -2.627   2.975  -8.515  -4.603  -2.640  -0.650   3.124 1.001  3000\nNA res[64]    -1.878   3.019  -7.923  -3.808  -1.879   0.207   3.832 1.001  3000\nNA res[65]    -7.955   2.986 -14.124  -9.906  -7.908  -5.914  -2.185 1.001  3000\nNA res[66]     5.629   3.022  -0.370   3.651   5.702   7.692  11.441 1.001  3000\nNA res[67]    -9.447   2.997 -15.297 -11.427  -9.472  -7.515  -3.360 1.002  1100\nNA res[68]     3.633   3.011  -2.241   1.639   3.558   5.638   9.647 1.002  1400\nNA res[69]     7.139   3.005   1.189   5.081   7.118   9.082  13.136 1.002  1300\nNA res[70]    -6.267   2.959 -11.956  -8.323  -6.249  -4.253  -0.493 1.001  3000\nNA res[71]     6.538   2.978   0.563   4.494   6.525   8.512  12.391 1.001  3000\nNA res[72]    -0.621   2.967  -6.432  -2.609  -0.683   1.364   5.125 1.001  3000\nNA res[73]    -0.989   2.943  -6.875  -2.968  -0.975   0.949   4.951 1.001  3000\nNA res[74]     1.375   2.946  -4.265  -0.614   1.336   3.269   7.187 1.001  2400\nNA res[75]    -1.399   2.934  -7.135  -3.371  -1.401   0.618   4.478 1.001  2700\nNA res[76]    -0.971   2.970  -6.912  -2.914  -1.004   1.027   4.841 1.001  3000\nNA res[77]    -3.549   2.958  -9.315  -5.540  -3.567  -1.572   2.143 1.001  3000\nNA res[78]     4.860   2.940  -0.988   2.887   4.805   6.835  10.606 1.001  3000\nNA res[79]     6.984   2.964   1.461   4.967   6.920   9.004  12.824 1.001  3000\nNA res[80]    -7.875   3.011 -13.687  -9.926  -7.936  -5.815  -1.954 1.001  3000\nNA res[81]     0.160   2.947  -5.484  -1.824   0.127   2.155   6.047 1.001  3000\nNA res[82]     2.734   2.915  -3.183   0.820   2.794   4.729   8.263 1.001  3000\nNA res[83]     2.626   2.932  -3.161   0.748   2.624   4.515   8.266 1.001  3000\nNA res[84]    -6.416   2.954 -12.297  -8.315  -6.382  -4.439  -0.826 1.001  3000\nNA res[85]    -2.326   3.020  -8.242  -4.293  -2.308  -0.307   3.564 1.001  3000\nNA res[86]    -1.054   3.030  -7.047  -3.116  -1.048   0.971   4.956 1.001  3000\nNA res[87]     0.823   3.034  -5.036  -1.236   0.832   2.846   6.820 1.001  3000\nNA res[88]    -3.700   3.021  -9.662  -5.732  -3.744  -1.656   2.141 1.001  3000\nNA res[89]     5.124   3.000  -0.700   3.125   5.166   7.053  10.967 1.001  3000\nNA res[90]    -3.973   3.026  -9.965  -5.967  -3.979  -1.965   2.004 1.001  3000\nNA res[91]     6.800   2.936   1.114   4.888   6.726   8.792  12.683 1.001  2100\nNA res[92]    -1.633   2.932  -7.382  -3.615  -1.645   0.303   4.342 1.002  1500\nNA res[93]    -5.027   2.937 -10.801  -6.975  -5.090  -3.114   0.895 1.002  1600\nNA res[94]    11.068   2.894   5.415   9.144  11.037  12.950  16.844 1.017   920\nNA res[95]    -5.145   2.871 -10.758  -7.025  -5.209  -3.257   0.653 1.002   920\nNA res[96]    -3.909   2.889  -9.670  -5.821  -3.975  -2.083   2.104 1.002  1000\nNA res[97]     1.670   2.927  -4.070  -0.286   1.655   3.647   7.528 1.001  2600\nNA res[98]    -1.855   2.902  -7.533  -3.814  -1.846   0.085   3.932 1.001  3000\nNA res[99]     0.809   2.905  -4.984  -1.113   0.807   2.832   6.509 1.001  3000\nNA res[100]    1.492   2.952  -4.294  -0.488   1.451   3.471   7.192 1.001  2000\nNA res[101]    0.647   2.987  -5.287  -1.341   0.649   2.697   6.465 1.002  1500\nNA res[102]   -0.289   2.974  -6.080  -2.262  -0.296   1.781   5.395 1.002  1600\nNA res[103]   -1.309   2.976  -7.063  -3.273  -1.394   0.728   4.790 1.004   440\nNA res[104]   11.580   2.959   5.725   9.638  11.567  13.515  17.434 1.003   770\nNA res[105]   -5.108   2.939 -10.788  -7.110  -5.108  -3.165   0.707 1.006   490\nNA sigma       5.090   0.453   4.294   4.775   5.059   5.360   6.091 1.002   980\nNA sigma.B    11.494   1.491   8.926  10.487  11.365  12.348  14.912 1.002   920\nNA deviance  637.702  11.556 618.449 629.190 636.668 645.140 663.417 1.001  3000\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 66.8 and DIC = 704.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   \n   #Priors\n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel.txt\")\n\nArrange the data as a list (as required by JAGS). As input, JAGS will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA.Xmat &lt;- model.matrix(~A,data.rcb)\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=A.Xmat,\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n         a0=rep(0,3), A0=diag(3)\n         )\n)\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the JAGS code via the R2jags interface.\n\ndata.rcb.r2jags.m &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 910\nNA \nNA Initializing model\n\nprint(data.rcb.r2jags.m)\n\nNA Inference for Bugs model at \"matrixModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA           mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]     0.550   1.010  -1.396  -0.163   0.543   1.212   2.618 1.001  3000\nNA beta[2]     0.624   0.969  -1.267  -0.021   0.609   1.267   2.494 1.002   830\nNA beta[3]     1.556   1.005  -0.458   0.886   1.564   2.248   3.495 1.001  3000\nNA gamma[1]   64.957  12.115  41.593  56.722  64.856  72.937  89.019 1.001  3000\nNA gamma[2]   65.491  12.061  40.343  57.573  65.450  73.443  89.230 1.001  3000\nNA gamma[3]   57.107  11.937  34.004  49.204  56.969  65.191  79.840 1.001  3000\nNA gamma[4]   56.660  11.705  32.676  48.922  56.796  64.418  79.244 1.002   980\nNA gamma[5]   55.496  12.269  30.868  47.463  55.494  63.576  79.632 1.001  3000\nNA gamma[6]   54.801  11.877  31.954  46.951  54.385  62.674  78.056 1.002  3000\nNA gamma[7]   60.234  11.740  36.788  52.823  60.472  67.916  83.053 1.001  3000\nNA gamma[8]   45.171  11.789  21.628  37.438  45.279  53.104  69.031 1.002  3000\nNA gamma[9]   89.740  11.870  66.877  81.784  89.985  97.893 112.876 1.001  3000\nNA gamma[10]  74.392  11.959  51.486  65.937  74.289  82.703  98.386 1.002  1500\nNA gamma[11]  45.824  11.994  22.631  37.864  45.861  53.737  69.411 1.002  1100\nNA gamma[12]  52.874  11.847  29.777  44.934  53.044  61.009  75.635 1.002  1800\nNA gamma[13]  49.828  12.010  26.670  41.934  49.809  57.903  73.611 1.001  3000\nNA gamma[14]  70.252  11.879  46.842  62.471  70.259  78.152  93.618 1.001  3000\nNA gamma[15]  56.956  11.790  34.318  48.968  56.782  65.102  79.771 1.001  3000\nNA gamma[16]  62.229  12.088  39.561  53.779  62.120  70.801  85.754 1.002  1500\nNA gamma[17]  56.154  11.923  32.379  48.260  56.301  64.248  79.156 1.004   840\nNA gamma[18]  56.302  11.789  32.839  48.585  56.477  64.399  79.221 1.001  3000\nNA gamma[19]  78.011  12.127  53.246  69.932  78.041  86.393 100.917 1.001  3000\nNA gamma[20]  55.445  11.822  32.414  47.480  55.477  63.566  78.456 1.002  3000\nNA gamma[21]  79.975  11.935  56.573  72.184  80.069  87.728 103.091 1.001  3000\nNA gamma[22]  39.667  11.800  16.288  31.836  39.723  47.341  62.882 1.001  3000\nNA gamma[23]  68.605  11.860  45.831  60.540  68.721  76.702  91.376 1.001  3000\nNA gamma[24]  59.858  12.057  36.038  51.799  59.900  67.940  83.312 1.001  2200\nNA gamma[25]  54.974  11.970  31.161  47.135  55.153  62.882  78.327 1.002  1400\nNA gamma[26]  62.397  11.915  38.745  54.113  62.260  70.437  86.239 1.002  3000\nNA gamma[27]  56.526  11.968  32.943  48.610  56.627  64.491  80.067 1.003  1800\nNA gamma[28]  56.062  12.002  33.315  48.254  56.158  64.016  79.264 1.001  3000\nNA gamma[29]  47.976  11.787  24.940  39.984  47.840  55.752  71.140 1.001  3000\nNA gamma[30]  47.866  11.894  24.408  40.161  47.877  55.955  70.787 1.001  3000\nNA gamma[31]  61.528  12.021  37.815  53.617  61.620  69.800  84.596 1.001  3000\nNA gamma[32]  70.047  11.805  46.872  62.230  70.224  78.030  93.180 1.001  3000\nNA gamma[33]  61.830  11.934  38.654  53.718  61.828  69.563  85.450 1.002  1800\nNA gamma[34]  70.909  12.075  47.408  62.788  70.805  78.794  95.338 1.002  3000\nNA gamma[35]  85.532  12.138  61.716  77.422  85.479  93.434 109.336 1.001  3000\nNA res[1]    -19.698  12.037 -43.684 -27.635 -19.684 -11.694   3.838 1.001  3000\nNA res[2]      0.587  12.062 -23.374  -7.403   0.579   8.638  24.005 1.001  2500\nNA res[3]     26.230  12.050   2.155  18.315  26.057  34.356  50.024 1.001  3000\nNA res[4]    -22.940  11.997 -46.569 -30.787 -23.060 -15.111   2.226 1.001  3000\nNA res[5]      6.542  11.989 -16.855  -1.311   6.542  14.381  31.699 1.001  3000\nNA res[6]     24.179  11.977   0.862  16.176  24.040  31.902  49.149 1.001  3000\nNA res[7]    -19.824  11.904 -42.484 -27.853 -19.648 -11.889   3.474 1.001  3000\nNA res[8]      4.872  11.923 -17.801  -3.160   4.933  12.798  28.368 1.001  3000\nNA res[9]     20.951  11.893  -1.904  12.827  21.045  28.785  44.008 1.001  3000\nNA res[10]   -21.576  11.660 -44.079 -29.235 -21.708 -13.770   2.548 1.002   920\nNA res[11]     8.523  11.658 -14.011   0.830   8.435  16.332  32.472 1.002  1100\nNA res[12]    19.489  11.657  -2.829  11.774  19.266  27.343  43.385 1.002   910\nNA res[13]   -22.465  12.167 -46.486 -30.516 -22.434 -14.565   2.114 1.001  3000\nNA res[14]    11.986  12.183 -11.847   3.842  12.016  20.032  36.714 1.001  3000\nNA res[15]    16.730  12.218  -7.281   8.522  16.737  24.706  41.406 1.001  3000\nNA res[16]   -22.029  11.845 -45.221 -29.925 -21.621 -14.244   0.444 1.001  3000\nNA res[17]    11.172  11.874 -12.338   3.326  11.433  18.964  33.977 1.001  3000\nNA res[18]    16.933  11.886  -6.385   8.941  17.228  24.796  39.651 1.001  3000\nNA res[19]   -24.908  11.723 -47.784 -32.552 -25.170 -17.491  -1.875 1.001  3000\nNA res[20]    11.841  11.745 -11.265   4.268  11.662  19.369  35.065 1.001  3000\nNA res[21]    20.133  11.744  -2.743  12.424  19.956  27.740  43.426 1.001  3000\nNA res[22]   -18.164  11.729 -41.484 -26.023 -18.365 -10.358   5.425 1.001  3000\nNA res[23]     9.664  11.740 -13.656   1.865   9.491  17.451  33.025 1.001  3000\nNA res[24]    14.399  11.768  -8.920   6.550  14.224  22.160  38.381 1.001  3000\nNA res[25]   -17.459  11.814 -40.240 -25.508 -17.731  -9.597   5.503 1.001  3000\nNA res[26]     2.112  11.834 -20.695  -5.859   1.791   9.903  25.291 1.001  3000\nNA res[27]    25.119  11.869   2.237  17.073  24.856  33.014  48.190 1.001  3000\nNA res[28]   -12.783  11.927 -36.486 -21.081 -12.685  -4.484  10.101 1.002  1600\nNA res[29]     7.751  11.951 -16.443  -0.606   7.820  16.063  30.397 1.001  2100\nNA res[30]    12.866  11.971 -10.922   4.637  12.799  21.163  36.114 1.002  1600\nNA res[31]   -23.404  11.959 -47.054 -31.381 -23.430 -15.471  -0.221 1.002  1200\nNA res[32]    10.809  11.960 -12.765   2.757  10.883  18.657  33.990 1.002  1400\nNA res[33]    17.359  11.972  -6.134   9.343  17.364  25.293  40.617 1.002  1100\nNA res[34]   -19.997  11.800 -42.736 -28.106 -20.202 -12.124   3.343 1.002  2500\nNA res[35]     6.359  11.812 -16.232  -1.893   6.081  14.115  29.804 1.002  1900\nNA res[36]    19.960  11.807  -2.519  11.753  19.730  27.900  43.458 1.002  2600\nNA res[37]   -19.902  11.980 -43.600 -27.871 -19.843 -12.058   3.587 1.001  3000\nNA res[38]     5.059  12.005 -18.429  -3.025   5.164  12.756  28.837 1.001  3000\nNA res[39]    20.566  11.996  -3.070  12.608  20.634  28.366  44.009 1.001  3000\nNA res[40]   -16.847  11.831 -39.884 -24.766 -17.012  -9.239   6.596 1.001  3000\nNA res[41]     5.057  11.855 -18.074  -2.883   4.927  12.687  28.394 1.001  3000\nNA res[42]    20.042  11.830  -3.241  12.206  20.003  27.718  43.766 1.001  3000\nNA res[43]   -26.596  11.746 -49.680 -34.580 -26.349 -18.627  -3.862 1.001  3000\nNA res[44]    10.592  11.735 -12.047   2.531  10.659  18.547  33.288 1.001  3000\nNA res[45]    22.535  11.759  -0.427  14.411  22.577  30.481  45.274 1.001  3000\nNA res[46]   -18.234  12.031 -41.669 -26.696 -18.076  -9.978   4.443 1.002  1200\nNA res[47]    10.165  12.061 -13.282   1.662  10.365  18.474  33.054 1.002  1500\nNA res[48]    15.501  12.060  -7.938   7.051  15.690  23.578  38.325 1.002  1200\nNA res[49]   -21.296  11.891 -44.247 -29.499 -21.509 -13.441   2.004 1.002  1000\nNA res[50]     7.208  11.902 -15.618  -0.939   6.981  15.152  30.441 1.002   860\nNA res[51]    21.986  11.895  -0.592  13.867  21.799  29.853  45.640 1.002  1000\nNA res[52]   -22.104  11.729 -44.941 -29.987 -22.312 -14.241   1.133 1.001  3000\nNA res[53]     9.556  11.751 -13.024   1.751   9.257  17.457  33.011 1.001  3000\nNA res[54]    19.797  11.734  -2.966  11.838  19.629  27.542  43.295 1.001  3000\nNA res[55]   -12.918  12.097 -35.949 -21.363 -12.982  -4.866  11.578 1.001  3000\nNA res[56]     3.979  12.106 -19.166  -4.374   3.889  12.063  28.610 1.001  3000\nNA res[57]    17.484  12.122  -5.688   8.981  17.300  25.557  42.114 1.001  3000\nNA res[58]   -18.315  11.797 -41.135 -26.620 -18.209 -10.520   4.471 1.001  3000\nNA res[59]     5.862  11.815 -16.900  -2.341   6.146  13.803  29.023 1.001  3000\nNA res[60]    19.383  11.783  -3.464  11.144  19.447  27.072  42.559 1.001  3000\nNA res[61]   -15.105  11.900 -38.125 -22.888 -15.284  -7.276   8.233 1.001  3000\nNA res[62]     9.306  11.923 -13.585   1.507   9.128  17.140  32.775 1.001  3000\nNA res[63]    16.323  11.956  -6.322   8.357  16.116  24.316  39.779 1.001  3000\nNA res[64]   -20.915  11.793 -44.085 -28.598 -21.048 -13.147   2.444 1.001  3000\nNA res[65]     0.307  11.820 -22.750  -7.598   0.334   7.975  23.864 1.001  3000\nNA res[66]    25.298  11.813   2.156  17.551  25.247  33.151  48.893 1.001  3000\nNA res[67]   -29.443  11.830 -52.213 -37.430 -29.692 -21.434  -6.575 1.001  3000\nNA res[68]    10.936  11.848 -11.706   2.810  10.811  18.885  33.881 1.001  3000\nNA res[69]    25.850  11.873   2.835  17.722  25.645  33.821  48.845 1.001  3000\nNA res[70]   -26.142  11.991 -49.697 -34.072 -26.243 -18.049  -2.711 1.001  2300\nNA res[71]    13.963  11.984  -9.101   6.160  13.990  22.029  37.049 1.001  3000\nNA res[72]    18.211  12.015  -4.983  10.314  18.104  26.258  41.685 1.001  2300\nNA res[73]   -20.794  11.932 -43.963 -28.725 -20.938 -12.958   3.106 1.002  1600\nNA res[74]     8.870  11.937 -14.459   0.895   8.787  16.905  32.822 1.001  2000\nNA res[75]    17.504  11.929  -5.274   9.402  17.317  25.457  41.765 1.002  1600\nNA res[76]   -21.046  11.851 -44.910 -28.922 -21.131 -12.843   2.003 1.002  3000\nNA res[77]     3.675  11.857 -19.985  -4.059   3.613  11.706  27.196 1.003  3000\nNA res[78]    23.492  11.881  -0.028  15.552  23.399  31.576  46.776 1.003  3000\nNA res[79]   -12.603  11.937 -35.961 -20.361 -12.719  -4.579  10.948 1.002  2900\nNA res[80]    -0.163  11.955 -23.303  -8.120  -0.343   7.789  23.405 1.002  2200\nNA res[81]    19.279  11.963  -4.001  11.319  19.160  27.293  42.755 1.002  3000\nNA res[82]   -16.766  11.961 -39.955 -24.721 -16.887  -9.030   6.016 1.001  3000\nNA res[83]    10.426  11.958 -13.000   2.504  10.510  18.221  32.859 1.001  3000\nNA res[84]    12.792  11.939 -10.192   4.873  12.720  20.537  35.661 1.001  3000\nNA res[85]   -21.347  11.725 -44.125 -28.923 -21.164 -13.609   1.848 1.001  3000\nNA res[86]     7.224  11.690 -15.758  -0.362   7.382  15.079  30.544 1.001  3000\nNA res[87]    20.510  11.739  -2.412  12.708  20.647  28.327  43.283 1.001  3000\nNA res[88]   -23.140  11.838 -46.005 -31.274 -23.099 -15.502  -0.066 1.001  3000\nNA res[89]    12.983  11.858  -9.972   4.729  13.009  20.680  36.860 1.001  3000\nNA res[90]    15.293  11.892  -7.122   6.928  15.405  23.032  38.696 1.001  3000\nNA res[91]   -13.173  11.935 -36.126 -21.376 -13.267  -5.416  10.293 1.001  3000\nNA res[92]     5.694  11.935 -17.082  -2.485   5.640  13.450  29.247 1.001  3000\nNA res[93]    13.708  11.936  -9.020   5.488  13.624  21.589  37.402 1.001  3000\nNA res[94]    -9.013  11.766 -31.685 -16.863  -9.233  -1.091  14.418 1.001  3000\nNA res[95]     2.073  11.780 -20.569  -5.756   1.871   9.830  25.639 1.001  3000\nNA res[96]    14.717  11.782  -8.181   6.737  14.422  22.602  37.767 1.001  3000\nNA res[97]   -18.561  11.906 -41.914 -26.360 -18.779 -10.492   4.073 1.002  1300\nNA res[98]     5.213  11.908 -18.272  -2.559   5.068  13.190  28.414 1.002  1100\nNA res[99]    19.285  11.939  -4.469  11.483  19.203  27.469  42.441 1.002  1300\nNA res[100]  -18.547  12.018 -42.274 -26.484 -18.430 -10.631   5.083 1.001  3000\nNA res[101]    7.907  12.020 -16.041  -0.084   8.072  15.733  31.471 1.001  3000\nNA res[102]   18.379  12.034  -5.679  10.333  18.514  26.429  41.930 1.001  3000\nNA res[103]  -21.652  12.104 -45.310 -29.603 -21.605 -13.692   1.709 1.001  2500\nNA res[104]   18.537  12.095  -5.105  10.797  18.645  26.404  42.117 1.001  1900\nNA res[105]   13.256  12.137 -10.949   5.489  13.253  21.341  36.897 1.001  2600\nNA sigma      20.838   1.809  17.597  19.556  20.736  21.936  24.751 1.002  1000\nNA sigma.B    63.500   7.812  50.201  58.005  62.978  68.138  80.806 1.001  3000\nNA deviance  934.350  11.459 914.767 926.442 933.465 941.520 959.367 1.004   460\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 65.5 and DIC = 999.9\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nFor a simple model with only two hierarchical levels, the model is the same as above. If you want to include finite-population standard deviations in the model you can use the following code.\n\nmodelString3=\"\nmodel {\n   #Likelihood (esimating site means (gamma.site)\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- gamma[Block[i]] + inprod(beta[], X[i,]) \n      y.err[i]&lt;- mu[i]-y[i]\n   }\n   for (i in 1:nBlock) {\n      gamma[i] ~ dnorm(0, tau.block)\n   }\n   #Priors\n   for (i in 1:nX) {\n     beta[i] ~ dnorm(0, 1.0E-6) #prior\n   }\n   sigma ~ dunif(0, 100)\n   tau &lt;- 1 / (sigma * sigma)\n   sigma.block ~ dunif(0, 100)\n   tau.block &lt;- 1 / (sigma.block * sigma.block)\n\n   sd.y &lt;- sd(y.err)\n   sd.block &lt;- sd(gamma)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString3, con = \"SDModel.txt\")\n\n#data list\nA.Xmat &lt;- model.matrix(~A,ddply(data.rcb,~Block,catcolwise(unique)))\ndata.rcb.list &lt;- with(data.rcb,\n        list(y=y,\n                 Block=Block,\n         X= A.Xmat,\n         n=nrow(data.rcb),\n         nBlock=length(levels(Block)),\n                 nX = ncol(A.Xmat)\n         )\n)\n\n#parameters and chain details\nparams &lt;- c(\"beta\",\"sigma\",\"sd.y\",'sd.block','sigma.block')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rcb.r2jagsSD &lt;- jags(data = data.rcb.list, inits = NULL, parameters.to.save = params,\n    model.file = \"SDModel.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 105\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 899\nNA \nNA Initializing model\n\nprint(data.rcb.r2jagsSD)\n\nNA Inference for Bugs model at \"SDModel.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA             mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\nNA beta[1]      41.715   2.196  37.449  40.231  41.710  43.183  45.995 1.001  3000\nNA beta[2]      27.928   1.209  25.537  27.146  27.918  28.713  30.317 1.002   980\nNA beta[3]      40.272   1.210  37.832  39.461  40.267  41.096  42.658 1.001  2300\nNA sd.block     11.358   0.519  10.353  11.029  11.345  11.706  12.370 1.001  3000\nNA sd.y          5.014   0.260   4.592   4.827   4.986   5.172   5.609 1.002  1300\nNA sigma         5.074   0.443   4.322   4.752   5.045   5.350   6.036 1.001  3000\nNA sigma.block  11.692   1.546   9.114  10.589  11.586  12.612  15.118 1.002  3000\nNA deviance    637.262  10.949 618.392 629.413 636.321 644.252 660.930 1.001  2200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 59.9 and DIC = 697.2\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nCalculate \\(R^2\\) from the posterior of the model.\n\ndata.rcb.mcmc.listSD &lt;- as.mcmc(data.rcb.r2jagsSD)\n\nXmat &lt;- model.matrix(~A, data.rcb)\ncoefs &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['beta']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nZ.var &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['sd.block']]^2\nR.var &lt;- data.rcb.r2jagsSD$BUGSoutput$sims.list[['sd.y']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.block &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.block &lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\nrbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional)\n\nNA                     Mean     Median      lower      upper\nNA R2.block       0.2927774 0.29248056 0.24902731 0.33605200\nNA R2.marginal    0.6500204 0.65101312 0.60509352 0.68965593\nNA R2.res         0.0572022 0.05628758 0.04596228 0.07055798\nNA R2.conditional 0.9427978 0.94371242 0.92944202 0.95403772"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#planned-comparisonsand-pairwise-tests",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#planned-comparisonsand-pairwise-tests",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Planned comparisonsand pairwise tests",
    "text": "Planned comparisonsand pairwise tests\nSince there are no restrictions on the type and number of comparisons derived from the posteriors, Bayesian analyses provide a natural framework for exploring additional contrasts and comparisons. For example, to compare all possible levels:\n\ncoefs &lt;- data.rcb.r2jags.m$BUGSoutput$sims.list[[c('beta')]]\nhead(coefs)\n\nNA            [,1]        [,2]       [,3]\nNA [1,] -1.0697767 -0.46647636  0.4808020\nNA [2,]  0.6186153  1.46210386  2.3592529\nNA [3,] -1.5100302  0.09180824  1.1835869\nNA [4,] -0.3127107  0.66392714 -0.5681012\nNA [5,]  1.5552936  1.06785499  2.6443403\nNA [6,]  0.7282182  0.59829747  2.8548669\n\nnewdata &lt;- data.frame(A=levels(data.rcb$A))\n# A Tukeys contrast matrix\nlibrary(multcomp)\ntuk.mat &lt;- contrMat(n=table(newdata$A), type=\"Tukey\")\nXmat &lt;- model.matrix(~A, data=newdata)\npairwise.mat &lt;- tuk.mat %*% Xmat\npairwise.mat\n\nNA       (Intercept) A2 A3\nNA 2 - 1           0  1  0\nNA 3 - 1           0  0  1\nNA 3 - 2           0 -1  1\n\ncomps &lt;- coefs %*% t(pairwise.mat)\n\nMCMCsum &lt;- function(x) {\n   data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n}\n\n(comps &lt;-plyr:::adply(comps,2,MCMCsum))\n\nNA      X1    Median       X0.         X25.      X50.     X75.    X100.      lower\nNA 1 2 - 1 0.6093838 -2.556240 -0.020575421 0.6093838 1.267051 4.786166 -1.2766747\nNA 2 3 - 1 1.5638199 -1.833977  0.886430287 1.5638199 2.248195 4.835948 -0.4024791\nNA 3 3 - 2 0.9310770 -4.672228 -0.003765539 0.9310770 1.864871 5.592247 -1.5970184\nNA      upper     lower.1  upper.1\nNA 1 2.479999  0.03512204 1.316762\nNA 2 3.539364  0.92897200 2.273364\nNA 3 3.728297 -0.03345687 1.823124"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#data-generation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Data generation",
    "text": "Data generation\nImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nslope &lt;- 30\nintercept &lt;- 200\nnBlock &lt;- 35\nnTime &lt;- 10\nsigma &lt;- 50\nsigma.block &lt;- 30\nn &lt;- nBlock*nTime\nBlock &lt;- gl(nBlock, k=1)\nTime &lt;- 1:10\nrho &lt;- 0.8\ndt &lt;- expand.grid(Time=Time,Block=Block)\nXmat &lt;- model.matrix(~-1+Block + Time, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\n#A.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,slope)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\n## Sum to zero block effects\n##block.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\n###A.effects &lt;- c(40,70,80)\n##all.effects &lt;- c(block.effects,intercept,slope)\n##lin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\neps &lt;- NULL\neps[1] &lt;- 0\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] #residuals\n}\ny &lt;- rnorm(n,lin.pred,sigma)+eps\n\n#OR\neps &lt;- NULL\n# first value cant be autocorrelated\neps[1] &lt;- rnorm(1,0,sigma)\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma)  #residuals\n}\ny &lt;- lin.pred + eps\ndata.rm &lt;- data.frame(y=y, dt)\nhead(data.rm)  #print out the first six rows of the data set\n\nNA          y Time Block\nNA 1 282.1142    1     1\nNA 2 321.1404    2     1\nNA 3 278.7700    3     1\nNA 4 285.8709    4     1\nNA 5 336.6390    5     1\nNA 6 333.5961    6     1\n\nggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method='lm') + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#exploratory-data-analysis-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~Time, data.rm)\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nwith(data.rm, interaction.plot(Time,Block,y))\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+Time, data.rm))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA Time         -0.7274           0.4675\nNA Tukey test   -0.9809           0.3267\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\n\nNA       Test     Pvalue \nNA -0.9808606  0.3266615\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nwith(data.rm,tukey.add.test(y,Time,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 0.3997341   Denom df = 305    p-value = 0.5277003\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\n\nSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\nlibrary(nlme)\ndata.rm.lme &lt;- lme(y~Time, random=~1|Block, data=data.rm)\nacf(resid(data.rm.lme), lag=10)\n\n\n\n\n\n\n\n\nConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#model-fitting-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Model fitting",
    "text": "Model fitting"
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#full-effect-parameterisation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nmodelString=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- beta0 + beta*Time[i] + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   }\n   \n   #Priors\n   beta0 ~ dnorm(0, 1.0E-6)\n   beta ~ dnorm(0, 1.0E-6) #prior\n   \n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString, con = \"fullModel2.txt\")\n\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         Time=Time,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block))\n             )\n)\n\nparams &lt;- c(\"beta0\",\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.f &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"fullModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 41\nNA    Total graph size: 1815\nNA \nNA Initializing model\n\nprint(data.rm.r2jags.f)\n\nNA Inference for Bugs model at \"fullModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\nNA beta        30.689   1.047   28.609   30.017   30.687   31.401   32.705 1.001\nNA beta0      189.009  12.648  164.589  180.318  189.054  197.523  213.976 1.001\nNA gamma[1]   -35.015  20.219  -74.991  -47.706  -35.021  -21.382    3.826 1.002\nNA gamma[2]   -52.026  20.114  -91.663  -65.012  -52.008  -38.575  -12.685 1.001\nNA gamma[3]    20.417  19.878  -19.313    7.462   20.587   33.579   60.228 1.001\nNA gamma[4]     0.671  20.295  -38.799  -12.839    0.882   14.490   39.807 1.001\nNA gamma[5]    67.812  19.967   29.683   54.189   67.514   81.150  109.392 1.001\nNA gamma[6]    36.338  19.760   -2.575   22.780   36.203   49.381   76.772 1.001\nNA gamma[7]    24.072  20.155  -14.701   10.740   24.009   36.728   63.397 1.001\nNA gamma[8]   -31.199  20.016  -70.564  -44.687  -31.149  -17.691    7.011 1.001\nNA gamma[9]    73.971  20.034   35.053   60.309   73.846   87.726  113.132 1.003\nNA gamma[10]   58.034  19.900   19.397   44.730   58.085   71.380   97.283 1.001\nNA gamma[11]  141.644  20.387  101.956  127.950  141.240  154.897  181.751 1.001\nNA gamma[12]    5.655  20.094  -32.833   -7.787    5.349   19.065   47.017 1.001\nNA gamma[13]  -44.187  20.168  -84.778  -57.576  -44.641  -30.571   -5.599 1.001\nNA gamma[14]  -23.866  19.908  -63.673  -37.311  -23.653  -10.578   14.435 1.001\nNA gamma[15]   30.407  20.239   -8.109   16.928   30.379   43.830   70.587 1.001\nNA gamma[16]  103.433  20.123   64.608   90.052  103.087  116.736  143.495 1.002\nNA gamma[17]   91.556  20.060   53.115   77.561   91.725  104.473  131.814 1.002\nNA gamma[18]  -63.563  20.127 -102.913  -77.195  -63.210  -50.190  -24.916 1.002\nNA gamma[19]   16.404  19.820  -21.892    2.880   16.232   29.420   55.497 1.001\nNA gamma[20]  -26.858  19.837  -66.283  -39.890  -26.676  -13.651   12.760 1.002\nNA gamma[21] -104.771  19.743 -143.174 -117.701 -105.347  -92.448  -64.620 1.001\nNA gamma[22]  -14.307  19.903  -54.617  -27.704  -14.041   -0.901   23.918 1.001\nNA gamma[23]  -81.493  19.863 -121.932  -94.860  -81.367  -67.618  -43.350 1.001\nNA gamma[24]  -86.520  20.067 -125.826 -100.133  -86.297  -73.003  -47.481 1.001\nNA gamma[25]  -47.166  20.417  -86.549  -61.568  -47.155  -33.479   -6.020 1.001\nNA gamma[26]  -92.375  19.497 -130.380 -105.540  -92.310  -79.043  -54.017 1.002\nNA gamma[27]   20.875  20.031  -18.328    6.625   20.873   34.275   60.661 1.002\nNA gamma[28]   74.464  19.909   36.179   61.091   74.369   87.433  114.480 1.001\nNA gamma[29]   -0.792  19.771  -39.202  -14.402   -0.589   12.677   36.999 1.001\nNA gamma[30]  -75.855  20.350 -116.077  -89.286  -76.179  -62.465  -35.911 1.001\nNA gamma[31]  -58.457  20.104  -97.479  -72.394  -58.390  -44.695  -19.492 1.001\nNA gamma[32]  -53.274  20.080  -91.482  -66.984  -53.201  -40.309  -13.057 1.001\nNA gamma[33]   15.405  20.131  -22.770    1.695   15.048   28.831   55.869 1.001\nNA gamma[34]   59.338  20.334   19.750   45.810   59.517   72.790   99.194 1.001\nNA gamma[35]   56.933  20.301   15.197   43.906   57.388   70.588   94.583 1.001\nNA res[1]      97.432  17.921   62.629   85.359   97.337  109.111  134.580 1.002\nNA res[2]     105.769  17.698   71.182   93.805  105.826  117.408  142.144 1.002\nNA res[3]      32.710  17.534   -1.148   20.909   32.522   44.178   68.460 1.002\nNA res[4]       9.121  17.431  -23.865   -2.637    8.871   20.532   44.239 1.002\nNA res[5]      29.201  17.391   -4.021   17.641   28.835   40.540   64.071 1.002\nNA res[6]      -4.531  17.414  -38.094  -16.092   -4.863    6.832   30.205 1.002\nNA res[7]    -107.185  17.500 -140.651 -118.950 -107.516  -95.741  -72.125 1.002\nNA res[8]     -37.350  17.647  -71.015  -49.217  -37.556  -25.708   -1.746 1.002\nNA res[9]     -67.307  17.855 -101.635  -79.246  -67.326  -55.699  -31.035 1.002\nNA res[10]    -78.614  18.121 -113.985  -90.735  -78.654  -66.764  -42.063 1.002\nNA res[11]     49.884  18.193   13.728   37.834   50.102   61.789   84.845 1.001\nNA res[12]     11.595  17.930  -23.183   -0.446   11.692   23.269   46.321 1.001\nNA res[13]     61.820  17.726   27.566   49.860   61.924   73.375   96.299 1.001\nNA res[14]     -3.458  17.582  -37.401  -15.192   -3.465    8.046   30.969 1.001\nNA res[15]    -10.511  17.499  -44.587  -21.767  -10.459    1.089   24.112 1.001\nNA res[16]     -2.243  17.479  -36.572  -13.604   -2.049    9.337   32.255 1.001\nNA res[17]    -50.520  17.522  -85.179  -61.867  -50.373  -38.782  -16.278 1.001\nNA res[18]    -62.585  17.627  -97.953  -73.969  -62.501  -50.564  -28.305 1.001\nNA res[19]    -42.079  17.792  -77.762  -53.482  -42.166  -30.142   -7.621 1.001\nNA res[20]      9.165  18.018  -26.988   -2.455    9.147   21.271   43.778 1.001\nNA res[21]    -77.926  17.501 -113.047  -89.469  -78.006  -66.052  -43.688 1.002\nNA res[22]    -73.189  17.257 -108.057  -84.536  -73.411  -61.442  -39.850 1.002\nNA res[23]    -14.228  17.075  -48.379  -25.553  -14.413   -2.773   18.665 1.002\nNA res[24]    -31.958  16.955  -66.594  -43.263  -32.141  -20.655    0.554 1.002\nNA res[25]     -7.975  16.900  -42.439  -19.028   -8.192    3.408   24.762 1.002\nNA res[26]     24.320  16.909   -9.262   13.308   24.202   35.576   57.313 1.002\nNA res[27]     38.799  16.983    5.216   27.563   38.571   50.247   72.421 1.002\nNA res[28]     69.516  17.120   35.582   58.330   69.291   80.872  103.259 1.002\nNA res[29]     55.153  17.321   20.895   43.887   54.848   66.615   89.335 1.002\nNA res[30]     28.976  17.581   -5.726   17.342   28.565   40.577   63.635 1.002\nNA res[31]   -121.587  17.872 -156.152 -133.567 -121.526 -109.634  -87.382 1.001\nNA res[32]   -100.256  17.618 -133.884 -111.966 -100.063  -88.659  -66.409 1.001\nNA res[33]    -57.168  17.423  -90.542  -68.902  -57.110  -45.627  -22.964 1.001\nNA res[34]    -17.580  17.290  -51.232  -29.462  -17.603   -6.075   16.449 1.001\nNA res[35]    -40.581  17.219  -74.430  -52.498  -40.599  -29.045   -6.738 1.001\nNA res[36]     57.619  17.212   23.794   45.812   57.457   69.114   91.210 1.001\nNA res[37]     61.388  17.269   27.008   49.782   61.277   72.884   95.367 1.004\nNA res[38]     56.259  17.389   21.473   44.753   56.294   67.702   90.416 1.001\nNA res[39]    109.316  17.570   73.992   97.577  109.390  120.643  143.528 1.001\nNA res[40]     52.088  17.811   16.206   39.944   52.135   63.655   86.709 1.001\nNA res[41]    -38.914  17.643  -73.656  -50.658  -38.634  -26.860   -5.046 1.001\nNA res[42]     77.326  17.420   42.616   65.720   77.575   89.215  111.120 1.001\nNA res[43]     50.865  17.258   16.915   39.497   51.222   62.617   84.564 1.001\nNA res[44]    110.679  17.159   76.469   99.239  111.186  122.203  144.204 1.001\nNA res[45]      4.790  17.122  -29.711   -6.616    5.343   16.301   38.330 1.001\nNA res[46]    -17.661  17.150  -52.491  -29.227  -17.113   -5.957   16.017 1.001\nNA res[47]     -7.311  17.242  -42.494  -18.933   -6.757    4.462   26.166 1.001\nNA res[48]     -3.089  17.396  -38.865  -14.616   -2.473    8.846   31.113 1.001\nNA res[49]    -65.133  17.611 -101.697  -76.777  -64.519  -53.174  -30.398 1.001\nNA res[50]    -63.661  17.886 -100.707  -75.693  -63.048  -51.501  -28.264 1.001\nNA res[51]    -31.518  17.144  -65.465  -43.209  -31.355  -19.712    1.672 1.001\nNA res[52]     14.815  16.893  -18.897    3.470   14.776   26.305   47.890 1.001\nNA res[53]     70.347  16.704   36.921   58.939   70.092   81.766  102.877 1.001\nNA res[54]    -50.853  16.579  -83.834  -62.055  -51.073  -39.619  -18.942 1.001\nNA res[55]     25.083  16.520   -8.057   14.027   24.935   36.357   56.754 1.001\nNA res[56]    -38.143  16.527  -71.222  -49.423  -38.357  -26.836   -6.450 1.001\nNA res[57]     -4.070  16.600  -36.763  -15.317   -4.236    7.197   27.724 1.001\nNA res[58]     33.306  16.738    0.513   22.191   33.191   44.522   65.598 1.001\nNA res[59]     20.080  16.940  -12.937    8.828   19.836   31.550   52.764 1.001\nNA res[60]    -12.900  17.204  -46.161  -24.276  -13.088   -1.463   20.472 1.001\nNA res[61]    -17.368  17.885  -51.348  -29.241  -17.425   -5.291   18.072 1.001\nNA res[62]      7.369  17.652  -26.193   -4.253    7.234   19.236   42.448 1.001\nNA res[63]     49.245  17.479   15.639   37.647   49.046   60.880   83.698 1.001\nNA res[64]    -64.174  17.368  -97.742  -75.711  -64.357  -52.534  -30.120 1.001\nNA res[65]   -134.249  17.320 -167.266 -146.107 -134.437 -122.586 -100.604 1.001\nNA res[66]    -37.107  17.334  -70.680  -48.991  -37.270  -25.543   -3.298 1.001\nNA res[67]     21.279  17.412  -12.492    9.532   21.267   33.096   55.105 1.001\nNA res[68]     37.284  17.552    3.245   25.564   37.265   49.184   71.420 1.001\nNA res[69]     63.944  17.753   29.095   52.068   63.864   75.938   98.781 1.001\nNA res[70]     95.234  18.013   60.203   83.123   95.270  107.455  130.632 1.001\nNA res[71]    -48.396  17.620  -83.102  -60.471  -48.230  -36.426  -13.894 1.001\nNA res[72]     16.818  17.403  -17.131    4.932   16.936   28.621   50.566 1.001\nNA res[73]    -10.912  17.247  -44.871  -22.609  -10.795    0.606   22.307 1.001\nNA res[74]      2.547  17.154  -31.372   -8.982    2.664   14.012   35.509 1.001\nNA res[75]    -13.113  17.125  -47.212  -24.603  -12.951   -1.578   20.054 1.001\nNA res[76]     32.577  17.159   -1.807   20.989   32.596   44.219   65.805 1.001\nNA res[77]      7.970  17.257  -27.008   -3.791    7.911   19.682   41.186 1.001\nNA res[78]     31.495  17.418   -3.602   19.914   31.400   43.395   65.506 1.001\nNA res[79]      4.718  17.639  -30.550   -6.860    4.806   16.702   39.109 1.001\nNA res[80]    -51.946  17.919  -87.119  -63.726  -51.922  -39.897  -17.025 1.001\nNA res[81]    -63.210  17.647  -97.791  -75.690  -63.212  -51.190  -29.440 1.002\nNA res[82]    -31.067  17.390  -65.386  -43.236  -31.079  -19.273    2.374 1.002\nNA res[83]     43.678  17.193    9.558   31.772   43.540   55.495   76.981 1.002\nNA res[84]     20.380  17.058  -13.497    8.805   20.360   32.176   53.934 1.002\nNA res[85]     54.597  16.986   21.096   43.276   54.749   66.249   87.971 1.002\nNA res[86]    124.353  16.979   90.938  113.063  124.583  135.902  157.927 1.002\nNA res[87]     67.176  17.037   33.903   55.809   67.439   78.691  101.104 1.002\nNA res[88]    -30.778  17.158  -64.046  -42.370  -30.407  -19.292    2.913 1.002\nNA res[89]    -55.098  17.342  -88.751  -66.851  -54.716  -43.392  -21.208 1.002\nNA res[90]    -73.427  17.586 -107.590  -85.203  -73.116  -61.615  -38.693 1.002\nNA res[91]    -39.879  17.759  -75.389  -51.772  -39.985  -28.068   -4.896 1.001\nNA res[92]     40.803  17.486    5.829   29.249   40.623   52.320   75.462 1.001\nNA res[93]      3.288  17.272  -30.880   -8.346    3.017   14.813   37.091 1.001\nNA res[94]      8.096  17.120  -26.071   -3.487    7.840   19.496   41.421 1.001\nNA res[95]    -18.230  17.031  -51.683  -29.609  -18.372   -6.678   14.819 1.001\nNA res[96]    -27.022  17.006  -60.187  -38.426  -27.134  -15.863    6.296 1.001\nNA res[97]    -19.513  17.045  -52.825  -30.880  -19.632   -8.423   13.668 1.001\nNA res[98]     37.064  17.149    3.831   25.570   37.124   48.265   70.659 1.001\nNA res[99]     21.843  17.315  -12.203   10.180   21.878   33.093   55.675 1.001\nNA res[100]    39.105  17.542    5.025   27.487   38.948   50.788   73.056 1.001\nNA res[101]    29.449  17.991   -5.205   17.155   29.781   41.807   63.442 1.001\nNA res[102]    49.685  17.766   15.218   37.673   50.072   61.814   82.819 1.001\nNA res[103]    -8.723  17.601  -43.029  -20.739   -8.303    3.244   24.590 1.001\nNA res[104]    54.477  17.498   20.833   42.653   54.759   66.407   87.773 1.001\nNA res[105]     4.508  17.456  -29.390   -7.159    4.817   16.421   37.676 1.001\nNA res[106]   -21.847  17.477  -55.720  -33.395  -21.535   -9.860   11.772 1.001\nNA res[107]    32.423  17.561   -2.084   20.791   32.811   44.448   66.578 1.001\nNA res[108]    70.203  17.706   35.619   58.406   70.311   82.156  104.571 1.001\nNA res[109]   -18.915  17.912  -54.616  -30.740  -18.911   -6.868   15.481 1.001\nNA res[110]   -79.501  18.176 -115.834  -91.501  -79.578  -67.080  -44.590 1.001\nNA res[111]   -35.407  17.785  -70.692  -46.976  -35.584  -23.160   -0.930 1.001\nNA res[112]   -16.834  17.533  -51.394  -28.356  -17.011   -4.647   17.012 1.001\nNA res[113]    -2.964  17.342  -37.311  -14.283   -3.253    9.068   30.829 1.001\nNA res[114]    17.958  17.211  -16.244    6.888   17.697   29.740   51.566 1.001\nNA res[115]    43.960  17.144    9.743   32.909   43.779   55.749   77.773 1.002\nNA res[116]     6.922  17.141  -27.302   -4.089    6.746   18.522   41.245 1.002\nNA res[117]   -42.437  17.202  -76.634  -53.389  -42.592  -31.097   -7.471 1.002\nNA res[118]    18.962  17.325  -15.305    7.723   18.882   30.432   53.655 1.003\nNA res[119]    54.157  17.511   19.505   42.669   54.007   65.841   88.924 1.003\nNA res[120]   -30.836  17.757  -66.091  -42.472  -31.031  -18.943    4.597 1.003\nNA res[121]    29.694  17.913   -5.583   17.603   29.789   41.163   65.302 1.001\nNA res[122]    -8.428  17.668  -42.894  -20.446   -8.260    2.790   26.838 1.001\nNA res[123]   -97.805  17.483 -132.040 -109.658  -97.722  -86.688  -63.003 1.001\nNA res[124]   -58.400  17.360  -92.551  -70.268  -58.013  -47.184  -24.118 1.001\nNA res[125]   -38.480  17.298  -72.858  -50.159  -38.072  -27.272   -4.162 1.001\nNA res[126]   -23.590  17.301  -58.378  -35.298  -23.103  -12.266   10.906 1.001\nNA res[127]     3.860  17.366  -31.068   -7.655    4.299   15.220   38.206 1.001\nNA res[128]    58.998  17.494   23.758   47.395   59.472   70.391   93.062 1.001\nNA res[129]    69.127  17.683   33.354   57.321   69.673   80.505  103.147 1.001\nNA res[130]    35.991  17.931   -0.747   24.279   36.423   47.825   70.397 1.001\nNA res[131]   -18.707  17.753  -53.599  -30.549  -18.607   -6.714   16.819 1.003\nNA res[132]   -14.747  17.516  -48.954  -26.537  -14.555   -2.845   20.061 1.003\nNA res[133]    10.374  17.340  -23.749   -1.370   10.597   22.133   44.664 1.003\nNA res[134]   -37.152  17.225  -70.611  -48.839  -37.026  -25.712   -3.136 1.002\nNA res[135]   -32.541  17.174  -65.860  -44.108  -32.536  -21.077    1.265 1.002\nNA res[136]    28.588  17.186   -4.848   17.066   28.625   39.992   62.044 1.002\nNA res[137]    23.576  17.262  -10.078   11.987   23.698   35.092   57.117 1.002\nNA res[138]   -10.078  17.401  -43.670  -21.709   -9.920    1.610   23.933 1.001\nNA res[139]   -16.016  17.601  -49.961  -27.799  -15.770   -4.358   18.774 1.001\nNA res[140]    48.626  17.861   14.303   36.410   48.905   60.610   83.764 1.001\nNA res[141]     4.593  17.780  -29.625   -7.191    4.424   16.505   38.788 1.001\nNA res[142]    57.463  17.544   23.749   45.968   57.368   69.300   91.358 1.001\nNA res[143]    44.743  17.368   11.543   33.225   44.780   56.414   78.495 1.001\nNA res[144]    47.987  17.253   14.659   36.564   48.140   59.625   81.776 1.001\nNA res[145]     2.009  17.202  -31.053   -9.596    2.094   13.679   35.963 1.001\nNA res[146]    23.279  17.214   -9.893   11.605   23.253   34.820   57.175 1.001\nNA res[147]   -15.427  17.290  -49.433  -27.118  -15.449   -4.007   19.070 1.001\nNA res[148]   -92.242  17.428 -126.419 -104.123  -92.223  -80.822  -57.405 1.001\nNA res[149]   -76.403  17.628 -110.319  -88.293  -76.440  -64.957  -41.280 1.001\nNA res[150]    27.022  17.887   -7.788   14.991   26.934   38.901   62.553 1.001\nNA res[151]    56.524  17.435   23.497   44.650   56.376   68.008   91.937 1.002\nNA res[152]    94.888  17.193   62.473   83.265   94.748  106.107  129.551 1.001\nNA res[153]    85.122  17.012   53.069   73.707   84.983   96.084  119.032 1.001\nNA res[154]    28.800  16.893   -3.079   17.754   28.668   39.844   62.380 1.001\nNA res[155]     3.921  16.839  -28.102   -7.248    3.904   15.053   37.301 1.001\nNA res[156]   -19.671  16.850  -51.711  -30.814  -19.649   -8.511   13.270 1.001\nNA res[157]   -48.454  16.926  -81.115  -59.563  -48.405  -37.301  -15.678 1.001\nNA res[158]   -12.976  17.067  -46.088  -24.257  -12.925   -1.706   20.012 1.001\nNA res[159]   -79.807  17.269 -113.216  -91.035  -79.682  -68.569  -46.865 1.001\nNA res[160]   -30.224  17.532  -64.395  -41.789  -29.847  -18.816    2.893 1.001\nNA res[161]   -10.710  17.984  -46.042  -22.979  -10.276    1.572   24.298 1.002\nNA res[162]   -82.452  17.740 -118.119  -94.601  -82.059  -70.317  -47.916 1.002\nNA res[163]   -48.077  17.555  -83.627  -59.974  -47.795  -35.994  -13.951 1.002\nNA res[164]    68.821  17.431   33.752   56.976   68.995   80.691  102.233 1.005\nNA res[165]    12.830  17.369  -22.123    1.019   12.897   24.719   45.628 1.002\nNA res[166]    38.006  17.370    3.280   26.085   37.983   49.937   71.003 1.002\nNA res[167]   -23.347  17.435  -58.240  -35.377  -23.237  -11.414    9.865 1.002\nNA res[168]    22.078  17.561  -13.256    9.887   22.185   34.157   55.218 1.001\nNA res[169]    15.237  17.749  -20.525    3.105   15.325   27.296   48.552 1.001\nNA res[170]    79.730  17.996   43.542   67.295   79.674   91.820  113.635 1.002\nNA res[171]    63.717  17.877   28.767   51.218   63.625   75.992   98.346 1.002\nNA res[172]    50.693  17.611   16.356   38.398   50.624   62.696   84.475 1.001\nNA res[173]    16.355  17.405  -17.386    4.388   16.412   28.087   49.899 1.001\nNA res[174]     5.229  17.259  -28.246   -6.690    5.281   16.848   38.937 1.001\nNA res[175]   -25.424  17.176  -58.537  -37.419  -25.426  -13.955    7.970 1.001\nNA res[176]   -60.299  17.157  -93.240  -72.218  -60.299  -48.754  -26.651 1.001\nNA res[177]   -17.707  17.202  -50.497  -29.605  -17.777   -6.222   15.477 1.001\nNA res[178]   -67.087  17.310  -99.859  -79.145  -67.067  -55.435  -33.398 1.001\nNA res[179]    21.851  17.480  -10.981    9.770   21.871   33.713   56.070 1.001\nNA res[180]   -40.647  17.711  -73.957  -52.771  -40.513  -28.613   -5.841 1.001\nNA res[181]   -19.458  17.542  -54.333  -31.085  -19.422   -7.722   15.640 1.001\nNA res[182]    13.382  17.297  -20.657    2.031   13.446   24.888   47.985 1.001\nNA res[183]    42.203  17.113    8.488   30.978   42.207   53.606   75.709 1.001\nNA res[184]    20.699  16.991  -12.648    9.734   20.572   32.070   53.897 1.001\nNA res[185]    22.419  16.933  -10.796   11.505   22.206   33.885   55.335 1.001\nNA res[186]    67.746  16.941   34.410   57.004   67.445   79.392  100.423 1.001\nNA res[187]   -17.017  17.012  -50.239  -27.920  -17.298   -5.427   15.763 1.001\nNA res[188]   -51.228  17.148  -84.441  -62.328  -51.548  -39.688  -18.069 1.001\nNA res[189]   -23.628  17.345  -57.805  -34.767  -23.760  -11.997   10.684 1.001\nNA res[190]   -39.945  17.603  -74.634  -51.215  -40.154  -28.208   -5.214 1.001\nNA res[191]    52.530  17.512   18.626   41.139   52.206   64.088   87.150 1.006\nNA res[192]    79.593  17.246   46.248   68.448   79.388   91.100  113.049 1.017\nNA res[193]    71.051  17.040   37.868   59.980   70.793   82.571  104.153 1.021\nNA res[194]   -14.917  16.897  -48.034  -25.902  -15.296   -3.462   18.041 1.005\nNA res[195]    -7.135  16.817  -39.943  -18.047   -7.544    4.215   25.897 1.005\nNA res[196]   -18.174  16.803  -51.133  -29.182  -18.623   -6.895   14.827 1.005\nNA res[197]   -16.439  16.854  -48.969  -27.463  -16.762   -5.147   16.557 1.004\nNA res[198]   -69.150  16.970 -102.173  -80.328  -69.361  -57.572  -35.980 1.004\nNA res[199]   -27.445  17.149  -60.854  -38.683  -27.690  -15.800    6.299 1.003\nNA res[200]   -71.100  17.389 -104.684  -82.727  -71.394  -59.189  -37.691 1.003\nNA res[201]     1.426  17.309  -34.407   -9.571    1.332   12.962   35.011 1.001\nNA res[202]    36.131  17.046    0.771   25.266   36.227   47.347   69.464 1.001\nNA res[203]     5.510  16.844  -28.967   -5.374    5.685   16.535   38.753 1.001\nNA res[204]    49.200  16.705   15.453   38.505   49.435   60.187   82.079 1.001\nNA res[205]   -10.960  16.631  -44.546  -21.713  -10.492   -0.147   21.703 1.001\nNA res[206]  -133.889  16.623 -167.539 -144.693 -133.301 -123.036 -101.431 1.001\nNA res[207]   -68.634  16.681 -102.616  -79.683  -67.975  -57.769  -36.182 1.001\nNA res[208]     2.212  16.804  -31.897   -8.838    2.830   13.283   34.752 1.001\nNA res[209]     2.431  16.991  -31.627   -8.767    3.000   13.550   35.269 1.001\nNA res[210]    41.968  17.240    6.811   30.539   42.440   53.467   75.061 1.001\nNA res[211]   -67.622  17.508 -102.075  -79.236  -67.541  -56.136  -32.937 1.001\nNA res[212]   -57.530  17.266  -91.092  -68.962  -57.500  -46.025  -22.873 1.001\nNA res[213]  -140.313  17.084 -173.718 -151.700 -140.155 -128.926 -105.859 1.002\nNA res[214]   -50.542  16.964  -83.840  -61.841  -50.386  -39.139  -16.241 1.002\nNA res[215]    55.074  16.909   22.122   43.726   55.352   66.374   89.195 1.002\nNA res[216]   100.133  16.919   67.118   88.860  100.592  111.422  133.899 1.002\nNA res[217]    80.975  16.993   48.141   69.704   81.338   92.108  115.080 1.002\nNA res[218]    65.212  17.132   32.285   53.430   65.470   76.536  100.152 1.001\nNA res[219]   -21.674  17.332  -55.118  -33.640  -21.456  -10.022   13.856 1.002\nNA res[220]    24.003  17.593   -9.530   11.811   24.101   35.907   59.852 1.002\nNA res[221]    60.185  17.783   25.820   48.457   59.813   72.436   96.171 1.001\nNA res[222]    26.825  17.539   -6.907   15.267   26.510   38.780   62.320 1.001\nNA res[223]   -37.765  17.355  -71.270  -49.250  -38.173  -25.946   -2.142 1.001\nNA res[224]   -33.962  17.232  -67.496  -45.390  -34.499  -22.046    1.130 1.001\nNA res[225]   -58.522  17.173  -91.829  -70.044  -59.083  -46.732  -23.657 1.001\nNA res[226]   -55.706  17.177  -88.390  -67.397  -56.263  -44.042  -21.182 1.001\nNA res[227]   -94.620  17.245 -127.457 -106.411  -95.119  -82.969  -60.749 1.001\nNA res[228]    19.371  17.375  -13.948    7.607   19.044   31.010   53.459 1.001\nNA res[229]    25.246  17.568   -8.529   13.626   24.828   37.075   59.410 1.001\nNA res[230]    84.355  17.820   50.467   72.405   83.982   96.570  118.968 1.001\nNA res[231]   -31.497  18.030  -66.696  -43.447  -31.785  -19.738    3.853 1.001\nNA res[232]   -33.555  17.750  -68.255  -45.064  -33.695  -22.067    1.221 1.001\nNA res[233]   -46.454  17.529  -80.525  -58.085  -46.585  -35.121  -11.905 1.001\nNA res[234]   -84.283  17.368 -118.128  -95.729  -84.398  -73.094  -50.480 1.001\nNA res[235]    23.792  17.270   -9.429   12.004   23.713   34.911   57.625 1.001\nNA res[236]   -37.979  17.234  -71.525  -49.693  -38.039  -26.826   -3.726 1.001\nNA res[237]    -0.851  17.262  -34.217  -12.666   -0.823   10.179   33.380 1.001\nNA res[238]    55.116  17.354   21.132   43.276   55.278   66.313   89.932 1.001\nNA res[239]    66.340  17.507   31.871   54.397   66.435   77.816  102.154 1.001\nNA res[240]    22.509  17.721  -12.331   10.444   22.467   34.188   58.690 1.001\nNA res[241]    48.779  17.985   13.144   36.603   49.101   61.358   82.744 1.001\nNA res[242]    54.607  17.728   19.723   42.634   54.853   67.014   88.097 1.001\nNA res[243]    -2.573  17.531  -36.920  -14.310   -2.250    9.522   31.153 1.001\nNA res[244]   -64.682  17.394  -98.704  -76.398  -64.353  -52.693  -31.136 1.001\nNA res[245]    59.232  17.320   25.143   47.518   59.597   70.983   92.873 1.001\nNA res[246]    19.963  17.309  -14.316    8.145   20.310   31.886   53.371 1.001\nNA res[247]   -70.443  17.361 -105.373  -82.320  -70.049  -58.397  -36.717 1.001\nNA res[248]   -23.463  17.476  -58.679  -35.360  -23.248  -11.587   10.598 1.001\nNA res[249]     2.831  17.652  -32.482   -8.951    3.087   14.786   37.287 1.001\nNA res[250]   -59.475  17.888  -95.014  -71.176  -59.315  -47.543  -24.802 1.001\nNA res[251]  -118.664  17.326 -151.645 -130.208 -118.894 -107.309  -83.446 1.002\nNA res[252]   -91.020  17.066 -123.512 -102.427  -91.199  -79.836  -57.008 1.001\nNA res[253]    -6.258  16.868  -38.555  -17.830   -6.464    4.709   27.751 1.001\nNA res[254]    36.251  16.734    4.248   24.639   35.981   47.226   70.217 1.001\nNA res[255]    13.667  16.664  -18.023    2.118   13.400   24.523   47.428 1.001\nNA res[256]   -21.601  16.659  -53.042  -33.062  -21.878  -10.646   12.267 1.001\nNA res[257]     5.310  16.721  -26.340   -6.021    5.033   16.411   39.015 1.001\nNA res[258]    21.015  16.847  -10.852    9.662   20.790   32.193   55.145 1.001\nNA res[259]    57.059  17.037   24.518   45.611   56.831   68.241   92.442 1.002\nNA res[260]    34.481  17.288    1.254   23.094   34.411   45.908   70.086 1.001\nNA res[261]    50.413  17.715   15.780   38.279   50.907   62.488   84.294 1.002\nNA res[262]     1.013  17.462  -32.559  -11.003    1.463   13.068   33.840 1.002\nNA res[263]   -13.632  17.268  -46.927  -25.466  -13.251   -1.826   18.917 1.002\nNA res[264]    28.083  17.137   -5.178   16.694   28.340   39.623   60.566 1.003\nNA res[265]    73.774  17.069   40.522   62.477   73.840   85.174  106.109 1.003\nNA res[266]   -36.234  17.065  -69.304  -47.505  -35.963  -24.747   -3.793 1.003\nNA res[267]   -22.093  17.125  -55.763  -33.330  -21.828  -10.616   10.070 1.003\nNA res[268]    14.160  17.249  -19.827    3.041   14.507   25.776   46.182 1.003\nNA res[269]   -59.954  17.435  -93.778  -71.235  -59.514  -48.101  -27.315 1.003\nNA res[270]   -22.812  17.681  -57.121  -34.293  -22.324  -10.851   10.918 1.003\nNA res[271]  -125.907  17.458 -160.933 -137.188 -125.802 -114.461  -92.053 1.001\nNA res[272]   -62.314  17.229  -97.496  -73.416  -62.473  -50.835  -29.085 1.001\nNA res[273]   -35.666  17.061  -70.145  -46.545  -35.800  -24.427   -3.232 1.001\nNA res[274]    -2.957  16.956  -37.392  -13.883   -2.813    8.075   29.213 1.001\nNA res[275]    -9.344  16.916  -43.467  -20.207   -9.343    1.681   22.885 1.001\nNA res[276]    22.554  16.940  -11.628   11.868   22.593   33.645   54.629 1.001\nNA res[277]    73.779  17.029   39.473   63.039   73.780   84.850  106.184 1.001\nNA res[278]   143.908  17.181  109.484  132.820  143.915  155.171  176.723 1.001\nNA res[279]   100.141  17.395   65.573   88.965  100.141  111.576  133.506 1.001\nNA res[280]   -46.044  17.669  -80.667  -57.400  -45.955  -34.590  -12.407 1.001\nNA res[281]    -5.699  17.595  -39.468  -17.381   -5.770    5.983   28.301 1.001\nNA res[282]     0.419  17.343  -33.174  -10.923    0.288   11.974   34.262 1.001\nNA res[283]   -12.869  17.152  -46.211  -24.317  -13.049   -1.401   20.398 1.001\nNA res[284]    12.545  17.023  -20.331    1.056   12.458   24.113   45.686 1.001\nNA res[285]    54.857  16.958   22.320   43.310   54.741   66.049   88.406 1.001\nNA res[286]    12.136  16.958  -20.207    0.644   11.947   23.530   45.348 1.001\nNA res[287]   -10.984  17.022  -43.192  -22.616  -10.977    0.294   22.275 1.001\nNA res[288]     4.979  17.150  -27.724   -6.762    5.048   16.454   38.007 1.001\nNA res[289]   -29.791  17.340  -62.952  -41.651  -29.779  -18.299    3.958 1.001\nNA res[290]   -25.671  17.591  -59.797  -37.708  -25.457  -14.066    8.558 1.001\nNA res[291]    28.550  17.963   -7.697   16.728   28.845   40.812   62.212 1.001\nNA res[292]    -9.089  17.712  -45.024  -20.553   -8.933    2.924   24.333 1.001\nNA res[293]   -49.732  17.521  -85.124  -61.052  -49.636  -38.060  -16.508 1.001\nNA res[294]   -58.677  17.390  -93.259  -69.572  -58.567  -47.134  -25.799 1.001\nNA res[295]   -57.955  17.322  -92.707  -68.933  -57.842  -46.344  -25.049 1.001\nNA res[296]    -3.734  17.317  -38.668  -14.858   -3.653    7.873   29.033 1.001\nNA res[297]    69.494  17.375   34.053   58.293   69.643   81.139  102.719 1.001\nNA res[298]    42.465  17.496    6.356   31.206   42.578   54.056   75.774 1.001\nNA res[299]     7.231  17.678  -29.387   -4.288    7.395   19.167   40.819 1.001\nNA res[300]   -23.337  17.919  -60.558  -34.994  -23.228  -11.302   10.882 1.001\nNA res[301]   -51.903  17.982  -88.248  -63.876  -52.309  -39.714  -17.303 1.001\nNA res[302]   -37.852  17.747  -73.574  -49.599  -38.337  -25.637   -3.466 1.001\nNA res[303]     9.383  17.571  -25.880   -2.168    9.060   21.337   43.272 1.001\nNA res[304]     6.786  17.457  -27.982   -4.692    6.466   18.794   40.534 1.001\nNA res[305]   -83.287  17.404 -117.576  -94.688  -83.511  -71.384  -50.132 1.001\nNA res[306]   -56.131  17.415  -90.605  -67.604  -56.382  -44.092  -22.768 1.001\nNA res[307]    29.387  17.488   -5.255   17.630   29.168   41.485   62.770 1.001\nNA res[308]    97.884  17.624   63.110   86.054   97.739  110.020  131.499 1.001\nNA res[309]    53.516  17.820   18.505   41.582   53.468   65.807   87.676 1.001\nNA res[310]   -20.057  18.074  -55.492  -32.169  -20.032   -7.574   14.442 1.001\nNA res[311]   101.300  17.549   67.220   89.168  101.137  113.302  135.233 1.001\nNA res[312]    83.175  17.312   49.506   71.360   82.808   95.098  116.774 1.001\nNA res[313]    71.785  17.135   37.980   60.064   71.677   83.600  105.227 1.001\nNA res[314]    88.437  17.022   55.339   76.901   88.506  100.014  122.116 1.001\nNA res[315]    -0.110  16.972  -33.336  -11.341    0.071   11.200   33.192 1.001\nNA res[316]   -26.794  16.987  -60.411  -38.159  -26.771  -15.599    6.418 1.001\nNA res[317]   -88.891  17.067 -122.585 -100.389  -88.824  -77.593  -55.404 1.001\nNA res[318]   -96.339  17.209 -130.866 -107.673  -96.328  -84.910  -62.716 1.001\nNA res[319]   -61.837  17.414  -96.679  -73.288  -61.840  -50.019  -28.051 1.001\nNA res[320]  -108.552  17.679 -143.474 -120.010 -108.694  -96.508  -74.344 1.001\nNA res[321]   -74.410  17.625 -109.575  -85.816  -74.485  -62.663  -39.753 1.001\nNA res[322]   -41.400  17.380  -75.871  -52.760  -41.519  -29.797   -7.178 1.001\nNA res[323]   -74.807  17.196 -108.901  -86.261  -74.760  -63.308  -41.163 1.001\nNA res[324]   -45.144  17.074  -79.209  -56.502  -45.026  -33.729  -12.103 1.001\nNA res[325]     4.537  17.016  -29.468   -6.710    4.542   15.856   37.618 1.001\nNA res[326]    59.794  17.022   26.116   48.375   59.758   71.282   93.240 1.001\nNA res[327]    40.165  17.092    6.814   28.767   40.355   51.655   73.479 1.001\nNA res[328]    30.285  17.226   -3.464   18.692   30.679   41.872   63.673 1.001\nNA res[329]    22.589  17.422  -10.860   10.756   23.053   34.110   56.454 1.001\nNA res[330]    92.703  17.678   58.616   80.766   93.284  104.245  126.965 1.001\nNA res[331]    95.283  17.977   59.650   83.167   95.192  107.412  130.221 1.001\nNA res[332]   112.719  17.720   77.424  100.669  112.724  124.693  147.222 1.001\nNA res[333]    70.443  17.522   35.436   58.422   70.438   82.385  104.268 1.001\nNA res[334]    69.514  17.384   34.563   57.587   69.353   81.410  102.556 1.001\nNA res[335]    70.134  17.309   35.643   58.219   69.855   81.817  103.521 1.001\nNA res[336]    -1.755  17.297  -35.726  -13.635   -1.976   10.018   31.941 1.001\nNA res[337]   -93.736  17.349 -127.604 -105.548  -93.768  -81.835  -60.000 1.001\nNA res[338]   -48.951  17.463  -82.779  -60.869  -49.029  -36.839  -14.994 1.001\nNA res[339]  -121.819  17.638 -156.071 -133.910 -122.015 -109.591  -87.286 1.001\nNA res[340]  -103.701  17.874 -138.279 -115.672 -103.713  -91.305  -68.978 1.001\nNA res[341]   -69.317  18.059 -103.485  -81.639  -69.415  -57.406  -33.457 1.001\nNA res[342]   -32.346  17.788  -65.785  -44.511  -32.362  -20.467    2.839 1.001\nNA res[343]   -21.629  17.575  -54.850  -33.597  -21.808   -9.882   13.449 1.001\nNA res[344]   -59.307  17.422  -92.307  -71.080  -59.415  -47.483  -24.425 1.001\nNA res[345]    -3.627  17.331  -36.621  -15.370   -3.984    8.092   31.060 1.001\nNA res[346]    78.394  17.304   46.202   66.537   78.021   90.261  112.620 1.001\nNA res[347]   100.999  17.339   68.667   89.254  100.770  113.032  135.290 1.001\nNA res[348]   -22.296  17.438  -55.281  -34.095  -22.507  -10.328   12.264 1.001\nNA res[349]    46.092  17.598   12.847   34.241   46.001   58.157   80.422 1.001\nNA res[350]    27.883  17.819   -5.915   15.648   27.735   40.121   63.077 1.001\nNA sigma       55.917   2.244   51.705   54.351   55.829   57.468   60.369 1.003\nNA sigma.B     64.474   8.406   50.251   58.466   63.695   69.675   83.144 1.006\nNA deviance  3809.753   9.145 3794.047 3803.114 3809.077 3815.608 3829.461 1.003\nNA           n.eff\nNA beta       3000\nNA beta0      3000\nNA gamma[1]   1800\nNA gamma[2]   3000\nNA gamma[3]   2100\nNA gamma[4]   3000\nNA gamma[5]   3000\nNA gamma[6]   3000\nNA gamma[7]   3000\nNA gamma[8]   3000\nNA gamma[9]   2800\nNA gamma[10]  3000\nNA gamma[11]  2100\nNA gamma[12]  3000\nNA gamma[13]  3000\nNA gamma[14]  3000\nNA gamma[15]  2500\nNA gamma[16]  1700\nNA gamma[17]  1700\nNA gamma[18]  1800\nNA gamma[19]  3000\nNA gamma[20]  1500\nNA gamma[21]  3000\nNA gamma[22]  3000\nNA gamma[23]  3000\nNA gamma[24]  3000\nNA gamma[25]  3000\nNA gamma[26]  1700\nNA gamma[27]  1500\nNA gamma[28]  3000\nNA gamma[29]  2300\nNA gamma[30]  3000\nNA gamma[31]  3000\nNA gamma[32]  3000\nNA gamma[33]  2500\nNA gamma[34]  3000\nNA gamma[35]  3000\nNA res[1]     1100\nNA res[2]     1000\nNA res[3]      990\nNA res[4]      940\nNA res[5]      910\nNA res[6]      880\nNA res[7]      860\nNA res[8]      850\nNA res[9]      840\nNA res[10]     840\nNA res[11]    3000\nNA res[12]    3000\nNA res[13]    3000\nNA res[14]    3000\nNA res[15]    3000\nNA res[16]    3000\nNA res[17]    3000\nNA res[18]    3000\nNA res[19]    3000\nNA res[20]    3000\nNA res[21]    1200\nNA res[22]    1200\nNA res[23]    1100\nNA res[24]    1000\nNA res[25]    1000\nNA res[26]     970\nNA res[27]     950\nNA res[28]    1200\nNA res[29]     920\nNA res[30]     920\nNA res[31]    3000\nNA res[32]    3000\nNA res[33]    3000\nNA res[34]    3000\nNA res[35]    3000\nNA res[36]    3000\nNA res[37]    3000\nNA res[38]    3000\nNA res[39]    3000\nNA res[40]    3000\nNA res[41]    3000\nNA res[42]    3000\nNA res[43]    3000\nNA res[44]    3000\nNA res[45]    3000\nNA res[46]    3000\nNA res[47]    3000\nNA res[48]    3000\nNA res[49]    3000\nNA res[50]    3000\nNA res[51]    3000\nNA res[52]    3000\nNA res[53]    3000\nNA res[54]    3000\nNA res[55]    3000\nNA res[56]    3000\nNA res[57]    3000\nNA res[58]    3000\nNA res[59]    3000\nNA res[60]    3000\nNA res[61]    3000\nNA res[62]    3000\nNA res[63]    3000\nNA res[64]    3000\nNA res[65]    3000\nNA res[66]    3000\nNA res[67]    3000\nNA res[68]    3000\nNA res[69]    3000\nNA res[70]    3000\nNA res[71]    3000\nNA res[72]    3000\nNA res[73]    3000\nNA res[74]    3000\nNA res[75]    3000\nNA res[76]    3000\nNA res[77]    3000\nNA res[78]    3000\nNA res[79]    3000\nNA res[80]    3000\nNA res[81]    1100\nNA res[82]    1100\nNA res[83]    1000\nNA res[84]     970\nNA res[85]     930\nNA res[86]     930\nNA res[87]    1100\nNA res[88]     860\nNA res[89]     850\nNA res[90]     850\nNA res[91]    3000\nNA res[92]    3000\nNA res[93]    3000\nNA res[94]    3000\nNA res[95]    3000\nNA res[96]    3000\nNA res[97]    3000\nNA res[98]    3000\nNA res[99]    3000\nNA res[100]   3000\nNA res[101]   2700\nNA res[102]   2700\nNA res[103]   2800\nNA res[104]   3000\nNA res[105]   3000\nNA res[106]   3000\nNA res[107]   3000\nNA res[108]   3000\nNA res[109]   3000\nNA res[110]   3000\nNA res[111]   3000\nNA res[112]   3000\nNA res[113]   3000\nNA res[114]   3000\nNA res[115]   3000\nNA res[116]   3000\nNA res[117]   3000\nNA res[118]   3000\nNA res[119]   3000\nNA res[120]   3000\nNA res[121]   3000\nNA res[122]   3000\nNA res[123]   3000\nNA res[124]   3000\nNA res[125]   3000\nNA res[126]   3000\nNA res[127]   3000\nNA res[128]   3000\nNA res[129]   3000\nNA res[130]   3000\nNA res[131]   3000\nNA res[132]   3000\nNA res[133]   3000\nNA res[134]   3000\nNA res[135]   3000\nNA res[136]   3000\nNA res[137]   3000\nNA res[138]   3000\nNA res[139]   3000\nNA res[140]   3000\nNA res[141]   2700\nNA res[142]   2700\nNA res[143]   2800\nNA res[144]   3000\nNA res[145]   3000\nNA res[146]   3000\nNA res[147]   3000\nNA res[148]   3000\nNA res[149]   3000\nNA res[150]   3000\nNA res[151]   1700\nNA res[152]   2200\nNA res[153]   2300\nNA res[154]   1900\nNA res[155]   1900\nNA res[156]   2000\nNA res[157]   2200\nNA res[158]   2300\nNA res[159]   2500\nNA res[160]   2700\nNA res[161]   1500\nNA res[162]   1500\nNA res[163]   1500\nNA res[164]   1000\nNA res[165]   1600\nNA res[166]   1700\nNA res[167]   1800\nNA res[168]   1900\nNA res[169]   2000\nNA res[170]   1600\nNA res[171]   3000\nNA res[172]   1900\nNA res[173]   1900\nNA res[174]   2000\nNA res[175]   2000\nNA res[176]   2100\nNA res[177]   2300\nNA res[178]   2400\nNA res[179]   2600\nNA res[180]   2800\nNA res[181]   3000\nNA res[182]   3000\nNA res[183]   3000\nNA res[184]   3000\nNA res[185]   3000\nNA res[186]   3000\nNA res[187]   3000\nNA res[188]   3000\nNA res[189]   3000\nNA res[190]   3000\nNA res[191]   1500\nNA res[192]    630\nNA res[193]    570\nNA res[194]   1600\nNA res[195]   1700\nNA res[196]   1800\nNA res[197]   1900\nNA res[198]   2000\nNA res[199]   2100\nNA res[200]   2300\nNA res[201]   3000\nNA res[202]   3000\nNA res[203]   3000\nNA res[204]   3000\nNA res[205]   3000\nNA res[206]   3000\nNA res[207]   3000\nNA res[208]   3000\nNA res[209]   3000\nNA res[210]   3000\nNA res[211]   2000\nNA res[212]   1900\nNA res[213]   1700\nNA res[214]   1600\nNA res[215]   1600\nNA res[216]   1700\nNA res[217]   1700\nNA res[218]   1900\nNA res[219]   1400\nNA res[220]   1400\nNA res[221]   3000\nNA res[222]   3000\nNA res[223]   3000\nNA res[224]   3000\nNA res[225]   3000\nNA res[226]   3000\nNA res[227]   3000\nNA res[228]   3000\nNA res[229]   3000\nNA res[230]   3000\nNA res[231]   3000\nNA res[232]   3000\nNA res[233]   3000\nNA res[234]   3000\nNA res[235]   3000\nNA res[236]   3000\nNA res[237]   3000\nNA res[238]   3000\nNA res[239]   3000\nNA res[240]   3000\nNA res[241]   3000\nNA res[242]   3000\nNA res[243]   3000\nNA res[244]   3000\nNA res[245]   3000\nNA res[246]   3000\nNA res[247]   3000\nNA res[248]   3000\nNA res[249]   3000\nNA res[250]   3000\nNA res[251]   1800\nNA res[252]   1800\nNA res[253]   1900\nNA res[254]   1900\nNA res[255]   2000\nNA res[256]   2100\nNA res[257]   2200\nNA res[258]   2400\nNA res[259]   3000\nNA res[260]   2800\nNA res[261]    930\nNA res[262]    880\nNA res[263]    830\nNA res[264]    790\nNA res[265]    750\nNA res[266]    740\nNA res[267]    720\nNA res[268]    710\nNA res[269]    710\nNA res[270]    710\nNA res[271]   3000\nNA res[272]   3000\nNA res[273]   3000\nNA res[274]   3000\nNA res[275]   3000\nNA res[276]   3000\nNA res[277]   3000\nNA res[278]   3000\nNA res[279]   3000\nNA res[280]   3000\nNA res[281]   2600\nNA res[282]   2600\nNA res[283]   2700\nNA res[284]   2800\nNA res[285]   3000\nNA res[286]   3000\nNA res[287]   3000\nNA res[288]   3000\nNA res[289]   3000\nNA res[290]   3000\nNA res[291]   3000\nNA res[292]   3000\nNA res[293]   3000\nNA res[294]   3000\nNA res[295]   3000\nNA res[296]   3000\nNA res[297]   3000\nNA res[298]   3000\nNA res[299]   3000\nNA res[300]   3000\nNA res[301]   3000\nNA res[302]   3000\nNA res[303]   3000\nNA res[304]   3000\nNA res[305]   3000\nNA res[306]   3000\nNA res[307]   3000\nNA res[308]   3000\nNA res[309]   3000\nNA res[310]   3000\nNA res[311]   3000\nNA res[312]   3000\nNA res[313]   3000\nNA res[314]   3000\nNA res[315]   3000\nNA res[316]   3000\nNA res[317]   3000\nNA res[318]   3000\nNA res[319]   3000\nNA res[320]   3000\nNA res[321]   2700\nNA res[322]   2800\nNA res[323]   2900\nNA res[324]   3000\nNA res[325]   3000\nNA res[326]   3000\nNA res[327]   3000\nNA res[328]   3000\nNA res[329]   3000\nNA res[330]   3000\nNA res[331]   3000\nNA res[332]   3000\nNA res[333]   3000\nNA res[334]   3000\nNA res[335]   3000\nNA res[336]   3000\nNA res[337]   3000\nNA res[338]   3000\nNA res[339]   3000\nNA res[340]   3000\nNA res[341]   3000\nNA res[342]   3000\nNA res[343]   3000\nNA res[344]   3000\nNA res[345]   3000\nNA res[346]   3000\nNA res[347]   3000\nNA res[348]   3000\nNA res[349]   3000\nNA res[350]   3000\nNA sigma       700\nNA sigma.B    1000\nNA deviance    720\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 41.8 and DIC = 3851.5\nNA DIC is an estimate of expected predictive error (lower deviance is better)."
  },
  {
    "objectID": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-01-block-anova-jags/index.html#matrix-parameterisation-1",
    "title": "Randomised Complete Block Anova (JAGS)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nmodelString2=\"\nmodel {\n   #Likelihood\n   for (i in 1:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   \n   #Priors\n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString2, con = \"matrixModel2.txt\")\n\nXmat &lt;- model.matrix(~Time,data.rm)\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=Xmat,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block)),\n         a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\n         )\n)\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\")\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.m &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel2.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 40\nNA    Total graph size: 2521\nNA \nNA Initializing model\n\nprint(data.rm.r2jags.m)\n\nNA Inference for Bugs model at \"matrixModel2.txt\", fit using jags,\nNA  2 chains, each with 4500 iterations (first 3000 discarded)\nNA  n.sims = 3000 iterations saved\nNA            mu.vect sd.vect     2.5%      25%      50%      75%    97.5%  Rhat\nNA beta[1]      0.118   0.993   -1.825   -0.560    0.127    0.772    2.136 1.002\nNA beta[2]      9.067   1.041    6.989    8.384    9.085    9.754   11.096 1.009\nNA gamma[1]   268.496  28.003  214.165  249.963  268.515  287.365  322.199 1.002\nNA gamma[2]   249.357  27.207  196.523  230.541  249.374  268.093  303.009 1.001\nNA gamma[3]   326.337  28.472  271.718  306.769  325.694  345.668  383.423 1.002\nNA gamma[4]   305.015  28.782  249.411  285.199  305.040  324.243  361.356 1.001\nNA gamma[5]   377.709  27.831  324.335  358.940  377.037  396.616  432.150 1.001\nNA gamma[6]   343.430  27.578  288.864  325.843  342.939  361.444  396.819 1.001\nNA gamma[7]   332.166  27.783  278.248  313.377  331.664  351.388  385.777 1.002\nNA gamma[8]   271.616  27.163  217.184  253.588  271.956  289.628  322.963 1.001\nNA gamma[9]   384.589  27.386  332.007  366.136  384.502  402.093  440.209 1.001\nNA gamma[10]  367.756  27.587  314.137  349.123  368.026  386.501  421.707 1.001\nNA gamma[11]  457.715  27.438  405.652  439.504  457.268  476.098  513.423 1.001\nNA gamma[12]  312.867  27.216  262.216  293.798  313.154  331.284  366.377 1.001\nNA gamma[13]  258.804  27.818  201.324  240.475  259.644  276.264  313.330 1.002\nNA gamma[14]  279.449  28.042  224.175  260.014  279.067  298.411  335.404 1.001\nNA gamma[15]  337.363  28.099  284.866  317.715  336.998  356.109  394.348 1.001\nNA gamma[16]  416.383  27.354  363.925  397.826  416.595  434.011  473.215 1.002\nNA gamma[17]  403.080  27.501  349.003  384.921  403.079  421.627  456.974 1.002\nNA gamma[18]  237.418  27.946  183.713  218.161  237.435  256.648  290.420 1.001\nNA gamma[19]  323.886  27.728  270.777  305.233  323.114  342.641  377.533 1.001\nNA gamma[20]  275.462  28.013  220.684  255.987  275.838  294.317  330.957 1.001\nNA gamma[21]  194.059  27.398  141.539  175.756  194.167  211.807  248.181 1.002\nNA gamma[22]  289.821  27.693  236.092  271.210  289.707  307.332  344.403 1.001\nNA gamma[23]  217.663  28.050  164.441  197.887  217.932  235.812  274.184 1.002\nNA gamma[24]  212.748  27.923  159.816  193.937  211.567  231.708  268.892 1.001\nNA gamma[25]  255.843  26.839  204.539  237.181  255.916  274.542  309.265 1.003\nNA gamma[26]  206.396  28.055  152.547  187.160  206.227  225.611  261.170 1.001\nNA gamma[27]  327.559  27.486  273.350  309.633  327.509  345.364  380.375 1.002\nNA gamma[28]  384.714  27.705  330.300  365.884  384.552  403.155  437.847 1.003\nNA gamma[29]  304.783  28.077  251.111  284.961  304.255  323.437  360.045 1.001\nNA gamma[30]  225.251  28.017  168.742  206.532  226.000  243.944  279.897 1.001\nNA gamma[31]  242.036  28.513  184.632  223.534  242.114  260.552  297.438 1.001\nNA gamma[32]  250.059  27.603  197.965  231.432  249.373  268.506  305.275 1.003\nNA gamma[33]  321.386  27.863  266.714  302.647  321.462  340.165  377.552 1.003\nNA gamma[34]  368.704  27.907  314.047  350.229  368.434  387.452  423.293 1.001\nNA gamma[35]  365.968  27.738  311.338  347.444  365.930  385.018  420.415 1.005\nNA res[1]       4.433  27.784  -48.877  -14.717    4.562   22.705   58.869 1.002\nNA res[2]      34.393  27.628  -18.119   15.474   34.478   52.583   88.788 1.002\nNA res[3]     -17.044  27.510  -69.264  -35.707  -16.883    0.972   37.088 1.002\nNA res[4]     -19.010  27.431  -70.907  -37.537  -18.756   -0.634   35.109 1.002\nNA res[5]      22.692  27.391  -29.376    4.107   22.942   41.160   77.308 1.001\nNA res[6]      10.582  27.391  -41.038   -7.968   10.837   28.907   65.778 1.001\nNA res[7]     -70.449  27.431 -121.778  -88.991  -69.960  -52.214  -14.834 1.001\nNA res[8]      21.008  27.509  -30.615    2.408   21.241   39.197   76.881 1.001\nNA res[9]      12.673  27.627  -39.023   -6.003   12.972   31.145   68.777 1.001\nNA res[10]     22.988  27.783  -29.250    4.168   22.937   41.683   79.296 1.001\nNA res[11]    -40.986  26.942  -94.515  -59.717  -41.262  -22.608   11.356 1.001\nNA res[12]    -57.652  26.765 -110.392  -76.201  -57.806  -39.438   -5.452 1.001\nNA res[13]     14.195  26.627  -37.606   -4.203   14.145   32.218   66.504 1.001\nNA res[14]    -29.460  26.529  -81.001  -47.601  -29.683  -11.482   22.755 1.001\nNA res[15]    -14.892  26.472  -66.711  -32.886  -15.051    3.149   37.784 1.001\nNA res[16]     15.000  26.456  -36.965   -3.255   14.703   33.011   68.048 1.001\nNA res[17]    -11.656  26.481  -63.530  -29.872  -12.202    6.352   41.041 1.001\nNA res[18]     -2.098  26.546  -53.836  -20.736   -2.884   15.886   51.209 1.001\nNA res[19]     40.030  26.652  -11.158   21.366   39.102   58.171   93.729 1.001\nNA res[20]    112.896  26.799   61.236   94.306  112.162  131.227  167.261 1.001\nNA res[21]   -173.333  28.266 -230.343 -192.502 -172.496 -153.934 -119.008 1.002\nNA res[22]   -146.973  28.095 -203.369 -166.084 -146.394 -127.937  -93.044 1.002\nNA res[23]    -66.390  27.961 -122.215  -85.409  -65.713  -47.492  -12.298 1.002\nNA res[24]    -62.498  27.865 -117.833  -81.428  -61.625  -43.630   -7.786 1.001\nNA res[25]    -16.892  27.808  -71.818  -35.704  -16.206    2.104   37.863 1.001\nNA res[26]     37.026  27.790  -17.773   18.140   37.505   55.996   91.643 1.001\nNA res[27]     73.127  27.811   18.616   54.532   73.671   91.725  127.823 1.001\nNA res[28]    125.466  27.871   71.518  106.855  125.737  144.036  180.266 1.001\nNA res[29]    132.725  27.969   78.584  113.924  133.004  151.381  187.753 1.001\nNA res[30]    128.171  28.106   73.520  109.247  128.243  147.168  184.155 1.001\nNA res[31]   -215.417  28.519 -271.907 -234.515 -215.477 -196.031 -160.014 1.001\nNA res[32]   -172.464  28.331 -228.398 -191.559 -172.626 -153.223 -117.470 1.001\nNA res[33]   -107.754  28.180 -163.519 -126.542 -107.988  -88.710  -53.225 1.001\nNA res[34]    -46.543  28.067 -102.122  -65.407  -46.791  -27.844    8.278 1.001\nNA res[35]    -47.922  27.992 -103.460  -66.793  -48.157  -29.112    7.029 1.001\nNA res[36]     71.901  27.955   16.630   52.816   71.633   90.451  126.471 1.001\nNA res[37]     97.292  27.958   42.344   78.098   97.282  115.785  151.917 1.001\nNA res[38]    113.786  27.999   59.110   94.749  113.857  132.397  169.334 1.001\nNA res[39]    188.465  28.078  134.278  169.268  188.455  207.026  244.767 1.001\nNA res[40]    152.859  28.196   98.827  133.783  152.725  171.438  209.588 1.001\nNA res[41]   -138.298  27.589 -192.214 -157.018 -137.668 -119.884  -85.511 1.001\nNA res[42]     -0.435  27.414  -54.092  -19.066    0.070   17.947   51.755 1.001\nNA res[43]     -5.275  27.278  -58.296  -23.835   -4.661   12.955   46.693 1.001\nNA res[44]     76.163  27.182   23.390   57.931   76.757   94.384  128.160 1.001\nNA res[45]     -8.105  27.124  -60.470  -26.335   -7.526   10.317   43.968 1.001\nNA res[46]     -8.933  27.107  -61.592  -27.005   -8.421    9.419   43.130 1.001\nNA res[47]     23.039  27.130  -29.532    5.000   23.698   41.485   75.548 1.001\nNA res[48]     48.884  27.192   -3.759   30.945   49.511   67.086  101.297 1.001\nNA res[49]      8.462  27.295  -44.134   -9.630    8.812   26.700   61.215 1.001\nNA res[50]     31.557  27.436  -20.973   13.381   31.746   50.020   84.424 1.001\nNA res[51]   -128.097  27.415 -180.770 -146.327 -127.910 -110.576  -73.568 1.001\nNA res[52]    -60.141  27.254 -112.252  -78.239  -59.732  -42.653   -5.561 1.001\nNA res[53]     17.013  27.132  -35.210   -1.266   17.578   34.265   71.525 1.001\nNA res[54]    -82.564  27.049 -134.784 -100.658  -82.293  -65.282  -27.984 1.002\nNA res[55]     14.994  27.006  -36.578   -3.173   15.282   32.302   69.851 1.002\nNA res[56]    -26.609  27.004  -78.326  -44.704  -26.580   -9.314   28.128 1.002\nNA res[57]     29.085  27.041  -22.987   10.998   29.221   46.243   83.856 1.002\nNA res[58]     88.084  27.118   35.657   69.858   88.230  105.263  142.463 1.004\nNA res[59]     96.481  27.235   43.718   78.058   96.354  113.557  150.554 1.004\nNA res[60]     85.123  27.391   32.058   66.554   85.013  102.661  139.651 1.005\nNA res[61]   -114.949  27.627 -168.618 -134.296 -114.394  -96.450  -60.949 1.001\nNA res[62]    -68.590  27.512 -121.492  -87.723  -68.056  -50.063  -14.649 1.001\nNA res[63]     -5.091  27.435  -57.658  -24.187   -4.767   13.171   48.423 1.001\nNA res[64]    -96.888  27.399 -149.185 -115.932  -96.721  -78.893  -43.557 1.001\nNA res[65]   -145.340  27.401 -197.553 -164.388 -145.424 -127.432  -91.411 1.001\nNA res[66]    -26.577  27.444  -78.399  -45.694  -26.594   -8.455   27.555 1.001\nNA res[67]     53.432  27.525    1.678   34.253   53.095   71.854  107.546 1.001\nNA res[68]     91.059  27.646   39.251   71.830   90.780  109.558  145.538 1.003\nNA res[69]    139.341  27.805   87.022  120.095  138.966  157.961  193.921 1.001\nNA res[70]    192.254  28.001  139.613  173.011  191.889  211.174  247.685 1.001\nNA res[71]   -140.697  26.945 -192.131 -158.462 -140.921 -122.682  -86.498 1.001\nNA res[72]    -53.861  26.803 -104.692  -71.382  -53.867  -36.045   -0.026 1.001\nNA res[73]    -59.969  26.701 -110.888  -77.531  -60.116  -42.419   -5.779 1.001\nNA res[74]    -24.888  26.639  -75.800  -42.339  -25.220   -7.759   28.792 1.001\nNA res[75]    -18.925  26.617  -70.569  -36.419  -19.228   -1.820   34.501 1.001\nNA res[76]     48.388  26.637   -3.281   30.722   48.168   65.796  102.013 1.001\nNA res[77]     45.403  26.697   -6.795   27.656   45.293   62.830   99.324 1.001\nNA res[78]     90.550  26.797   38.062   72.851   90.464  108.076  144.636 1.001\nNA res[79]     85.396  26.937   32.309   67.374   84.992  103.181  139.994 1.001\nNA res[80]     50.353  27.116   -2.874   32.360   50.071   68.275  105.635 1.001\nNA res[81]   -163.315  27.183 -218.512 -180.651 -163.268 -145.149 -111.213 1.001\nNA res[82]   -109.549  27.050 -164.522 -126.836 -109.407  -91.446  -57.185 1.001\nNA res[83]    -13.182  26.957  -68.058  -30.309  -13.185    4.672   38.759 1.001\nNA res[84]    -14.857  26.904  -69.375  -32.207  -15.130    2.965   37.149 1.001\nNA res[85]     40.982  26.891  -12.987   23.736   40.580   58.715   93.178 1.001\nNA res[86]    132.360  26.918   78.198  115.020  131.926  150.358  184.466 1.001\nNA res[87]     96.805  26.986   42.362   79.456   96.215  114.807  149.347 1.001\nNA res[88]     20.474  27.093  -33.942    3.067   19.924   38.470   72.341 1.001\nNA res[89]     17.776  27.240  -36.170    0.154   17.205   35.757   69.869 1.001\nNA res[90]     21.069  27.426  -33.512    3.375   20.566   38.921   73.507 1.001\nNA res[91]   -139.087  27.401 -192.251 -157.857 -139.429 -120.731  -85.751 1.001\nNA res[92]    -36.783  27.249  -89.549  -55.363  -37.012  -18.606   16.371 1.001\nNA res[93]    -52.676  27.136 -105.357  -71.142  -52.862  -34.505    0.149 1.001\nNA res[94]    -26.246  27.062  -78.812  -44.487  -26.694   -8.200   27.216 1.001\nNA res[95]    -30.949  27.028  -83.569  -49.064  -31.432  -12.883   22.822 1.001\nNA res[96]    -18.119  27.035  -70.366  -36.181  -18.630   -0.071   35.799 1.001\nNA res[97]     11.013  27.081  -42.062   -7.308   10.371   29.090   64.960 1.001\nNA res[98]     89.212  27.167   36.723   70.825   88.487  107.474  143.357 1.001\nNA res[99]     95.614  27.293   43.369   77.242   94.962  114.096  150.199 1.001\nNA res[100]   134.498  27.457   80.966  116.052  134.051  153.002  189.417 1.001\nNA res[101]   -76.108  27.204 -130.082  -94.332  -75.847  -57.878  -24.353 1.001\nNA res[102]   -34.250  27.032  -87.681  -52.048  -34.152  -16.151   17.341 1.001\nNA res[103]   -71.035  26.900 -124.029  -88.589  -71.012  -53.026  -19.557 1.001\nNA res[104]    13.787  26.807  -38.887   -3.638   13.685   31.994   65.003 1.001\nNA res[105]   -14.560  26.754  -67.146  -32.253  -14.734    3.512   36.383 1.001\nNA res[106]   -19.293  26.742  -72.025  -36.855  -19.308   -1.263   32.398 1.001\nNA res[107]    56.600  26.770    3.888   38.981   56.413   74.590  108.690 1.001\nNA res[108]   116.002  26.839   62.639   98.370  115.911  133.987  168.211 1.001\nNA res[109]    48.507  26.948   -4.467   30.803   48.406   66.425  100.281 1.001\nNA res[110]     9.543  27.096  -43.482   -8.442    9.395   27.710   61.257 1.001\nNA res[111]  -132.106  27.068 -185.510 -150.360 -132.361 -113.290  -81.768 1.002\nNA res[112]   -91.911  26.941 -144.896 -110.112  -92.368  -73.497  -41.675 1.002\nNA res[113]   -56.418  26.853 -110.121  -74.546  -56.821  -38.144   -6.066 1.002\nNA res[114]   -13.873  26.805  -66.754  -32.098  -14.064    4.250   36.754 1.002\nNA res[115]    33.751  26.798  -19.527   15.438   33.439   51.710   84.504 1.002\nNA res[116]    18.335  26.831  -34.945   -0.042   17.997   36.508   69.231 1.003\nNA res[117]    -9.402  26.904  -62.947  -27.827   -9.801    8.739   41.996 1.003\nNA res[118]    73.620  27.018   20.102   55.043   73.349   91.735  125.589 1.003\nNA res[119]   130.437  27.170   77.165  111.769  130.243  148.448  183.288 1.003\nNA res[120]    67.067  27.362   14.395   48.375   66.985   85.315  120.963 1.004\nNA res[121]   -62.784  27.602 -117.124  -80.547  -63.554  -44.541   -5.745 1.002\nNA res[122]   -79.284  27.432 -132.901  -96.959  -80.152  -61.213  -23.216 1.002\nNA res[123]  -147.038  27.300 -200.594 -164.755 -147.987 -129.337  -91.122 1.002\nNA res[124]   -86.011  27.208 -139.363 -103.583  -87.079  -68.166  -30.561 1.001\nNA res[125]   -44.469  27.155  -96.901  -61.934  -45.519  -26.882   10.835 1.001\nNA res[126]    -7.957  27.143  -60.233  -25.501   -9.007    9.627   47.109 1.001\nNA res[127]    41.116  27.170  -10.769   23.449   40.140   58.470   96.951 1.001\nNA res[128]   117.876  27.237   66.278   99.824  117.103  135.377  173.458 1.003\nNA res[129]   149.628  27.343   98.060  131.402  148.903  167.283  205.474 1.002\nNA res[130]   138.114  27.489   85.817  119.560  137.512  155.791  194.856 1.002\nNA res[131]  -111.510  27.838 -167.229 -130.194 -111.277  -92.014  -56.583 1.002\nNA res[132]   -85.928  27.671 -141.664 -104.693  -85.568  -66.741  -30.867 1.002\nNA res[133]   -39.184  27.543  -95.068  -57.826  -39.100  -20.158   15.452 1.002\nNA res[134]   -65.087  27.454 -120.528  -83.757  -65.031  -46.398  -10.806 1.002\nNA res[135]   -38.854  27.404  -94.235  -57.313  -38.905  -20.233   15.932 1.002\nNA res[136]    43.897  27.394  -11.085   25.502   43.662   62.725   99.335 1.002\nNA res[137]    60.508  27.423    5.661   42.259   60.002   79.149  116.033 1.002\nNA res[138]    48.476  27.491   -6.511   30.204   47.961   67.171  103.838 1.002\nNA res[139]    64.161  27.599    9.521   45.658   63.389   82.927  119.918 1.002\nNA res[140]   150.425  27.745   95.799  131.748  149.642  169.252  206.380 1.003\nNA res[141]   -91.850  27.881 -148.268 -110.602  -91.492  -72.233  -39.029 1.001\nNA res[142]   -17.357  27.708  -73.303  -36.036  -17.162    1.868   35.260 1.001\nNA res[143]    -8.456  27.572  -63.972  -27.298   -8.090   10.649   43.948 1.001\nNA res[144]    16.411  27.476  -38.675   -2.242   16.566   35.463   68.828 1.001\nNA res[145]    -7.945  27.418  -62.620  -26.453   -7.775   11.028   44.009 1.001\nNA res[146]    34.948  27.400  -19.537   16.194   35.119   53.767   87.148 1.001\nNA res[147]    17.864  27.422  -36.519   -0.769   17.959   36.562   70.397 1.001\nNA res[148]   -37.328  27.483  -92.124  -55.967  -37.389  -18.410   15.393 1.001\nNA res[149]     0.133  27.583  -54.930  -18.289    0.029   18.855   53.369 1.001\nNA res[150]   125.181  27.722   69.744  106.772  124.931  144.185  179.294 1.001\nNA res[151]   -45.913  27.112 -101.600  -63.384  -46.504  -27.406    6.233 1.002\nNA res[152]    14.074  26.949  -41.422   -3.190   13.364   32.310   66.645 1.001\nNA res[153]    25.930  26.825  -28.692    8.617   25.258   43.842   78.335 1.001\nNA res[154]    -8.769  26.741  -62.803  -25.915   -9.134    8.810   43.512 1.001\nNA res[155]   -12.026  26.698  -66.536  -29.146  -12.358    5.538   40.695 1.001\nNA res[156]   -13.996  26.695  -68.442  -31.190  -14.374    3.511   39.057 1.001\nNA res[157]   -21.157  26.732  -75.444  -38.415  -21.645   -3.442   32.176 1.001\nNA res[158]    35.944  26.810  -17.938   18.462   35.409   53.573   89.820 1.001\nNA res[159]    -9.265  26.928  -63.044  -26.958   -9.879    8.560   44.932 1.001\nNA res[160]    61.941  27.086    8.272   44.148   61.146   79.841  116.112 1.001\nNA res[161]  -111.720  27.299 -164.219 -130.281 -111.941  -93.352  -57.961 1.002\nNA res[162]  -161.840  27.142 -214.803 -180.270 -162.077 -143.321 -108.335 1.002\nNA res[163]  -105.843  27.025 -158.096 -124.374 -106.136  -87.584  -52.496 1.002\nNA res[164]    32.678  26.947  -18.932   14.279   32.115   50.832   85.368 1.002\nNA res[165]    -1.691  26.909  -53.352  -20.188   -2.310   16.517   51.266 1.002\nNA res[166]    45.107  26.912   -6.211   26.808   44.518   63.279   97.999 1.002\nNA res[167]     5.376  26.954  -46.126  -12.677    4.882   23.392   58.513 1.002\nNA res[168]    72.424  27.037   20.741   54.308   72.204   90.456  125.956 1.002\nNA res[169]    87.205  27.159   35.982   68.965   87.205  105.288  141.275 1.002\nNA res[170]   173.321  27.321  121.880  155.039  173.059  191.695  228.030 1.003\nNA res[171]   -26.750  27.741  -79.583  -45.635  -26.886   -7.432   26.470 1.001\nNA res[172]   -18.152  27.612  -70.442  -37.010  -18.270    0.993   35.150 1.001\nNA res[173]   -30.868  27.522  -82.411  -49.842  -30.897  -11.752   22.359 1.001\nNA res[174]   -20.371  27.471  -72.112  -39.216  -20.399   -1.090   32.975 1.001\nNA res[175]   -29.402  27.460  -81.284  -48.575  -29.553   -9.855   23.536 1.001\nNA res[176]   -42.654  27.488  -94.488  -61.958  -42.932  -23.151   10.118 1.001\nNA res[177]    21.559  27.555  -30.491    2.158   21.213   40.881   74.590 1.001\nNA res[178]    -6.198  27.662  -58.988  -25.815   -6.244   13.036   47.593 1.001\nNA res[179]   104.363  27.807   51.627   84.332  104.497  123.625  158.553 1.001\nNA res[180]    63.487  27.989   10.897   43.415   63.657   82.927  118.256 1.002\nNA res[181]  -116.426  27.477 -170.058 -135.254 -115.973  -98.040  -63.859 1.001\nNA res[182]   -61.964  27.315 -115.028  -80.596  -61.367  -43.741   -8.748 1.001\nNA res[183]   -11.520  27.191  -65.196  -29.915  -10.904    6.736   41.954 1.001\nNA res[184]   -11.403  27.107  -64.157  -29.928  -10.938    7.033   42.438 1.001\nNA res[185]    11.940  27.063  -40.738   -6.618   12.599   30.143   65.828 1.001\nNA res[186]    78.890  27.059   26.180   60.481   79.578   97.004  132.471 1.001\nNA res[187]    15.749  27.094  -36.961   -2.786   16.169   33.923   68.912 1.001\nNA res[188]     3.160  27.170  -49.311  -15.573    3.325   21.357   56.992 1.001\nNA res[189]    52.382  27.285   -0.331   33.687   52.699   70.616  106.503 1.001\nNA res[190]    57.688  27.439    4.937   38.848   57.982   75.882  112.335 1.001\nNA res[191]   -39.277  27.823  -93.946  -58.037  -39.479  -19.984   14.965 1.001\nNA res[192]     9.409  27.662  -44.962   -9.063    9.312   28.464   63.569 1.001\nNA res[193]    22.489  27.540  -32.351    3.890   22.479   41.387   76.748 1.001\nNA res[194]   -41.856  27.457  -96.305  -60.387  -41.784  -23.180   12.866 1.001\nNA res[195]   -12.452  27.413  -66.510  -31.155  -12.405    6.113   42.359 1.001\nNA res[196]    -1.869  27.408  -55.985  -20.561   -1.913   16.685   53.271 1.001\nNA res[197]    21.489  27.443  -32.127    2.711   21.389   39.871   76.814 1.001\nNA res[198]    -9.600  27.518  -62.931  -28.632   -9.617    8.338   45.590 1.002\nNA res[199]    53.727  27.631   -0.059   34.427   53.676   71.736  108.974 1.002\nNA res[200]    31.695  27.783  -22.231   12.143   31.743   49.979   87.523 1.002\nNA res[201]   -86.890  27.181 -140.177 -104.770  -87.118  -68.603  -35.106 1.001\nNA res[202]   -30.563  27.031  -83.642  -48.350  -31.011  -12.345   20.910 1.002\nNA res[203]   -39.562  26.920  -92.523  -57.243  -39.987  -21.390   11.887 1.002\nNA res[204]    25.751  26.850  -27.313    8.074   25.467   43.881   77.877 1.002\nNA res[205]   -12.787  26.819  -65.884  -30.405  -13.246    5.237   39.562 1.002\nNA res[206]  -114.094  26.829 -166.864 -131.854 -114.195  -96.423  -61.166 1.002\nNA res[207]   -27.215  26.880  -80.040  -44.971  -27.431   -9.544   26.079 1.002\nNA res[208]    65.253  26.970   12.034   47.409   65.333   82.874  119.333 1.002\nNA res[209]    87.094  27.100   33.577   69.194   87.081  104.945  141.467 1.003\nNA res[210]   148.253  27.269   94.421  130.305  148.113  166.284  203.466 1.002\nNA res[211]  -161.236  27.503 -215.889 -178.736 -161.122 -142.886 -108.375 1.001\nNA res[212]  -129.522  27.348 -183.477 -146.819 -129.294 -111.282  -76.871 1.001\nNA res[213]  -190.682  27.231 -243.930 -208.062 -190.524 -172.562 -138.450 1.001\nNA res[214]   -79.289  27.154 -132.322  -96.647  -79.209  -61.143  -27.250 1.001\nNA res[215]    47.949  27.116   -4.985   30.474   48.166   66.107  100.695 1.001\nNA res[216]   114.631  27.119   61.912   96.825  114.839  132.864  167.496 1.001\nNA res[217]   117.095  27.161   63.624   99.215  117.216  135.276  169.783 1.001\nNA res[218]   122.954  27.243   68.976  105.183  123.040  140.999  175.833 1.001\nNA res[219]    57.691  27.364    2.985   40.012   57.620   75.929  110.855 1.001\nNA res[220]   124.990  27.525   70.453  107.235  124.767  143.105  178.606 1.001\nNA res[221]   -28.457  27.809  -85.147  -46.709  -28.877   -8.749   24.203 1.001\nNA res[222]   -40.195  27.642  -96.634  -58.154  -40.465  -20.895   12.714 1.001\nNA res[223]   -83.163  27.513 -139.662 -101.235  -83.213  -63.910  -30.005 1.001\nNA res[224]   -57.737  27.424 -113.878  -75.793  -57.651  -38.683   -5.179 1.001\nNA res[225]   -60.675  27.373 -116.422  -78.560  -60.376  -41.718   -8.273 1.001\nNA res[226]   -36.237  27.363  -91.706  -54.143  -35.975  -17.495   16.397 1.001\nNA res[227]   -53.528  27.391 -108.591  -71.787  -53.313  -34.530   -0.302 1.001\nNA res[228]    82.085  27.460   27.176   63.546   82.505  100.972  135.212 1.001\nNA res[229]   109.582  27.567   54.642   90.994  110.106  128.598  162.713 1.001\nNA res[230]   190.314  27.713  135.634  171.529  190.765  209.465  244.223 1.001\nNA res[231]  -120.252  27.694 -175.928 -138.875 -119.474 -101.313  -68.003 1.001\nNA res[232]  -100.688  27.530 -155.801 -119.226  -99.978  -81.979  -48.238 1.001\nNA res[233]   -91.964  27.405 -146.450 -110.666  -91.280  -73.148  -39.619 1.001\nNA res[234]  -108.171  27.319 -162.489 -126.710 -107.330  -89.436  -55.884 1.001\nNA res[235]    21.527  27.273  -32.755    3.074   22.292   40.459   73.543 1.001\nNA res[236]   -18.622  27.266  -73.068  -36.988  -18.139    0.348   33.406 1.001\nNA res[237]    40.129  27.299  -14.569   21.865   40.523   59.257   92.027 1.001\nNA res[238]   117.718  27.372   62.996   99.591  118.175  136.830  169.568 1.001\nNA res[239]   150.564  27.484   95.949  132.308  150.955  169.833  203.577 1.001\nNA res[240]   128.355  27.634   73.287  109.633  128.714  147.821  181.492 1.001\nNA res[241]   -43.717  26.592  -96.442  -61.945  -43.778  -25.366    7.706 1.002\nNA res[242]   -16.266  26.421  -68.915  -34.345  -16.394    1.657   34.738 1.002\nNA res[243]   -51.824  26.290 -104.250  -69.666  -51.904  -34.191   -0.773 1.002\nNA res[244]   -92.310  26.200 -144.512 -109.770  -92.356  -74.742  -41.007 1.002\nNA res[245]    53.225  26.151    1.165   35.773   53.155   70.545  104.484 1.002\nNA res[246]    35.579  26.143  -16.717   18.099   35.633   52.862   86.869 1.001\nNA res[247]   -33.204  26.177  -85.575  -50.833  -33.098  -15.939   18.146 1.001\nNA res[248]    35.397  26.252  -16.792   17.809   35.174   52.691   87.176 1.001\nNA res[249]    83.314  26.368   30.491   65.761   83.032  100.631  135.542 1.001\nNA res[250]    42.630  26.524   -9.672   24.976   42.309   59.916   95.055 1.001\nNA res[251]  -206.923  27.800 -260.529 -225.880 -206.782 -187.968 -153.986 1.001\nNA res[252]  -157.656  27.629 -211.249 -176.383 -157.567 -138.906 -104.887 1.001\nNA res[253]   -51.271  27.496 -104.867  -70.030  -51.271  -32.412    1.659 1.001\nNA res[254]    12.860  27.402  -40.341   -5.856   13.103   31.625   65.496 1.001\nNA res[255]    11.898  27.348  -41.371   -7.218   11.887   30.504   64.701 1.001\nNA res[256]    -1.747  27.333  -54.813  -20.847   -1.807   16.587   50.997 1.001\nNA res[257]    46.786  27.357   -6.081   27.648   46.704   65.069  100.383 1.001\nNA res[258]    84.114  27.422   31.253   65.110   83.938  102.405  137.813 1.001\nNA res[259]   141.780  27.525   88.996  122.717  141.448  159.933  195.838 1.001\nNA res[260]   140.825  27.667   88.214  121.349  140.766  158.831  195.301 1.001\nNA res[261]   -45.758  27.262  -98.677  -63.456  -45.803  -28.120    8.017 1.002\nNA res[262]   -73.536  27.103 -126.119  -91.086  -73.677  -55.880  -19.998 1.001\nNA res[263]   -66.558  26.984 -118.944  -84.155  -66.732  -49.089  -12.830 1.001\nNA res[264]    -3.221  26.904  -55.491  -20.926   -3.268   14.217   50.110 1.001\nNA res[265]    64.093  26.864   12.275   46.220   64.035   81.487  117.920 1.001\nNA res[266]   -24.292  26.865  -75.837  -42.281  -24.457   -6.832   29.548 1.001\nNA res[267]    11.470  26.906  -40.398   -6.459   11.267   29.011   65.223 1.001\nNA res[268]    69.347  26.987   17.413   51.396   68.896   87.025  123.112 1.001\nNA res[269]    16.854  27.108  -35.179   -1.244   16.533   34.478   70.967 1.001\nNA res[270]    75.619  27.268   23.067   57.629   75.329   93.227  129.655 1.001\nNA res[271]  -225.643  27.480 -278.886 -243.823 -225.694 -207.386 -171.764 1.003\nNA res[272]  -140.428  27.320 -193.206 -158.567 -140.609 -122.309  -86.874 1.003\nNA res[273]   -92.158  27.198 -145.009 -110.346  -92.252  -74.095  -38.573 1.003\nNA res[274]   -37.826  27.116  -90.730  -55.873  -38.139  -19.735   15.494 1.002\nNA res[275]   -22.591  27.074  -75.243  -40.456  -22.982   -4.368   30.621 1.002\nNA res[276]    30.930  27.072  -22.094   13.172   30.419   49.127   84.548 1.002\nNA res[277]   103.777  27.109   50.599   85.820  102.988  121.948  157.742 1.002\nNA res[278]   195.528  27.187  142.165  177.513  194.832  213.701  249.967 1.002\nNA res[279]   173.383  27.304  119.414  155.128  172.627  191.796  228.591 1.001\nNA res[280]    48.822  27.460   -5.443   30.590   47.952   67.271  104.943 1.001\nNA res[281]  -100.761  27.924 -155.664 -119.211 -100.098  -81.104  -47.734 1.001\nNA res[282]   -73.020  27.777 -127.369  -91.266  -72.520  -53.694  -20.563 1.001\nNA res[283]   -64.686  27.669 -119.132  -82.823  -64.075  -45.417  -12.494 1.001\nNA res[284]   -17.650  27.599  -72.206  -35.666  -17.169    1.377   35.020 1.001\nNA res[285]    46.285  27.569   -7.652   28.252   46.815   65.328   99.260 1.001\nNA res[286]    25.187  27.577  -28.572    7.101   25.553   44.083   78.569 1.001\nNA res[287]    23.689  27.625  -29.512    5.427   23.865   42.895   77.192 1.001\nNA res[288]    61.274  27.713    8.220   42.691   61.404   80.495  114.966 1.001\nNA res[289]    48.126  27.838   -5.127   29.210   48.155   67.369  102.003 1.001\nNA res[290]    73.868  28.002   20.444   54.993   73.670   93.109  128.079 1.001\nNA res[291]   -62.043  27.792 -115.822  -80.494  -62.779  -43.673   -4.435 1.001\nNA res[292]   -78.060  27.657 -131.681  -96.527  -78.957  -60.027  -21.083 1.001\nNA res[293]   -97.080  27.560 -150.130 -115.697  -97.882  -79.080  -40.450 1.001\nNA res[294]   -84.403  27.502 -137.305 -103.193  -85.127  -66.419  -27.873 1.001\nNA res[295]   -62.058  27.484 -115.409  -80.974  -62.634  -43.885   -5.395 1.001\nNA res[296]    13.785  27.505  -39.891   -5.375   13.338   32.209   69.885 1.001\nNA res[297]   108.636  27.565   55.243   89.343  108.404  127.130  164.689 1.001\nNA res[298]   103.229  27.665   49.562   83.789  102.942  122.033  159.137 1.001\nNA res[299]    89.617  27.803   35.620   70.171   89.485  108.312  146.507 1.001\nNA res[300]    80.672  27.979   26.414   60.959   80.808   99.562  137.744 1.001\nNA res[301]  -141.883  28.305 -196.509 -160.420 -141.920 -123.442  -84.721 1.001\nNA res[302]  -106.210  28.157 -160.120 -124.453 -106.309  -87.823  -49.394 1.001\nNA res[303]   -37.352  28.047  -91.475  -55.335  -37.285  -19.229   19.895 1.001\nNA res[304]   -18.327  27.975  -72.662  -36.286  -18.237   -0.397   38.713 1.001\nNA res[305]   -86.778  27.942 -140.890 -105.114  -86.791  -68.788  -29.209 1.001\nNA res[306]   -37.999  27.947  -91.849  -56.327  -37.867  -20.131   19.461 1.001\nNA res[307]    69.141  27.991   14.641   50.665   69.178   86.859  126.516 1.001\nNA res[308]   159.261  28.074  104.599  140.704  159.387  177.180  216.359 1.001\nNA res[309]   136.516  28.195   80.873  118.244  136.554  154.819  193.892 1.001\nNA res[310]    84.565  28.354   28.496   66.131   84.693  102.951  142.719 1.002\nNA res[311]     8.480  27.336  -47.030   -9.792    8.978   27.073   59.498 1.003\nNA res[312]    11.977  27.128  -43.062   -5.921   12.427   30.119   62.594 1.003\nNA res[313]    22.210  26.959  -32.858    4.243   22.751   40.219   72.580 1.003\nNA res[314]    60.484  26.829    5.199   42.716   60.942   78.598  110.330 1.002\nNA res[315]    -6.440  26.739  -61.507  -24.246   -6.091   11.508   43.556 1.002\nNA res[316]   -11.502  26.689  -66.443  -29.364  -11.033    6.418   38.397 1.002\nNA res[317]   -51.977  26.680 -107.165  -69.981  -51.390  -33.782   -1.318 1.002\nNA res[318]   -37.802  26.711  -93.392  -55.747  -37.309  -19.574   12.624 1.002\nNA res[319]    18.322  26.783  -37.480    0.309   19.012   36.680   68.620 1.001\nNA res[320]    -6.770  26.895  -62.754  -24.603   -5.931   11.437   44.809 1.001\nNA res[321]  -169.878  27.641 -225.412 -188.557 -170.087 -150.812 -115.846 1.003\nNA res[322]  -115.245  27.475 -170.155 -133.481 -115.610  -96.509  -61.338 1.002\nNA res[323]  -127.030  27.348 -181.991 -145.028 -127.405 -108.460  -73.235 1.002\nNA res[324]   -75.745  27.260 -130.493  -94.150  -76.056  -57.347  -21.877 1.002\nNA res[325]    -4.442  27.211  -59.066  -22.893   -4.731   14.091   49.373 1.002\nNA res[326]    72.438  27.202   17.923   54.115   72.083   90.962  125.624 1.002\nNA res[327]    74.431  27.233   20.320   56.096   74.054   93.130  128.264 1.001\nNA res[328]    86.173  27.303   32.254   68.056   86.103  104.893  140.473 1.001\nNA res[329]   100.100  27.413   46.110   82.027  100.161  118.754  154.311 1.001\nNA res[330]   191.836  27.562  137.087  173.612  191.900  210.527  246.650 1.001\nNA res[331]    -3.568  27.677  -58.111  -22.218   -3.240   14.872   51.330 1.001\nNA res[332]    35.490  27.505  -18.504   16.700   35.467   53.896   89.971 1.001\nNA res[333]    14.836  27.372  -38.705   -3.945   14.883   33.326   69.479 1.001\nNA res[334]    35.529  27.278  -17.795   16.742   35.624   53.845   89.880 1.001\nNA res[335]    57.772  27.224    4.745   39.406   57.712   76.032  112.258 1.001\nNA res[336]     7.505  27.209  -45.201  -11.277    7.241   25.550   61.787 1.001\nNA res[337]   -62.853  27.234 -115.231  -81.656  -63.102  -44.830   -8.470 1.001\nNA res[338]     3.554  27.299  -48.771  -15.196    3.261   21.590   57.887 1.001\nNA res[339]   -47.691  27.403 -100.181  -66.462  -48.175  -29.624    6.870 1.001\nNA res[340]    -7.951  27.546  -60.486  -26.990   -8.417   10.133   47.240 1.001\nNA res[341]  -167.838  27.534 -221.260 -186.767 -167.997 -149.518 -113.530 1.005\nNA res[342]  -109.245  27.409 -162.241 -128.030 -109.426  -91.138  -55.319 1.005\nNA res[343]   -76.905  27.323 -129.335  -95.553  -76.780  -58.727  -23.086 1.004\nNA res[344]   -92.961  27.276 -144.845 -111.585  -92.688  -74.742  -39.601 1.004\nNA res[345]   -15.658  27.269  -67.414  -34.298  -15.410    2.716   37.316 1.004\nNA res[346]    87.984  27.302   36.319   69.290   88.171  106.278  140.902 1.003\nNA res[347]   132.212  27.374   80.204  113.503  132.167  150.380  185.832 1.003\nNA res[348]    30.540  27.486  -21.770   11.869   30.461   48.883   84.590 1.003\nNA res[349]   120.550  27.636   67.565  101.960  120.450  138.866  174.815 1.002\nNA res[350]   123.963  27.825   70.604  105.055  123.691  142.633  178.437 1.002\nNA sigma       86.053   4.095   78.239   83.300   85.994   88.737   94.268 1.013\nNA sigma.B    317.817  38.958  252.469  291.832  313.873  339.888  405.125 1.002\nNA deviance  4111.820  21.476 4069.687 4097.266 4111.765 4126.251 4152.352 1.008\nNA           n.eff\nNA beta[1]    1400\nNA beta[2]     230\nNA gamma[1]   1000\nNA gamma[2]   3000\nNA gamma[3]   1300\nNA gamma[4]   2800\nNA gamma[5]   3000\nNA gamma[6]   3000\nNA gamma[7]   1800\nNA gamma[8]   3000\nNA gamma[9]   1800\nNA gamma[10]  2900\nNA gamma[11]  3000\nNA gamma[12]  1800\nNA gamma[13]  1900\nNA gamma[14]  3000\nNA gamma[15]  3000\nNA gamma[16]  1300\nNA gamma[17]  1100\nNA gamma[18]  3000\nNA gamma[19]  3000\nNA gamma[20]  3000\nNA gamma[21]  3000\nNA gamma[22]  3000\nNA gamma[23]  1800\nNA gamma[24]  3000\nNA gamma[25]   810\nNA gamma[26]  3000\nNA gamma[27]  1500\nNA gamma[28]   570\nNA gamma[29]  3000\nNA gamma[30]  3000\nNA gamma[31]  3000\nNA gamma[32]   660\nNA gamma[33]   730\nNA gamma[34]  3000\nNA gamma[35]   340\nNA res[1]     1100\nNA res[2]     1200\nNA res[3]     1400\nNA res[4]     1700\nNA res[5]     2100\nNA res[6]     2700\nNA res[7]     3000\nNA res[8]     3000\nNA res[9]     3000\nNA res[10]    3000\nNA res[11]    3000\nNA res[12]    3000\nNA res[13]    3000\nNA res[14]    3000\nNA res[15]    3000\nNA res[16]    3000\nNA res[17]    3000\nNA res[18]    3000\nNA res[19]    3000\nNA res[20]    3000\nNA res[21]    1200\nNA res[22]    1400\nNA res[23]    1700\nNA res[24]    2000\nNA res[25]    2500\nNA res[26]    3000\nNA res[27]    3000\nNA res[28]    3000\nNA res[29]    3000\nNA res[30]    3000\nNA res[31]    3000\nNA res[32]    3000\nNA res[33]    3000\nNA res[34]    3000\nNA res[35]    3000\nNA res[36]    3000\nNA res[37]    3000\nNA res[38]    3000\nNA res[39]    3000\nNA res[40]    3000\nNA res[41]    3000\nNA res[42]    3000\nNA res[43]    3000\nNA res[44]    3000\nNA res[45]    3000\nNA res[46]    3000\nNA res[47]    3000\nNA res[48]    3000\nNA res[49]    3000\nNA res[50]    3000\nNA res[51]    3000\nNA res[52]    2800\nNA res[53]    2200\nNA res[54]    1700\nNA res[55]    1400\nNA res[56]    1200\nNA res[57]    1000\nNA res[58]     710\nNA res[59]     630\nNA res[60]     550\nNA res[61]    2000\nNA res[62]    2400\nNA res[63]    3000\nNA res[64]    3000\nNA res[65]    3000\nNA res[66]    3000\nNA res[67]    3000\nNA res[68]    3000\nNA res[69]    3000\nNA res[70]    3000\nNA res[71]    3000\nNA res[72]    3000\nNA res[73]    3000\nNA res[74]    3000\nNA res[75]    3000\nNA res[76]    3000\nNA res[77]    3000\nNA res[78]    3000\nNA res[79]    3000\nNA res[80]    3000\nNA res[81]    2100\nNA res[82]    2600\nNA res[83]    3000\nNA res[84]    3000\nNA res[85]    3000\nNA res[86]    3000\nNA res[87]    3000\nNA res[88]    3000\nNA res[89]    3000\nNA res[90]    3000\nNA res[91]    3000\nNA res[92]    3000\nNA res[93]    3000\nNA res[94]    3000\nNA res[95]    3000\nNA res[96]    3000\nNA res[97]    3000\nNA res[98]    3000\nNA res[99]    3000\nNA res[100]   3000\nNA res[101]   3000\nNA res[102]   3000\nNA res[103]   3000\nNA res[104]   3000\nNA res[105]   3000\nNA res[106]   3000\nNA res[107]   3000\nNA res[108]   3000\nNA res[109]   3000\nNA res[110]   3000\nNA res[111]   1700\nNA res[112]   1400\nNA res[113]   1100\nNA res[114]    980\nNA res[115]    840\nNA res[116]    740\nNA res[117]    650\nNA res[118]    580\nNA res[119]    600\nNA res[120]    480\nNA res[121]   1600\nNA res[122]   2000\nNA res[123]   2400\nNA res[124]   3000\nNA res[125]   3000\nNA res[126]   3000\nNA res[127]   3000\nNA res[128]   3000\nNA res[129]   3000\nNA res[130]   3000\nNA res[131]   3000\nNA res[132]   3000\nNA res[133]   3000\nNA res[134]   3000\nNA res[135]   3000\nNA res[136]   3000\nNA res[137]   3000\nNA res[138]   3000\nNA res[139]   2700\nNA res[140]   3000\nNA res[141]   3000\nNA res[142]   3000\nNA res[143]   3000\nNA res[144]   3000\nNA res[145]   3000\nNA res[146]   3000\nNA res[147]   3000\nNA res[148]   3000\nNA res[149]   3000\nNA res[150]   3000\nNA res[151]   1600\nNA res[152]   1900\nNA res[153]   2300\nNA res[154]   3000\nNA res[155]   3000\nNA res[156]   3000\nNA res[157]   3000\nNA res[158]   3000\nNA res[159]   3000\nNA res[160]   3000\nNA res[161]   1000\nNA res[162]   1200\nNA res[163]   1400\nNA res[164]   1700\nNA res[165]   2100\nNA res[166]   2600\nNA res[167]   3000\nNA res[168]   3000\nNA res[169]   3000\nNA res[170]   3000\nNA res[171]   3000\nNA res[172]   3000\nNA res[173]   3000\nNA res[174]   3000\nNA res[175]   3000\nNA res[176]   3000\nNA res[177]   2700\nNA res[178]   2200\nNA res[179]   2000\nNA res[180]   1500\nNA res[181]   3000\nNA res[182]   3000\nNA res[183]   3000\nNA res[184]   3000\nNA res[185]   3000\nNA res[186]   3000\nNA res[187]   3000\nNA res[188]   3000\nNA res[189]   3000\nNA res[190]   3000\nNA res[191]   3000\nNA res[192]   3000\nNA res[193]   3000\nNA res[194]   3000\nNA res[195]   3000\nNA res[196]   2500\nNA res[197]   2000\nNA res[198]   1700\nNA res[199]   1400\nNA res[200]   1200\nNA res[201]   3000\nNA res[202]   3000\nNA res[203]   2700\nNA res[204]   2100\nNA res[205]   1700\nNA res[206]   1400\nNA res[207]   1200\nNA res[208]   1000\nNA res[209]    890\nNA res[210]   1100\nNA res[211]   3000\nNA res[212]   3000\nNA res[213]   3000\nNA res[214]   3000\nNA res[215]   3000\nNA res[216]   3000\nNA res[217]   3000\nNA res[218]   3000\nNA res[219]   3000\nNA res[220]   3000\nNA res[221]   1800\nNA res[222]   2200\nNA res[223]   2800\nNA res[224]   3000\nNA res[225]   3000\nNA res[226]   3000\nNA res[227]   3000\nNA res[228]   3000\nNA res[229]   3000\nNA res[230]   3000\nNA res[231]   3000\nNA res[232]   3000\nNA res[233]   3000\nNA res[234]   3000\nNA res[235]   3000\nNA res[236]   3000\nNA res[237]   3000\nNA res[238]   3000\nNA res[239]   3000\nNA res[240]   3000\nNA res[241]    900\nNA res[242]   1000\nNA res[243]   1200\nNA res[244]   1400\nNA res[245]   1800\nNA res[246]   2200\nNA res[247]   2800\nNA res[248]   3000\nNA res[249]   3000\nNA res[250]   3000\nNA res[251]   3000\nNA res[252]   3000\nNA res[253]   3000\nNA res[254]   3000\nNA res[255]   3000\nNA res[256]   3000\nNA res[257]   3000\nNA res[258]   3000\nNA res[259]   2600\nNA res[260]   2100\nNA res[261]   1600\nNA res[262]   1900\nNA res[263]   2400\nNA res[264]   3000\nNA res[265]   3000\nNA res[266]   3000\nNA res[267]   3000\nNA res[268]   3000\nNA res[269]   3000\nNA res[270]   3000\nNA res[271]    590\nNA res[272]    650\nNA res[273]    730\nNA res[274]    830\nNA res[275]    960\nNA res[276]   1100\nNA res[277]   1200\nNA res[278]   1500\nNA res[279]   1800\nNA res[280]   2500\nNA res[281]   3000\nNA res[282]   3000\nNA res[283]   3000\nNA res[284]   3000\nNA res[285]   3000\nNA res[286]   3000\nNA res[287]   3000\nNA res[288]   3000\nNA res[289]   3000\nNA res[290]   2800\nNA res[291]   3000\nNA res[292]   3000\nNA res[293]   3000\nNA res[294]   3000\nNA res[295]   3000\nNA res[296]   3000\nNA res[297]   3000\nNA res[298]   3000\nNA res[299]   3000\nNA res[300]   3000\nNA res[301]   3000\nNA res[302]   3000\nNA res[303]   3000\nNA res[304]   3000\nNA res[305]   3000\nNA res[306]   3000\nNA res[307]   3000\nNA res[308]   2700\nNA res[309]   2300\nNA res[310]   1700\nNA res[311]    610\nNA res[312]    670\nNA res[313]    760\nNA res[314]    860\nNA res[315]    990\nNA res[316]   1200\nNA res[317]   1400\nNA res[318]   1700\nNA res[319]   2100\nNA res[320]   2600\nNA res[321]    770\nNA res[322]    870\nNA res[323]   1000\nNA res[324]   1200\nNA res[325]   1400\nNA res[326]   1600\nNA res[327]   2000\nNA res[328]   2600\nNA res[329]   2700\nNA res[330]   3000\nNA res[331]   3000\nNA res[332]   3000\nNA res[333]   3000\nNA res[334]   3000\nNA res[335]   3000\nNA res[336]   3000\nNA res[337]   3000\nNA res[338]   3000\nNA res[339]   3000\nNA res[340]   3000\nNA res[341]    350\nNA res[342]    380\nNA res[343]    420\nNA res[344]    460\nNA res[345]    510\nNA res[346]    570\nNA res[347]    810\nNA res[348]    740\nNA res[349]   1200\nNA res[350]   1500\nNA sigma       130\nNA sigma.B    3000\nNA deviance    200\nNA \nNA For each parameter, n.eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor (at convergence, Rhat=1).\nNA \nNA DIC info (using the rule, pD = var(deviance)/2)\nNA pD = 229.5 and DIC = 4341.4\nNA DIC is an estimate of expected predictive error (lower deviance is better).\n\n\nGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\nmodelString3=\"\nmodel {\n   #Likelihood\n   y[1]~dnorm(mu[1],tau)\n   mu[1] &lt;- eta1[1]\n   eta1[1] ~ dnorm(eta[1], taueps)\n   eta[1] &lt;- inprod(beta[],X[1,]) + gamma[Block[1]]\n   res[1] &lt;- y[1]-mu[1]\n   for (i in 2:n) {\n      y[i]~dnorm(mu[i],tau)\n      mu[i] &lt;- eta1[i]\n      eta1[i] ~ dnorm(temp[i], taueps)\n      temp[i] &lt;- eta[i] + -rho*(mu[i-1]-y[i-1])\n      eta[i] &lt;- inprod(beta[],X[i,]) + gamma[Block[i]]\n      res[i] &lt;- y[i]-mu[i]\n   } \n   beta ~ dmnorm(a0,A0)\n   for (i in 1:nBlock) {\n     gamma[i] ~ dnorm(0, tau.B) #prior\n   }\n   rho ~ dunif(-1,1)\n   tau &lt;- pow(sigma,-2)\n   sigma &lt;- z/sqrt(chSq) \n   z ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq ~ dgamma(0.5, 0.5)\n   taueps &lt;- pow(sigma.eps,-2)\n   sigma.eps &lt;- z/sqrt(chSq.eps) \n   z.eps ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.eps ~ dgamma(0.5, 0.5)\n   tau.B &lt;- pow(sigma.B,-2)\n   sigma.B &lt;- z/sqrt(chSq.B) \n   z.B ~ dnorm(0, 0.0016)I(0,)  #1/25^2 = 0.0016\n   chSq.B ~ dgamma(0.5, 0.5)\n   sd.y &lt;- sd(res)\n   sd.block &lt;- sd(gamma)\n }\n\"\n\n## write the model to a text file\nwriteLines(modelString3, con = \"matrixModel3.txt\")\n\nXmat &lt;- model.matrix(~Time,data.rm)\ndata.rm.list &lt;- with(data.rm,\n        list(y=y,\n                 Block=as.numeric(Block),\n         X=Xmat,\n         n=nrow(data.rm),\n         nBlock=length(levels(Block)),\n         a0=rep(0,ncol(Xmat)), A0=diag(ncol(Xmat))\n         )\n)\n\nparams &lt;- c(\"beta\",'gamma',\"sigma\",\"sigma.B\",\"res\",'sigma.eps','rho','sd.y','sd.block')\nadaptSteps = 1000\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.r2jags.mt &lt;- jags(data = data.rm.list, inits = NULL, parameters.to.save = params,\n    model.file = \"matrixModel3.txt\", n.chains = nChains, n.iter = nIter,\n    n.burnin = burnInSteps, n.thin = thinSteps)\n\nNA Compiling model graph\nNA    Resolving undeclared variables\nNA    Allocating nodes\nNA Graph information:\nNA    Observed stochastic nodes: 350\nNA    Unobserved stochastic nodes: 393\nNA    Total graph size: 3931\nNA \nNA Initializing model\n\ndata.rm.mt.mcmc &lt;- data.rm.r2jags.mt$BUGSoutput$sims.matrix\nsummary(as.mcmc(data.rm.mt.mcmc[,grep('beta|sigma|rho',colnames(data.rm.mt.mcmc))]))\n\nNA \nNA Iterations = 1:3000\nNA Thinning interval = 1 \nNA Number of chains = 1 \nNA Sample size per chain = 3000 \nNA \nNA 1. Empirical mean and standard deviation for each variable,\nNA    plus standard error of the mean:\nNA \nNA               Mean      SD Naive SE Time-series SE\nNA beta[1]     0.1093  1.0090 0.018422       0.018422\nNA beta[2]     9.1917  1.0421 0.019026       0.019026\nNA rho         0.6991  0.1635 0.002985       0.002985\nNA sigma      63.1901  7.6813 0.140241       0.140241\nNA sigma.B   314.5779 38.4731 0.702419       0.702419\nNA sigma.eps  32.8263  9.4551 0.172625       0.172625\nNA \nNA 2. Quantiles for each variable:\nNA \nNA               2.5%      25%       50%      75%    97.5%\nNA beta[1]    -1.8402  -0.5698   0.09792   0.7920   2.0576\nNA beta[2]     7.2315   8.4966   9.19938   9.8961  11.2232\nNA rho         0.4549   0.5508   0.68313   0.8453   0.9746\nNA sigma      49.8811  56.9814  62.70134  69.5262  77.4328\nNA sigma.B   251.2601 286.7395 310.40901 338.2187 398.3167\nNA sigma.eps  14.8598  25.5907  35.65555  40.2488  46.3558\n\n#head(data.rm.r2jags.mt$BUGSoutput$sims.list[[c('beta','rho','sigma')]]) \n#print(data.rm.r2jags.mt)\ndata.rm.mcmc.list.mt &lt;- as.mcmc(data.rm.r2jags.mt)\nData.Rm.mcmc.list.mt &lt;- data.rm.mcmc.list.mt\n\n# R2 calculations\nXmat &lt;- model.matrix(~Time, data.rm)\ncoefs &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['beta']]\nfitted &lt;- coefs %*% t(Xmat)\nX.var &lt;- aaply(fitted,1,function(x){var(x)})\nX.var[1:10]\n\nNA        1        2        3        4        5        6        7        8 \nNA 814.3418 591.0181 634.1438 685.5362 900.1883 740.2397 864.5962 435.7952 \nNA        9       10 \nNA 672.4743 584.2064\n\nZ.var &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['sd.block']]^2\nR.var &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['sd.y']]^2\nR2.marginal &lt;- (X.var)/(X.var+Z.var+R.var)\nR2.marginal &lt;- data.frame(Mean=mean(R2.marginal), Median=median(R2.marginal), HPDinterval(as.mcmc(R2.marginal)))\nR2.conditional &lt;- (X.var+Z.var)/(X.var+Z.var+R.var)\nR2.conditional &lt;- data.frame(Mean=mean(R2.conditional),\n   Median=median(R2.conditional), HPDinterval(as.mcmc(R2.conditional)))\nR2.block &lt;- (Z.var)/(X.var+Z.var+R.var)\nR2.block &lt;- data.frame(Mean=mean(R2.block), Median=median(R2.block), HPDinterval(as.mcmc(R2.block)))\nR2.res&lt;-(R.var)/(X.var+Z.var+R.var)\nR2.res &lt;- data.frame(Mean=mean(R2.res), Median=median(R2.res), HPDinterval(as.mcmc(R2.res)))\n\n(r2 &lt;- rbind(R2.block=R2.block, R2.marginal=R2.marginal, R2.res=R2.res, R2.conditional=R2.conditional))\n\nNA                      Mean     Median      lower     upper\nNA R2.block       0.52595295 0.52197768 0.40289527 0.6376026\nNA R2.marginal    0.07190426 0.06983026 0.03887103 0.1087763\nNA R2.res         0.40214279 0.40351594 0.28261806 0.5200679\nNA R2.conditional 0.59785721 0.59648406 0.47993214 0.7173819\n\n\nIt would appear that the incorporation of a first order autocorrelation structure is indeed appropriate. The degree of correlation between successive points is \\(0.733\\). Let’s have a look at a summary figure.\n\ncoefs &lt;- data.rm.r2jags.mt$BUGSoutput$sims.list[['beta']]\nnewdata &lt;- with(data.rm, data.frame(Time=seq(min(Time, na.rm=TRUE), max(Time, na.rm=TRUE), len=100)))\nXmat &lt;- model.matrix(~Time, newdata)\npred &lt;- (coefs %*% t(Xmat))\npred &lt;- adply(pred, 2, function(x) {\n   data.frame(Mean=mean(x), Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n})\nnewdata &lt;- cbind(newdata, pred)\n#Also calculate the partial observations\nXmat &lt;- model.matrix(~Time, data.rm)\npred &lt;- colMeans(as.vector(coefs %*% t(Xmat))+data.rm.r2jags.mt$BUGSoutput$sims.list[['res']])\npart.obs &lt;- cbind(data.rm,Median=pred)\n\nggplot(newdata, aes(y=Median, x=Time)) +\n  geom_point(data=part.obs, aes(y=Median))+\n  geom_ribbon(aes(ymin=lower, ymax=upper), fill='blue',alpha=0.2) +\n  geom_line()+\n  scale_x_continuous('Time') +\n  scale_y_continuous('Y') +\n  theme_classic() +\n  theme(axis.title.y = element_text(vjust=2, size=rel(1.2)),\n        axis.title.x = element_text(vjust=-2, size=rel(1.2)),\n        plot.margin=unit(c(0.5,0.5,2,2), 'lines'))"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "",
    "text": "This tutorial will focus on the use of Bayesian estimation to fit simple linear regression models. BUGS (Bayesian inference Using Gibbs Sampling) is an algorithm and supporting language (resembling R) dedicated to performing the Gibbs sampling implementation of Markov Chain Monte Carlo (MCMC) method. Dialects of the BUGS language are implemented within three main projects:\nWhilst the above programs can be used stand-alone, they do offer the rich data pre-processing and graphical capabilities of R, and thus, they are best accessed from within R itself. As such there are multiple packages devoted to interfacing with the various software implementations:\nThis tutorial will demonstrate how to fit models in Stan (Gelman, Lee, and Guo (2015)) using the package rstan (Stan Development Team (2018)) as interface, which also requires to load some other packages."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#introduction",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#introduction",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous tutorial (nested ANOVA), we introduced the concept of employing sub-replicates that are nested within the main treatment levels as a means of absorbing some of the unexplained variability that would otherwise arise from designs in which sampling units are selected from amongst highly heterogeneous conditions. Such (nested) designs are useful in circumstances where the levels of the main treatment (such as burnt and un-burnt sites) occur at a much larger temporal or spatial scale than the experimental/sampling units (e.g. vegetation monitoring quadrats). For circumstances in which the main treatments can be applied (or naturally occur) at the same scale as the sampling units (such as whether a stream rock is enclosed by a fish proof fence or not), an alternative design is available. In this design (randomised complete block design), each of the levels of the main treatment factor are grouped (blocked) together (in space and/or time) and therefore, whilst the conditions between the groups (referred to as “blocks”) might vary substantially, the conditions under which each of the levels of the treatment are tested within any given block are far more homogeneous.\nIf any differences between blocks (due to the heterogeneity) can account for some of the total variability between the sampling units (thereby reducing the amount of variability that the main treatment(s) failed to explain), then the main test of treatment effects will be more powerful/sensitive. As an simple example of a randomised complete block (RCB) design, consider an investigation into the roles of different organism scales (microbial, macro invertebrate and vertebrate) on the breakdown of leaf debris packs within streams. An experiment could consist of four treatment levels - leaf packs protected by fish-proof mesh, leaf packs protected by fine macro invertebrate exclusion mesh, leaf packs protected by dissolving antibacterial tablets, and leaf packs relatively unprotected as controls. As an acknowledgement that there are many other unmeasured factors that could influence leaf pack breakdown (such as flow velocity, light levels, etc) and that these are likely to vary substantially throughout a stream, the treatments are to be arranged into groups or “blocks” (each containing a single control, microbial, macro invertebrate and fish protected leaf pack). Blocks of treatment sets are then secured in locations haphazardly selected throughout a particular reach of stream. Importantly, the arrangement of treatments in each block must be randomized to prevent the introduction of some systematic bias - such as light angle, current direction etc.\nBlocking does however come at a cost. The blocks absorb both unexplained variability as well as degrees of freedom from the residuals. Consequently, if the amount of the total unexplained variation that is absorbed by the blocks is not sufficiently large enough to offset the reduction in degrees of freedom (which may result from either less than expected heterogeneity, or due to the scale at which the blocks are established being inappropriate to explain much of the variation), for a given number of sampling units (leaf packs), the tests of main treatment effects will suffer power reductions. Treatments can also be applied sequentially or repeatedly at the scale of the entire block, such that at any single time, only a single treatment level is being applied (see the lower two sub-figures above). Such designs are called repeated measures. A repeated measures ANOVA is to an single factor ANOVA as a paired t-test is to a independent samples t-test. One example of a repeated measures analysis might be an investigation into the effects of a five different diet drugs (four doses and a placebo) on the food intake of lab rats. Each of the rats (“subjects”) is subject to each of the four drugs (within subject effects) which are administered in a random order. In another example, temporal recovery responses of sharks to bi-catch entanglement stresses might be simulated by analyzing blood samples collected from captive sharks (subjects) every half hour for three hours following a stress inducing restraint. This repeated measures design allows the anticipated variability in stress tolerances between individual sharks to be accounted for in the analysis (so as to permit more powerful test of the main treatments). Furthermore, by performing repeated measures on the same subjects, repeated measures designs reduce the number of subjects required for the investigation. Essentially, this is a randomised complete block design except that the within subject (block) effect (e.g. time since stress exposure) cannot be randomised.\nTo suppress contamination effects resulting from the proximity of treatment sampling units within a block, units should be adequately spaced in time and space. For example, the leaf packs should not be so close to one another that the control packs are effected by the antibacterial tablets and there should be sufficient recovery time between subsequent drug administrations. In addition, the order or arrangement of treatments within the blocks must be randomized so as to prevent both confounding as well as computational complications. Whilst this is relatively straight forward for the classic randomized complete block design (such as the leaf packs in streams), it is logically not possible for repeated measures designs. Blocking factors are typically random factors that represent all the possible blocks that could be selected. As such, no individual block can truly be replicated. Randomised complete block and repeated measures designs can therefore also be thought of as un-replicated factorial designs in which there are two or more factors but that the interactions between the blocks and all the within block factors are not replicated."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#linear-models",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#linear-models",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Linear models",
    "text": "Linear models\nThe linear models for two and three factor nested design are:\n\\[\ny_{ij} = \\mu + \\beta_i + \\alpha_j + \\epsilon_{ij},\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\beta\\alpha)_{ij} + (\\beta\\gamma)_{ik} + (\\alpha\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 1)}\n\\]\n\\[\ny_{ijk} = \\mu + \\beta_i + \\alpha_j + \\gamma_k + (\\alpha\\gamma)_{jk} + \\epsilon_{ijk}, \\;\\;\\; \\text{(Model 2)},\n\\]\nwhere \\(\\mu\\) is the overall mean, \\(\\beta\\) is the effect of the Blocking Factor B (\\(\\sum \\beta=0\\)), \\(\\alpha\\) and \\(\\gamma\\) are the effects of withing block Factor A and Factor C, respectively, and \\(\\epsilon \\sim N(0,\\sigma^2)\\) is the random unexplained or residual component.\nTests for the effects of blocks as well as effects within blocks assume that there are no interactions between blocks and the within block effects. That is, it is assumed that any effects are of similar nature within each of the blocks. Whilst this assumption may well hold for experiments that are able to consciously set the scale over which the blocking units are arranged, when designs utilize arbitrary or naturally occurring blocking units, the magnitude and even polarity of the main effects are likely to vary substantially between the blocks. The preferred (non-additive or “Model 1”) approach to un-replicated factorial analysis of some bio-statisticians is to include the block by within subject effect interactions (e.g. \\(\\beta\\alpha\\)). Whilst these interaction effects cannot be formally tested, they can be used as the denominators in F-ratio calculations of their respective main effects tests. Proponents argue that since these blocking interactions cannot be formally tested, there is no sound inferential basis for using these error terms separately. Alternatively, models can be fitted additively (“Model 2”) whereby all the block by within subject effect interactions are pooled into a single residual term (\\(\\epsilon\\)). Although the latter approach is simpler, each of the within subject effects tests do assume that there are no interactions involving the blocks and that perhaps even more restrictively, that sphericity holds across the entire design."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#assumptions",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#assumptions",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Assumptions",
    "text": "Assumptions\nAs with other ANOVA designs, the reliability of hypothesis tests is dependent on the residuals being:\n\nnormally distributed. Boxplots using the appropriate scale of replication (reflecting the appropriate residuals/F-ratio denominator should be used to explore normality. Scale transformations are often useful.\nequally varied. Boxplots and plots of means against variance (using the appropriate scale of replication) should be used to explore the spread of values. Residual plots should reveal no patterns. Scale transformations are often useful.\nindependent of one another. Although the observations within a block may not strictly be independent, provided the treatments are applied or ordered randomly within each block or subject, within block proximity effects on the residuals should be random across all blocks and thus the residuals should still be independent of one another. Nevertheless, it is important that experimental units within blocks are adequately spaced in space and time so as to suppress contamination or carryover effects."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Data generation",
    "text": "Data generation\nImagine we has designed an experiment in which we intend to measure a response (y) to one of treatments (three levels; “a1”, “a2” and “a3”). Unfortunately, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability you decide to apply a design (RCB) in which each of the treatments within each of 35 blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nlibrary(plyr)\nset.seed(123)\nnTreat &lt;- 3\nnBlock &lt;- 35\nsigma &lt;- 5\nsigma.block &lt;- 12\nn &lt;- nBlock*nTreat\nBlock &lt;- gl(nBlock, k=1)\nA &lt;- gl(nTreat,k=1)\ndt &lt;- expand.grid(A=A,Block=Block)\n#Xmat &lt;- model.matrix(~Block + A + Block:A, data=dt)\nXmat &lt;- model.matrix(~-1+Block + A, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = 40, sd = sigma.block)\nA.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~-1+A,data=dt))\n## Sum to zero block effects\nblock.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\nA.effects &lt;- c(40,70,80)\nall.effects &lt;- c(block.effects,A.effects)\nlin.pred &lt;- Xmat %*% all.effects\n\n\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\ny &lt;- rnorm(n,lin.pred,sigma)\ndata.rcb &lt;- data.frame(y=y, expand.grid(A=A, Block=Block))\nhead(data.rcb)  #print out the first six rows of the data set\n\nNA          y A Block\nNA 1 45.80853 1     1\nNA 2 66.71784 2     1\nNA 3 93.29238 3     1\nNA 4 43.10101 1     2\nNA 5 73.20697 2     2\nNA 6 91.77487 3     2"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~A, data.rcb)\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. . More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nlibrary(car)\nwith(data.rcb, interaction.plot(A,Block,y))\n\n#OR with ggplot\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(data.rcb, aes(y=y, x=A, group=Block,color=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+A, data.rcb))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA A                                    \nNA Tukey test   -1.4163           0.1567\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+A, data.rcb))\n\nNA       Test     Pvalue \nNA -1.4163343  0.1566776\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nlibrary(asbio)\nwith(data.rcb,tukey.add.test(y,A,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 2.0060029   Denom df = 67    p-value = 0.1613102\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (A). Any trends appear to be reasonably consistent between Blocks."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Model fitting",
    "text": "Model fitting\nFull parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij)} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\beta_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)).\nMatrix parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}=\\boldsymbol \\beta \\boldsymbol X + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\boldsymbol \\beta \\sim MVN(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\). The full parameterisation, shows the effects parameterisation in which there is an intercept (\\(\\alpha_0\\)) and two treatment effects (\\(\\beta_i\\), where \\(i\\) is \\(1,2\\)). The matrix parameterisation is a compressed notation, In this parameterisation, there are three alpha parameters (one representing the mean of treatment a1, and the other two representing the treatment effects (differences between a2 and a1 and a3 and a1). In generating priors for each of these three alpha parameters, we could loop through each and define a non-informative normal prior to each (as in the Full parameterisation version). However, it turns out that it is more efficient (in terms of mixing and thus the number of necessary iterations) to define the priors from a multivariate normal distribution. This has as many means as there are parameters to estimate (\\(3\\)) and a \\(3\\times3\\) matrix of zeros and \\(100\\) in the diagonals.\n\\[\n\\boldsymbol \\mu =\n  \\begin{bmatrix} 0  \\\\ 0  \\\\ 0 \\end{bmatrix}, \\;\\;\\; \\sigma^2 \\sim   \n  \\begin{bmatrix}\n   1000000 & 0 & 0 \\\\\n   0 & 1000000 & 0 \\\\\n   0 & 0 & 1000000\n   \\end{bmatrix}.\n\\]\nHierarchical parameterisation\n\\[\ny_{ijk} \\sim N(\\mu_{ij}, \\sigma^2), \\;\\;\\; \\mu_{ij}= \\beta_0 + \\beta_i + \\gamma_{j(i)},\n\\]\nwhere \\(\\gamma_{ij} \\sim N(0, \\sigma^2_B)\\), \\(\\beta_0, \\beta_i \\sim N(0, 1000000)\\), and \\(\\sigma^2, \\sigma^2_B \\sim \\text{Cauchy(0, 25)}\\).\nRather than assume a specific variance-covariance structure, just like lme we can incorporate an appropriate structure to account for different dependency/correlation structures in our data. In RCB designs, it is prudent to capture the residuals to allow checks that there are no outstanding dependency issues following model fitting."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#full-means-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#full-means-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Full means parameterisation",
    "text": "Full means parameterisation\n\nrstanString=\"\ndata{\n   int n;\n   int nA;\n   int nB;\n   vector [n] y;\n   int A[n];\n   int B[n];\n}\n\nparameters{\n  real alpha[nA];\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha ~ normal( 0 , 100 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha[A[i]] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString, con = \"fullModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\ndata.rcb.list &lt;- with(data.rcb, list(y=y, A=as.numeric(A), B=as.numeric(Block),\n  n=nrow(data.rcb), nB=length(levels(Block)),nA=length(levels(A))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nStart the Stan model (check the model, load data into the model, specify the number of chains and compile the model). Load the rstan package.\n\nlibrary(rstan)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.c &lt;- stan(data = data.rcb.list, file = \"fullModel.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.7e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.243 seconds (Warm-up)\nNA Chain 1:                0.111 seconds (Sampling)\nNA Chain 1:                0.354 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 7e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.237 seconds (Warm-up)\nNA Chain 2:                0.105 seconds (Sampling)\nNA Chain 2:                0.342 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.c, par = c(\"alpha\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha[1] 41.55    0.10 2.13 37.40 40.15 41.54 42.94 45.68   441 1.01\nNA alpha[2] 69.50    0.11 2.16 65.31 68.08 69.50 70.86 73.85   419 1.01\nNA alpha[3] 81.85    0.10 2.12 77.71 80.45 81.80 83.21 85.95   425 1.01\nNA sigma     5.07    0.01 0.44  4.29  4.75  5.05  5.35  6.00  2176 1.00\nNA sigma_B  11.71    0.03 1.60  9.10 10.58 11.55 12.63 15.35  3757 1.00\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:08:01 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.c.df &lt;-as.data.frame(extract(data.rcb.rstan.c))\nhead(data.rcb.rstan.c.df)\n\nNA    alpha.1  alpha.2  alpha.3    sigma  sigma_B      lp__\nNA 1 40.92583 67.75321 79.80043 4.655019 10.81956 -319.5226\nNA 2 41.66058 72.22219 83.44578 4.325701 15.14545 -323.5076\nNA 3 41.23380 69.83747 82.08849 5.818799 10.75345 -329.9066\nNA 4 40.46224 68.34375 80.72886 4.690904 11.07078 -317.4154\nNA 5 38.51851 66.61851 79.33672 4.498886 11.73147 -318.8312\nNA 6 41.81091 69.41454 82.56791 6.062546  9.78244 -322.8856\n\ndata.rcb.mcmc.c&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.c)\n\nlibrary(coda)\nMCMCsum &lt;- function(x) {\n   data.frame(Median=median(x, na.rm=TRUE), t(quantile(x,na.rm=TRUE)),\n              HPDinterval(as.mcmc(x)),HPDinterval(as.mcmc(x),p=0.5))\n}\n\nplyr:::adply(as.matrix(data.rcb.rstan.c.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1 alpha.1   41.535980   32.655709   40.149101   41.535980   42.944616\nNA 2 alpha.2   69.496977   61.677410   68.079227   69.496977   70.861836\nNA 3 alpha.3   81.799857   73.717659   80.452339   81.799857   83.205086\nNA 4   sigma    5.048656    3.677651    4.748179    5.048656    5.349604\nNA 5 sigma_B   11.554897    7.586198   10.576991   11.554897   12.629219\nNA 6    lp__ -321.666229 -342.185020 -325.626894 -321.666229 -318.292350\nNA         X100.       lower       upper     lower.1    upper.1\nNA 1   50.527411   37.476978   45.726479   40.282467   43.04881\nNA 2   78.798149   65.297631   73.829659   68.437537   71.17360\nNA 3   90.863749   77.678551   85.948313   80.381917   83.10695\nNA 4    6.931274    4.212814    5.902948    4.673778    5.25575\nNA 5   19.103937    8.800251   14.887844   10.404933   12.39880\nNA 6 -307.824839 -333.210469 -312.359491 -325.688234 -318.38516"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#full-effect-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#full-effect-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Full effect parameterisation",
    "text": "Full effect parameterisation\n\nrstan2String=\"\ndata{\n   int n;\n   int nB;\n   vector [n] y;\n   int A2[n];\n   int A3[n];\n   int B[n];\n}\n\nparameters{\n  real alpha0;\n  real alpha2;\n  real alpha3;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] beta;\n  real&lt;lower=0&gt; sigma_B;\n}\n \nmodel{\n    real mu[n];\n\n    // Priors\n    alpha0 ~ normal( 0 , 1000 );\n    alpha2 ~ normal( 0 , 1000 );\n    alpha3 ~ normal( 0 , 1000 );\n    beta ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    for ( i in 1:n ) {\n        mu[i] = alpha0 + alpha2*A2[i] + \n               alpha3*A3[i] + beta[B[i]];\n    }\n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstan2String, con = \"full2Model.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nA2 &lt;- ifelse(data.rcb$A=='2',1,0)\nA3 &lt;- ifelse(data.rcb$A=='3',1,0)\ndata.rcb.list &lt;- with(data.rcb, list(y=y, A2=A2, A3=A3, B=as.numeric(Block),\n   n=nrow(data.rcb), nB=length(levels(Block))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"alpha0\",\"alpha2\",\"alpha3\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.f &lt;- stan(data = data.rcb.list, file = \"full2Model.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.2e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.421 seconds (Warm-up)\nNA Chain 1:                0.165 seconds (Sampling)\nNA Chain 1:                0.586 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.445 seconds (Warm-up)\nNA Chain 2:                0.175 seconds (Sampling)\nNA Chain 2:                0.62 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.f, par = c(\"alpha0\", \"alpha2\", \"alpha3\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA alpha0  41.73    0.14 2.17 37.48 40.25 41.70 43.24 46.17   253    1\nNA alpha2  27.91    0.03 1.23 25.49 27.10 27.93 28.70 30.30  1991    1\nNA alpha3  40.24    0.03 1.19 37.85 39.44 40.26 41.04 42.52  2033    1\nNA sigma    5.08    0.01 0.46  4.28  4.76  5.05  5.37  6.07  1685    1\nNA sigma_B 11.73    0.03 1.57  9.15 10.63 11.57 12.63 15.27  2206    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:08:33 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.f.df &lt;-as.data.frame(extract(data.rcb.rstan.f))\nhead(data.rcb.rstan.f.df)\n\nNA     alpha0   alpha2   alpha3    sigma  sigma_B      lp__\nNA 1 39.95904 28.82693 40.37188 5.244175 11.47116 -318.3557\nNA 2 40.26174 27.82831 41.60322 5.111554 12.84181 -311.5651\nNA 3 39.07137 28.27831 38.76523 4.700475 12.79742 -315.8146\nNA 4 41.14351 30.10751 40.30998 4.690441 13.88115 -325.3576\nNA 5 42.56938 26.98799 39.34253 5.384412 10.29316 -321.9314\nNA 6 40.45307 29.22397 41.05599 5.021199 12.07345 -314.9324\n\ndata.rcb.mcmc.f&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.f)\n\nplyr:::adply(as.matrix(data.rcb.rstan.f.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1  alpha0   41.698390   34.473108   40.246925   41.698390   43.235888\nNA 2  alpha2   27.929974   23.249223   27.096648   27.929974   28.701302\nNA 3  alpha3   40.264280   35.080170   39.437859   40.264280   41.043282\nNA 4   sigma    5.052888    3.786100    4.757611    5.052888    5.366813\nNA 5 sigma_B   11.567793    7.661363   10.634201   11.567793   12.631537\nNA 6    lp__ -321.145807 -342.911147 -325.003745 -321.145807 -317.615781\nNA         X100.       lower       upper     lower.1     upper.1\nNA 1   48.769516   37.757085   46.360518   40.444880   43.392733\nNA 2   32.665468   25.650646   30.418299   27.152255   28.743241\nNA 3   44.938585   37.833235   42.486534   39.485525   41.085516\nNA 4    7.054157    4.214457    5.954861    4.688346    5.269776\nNA 5   19.027742    9.063056   15.130256   10.347573   12.291484\nNA 6 -306.630367 -332.364491 -310.979729 -323.592133 -316.356397"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n}\ntransformed parameters {\n  vector[n] mu;    \n  \n  mu = X*beta;\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel.stan\")\n\nArrange the data as a list (as required by Stan). As input, Stan will need to be supplied with: the response variable, the predictor matrix, the number of predictors, the total number of observed items. This all needs to be contained within a list object. We will create two data lists, one for each of the hypotheses.\n\nXmat &lt;- model.matrix(~A, data=data.rcb)\ndata.rcb.list &lt;- with(data.rcb, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rcb), nB=length(levels(Block))))\n\nDefine the nodes (parameters and derivatives) to monitor and the chain parameters.\n\nparams &lt;- c(\"beta\",\"sigma\",\"sigma_B\")\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\nNow run the Stan code via the rstan interface.\n\ndata.rcb.rstan.d &lt;- stan(data = data.rcb.list, file = \"matrixModel.stan\", \n                         chains = nChains, pars = params, iter = nIter, \n                         warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3.9e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 0.343 seconds (Warm-up)\nNA Chain 1:                0.128 seconds (Sampling)\nNA Chain 1:                0.471 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 8e-06 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 0.338 seconds (Warm-up)\nNA Chain 2:                0.127 seconds (Sampling)\nNA Chain 2:                0.465 seconds (Total)\nNA Chain 2:\n\nprint(data.rcb.rstan.d, par = c(\"beta\", \"sigma\", \"sigma_B\"))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nNA beta[1] 41.70    0.12 2.19 37.16 40.31 41.76 43.18 45.87   311 1.01\nNA beta[2] 27.93    0.02 1.20 25.55 27.14 27.93 28.71 30.22  2830 1.00\nNA beta[3] 40.25    0.02 1.25 37.78 39.41 40.26 41.09 42.71  2510 1.00\nNA sigma    5.06    0.01 0.45  4.28  4.74  5.04  5.33  6.03  1684 1.00\nNA sigma_B 11.71    0.03 1.52  9.13 10.61 11.58 12.64 15.06  2905 1.00\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:06 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\ndata.rcb.rstan.d.df &lt;-as.data.frame(extract(data.rcb.rstan.d))\nhead(data.rcb.rstan.d.df)\n\nNA     beta.1   beta.2   beta.3    sigma   sigma_B      lp__\nNA 1 43.53750 26.19984 38.88439 4.383973 11.909139 -311.6449\nNA 2 40.00707 29.45451 40.75455 5.187455 12.559968 -318.1385\nNA 3 41.04678 27.38811 40.37273 4.133845 12.234813 -313.0129\nNA 4 40.53589 28.77436 41.33340 4.601260  9.623768 -316.7945\nNA 5 39.19210 28.78798 41.84681 4.408715 12.655509 -313.2638\nNA 6 41.09020 28.31770 40.18192 4.504707 11.065015 -311.0851\n\ndata.rcb.mcmc.d&lt;-rstan:::as.mcmc.list.stanfit(data.rcb.rstan.d)\n\nplyr:::adply(as.matrix(data.rcb.rstan.d.df),2,MCMCsum)\n\nNA        X1      Median         X0.        X25.        X50.        X75.\nNA 1  beta.1   41.758141   33.279797   40.306435   41.758141   43.178469\nNA 2  beta.2   27.931300   23.108350   27.136590   27.931300   28.712865\nNA 3  beta.3   40.260645   34.766550   39.412227   40.260645   41.086043\nNA 4   sigma    5.035827    3.837230    4.738249    5.035827    5.330248\nNA 5 sigma_B   11.582596    7.470131   10.613220   11.582596   12.637936\nNA 6    lp__ -320.912804 -347.661595 -324.937268 -320.912804 -317.463916\nNA        X100.       lower       upper     lower.1     upper.1\nNA 1   51.15476   37.008800   45.713133   40.384918   43.218462\nNA 2   32.92478   25.618576   30.287537   27.029125   28.593104\nNA 3   44.64738   37.866252   42.759044   39.451019   41.116210\nNA 4    7.73581    4.288374    6.031137    4.719787    5.304595\nNA 5   19.08792    8.929760   14.729667   10.377374   12.370289\nNA 6 -306.91630 -332.294151 -311.520232 -324.167208 -316.967283"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#data-generation-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Data generation",
    "text": "Data generation\nImagine now that we has designed an experiment to investigate the effects of a continuous predictor (\\(x\\), for example time) on a response (\\(y\\)). Again, the system that we intend to sample is spatially heterogeneous and thus will add a great deal of noise to the data that will make it difficult to detect a signal (impact of treatment). Thus in an attempt to constrain this variability, we again decide to apply a design (RCB) in which each of the levels of \\(X\\) (such as time) treatments within each of \\(35\\) blocks dispersed randomly throughout the landscape. As this section is mainly about the generation of artificial data (and not specifically about what to do with the data), understanding the actual details are optional and can be safely skipped.\n\nset.seed(123)\nslope &lt;- 30\nintercept &lt;- 200\nnBlock &lt;- 35\nnTime &lt;- 10\nsigma &lt;- 50\nsigma.block &lt;- 30\nn &lt;- nBlock*nTime\nBlock &lt;- gl(nBlock, k=1)\nTime &lt;- 1:10\nrho &lt;- 0.8\ndt &lt;- expand.grid(Time=Time,Block=Block)\nXmat &lt;- model.matrix(~-1+Block + Time, data=dt)\nblock.effects &lt;- rnorm(n = nBlock, mean = intercept, sd = sigma.block)\n#A.effects &lt;- c(30,40)\nall.effects &lt;- c(block.effects,slope)\nlin.pred &lt;- Xmat %*% all.effects\n\n# OR\nXmat &lt;- cbind(model.matrix(~-1+Block,data=dt),model.matrix(~Time,data=dt))\n## Sum to zero block effects\n##block.effects &lt;- rnorm(n = nBlock, mean = 0, sd = sigma.block)\n###A.effects &lt;- c(40,70,80)\n##all.effects &lt;- c(block.effects,intercept,slope)\n##lin.pred &lt;- Xmat %*% all.effects\n\n## the quadrat observations (within sites) are drawn from\n## normal distributions with means according to the site means\n## and standard deviations of 5\neps &lt;- NULL\neps[1] &lt;- 0\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] #residuals\n}\ny &lt;- rnorm(n,lin.pred,sigma)+eps\n\n#OR\neps &lt;- NULL\n# first value cant be autocorrelated\neps[1] &lt;- rnorm(1,0,sigma)\nfor (j in 2:n) {\n  eps[j] &lt;- rho*eps[j-1] + rnorm(1, mean = 0, sd = sigma)  #residuals\n}\ny &lt;- lin.pred + eps\ndata.rm &lt;- data.frame(y=y, dt)\nhead(data.rm)  #print out the first six rows of the data set\n\nNA          y Time Block\nNA 1 282.1142    1     1\nNA 2 321.1404    2     1\nNA 3 278.7700    3     1\nNA 4 285.8709    4     1\nNA 5 336.6390    5     1\nNA 6 333.5961    6     1\n\nggplot(data.rm, aes(y=y, x=Time)) + geom_smooth(method='lm') + geom_point() + facet_wrap(~Block)"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#exploratory-data-analysis-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nNormality and Homogeneity of variance\n\nboxplot(y~Time, data.rm)\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=factor(Time))) + geom_boxplot()\n\n\n\n\n\n\n\n\nConclusions:\n\nthere is no evidence that the response variable is consistently non-normal across all populations - each boxplot is approximately symmetrical.\nthere is no evidence that variance (as estimated by the height of the boxplots) differs between the five populations. More importantly, there is no evidence of a relationship between mean and variance - the height of boxplots does not increase with increasing position along the \\(y\\)-axis. Hence it there is no evidence of non-homogeneity\n\nObvious violations could be addressed either by:\n\ntransform the scale of the response variables (to address normality, etc). Note transformations should be applied to the entire response variable (not just those populations that are skewed).\n\nBlock by within-Block interaction\n\nwith(data.rm, interaction.plot(Time,Block,y))\n\n\n\n\n\n\n\nggplot(data.rm, aes(y=y, x=Time, color=Block, group=Block)) + geom_line() +\n  guides(color=guide_legend(ncol=3))\n\n\n\n\n\n\n\nresidualPlots(lm(y~Block+Time, data.rm))\n\n\n\n\n\n\n\n\nNA            Test stat Pr(&gt;|Test stat|)\nNA Block                                \nNA Time         -0.7274           0.4675\nNA Tukey test   -0.9809           0.3267\n\n# the Tukey's non-additivity test by itself can be obtained via an internal function\n# within the car package\ncar:::tukeyNonaddTest(lm(y~Block+Time, data.rm))\n\nNA       Test     Pvalue \nNA -0.9808606  0.3266615\n\n# alternatively, there is also a Tukey's non-additivity test within the\n# asbio package\nwith(data.rm,tukey.add.test(y,Time,Block))\n\nNA \nNA Tukey's one df test for additivity \nNA F = 0.3997341   Denom df = 305    p-value = 0.5277003\n\n\nConclusions:\n\nthere is no visual or inferential evidence of any major interactions between Block and the within-Block effect (Time). Any trends appear to be reasonably consistent between Blocks.\n\nSphericity\nSince the levels of Time cannot be randomly assigned, it is likely that sphericity is not met. We can explore whether there is an auto-correlation patterns in the residuals. Note, as there was only ten time periods, it does not make logical sense to explore lags above \\(10\\).\n\nlibrary(nlme)\ndata.rm.lme &lt;- lme(y~Time, random=~1|Block, data=data.rm)\nacf(resid(data.rm.lme), lag=10)\n\n\n\n\n\n\n\n\nConclusions:\nThe autocorrelation factor (ACF) at a range of lags up to \\(10\\), indicate that there is a cyclical pattern of residual auto-correlation. We really should explore incorporating some form of correlation structure into our model."
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#model-fitting-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Model fitting",
    "text": "Model fitting"
  },
  {
    "objectID": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation-1",
    "href": "tutorials/2020-02-10-block-anova-stan/index.html#matrix-parameterisation-1",
    "title": "Randomised Complete Block Anova (Stan)",
    "section": "Matrix parameterisation",
    "text": "Matrix parameterisation\n\nrstanString2=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n}\ntransformed parameters {\n  vector[n] mu;    \n  \n  mu = X*beta;\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString2, con = \"matrixModel2.stan\")\n\nXmat &lt;- model.matrix(~Time, data=data.rm)\ndata.rm.list &lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rm), nB=length(levels(Block))))\n\nparams &lt;- c('beta','sigma','sigma_B')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.rstan.d  &lt;- stan(data = data.rm.list, file = \"matrixModel2.stan\", \n                            chains = nChains, pars = params, iter = nIter, \n                            warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 3e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.194 seconds (Warm-up)\nNA Chain 1:                0.285 seconds (Sampling)\nNA Chain 1:                1.479 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 1.6e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 1.21 seconds (Warm-up)\nNA Chain 2:                0.283 seconds (Sampling)\nNA Chain 2:                1.493 seconds (Total)\nNA Chain 2:\n\nprint(data.rm.rstan.d , par = c('beta','sigma','sigma_B'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1] 186.70    0.72 11.96 162.54 178.89 186.74 194.88 209.68   272    1\nNA beta[2]  30.79    0.02  1.02  28.82  30.10  30.77  31.48  32.75  2076    1\nNA sigma    55.90    0.04  2.21  51.64  54.35  55.83  57.39  60.29  2729    1\nNA sigma_B  64.52    0.19  8.76  50.39  58.35  63.49  69.44  83.96  2044    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:12 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1).\n\n\nGiven that Time cannot be randomized, there is likely to be a temporal dependency structure to the data. The above analyses assume no temporal dependency - actually, they assume that the variance-covariance matrix demonstrates a structure known as sphericity. Lets specifically model in a first order autoregressive correlation structure in an attempt to accommodate the expected temporal autocorrelation.\n\nrstanString3=\"\ndata{\n   int n;\n   int nX;\n   int nB;\n   vector [n] y;\n   matrix [n,nX] X;\n   int B[n];\n   vector [n] tgroup;\n}\n\nparameters{\n  vector [nX] beta;\n  real&lt;lower=0&gt; sigma;\n  vector [nB] gamma;\n  real&lt;lower=0&gt; sigma_B;\n  real ar;\n}\ntransformed parameters {\n  vector[n] mu;    \n  vector[n] E;\n  vector[n] res;\n\n  mu = X*beta;\n  for (i in 1:n) {\n     E[i] = 0;\n  }\n  for (i in 1:n) {\n    mu[i] = mu[i] + gamma[B[i]];\n    res[i] = y[i] - mu[i];\n    if(i&gt;0 && i &lt; n && tgroup[i+1] == tgroup[i]) {\n      E[i+1] = res[i];\n    }\n    mu[i] = mu[i] + (E[i] * ar);\n  }\n} \nmodel{\n    // Priors\n    beta ~ normal( 0 , 100 );\n    gamma ~ normal( 0 , sigma_B );\n    sigma_B ~ cauchy( 0 , 25 );\n    sigma ~ cauchy( 0 , 25 );\n    \n    y ~ normal( mu , sigma );\n}\n\n\"\n\n## write the model to a text file\nwriteLines(rstanString3, con = \"matrixModel3.stan\")\n\nXmat &lt;- model.matrix(~Time, data=data.rm)\ndata.rm.list &lt;- with(data.rm, list(y=y, X=Xmat, nX=ncol(Xmat),\n  B=as.numeric(Block),\n  n=nrow(data.rm), nB=length(levels(Block)),\n  tgroup=as.numeric(Block)))\n\nparams &lt;- c('beta','sigma','sigma_B','ar')\nburnInSteps = 3000\nnChains = 2\nnumSavedSteps = 3000\nthinSteps = 1\nnIter = burnInSteps+ceiling((numSavedSteps * thinSteps)/nChains)\n\ndata.rm.rstan.d  &lt;- stan(data = data.rm.list, file = \"matrixModel3.stan\", \n                            chains = nChains, pars = params, iter = nIter, \n                            warmup = burnInSteps, thin = thinSteps)\n\nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nNA Chain 1: \nNA Chain 1: Gradient evaluation took 5.7e-05 seconds\nNA Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.57 seconds.\nNA Chain 1: Adjust your expectations accordingly!\nNA Chain 1: \nNA Chain 1: \nNA Chain 1: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 1: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 1: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 1: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 1: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 1: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 1: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 1: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 1: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 1: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 1: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 1: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 1: \nNA Chain 1:  Elapsed Time: 1.438 seconds (Warm-up)\nNA Chain 1:                0.502 seconds (Sampling)\nNA Chain 1:                1.94 seconds (Total)\nNA Chain 1: \nNA \nNA SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nNA Chain 2: \nNA Chain 2: Gradient evaluation took 2.7e-05 seconds\nNA Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nNA Chain 2: Adjust your expectations accordingly!\nNA Chain 2: \nNA Chain 2: \nNA Chain 2: Iteration:    1 / 4500 [  0%]  (Warmup)\nNA Chain 2: Iteration:  450 / 4500 [ 10%]  (Warmup)\nNA Chain 2: Iteration:  900 / 4500 [ 20%]  (Warmup)\nNA Chain 2: Iteration: 1350 / 4500 [ 30%]  (Warmup)\nNA Chain 2: Iteration: 1800 / 4500 [ 40%]  (Warmup)\nNA Chain 2: Iteration: 2250 / 4500 [ 50%]  (Warmup)\nNA Chain 2: Iteration: 2700 / 4500 [ 60%]  (Warmup)\nNA Chain 2: Iteration: 3001 / 4500 [ 66%]  (Sampling)\nNA Chain 2: Iteration: 3450 / 4500 [ 76%]  (Sampling)\nNA Chain 2: Iteration: 3900 / 4500 [ 86%]  (Sampling)\nNA Chain 2: Iteration: 4350 / 4500 [ 96%]  (Sampling)\nNA Chain 2: Iteration: 4500 / 4500 [100%]  (Sampling)\nNA Chain 2: \nNA Chain 2:  Elapsed Time: 2.343 seconds (Warm-up)\nNA Chain 2:                0.483 seconds (Sampling)\nNA Chain 2:                2.826 seconds (Total)\nNA Chain 2:\n\nprint(data.rm.rstan.d , par = c('beta','sigma','sigma_B','ar'))\n\nNA Inference for Stan model: anon_model.\nNA 2 chains, each with iter=4500; warmup=3000; thin=1; \nNA post-warmup draws per chain=1500, total post-warmup draws=3000.\nNA \nNA           mean se_mean    sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nNA beta[1] 179.17    0.24 12.76 153.91 170.91 179.06 187.48 204.24  2816    1\nNA beta[2]  31.29    0.02  1.70  27.91  30.17  31.29  32.43  34.72  5406    1\nNA sigma    48.73    0.03  1.99  45.11  47.33  48.63  50.06  52.72  5115    1\nNA sigma_B  49.78    0.26 10.61  30.48  42.28  49.07  56.56  73.10  1604    1\nNA ar        0.78    0.00  0.05   0.68   0.75   0.78   0.81   0.88  2786    1\nNA \nNA Samples were drawn using NUTS(diag_e) at Mon Jul 22 12:09:49 2024.\nNA For each parameter, n_eff is a crude measure of effective sample size,\nNA and Rhat is the potential scale reduction factor on split chains (at \nNA convergence, Rhat=1)."
  }
]