[
  {
    "objectID": "research/volley/volley.html",
    "href": "research/volley/volley.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or “modules” form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match (\\(y_h\\) and \\(y_a\\)); (2) the module of the binary indicator for the number of sets played (\\(d^s\\)); (3) the module of the binary indicator for the winner of the match (\\(d^m\\)). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\n\n\nIn the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters.\n\n\n\nIn the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]\n\n\n\nThe last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\).\n\n\n\nAlthough the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "href": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters."
  },
  {
    "objectID": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "href": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]"
  },
  {
    "objectID": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "href": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "The last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\)."
  },
  {
    "objectID": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "href": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Although the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/reviewNL/reviewNL.html",
    "href": "research/reviewNL/reviewNL.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Introduction\nIn the Netherlands, the Dutch National Health Care Institute (Zorginstituut Nederland or ZIN) is the body in charge of issuing recommendations and guidance on good practice in health economic evaluations, not just for pharmaceutical products, but also in relation to other fields of application such as medical devices, long-term care and forensics. In 2016, ZIN issued an update on the guidance for health economic evaluations, which aggregated into a single document and revised three separately published guidelines for pharmacoeconomics evaluation, outcomes research and costing manual. The novel aspects and future policy direction introduced by these guidelines have already been object of discussion, particularly with respect to the potential impact and concerns associated with their implementation in standard health economics practice in the Netherlands. Given the importance covered by these guidelines, an assessment of their impact on economic evaluation practice is desirable.\nThe objective of this paper was to review the evolution of health economic evaluation practice in the Netherlands before and after the introduction of the ZIN’s 2016 guidelines. Based on some key components within the health economics framework addressed by the new guidelines, we specifically focus on reviewing the statistical methods, missing data methods and software implemented by health economists. Given the intrinsic complexity of analysing health economics data, the choice of the analytical approaches to deal with these problems as well as transparent information on their implementation is crucial in determining the degree of confidence that decision-makers should have towards cost-effectiveness results obtained from these studies\n\n\nThe ZIN 2016 guidelines\nThe main objective of the guidelines is to ensure the comparability and quality of health economic evaluations in the Netherlands, therefore facilitating the task of the decision-maker regarding the reimbursement of new health care interventions. Following the example of guidelines issued by decision-making bodies in other countries, including the National Institute for Health and Care Excellence (NICE) in the UK, the recommended features for economic evaluations are summarised in a typical scenario referred to as ‘reference case’, although deviations from it are allowed when properly justified.\nBased on the structure of the reference case, four essential components of a health economic evaluation are identified: framework, analytic approach, input data and reporting. For the purpose of the review, we only focus on these components in the reference case as the main elements upon which evaluating health economics practice.\n\n\nMethods\nWe performed a bibliographic search in June 2021 using search engines of two online full-text journal repositories: (1) PubMed and (2) Zorginstituut. These sources were chosen to maximise the number of studies that could be accessed given the scoping nature of the review and the lack of a search strategy based on a pre-defined and rigid approach typical of systematic reviews. Articles were considered eligible for the review only if they were cost-effectiveness or cost-utility analyses targeting a Dutch population. To allow the inclusion of a reasonable amount of studies, the key words used in the search strategy were (cost-effectiveness OR cost-utility OR economic evaluation), and we targeted studies published between January 2016 and April 2021.\n\n\nAnalytic approaches\nAlmost all reviewed empirical analyses used bootstrapping (95%), although the number of replications varied largely across the studies, with the most popular choices being 5000 (55%) followed by 2000 (29%). Studies showed even more variability in the choice of the methods used in combination with bootstrapping. Seven general classes of statistical approaches were identified, among which unadjusted methods were the most popular choice across both time periods. A clear change in the type of statistical methods used between the two periods is denoted by a strong decrease (from 64 to 39) in the number of unadjusted analyses in 2016–2020 compared to the earlier period, which is compensated by a rise in the number of adjusted analyses using either SUR or linear mixed effects model (LMM) methods.\nFrom Figure 1 we can look at the different type and combination of software programs used as an indication of the implementation preferences of analysts for health economic evaluations. Although in principle the choice of software should have no impact on the quality of the statistical methods implemented, it has been highlighted how use of simpler software (e.g. spreadsheet calculators such as Excel) may become increasingly cumbersome for matching more realistic and therefore complex modelling requirements.\n\n\n\n\n\n\nFigure 1: Heatmap of the combination of software programs used\n\n\n\nThe most popular software was SPSS, chosen by 87 (52%) of the studies, either in the base-case (33%) or secondary (19%) analyses, often used in combination with Excel or by itself. When either STATA (26%) or R (13%) was used in the base-case analysis, SPSS was still the most popular choice in secondary analyses. Other combinations of software were less frequently chosen, even though 38 (23%) of the studies were unclear about the software implemented.\n\n\nMissing data methods\nAcross both periods limited changes are observed in terms of order of choice for missing data methods, with MI being the most popular base-case analysis, followed by complete case analysis (CCA), as the most popular SA choice. However, two noticeable variations in the frequency of these methods are observed between the two periods. First, the proportion of studies using MI in the base-case analysis has considerably increased over time (from 28 to 39%), which is compensated by a decrease in the proportion of less advanced methods such as CCA (from 14 to 5%) and single imputation (SI) (from 21 to 16%). Second, the number of studies not clearly reporting the methods has also considerably decreased (from 12 to 5%). The observed trend between the two periods may be the result of the specific recommendations from the 2016 guidelines in regards to the ‘optimal’ missing data strategy, resulting in a more frequent adoption of MI techniques and, at the same time, a less frequent use of CCA in the base-case analysis. However, in contrast to these guidelines, a large number of studies still does not perform any SA to missing data assumptions (about 65% in 2010–2015 and 63% in 2016–2020).\nMost of the studies lie in the middle and lower parts of the plot, and are associated with a limited or sufficient quality of information. However, only a few of these studies rely on very strong and unjustified missing data assumptions, while the majority provides either adequate justifications or uses methods associated with weak assumptions. Only 11 (14%) studies are associated with both high-quality scores and less restrictive missingness assumptions. No study was associated with either full information or adequate justifications for the assumptions explored in base-case and sensitivity analysis.\n\n\nDiscussion\nDescriptive information extracted from the reviewed studies provides some first insights about changes in practice in the years following the publication of the guidelines. First, a clear trend is observed towards an increase in the adoption of a societal and health care perspective and of CUA as the reference base-case analysis approach. Second, a similar increment is observed in the use of recommended instruments for the collection and valuation of health economic outcomes, such as EQ-5D-5L for QALYs and friction method for costs. Most of these changes are in accordance with the 2016 guidelines, which are likely to have played a role in influencing analysts and practitioners towards a clearer and more standardised way to report health economic results.\nWhen looking at the type of statistical methods used to perform the analysis, an important shift occurs between the two periods towards the use of methods that allow for regression adjustment, with a considerable uptake in the use of SURs and LMMs in the context of empirical analyses. These techniques are strongly supported by the 2016 guidelines in that they allow us to correct for potential bias due to confounding effects, deal with clustered data and formally take into account the correlation between costs and effects. Bootstrapping remains the most popular methods to quantify uncertainty around parameter estimates across both periods. However, the health economic analysis framework requires that the level of complexity of the analysis model is reflected in the way uncertainty surrounding the estimates is generated.\nThe transition between the two time periods reveals an increase in the use of MI techniques in the base-case analysis together with a decrease in the overall use of CCA. This change is in line with the 2016 guidelines which warns about the inherent limitations and potential bias of simple methods (e.g. CCA) when compared to MI as the potential reference method to handle missing values. Nevertheless, improvements are still needed given that many studies (more than 6%) performed the analysis under a single missing data assumption. This is not ideal since by definition missing data assumptions can never be checked, making the results obtained under a specific method (i.e. assumption) potentially biased.\n\n\nConclusions\nGiven the complexity of the health economics framework, the implementation of simple but likely inadequate analytic approaches may lead to imprecise cost-effectiveness results. This is a potentially serious issue for bodies such as ZIN in the Netherlands that use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new health care interventions. Our review shows, over time, a change in common practice with respect to different analysis components in accordance with the recent ZIN’s 2016 guidelines. This is an encouraging movement towards the standardised use of more suitable and robust analytic methods in terms of both statistical, uncertainty and missing data analysis. Improvements are however still needed, particularly in the choice of adequate statistical techniques to deal with the complexity of the data analysed and in the assessment of the impact of alternative missing data assumptions on the results in SA."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html",
    "href": "research/mnarHTA/mnarHTA.html",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "",
    "text": "Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "href": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "Modelling framework",
    "text": "Modelling framework\nThe distribution of the observed responses \\(\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})\\) is specified in terms of a model for the utility and cost variables at time \\(j=\\{0,1,2\\}\\), which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for \\(\\boldsymbol y_{ijt}\\) is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on \\([0,1]\\) through the transformation \\(u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}\\), and fit the model to these transformed variables. To account for the structural values \\(u_{ij} = 1\\) and \\(c_{ij} = 0\\) we use a hurdle approach by including in the model the indicator variables \\(d^u_{ij}:=\\mathbb{I}(u_{ij}=1)\\) and \\(d^c_{ij}:=\\mathbb{I}(c_{ij}=0)\\), which take value \\(1\\) if subject \\(i\\) is associated with a structural value at time \\(j\\) and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time \\(j=1,2\\), the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at \\(j=0\\)) and the utilities at the previous times (only at \\(j=\\{1,2\\}\\)). The model is summarised by Figure 1:\n\n\n\n\n\n\nFigure 1: Longitudinal model for missingness.\n\n\n\nWe use partial identifying restrictions to link the observed data distribution \\(p(\\boldsymbol y_{obs},\\boldsymbol r)\\) to the extrapolation distribution \\(p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)\\) and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern \\(\\boldsymbol y^{\\boldsymbol r}_{mis}\\) by averaging across the corresponding components that are observed and add the sensitivity parameters \\(\\boldsymbol \\Delta_j\\).\nWe define \\(\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})\\) to be time-specific location shifts at the marginal mean in each pattern and set \\(\\boldsymbol \\Delta_j = \\boldsymbol 0\\) as the benchmark scenario. We then explore departures from this benchmark using alternative priors on \\(\\boldsymbol \\Delta_j\\), which are calibrated using the observed standard deviations for costs and utilities at each time \\(j\\) to define the amplitude of the departures from \\(\\boldsymbol \\Delta_j=\\boldsymbol 0\\)."
  },
  {
    "objectID": "research/lmmHTA/lmmHTA.html",
    "href": "research/lmmHTA/lmmHTA.html",
    "title": "Linear mixed models to handle missing at random data in trial based economic evaluations",
    "section": "",
    "text": "Introduction\nCost‐effectiveness analyses (CEAs) conducted alongside randomised controlled trials are an important source of information for decision-makers in the process of technology appraisal (Ramsey et al., 2015). The analysis is based on healthcare outcome data and health service use, typically collected at multiple time points and then combined into overall measures of effectiveness and cost. A popular approach to handle missingness is to discard the participants with incomplete observations (complete case analysis or CCA), allowing for derivation of the overall measures based on the completers alone. We note that slightly different definitions of CCA are possible, depending on the form of the model of interest, the type of missingness and the inclusion of observed covariates. This approach, although appealing by its simplicity, has well-recognised limitations including loss of efficiency and an increased risk of bias. We propose the use of linear mixed effects models (LMMs) as an alternative approach under MAR. LMMs are commonly used for the modelling of dependent data (e.g. repeated-measures) and belong to the general class of likelihood-based methods. LMMs appear surprisingly uncommon for the analysis of repeated measures in trial-based CEA, perhaps because of a lack of awareness or familiarity with fitting LMMs.\n\n\nMethods\nLinear mixed model extends the usual linear model framework by the addition of “random effect” terms, which can take into account the dependence between observations.\n\\[\nY_{ij}=\\beta_1+\\beta_2 X_{i1}+\\ldots+\\beta_(P+1) X_{iP}+\\omega_i+\\epsilon_{ij},\n\\tag{1}\\]\nwhere \\(Y_{ij}\\) denotes the outcome repeatedly collected for each individual \\(i=1,\\ldots,N\\) at multiple times \\(j=1,\\ldots,J\\). The model parameters commonly referred to as fixed effects include an intercept \\(\\beta_1\\) and the coefficients \\((\\beta_2,\\ldots,\\beta_{(P+1)})\\) associated with the predictors \\(X_{i1},\\ldots,X_{iP}\\), while \\(\\omega_i\\) and \\(\\epsilon_{ij}\\) are two random terms: \\(\\epsilon_{ij}\\) is the usual error term and \\(\\omega_i\\) is a random intercept which captures variation in outcomes between individuals. The models can be extended to deal with more complex structures, for example by allowing the effect of the covariates to vary across individuals (random slope) or a different covariance structure of the errors. LMMs can be fitted even if some outcome data are missing and provide correct inferences under MAR.\nA particular type of LMMs commonly used in the analysis of repeated measures in clinical trials is referred to as Mixed Model for Repeated Measurement (MMRM). The model includes a categorical effect for time, an interaction between time and treatment arm, and allows errors to have different variance and correlation over time (i.e. unstructured covariance structure). Figure 1 shows some examples of possible covariance structures that may be explored for LMMs.\n\n\n\n\n\n\nFigure 1: Some examples of covariance structures in LMM\n\n\n\nIncremental (between-group) or marginal (within-group) estimates for aggregated outcomes over the trial period, such as quality-adjusted life years (QALYs) or total costs can be retrieved as linear combinations of the parameter estimates from Equation 1. For example, the mean difference in total cost is obtained by summing up the estimated differences at each follow-up point, while differences on a QALY scale can be obtained as weighted linear combinations of the coefficient estimates of the utility model.\n\n\nConclusions\nWe believe LMMs represent an alternative approach which can overcome some of these limitations.\n\nFirst, practitioners may be more comfortable with the standard regression framework.\nSecond, LMMs can be tailored to address other data features (e.g. cluster-randomised trials or non-normal distribution) while also easily combined with bootstrapping.\nThird, LMMs do not rely on imputation, and results are therefore deterministic and easily reproducible, whereas the Monte Carlo error associated with multiple imputation may cause results to vary from one imputation to another, unless the number of imputations is sufficiently large.\n\nAlthough the methodology illustrated is already known, particularly in the area of statistical analyses, to our knowledge LMMs have rarely been applied to health economic data collected alongside randomised trials. We believe the proposed methods is preferable to a complete-case analysis when CEA data are incomplete, and that it can offer an interesting alternative to imputation methods."
  },
  {
    "objectID": "research/hurdleHTA/hurdleHTA.html",
    "href": "research/hurdleHTA/hurdleHTA.html",
    "title": "Bayesian Modelling for Health Economic Evaluations",
    "section": "",
    "text": "Modelling Framework\nWe propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables \\((e_{it}, c_{it})\\) calculated for the \\(i-\\)th person in group \\(t\\) of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator \\(t\\). Equation 1 specifies the joint distribution \\(p(e_i,c_i)\\) as\n\\[\np(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i)\n\\tag{1}\\]\nwhere, for example, \\(p(e_i)\\) is the marginal distribution of the QALYs and \\(p(c_i\\mid e_i)\\) is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. \\(p(e_i)\\) or \\(p(e_i\\mid c_i)\\), which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either \\(e_i\\) determines \\(c_i\\) or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution \\(p(e_i,c_i)\\) in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution \\(p(e_i \\mid \\boldsymbol \\theta_e)\\) indexed by a set of parameters \\(\\boldsymbol \\theta_e\\) comprising a location \\(\\boldsymbol \\phi_{ie}\\) and a set of ancillary parameters \\(\\boldsymbol\\psi_e\\) typically including some measure of marginal variance \\(\\sigma^2_e\\). Equation 2 models the location parameter using a generalised linear structure\n\\[\ng_e(\\phi_{ie})= \\alpha_0 \\,\\,[+ \\ldots]\n\\tag{2}\\]\nwhere \\(\\alpha\\_0\\) is the intercept and the notation \\([+\\ldots]\\) indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version \\(x_i^{\\star} = (x_i - \\bar{x})\\) is used, the parameter \\(\\mu_e = g_e^{-1}(\\alpha_0)\\) represents the population average QALYs. For the costs, we consider a conditional model \\(p(c_i\\mid e_i,\\boldsymbol\\theta_c)\\), which explicitly depends on the QALYs, as well as on a set of quantities \\(\\boldsymbol\\theta_c\\), again comprising a location \\(\\phi_{ic}\\) and ancillary parameters \\(\\boldsymbol \\psi_{c}\\). For example, when normal distributions are assumed for both \\(p(e_i \\mid \\boldsymbol \\theta\\_e)\\) and \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\), i.e. bivariate normal on both outcomes, the ancillary parameters \\(\\boldsymbol\\psi_c\\) include a conditional variance \\(\\tau^2_c\\), which can be expressed as a function of the marginal variance \\(\\sigma^2_c\\). More specifically, the conditional variance of \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\) is a function of the marginal effectiveness and cost variances and has the closed form \\(\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2\\), where \\(\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}\\) and \\(\\rho\\) is the parameter capturing the correlation between the variables.\nEquation 3 models the location as a function of the QALYs as\n\\[\ng\\_c(\\phi\\_{ic}) = \\beta\\_{0} + \\beta\\_{1}(e\\_{i}-\\mu\\_{e})\\,\\,[+\\ldots]\n\\tag{3}\\]\nHere, \\((e_i-\\mu_e)\\) is the centered version of the QALYs, while \\(\\beta_{1}\\) quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, \\(\\mu_c = g_c^{-1}(\\beta_{0})\\) is the estimated population average cost. The Figure 1 shows a graphical representation of the general modelling framework.\n\n\n\n\n\n\nFigure 1: Modelling framework.\n\n\n\nThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\n\n\nExample\nThree model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. Figure 2 shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and \\(90\\%\\) HPD intervals.\n\n\n\n\n\n\nFigure 2: Imputed QALYs under alternative model specifications.\n\n\n\nThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\n\n\nConclusions\nWe have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n\nJointly model costs and QALYs;\nAccount for skewness and structural values;\nAssess the robustness of the results under a set of differing missingness assumptions.\n\nThe original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses.\n\n\n\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses."
  },
  {
    "objectID": "index.html#research-and-work",
    "href": "index.html#research-and-work",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "research/bookHTA/bookHTA.html",
    "href": "research/bookHTA/bookHTA.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Introduction\nThe type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and \\(P\\)-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n\nwhether the treatments under evaluation are cost-effective given the available evidence and\nwhether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).\n\nThis corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\n\n\nBayesian methods in HTA\nThere are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in Figure 1.\n\n\n\n\n\n\nFigure 1: Diagram representation of the process for health economic evaluation.\n\n\n\nThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g. the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the available evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n\nwhether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and\nwhether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.\n\nWhile the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\n\n\nConclusions\nHTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions."
  },
  {
    "objectID": "research/jointHTA/jointHTA.html",
    "href": "research/jointHTA/jointHTA.html",
    "title": "Joint Longitudinal Models for Dealing With Missing at Random Data in Trial-Based Economic Evaluations",
    "section": "",
    "text": "Introduction\nIn trial-based economic evaluation, some individuals are typically associated with missing data at some time point, so that their corresponding aggregated outcomes (e.g. quality-adjusted life-years) cannot be evaluated. Restricting the analysis to the complete cases is inefficient and can result in biased estimates, while imputation methods are often implemented under a missing at random (MAR) assumption. We propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR.\n\n\nStandard approach in trial-based CEA\nAccording to recent reviews, standard practice in trial-based CEAs handles missingness at the level of the aggregated outcomes and baseline variables. Indeed, estimates of interest are obtained by directly modeling the aggregated outcomes rather than the utility and cost data at each time. This requires the analyst to process the data collected on individual \\(i\\) at time \\(j\\) in treatment \\(t\\), to derive the aggregated measures over the study duration.\nFigure 1 shows a typical data set of trial-based CEA, formed by the sets of utility and cost variables collected at baseline \\(j = 0\\) and some follow-ups \\(j = 1,\\ldots,J\\). The graph represents the standard procedure for processing the data and identifying the variables used in the analysis.\n\n\n\n\n\n\nFigure 1: Schematic representation of the standard procedure for processing trial-based CEA data\n\n\n\nA general limitation of any aggregated method is to ignore the longitudinal nature of the data and discard all follow-up values for partially observed individuals. Conversely, methods that handle missingness at each time point account for the longitudinal structure, incorporate all available evidence, and potentially make the missingness assumptions (e.g. missing at random or MAR) more reasonable.\n\n\nMethods\nWe propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR. We compare the results from methods that handle missingness at an aggregated (case deletion, baseline imputation, and joint aggregated models) and disaggregated (joint longitudinal models) level under MAR. The methods are compared using a simulation study and applied to data from 2 real case studies.\n\n\nConclusions\nJoint longitudinal models provide an alternative and potentially less biased approach for handling missing data with respect to current practice under a missing at random assumption. Methods that ignore some of the available information may be associated with biased results and mislead the decision-making process. This is a potentially serious issue for those who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options."
  },
  {
    "objectID": "research/missingHE/missingHE.html",
    "href": "research/missingHE/missingHE.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variables, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitivity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot can produce graphs, such as those shown in Figure 1, which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\n\n\n\n\n\nFigure 1: Plot of observed (black dots) and imputed (red dots and lines) effectiveness and cost data by treatment group.\n\n\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "research/partsurvHTA/partsurvHTA.html",
    "href": "research/partsurvHTA/partsurvHTA.html",
    "title": "A Bayesian Framework for Patient-Level Partitioned Survival Cost-Utility Analysis",
    "section": "",
    "text": "Modelling Framework\nwe extend the current methods for modelling trial-based partitioned survival cost-utility data, taking advantage of the flexibility of the Bayesian approach, and specify a joint probabilistic model for the health economic outcomes. We propose a general framework that is able to account for the multiple types of complexities affecting individual level data (correlation, missingness, skewness and structural values), while also explicitly modelling the dependence relationships between different types of quality of life and cost components.\nConsider a clinical trial in which patient-level information on a set of suitably defined effectiveness and cost variables is collected at \\(J\\) time points on \\(N\\) individuals, who have been allocated to \\(T\\) intervention groups. Assume that the primary endpoint of the trial is OS, while secondary endpoints include PFS, a self-reported health-related quality of life questionnaire (e.g. EQ-5D) and health records on different types of services (e.g. drug frequency and dosage, hospital visits, etc.). Following standard health economic notation, we denote with \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) the two sets of health economic outcomes (effectiveness and costs) collected for the \\(i\\)-th individual in treatment \\(t\\) of the trial. For simplicity, we define \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) based on the variables used in the analysis.\nThe effectiveness outcomes are represented by pre-progression (\\(e^{PFS}\\_{it}=\\text{QAS}^{\\text{PFS}}\\)) and post-progression (\\(e^{PPS}\\_{it}=\\text{QAS}^{\\text{PPS}}\\)) QAS data calculated using survival and utility data collected up to and beyond progression. We denote the full set of effectiveness variables as \\(\\boldsymbol e_{it}=(e^{\\text{PFS}}\\_{it},e^{\\text{PPS}}\\_{it})\\), formed by the pre and post-progression components. The cost outcomes are represented by a set of \\(K\\) variables (\\(c\\_{it}=c^k\\_{it}\\), for \\(k=1,\\ldots,K\\)) calculated based on \\(K\\) different types of health services and associated unit prices. We denote the full set of cost variables as \\(\\boldsymbol c\\_{it}=(c^1\\_{it},\\ldots,c^K\\_{it})\\), formed by the \\(K\\) different cost components.\nThe objective of the economic evaluation is to perform a patient-level partitioned survival cost-utility analysis by specifying a joint model \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) denotes the full set of model parameters. Among these parameters, interest is in the marginal mean effectiveness and costs \\(\\boldsymbol \\mu=(\\mu\\_{et},\\mu\\_{ct})\\) which are used to inform the decision-making process. Different approaches can be used to specify \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\). Here, we express the joint distribution as\n\\[\np(\\boldsymbol e_{it}, \\boldsymbol c_{it} \\mid \\boldsymbol \\theta) = p(\\boldsymbol e_{it} \\mid \\boldsymbol \\theta_e)p(\\boldsymbol c_{it} \\mid \\boldsymbol  e_{it} , \\boldsymbol  \\theta_c),\n\\tag{1}\\]\nwhere \\(p(\\boldsymbol e_{it} \\mid \\boldsymbol  \\theta_e)\\) is the marginal distribution of the effectiveness and \\(p(\\boldsymbol  c_{it} \\mid \\boldsymbol  e_{it} \\boldsymbol  \\theta_c)\\) is the conditional distribution of the costs given the effectiveness, respectively indexed by \\(\\boldsymbol  \\theta_e\\) and \\(\\boldsymbol  \\theta_c\\), with \\(\\boldsymbol  \\theta=(\\boldsymbol  \\theta_e,\\boldsymbol  \\theta_c)\\). We specify the model in Equation 1 in terms of a marginal distribution for the effectiveness and a conditional distribution for the costs. A key advantage of using a conditional factorisation, compared to a multivariate marginal approach, is that univariate models for each variable can be flexibly specified to tackle the idiosyncrasies of the data (e.g. non-normality ans spikes) while also capturing the potential correlation between the variables. We now describe how the two factors on the right-hand side of the Equation can be specified.\nFigure 1 provides a visual representation of the proposed modelling framework.\n\n\n\n\n\n\nFigure 1: Visual representation of the proposed modelling framework\n\n\n\nThe effectiveness and cost distributions are represented in terms of combined “modules”- the red and blue boxes - in which the random quantities are linked through logical relationships. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in each module.\n\n\nConclusions\nAlthough our approach may not be applicable to all cases, the data analysed are very much representative of the “typical” data used in partitioned survival cost-utility analysis alongside clinical trials. Thus, it is highly likely that the same features apply to other real cases. This is a very important, if somewhat overlooked problem, as methods that do not take into account the complexities affecting patient-level data, while being easier to implement and well established among practitioners, may ultimately mislead cost-effectiveness conclusions and bias the decision-making process."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html",
    "href": "research/reviewQES/reviewQES.html",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "",
    "text": "We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a Quality Evaluation Scheme (QES), providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#descriptive-review",
    "href": "research/reviewQES/reviewQES.html#descriptive-review",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Descriptive Review",
    "text": "Descriptive Review\n\n\n\n\n\n\nFigure 2: Missingness methods by outcome and period.\n\n\n\nFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects (\\(\\approx 45\\%\\) and \\(\\approx 38\\%\\)) and costs ( \\(\\approx 50\\%\\) and \\(\\approx 62\\%\\)). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#quality-assessment",
    "href": "research/reviewQES/reviewQES.html#quality-assessment",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Quality assessment",
    "text": "Quality assessment\nGenerally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the QES. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than \\(7\\%\\) of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making."
  }
]